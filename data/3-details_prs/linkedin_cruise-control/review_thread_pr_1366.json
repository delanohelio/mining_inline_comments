{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTExOTQ0MTE5", "number": 1366, "reviewThreads": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOToyMTowMVrOE0hIOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTo0MDowOFrOE0_ZPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTA0MTg0OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/InvalidPrometheusResultException.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOToyMTowMVrOHsSbXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMToxNTo1NVrOHsV5tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwMTMwOA==", "bodyText": "Nit: 2017 -> 2020", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516201308", "createdAt": "2020-11-02T19:21:01Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/InvalidPrometheusResultException.java", "diffHunk": "@@ -0,0 +1,18 @@\n+/*\n+ * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1ODIzMQ==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516258231", "createdAt": "2020-11-02T21:15:55Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/InvalidPrometheusResultException.java", "diffHunk": "@@ -0,0 +1,18 @@\n+/*\n+ * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwMTMwOA=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTA1NjczOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/InvalidPrometheusResultException.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOToyNTozMFrOHsSkVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOToxMDozMlrOHs80wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwMzYwNg==", "bodyText": "Should InvalidPrometheusResultException extend from SamplingException? More specifically, what is the additional benefit of using InvalidPrometheusResultException rather than SamplingException?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516203606", "createdAt": "2020-11-02T19:25:30Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/InvalidPrometheusResultException.java", "diffHunk": "@@ -0,0 +1,18 @@\n+/*\n+ * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException;\n+\n+/**\n+ * The exception indicates that the broker with which the metric is associated could not be determined.\n+ */\n+public class InvalidPrometheusResultException extends KafkaCruiseControlException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5NTkzOA==", "bodyText": "Multiple Kafka clusters can use the same Prometheus server to publish metrics. In such a case, the Prometheus API queries may return metrics from hosts that are not part of the cluster being managed by Cruise Control. When we receive such query results, we simply want to ignore them and move on to the other query results. We do not want to abandon the entire sampling run because some of the query results were invalid.\nThe current usage of the SamplingException seems to be in cases where we want to abandon the current sampling run because of an irrecoverable error.\nTo distinguish from these errors, we have created InvalidPrometheusResultException, which are not supposed to abandon the sampling execution, but simply skip the current metric result being evaluated because it is invalid.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516895938", "createdAt": "2020-11-03T19:10:32Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/InvalidPrometheusResultException.java", "diffHunk": "@@ -0,0 +1,18 @@\n+/*\n+ * Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException;\n+\n+/**\n+ * The exception indicates that the broker with which the metric is associated could not be determined.\n+ */\n+public class InvalidPrometheusResultException extends KafkaCruiseControlException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwMzYwNg=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTA2Mzk1OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOToyNzoyNlrOHsSocQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDozODowMlrOHscXqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNDY1Nw==", "bodyText": "Can we move the hardcoded string constants (e.g. \"/api/v1/query_range\", \"query\") to static variables?\nCan we indicate the expected values for parameters -- e.g. start and end requires UNIX timestamp in seconds (are they inclusive or exclusive?), step is in seconds.\nDoes step accepts values with a decimal point? Would the current integer division cause loss of accuracy?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516204657", "createdAt": "2020-11-02T19:27:26Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + \"/api/v1/query_range\");\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf(startTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf(endTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf(_samplingIntervalMs / MILLIS_IN_SECOND)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2NDIwMA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516364200", "createdAt": "2020-11-03T00:38:02Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + \"/api/v1/query_range\");\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf(startTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf(endTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf(_samplingIntervalMs / MILLIS_IN_SECOND)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNDY1Nw=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTA3MjU2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOToyOTo1N1rOHsSttg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMToyMTozM1rOHsWEuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNjAwNg==", "bodyText": "200 -> HttpServletResponse.SC_OK", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516206006", "createdAt": "2020-11-02T19:29:57Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + \"/api/v1/query_range\");\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf(startTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf(endTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf(_samplingIntervalMs / MILLIS_IN_SECOND)));\n+\n+        httpPost.setEntity(new UrlEncodedFormEntity(data));\n+        try (CloseableHttpResponse response = _httpClient.execute(httpPost)) {\n+            int responseCode = response.getStatusLine().getStatusCode();\n+            HttpEntity entity = response.getEntity();\n+            InputStream content = entity.getContent();\n+            String responseString = IOUtils.toString(content, StandardCharsets.UTF_8);\n+            if (responseCode != 200) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2MTA1MQ==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516261051", "createdAt": "2020-11-02T21:21:33Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + \"/api/v1/query_range\");\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf(startTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf(endTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf(_samplingIntervalMs / MILLIS_IN_SECOND)));\n+\n+        httpPost.setEntity(new UrlEncodedFormEntity(data));\n+        try (CloseableHttpResponse response = _httpClient.execute(httpPost)) {\n+            int responseCode = response.getStatusLine().getStatusCode();\n+            HttpEntity entity = response.getEntity();\n+            InputStream content = entity.getContent();\n+            String responseString = IOUtils.toString(content, StandardCharsets.UTF_8);\n+            if (responseCode != 200) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNjAwNg=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTA3NDM3OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOTozMDozNVrOHsSu5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMTo1Nzo0MlrOHsXI0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNjMwOQ==", "bodyText": "Do we allow null values for samplingIntervalMs? If not, can we use int rather than Integer?\nShould we add sanit checks to ensure that httpClient and prometheusEndpoint are not null?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516206309", "createdAt": "2020-11-02T19:30:35Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI3ODQ4Mw==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516278483", "createdAt": "2020-11-02T21:57:42Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwNjMwOQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTEzMTU5OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxOTo0Nzo0NlrOHsTR2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMToyMjozM1rOHsWGjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIxNTI1OQ==", "bodyText": "Can we move the hardcoded string \"success\" to a static variable?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516215259", "createdAt": "2020-11-02T19:47:46Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + \"/api/v1/query_range\");\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf(startTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf(endTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf(_samplingIntervalMs / MILLIS_IN_SECOND)));\n+\n+        httpPost.setEntity(new UrlEncodedFormEntity(data));\n+        try (CloseableHttpResponse response = _httpClient.execute(httpPost)) {\n+            int responseCode = response.getStatusLine().getStatusCode();\n+            HttpEntity entity = response.getEntity();\n+            InputStream content = entity.getContent();\n+            String responseString = IOUtils.toString(content, StandardCharsets.UTF_8);\n+            if (responseCode != 200) {\n+                throw new IOException(String.format(\"Received non-success response code on Prometheus API HTTP call,\"\n+                                                    + \" response code = %s, response body = %s\",\n+                                                    responseCode, responseString));\n+            }\n+            PrometheusResponse prometheusResponse = GSON.fromJson(responseString, PrometheusResponse.class);\n+            if (prometheusResponse == null) {\n+                throw new IOException(String.format(\n+                    \"No response received from Prometheus API query, response body = %s\", responseString));\n+            }\n+\n+            if (!\"success\".equals(prometheusResponse.status())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2MTUxOQ==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516261519", "createdAt": "2020-11-02T21:22:33Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final Gson GSON = new Gson();\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final Integer _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             Integer samplingIntervalMs) {\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + \"/api/v1/query_range\");\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf(startTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf(endTimeMs / MILLIS_IN_SECOND)));\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf(_samplingIntervalMs / MILLIS_IN_SECOND)));\n+\n+        httpPost.setEntity(new UrlEncodedFormEntity(data));\n+        try (CloseableHttpResponse response = _httpClient.execute(httpPost)) {\n+            int responseCode = response.getStatusLine().getStatusCode();\n+            HttpEntity entity = response.getEntity();\n+            InputStream content = entity.getContent();\n+            String responseString = IOUtils.toString(content, StandardCharsets.UTF_8);\n+            if (responseCode != 200) {\n+                throw new IOException(String.format(\"Received non-success response code on Prometheus API HTTP call,\"\n+                                                    + \" response code = %s, response body = %s\",\n+                                                    responseCode, responseString));\n+            }\n+            PrometheusResponse prometheusResponse = GSON.fromJson(responseString, PrometheusResponse.class);\n+            if (prometheusResponse == null) {\n+                throw new IOException(String.format(\n+                    \"No response received from Prometheus API query, response body = %s\", responseString));\n+            }\n+\n+            if (!\"success\".equals(prometheusResponse.status())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIxNTI1OQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTE3NzkwOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDowMjowM1rOHsTuYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMToyNDowOFrOHsWJbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyMjU2MQ==", "bodyText": "Nit: Can we use com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.SEC_TO_MS rather than introducing a new variable?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516222561", "createdAt": "2020-11-02T20:02:03Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2MjI1NA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516262254", "createdAt": "2020-11-02T21:24:08Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyMjU2MQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTE4NTk1OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDowNDozOVrOHsTzSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwOToxNDozMVrOHslv9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyMzgxOA==", "bodyText": "Can we update the JavaDoc to indicate the configs that this class takes -- e.g. similar to the JavaDoc of the pluggable com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516223818", "createdAt": "2020-11-02T20:04:39Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUxNzg3OA==", "bodyText": "Updated JavaDoc.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516517878", "createdAt": "2020-11-03T09:14:31Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyMzgxOA=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTE5Mzc1OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDowNjo1OVrOHsT3_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMToyNzo1OFrOHsWQqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyNTAyMQ==", "bodyText": "Nit: Can we use com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.SEC_TO_MS rather than introducing a new variable?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516225021", "createdAt": "2020-11-02T20:06:59Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2NDEwNQ==", "bodyText": "Done", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516264105", "createdAt": "2020-11-02T21:27:58Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final int MILLIS_IN_SECOND = 1000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyNTAyMQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTIwNDc0OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDoxMDoyOFrOHsT-tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMToyODo0NVrOHsWSPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyNjc0MA==", "bodyText": "Can we make this an int rather than Integer?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516226740", "createdAt": "2020-11-02T20:10:28Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2NDUxMA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516264510", "createdAt": "2020-11-02T21:28:45Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyNjc0MA=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTIxMDE0OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDoxMjoxOFrOHsUB7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMTo1Mjo0MVrOHsW_Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyNzU2Ng==", "bodyText": "Is this try catch block that throws IllegalArgumentException and catches it to throw ConfigException needed? Can't we drop the try-catch block and move the following ConfigException to here?\n            throw new ConfigException(\n                String.format(\"Prometheus endpoint URI is malformed, \"\n                              + \"expected schema://host:port, provided %s\", endpoint));", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516227566", "createdAt": "2020-11-02T20:12:18Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI3NjA3MQ==", "bodyText": "The HttpHost.create(endpoint) also throws an IllegalArgumentException when the hostname is malformed.\nhttps://github.com/apache/httpcomponents-core/blob/4.4.x/httpcore/src/main/java/org/apache/http/HttpHost.java#L122\nThis catch block is meant to catch both of these.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516276071", "createdAt": "2020-11-02T21:52:41Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyNzU2Ng=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTIzNzk2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDoyMDo1MVrOHsUSfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0MTo0NFrOHs_tjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzMTgwNw==", "bodyText": "Should this method relinquish underlying resources? -- e.g.\n        _hostToBrokerIdMap.clear();\n        _metricToPrometheusQueryMap.clear();", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516231807", "createdAt": "2020-11-02T20:20:51Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI3MjI1Ng==", "bodyText": "I decided to make the httpClient a field, and close it here.\nIs there any advantage to clearing the Maps here? They are going to be garbage-cleaned anyway.\nI guess it makes sense to close open handles, etc. which is why I closed the httpClient.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516272256", "createdAt": "2020-11-02T21:44:48Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzMTgwNw=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MzI0Ng==", "bodyText": "Is there any advantage to clearing the Maps here? They are going to be garbage-cleaned anyway.\n\n@wyuka TL;DR. I think there is no need to apply the suggestion I made.\n-- more --\nGarbage collection (GC) would trigger only when JVM thinks it needs a GC based on the heap size. Also, GC will not trigger as long as there are references to PrometheusMetricSampler; hence, it can potentially keep unused resources longer. That being said, clearing resources in close() will also not trigger GC, but will make those resources available for GC earlier. However, presumably references to PrometheusMetricSampler are not expected to linger around for a long time after it is closed. Thus, please feel free to ignore my suggestion.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516943246", "createdAt": "2020-11-03T20:41:44Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzMTgwNw=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTI1NTg1OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDoyNjozNlrOHsUdAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0MzozOVrOHs_xPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNDQ5OA==", "bodyText": "Nit: Would (maybe a batched -- i.e. not individual) trace-level logging help with debugging potential issues?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516234498", "createdAt": "2020-11-02T20:26:36Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5ODA2OQ==", "bodyText": "We can provide a count of total number of results that were found to be invalid. That can be an INFO log.\nWe can also provide trace level logging for every metric that was invalid (individually). What do you think?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516898069", "createdAt": "2020-11-03T19:14:30Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNDQ5OA=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0NDE5MQ==", "bodyText": "Sure, both suggestions sound reasonable to me.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516944191", "createdAt": "2020-11-03T20:43:39Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNDQ5OA=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTI3MjIwOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDozMTozOFrOHsUm0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo0ODowMVrOHs_5TQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNzAwOQ==", "bodyText": "Does it help to also log brokers found in Kafka cluster metadata as part of this message?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516237009", "createdAt": "2020-11-02T20:31:38Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.\n+                     */\n+                }\n+            }\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addBrokerMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new BrokerMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addTopicMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new TopicMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addPartitionMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+        int partition = getPartition(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new PartitionMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, partition, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int getBrokerId(Cluster cluster, PrometheusQueryResult queryResult) throws\n+        InvalidPrometheusResultException {\n+        String hostPort = queryResult.metric().instance();\n+        if (hostPort == null) {\n+            throw new InvalidPrometheusResultException(\"Instance returned as part of Prometheus API response is null.\");\n+        }\n+        Integer brokerId;\n+\n+        String hostName = hostPort.split(\":\")[0];\n+        brokerId = getBrokerIdForHostName(hostName, cluster);\n+        if (brokerId == null) {\n+            throw new InvalidPrometheusResultException(String.format(\n+                \"Unexpected host %s, does not map to any broker found from Kafka cluster metadata.\", hostName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5Njc5NQ==", "bodyText": "We are not really logging this message anywhere. We can consider removing the message entirely here, and providing a batched trace-level log that counts the total number of results that were invalid.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516896795", "createdAt": "2020-11-03T19:12:05Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.\n+                     */\n+                }\n+            }\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addBrokerMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new BrokerMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addTopicMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new TopicMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addPartitionMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+        int partition = getPartition(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new PartitionMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, partition, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int getBrokerId(Cluster cluster, PrometheusQueryResult queryResult) throws\n+        InvalidPrometheusResultException {\n+        String hostPort = queryResult.metric().instance();\n+        if (hostPort == null) {\n+            throw new InvalidPrometheusResultException(\"Instance returned as part of Prometheus API response is null.\");\n+        }\n+        Integer brokerId;\n+\n+        String hostName = hostPort.split(\":\")[0];\n+        brokerId = getBrokerIdForHostName(hostName, cluster);\n+        if (brokerId == null) {\n+            throw new InvalidPrometheusResultException(String.format(\n+                \"Unexpected host %s, does not map to any broker found from Kafka cluster metadata.\", hostName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNzAwOQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MjE1OQ==", "bodyText": "I have decided to add the brokers found in Kafka clusters metadata here. I am later logging it at TRACE level.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516942159", "createdAt": "2020-11-03T20:39:24Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.\n+                     */\n+                }\n+            }\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addBrokerMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new BrokerMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addTopicMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new TopicMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addPartitionMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+        int partition = getPartition(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new PartitionMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, partition, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int getBrokerId(Cluster cluster, PrometheusQueryResult queryResult) throws\n+        InvalidPrometheusResultException {\n+        String hostPort = queryResult.metric().instance();\n+        if (hostPort == null) {\n+            throw new InvalidPrometheusResultException(\"Instance returned as part of Prometheus API response is null.\");\n+        }\n+        Integer brokerId;\n+\n+        String hostName = hostPort.split(\":\")[0];\n+        brokerId = getBrokerIdForHostName(hostName, cluster);\n+        if (brokerId == null) {\n+            throw new InvalidPrometheusResultException(String.format(\n+                \"Unexpected host %s, does not map to any broker found from Kafka cluster metadata.\", hostName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNzAwOQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0NjI1Mw==", "bodyText": "Sure, that sounds good. This would also eliminate the overhead of creating this formatted string.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516946253", "createdAt": "2020-11-03T20:48:01Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSampler.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.http.HttpHost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigDef;\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfigUtils;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.BrokerMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.PartitionMetric;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.TopicMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.AbstractMetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.cruisecontrol.common.config.ConfigDef.Type.CLASS;\n+\n+/**\n+ * Metric sampler that fetches Kafka metrics from a Prometheus server and converts them to samples.\n+ */\n+public class PrometheusMetricSampler extends AbstractMetricSampler {\n+    // Config name visible to tests\n+    static final String PROMETHEUS_SERVER_ENDPOINT_CONFIG = \"prometheus.server.endpoint\";\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG = \"prometheus.query.resolution.step.ms\";\n+    private static final Integer DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS = 60_000;\n+    private static final int MILLIS_IN_SECOND = 1000;\n+\n+    // Config name visible to tests\n+    static final String PROMETHEUS_QUERY_SUPPLIER_CONFIG = \"prometheus.query.supplier\";\n+    private static final Class<?> DEFAULT_PROMETHEUS_QUERY_SUPPLIER = DefaultPrometheusQuerySupplier.class;\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(PrometheusMetricSampler.class);\n+\n+    protected Integer _samplingIntervalMs;\n+    protected Map<String, Integer> _hostToBrokerIdMap = new HashMap<>();\n+    protected PrometheusAdapter _prometheusAdapter;\n+    protected Map<RawMetricType, String> _metricToPrometheusQueryMap;\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        super.configure(configs);\n+        configureSamplingInterval(configs);\n+        configurePrometheusAdapter(configs);\n+        configureQueryMap(configs);\n+    }\n+\n+    private void configureSamplingInterval(Map<String, ?> configs) {\n+        _samplingIntervalMs = DEFAULT_PROMETHEUS_METRICS_SAMPLING_INTERVAL_MS;\n+        if (configs.containsKey(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG)) {\n+            String samplingIntervalMsString = (String) configs.get(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG);\n+            try {\n+                _samplingIntervalMs = Integer.parseInt(samplingIntervalMsString);\n+            } catch (NumberFormatException e) {\n+                throw new ConfigException(\"%s config should be a positive number, provided %s\",\n+                    samplingIntervalMsString);\n+            }\n+\n+            if (_samplingIntervalMs <= 0) {\n+                throw new ConfigException(String.format(\"%s config should be set to positive,\"\n+                                                        + \" provided %d.\",\n+                                                        PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG,\n+                                                        _samplingIntervalMs));\n+            }\n+        }\n+    }\n+\n+    private void configurePrometheusAdapter(Map<String, ?> configs) {\n+        final String endpoint = (String) configs.get(PROMETHEUS_SERVER_ENDPOINT_CONFIG);\n+        if (endpoint == null) {\n+            throw new ConfigException(String.format(\n+                \"%s config is required by Prometheus metric sampler\", PROMETHEUS_SERVER_ENDPOINT_CONFIG));\n+        }\n+\n+        try {\n+            HttpHost host = HttpHost.create(endpoint);\n+            if (host.getPort() < 0) {\n+                throw new IllegalArgumentException();\n+            }\n+            CloseableHttpClient httpClient = HttpClients.createDefault();\n+            _prometheusAdapter = new PrometheusAdapter(httpClient, host, _samplingIntervalMs);\n+        } catch (IllegalArgumentException ex) {\n+            throw new ConfigException(\n+                String.format(\"Prometheus endpoint URI is malformed, \"\n+                              + \"expected schema://host:port, provided %s\", endpoint));\n+        }\n+    }\n+\n+    private void configureQueryMap(Map<String, ?> configs) {\n+        String prometheusQuerySupplierClassName = (String) configs.get(PROMETHEUS_QUERY_SUPPLIER_CONFIG);\n+        Class<?> prometheusQuerySupplierClass = DEFAULT_PROMETHEUS_QUERY_SUPPLIER;\n+        if (prometheusQuerySupplierClassName != null) {\n+            prometheusQuerySupplierClass = (Class<?>) ConfigDef.parseType(PROMETHEUS_QUERY_SUPPLIER_CONFIG,\n+                prometheusQuerySupplierClassName, CLASS);\n+            if (!PrometheusQuerySupplier.class.isAssignableFrom(prometheusQuerySupplierClass)) {\n+                throw new ConfigException(String.format(\n+                    \"Invalid %s is provided to prometheus metric sampler, provided %s\",\n+                    PROMETHEUS_QUERY_SUPPLIER_CONFIG, prometheusQuerySupplierClass));\n+            }\n+        }\n+        PrometheusQuerySupplier prometheusQuerySupplier = KafkaCruiseControlConfigUtils.getConfiguredInstance(\n+            prometheusQuerySupplierClass, PrometheusQuerySupplier.class, Collections.emptyMap());\n+        _metricToPrometheusQueryMap = prometheusQuerySupplier.get();\n+    }\n+\n+    @Override\n+    public void close() {\n+        // do nothing\n+    }\n+\n+    private Integer getBrokerIdForHostName(String host, Cluster cluster) {\n+        Integer cachedId = _hostToBrokerIdMap.get(host);\n+        if (cachedId != null) {\n+            return cachedId;\n+        }\n+        mapNodesToClusterId(cluster);\n+        return _hostToBrokerIdMap.get(host);\n+    }\n+\n+    private void mapNodesToClusterId(Cluster cluster) {\n+        for (Node node : cluster.nodes()) {\n+            _hostToBrokerIdMap.put(node.host(), node.id());\n+        }\n+    }\n+\n+    @Override\n+    protected int retrieveMetricsForProcessing(MetricSamplerOptions metricSamplerOptions) throws SamplingException {\n+        int metricsAdded = 0;\n+        for (Map.Entry<RawMetricType, String> metricToQueryEntry : _metricToPrometheusQueryMap.entrySet()) {\n+            final RawMetricType metricType = metricToQueryEntry.getKey();\n+            final String prometheusQuery = metricToQueryEntry.getValue();\n+            final List<PrometheusQueryResult> prometheusQueryResults;\n+            try {\n+                prometheusQueryResults = _prometheusAdapter.queryMetric(prometheusQuery,\n+                                                                        metricSamplerOptions.startTimeMs(),\n+                                                                        metricSamplerOptions.endTimeMs());\n+            } catch (IOException e) {\n+                LOG.error(\"Error when attempting to query Prometheus metrics\", e);\n+                throw new SamplingException(\"Could not query metrics from Prometheus\");\n+            }\n+            for (PrometheusQueryResult result : prometheusQueryResults) {\n+                try {\n+                    switch (metricType.metricScope()) {\n+                        case BROKER:\n+                            metricsAdded += addBrokerMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case TOPIC:\n+                            metricsAdded += addTopicMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        case PARTITION:\n+                            metricsAdded += addPartitionMetrics(metricSamplerOptions.cluster(), metricType, result);\n+                            break;\n+                        default:\n+                            // Not supported.\n+                            break;\n+                    }\n+                } catch (InvalidPrometheusResultException e) {\n+                    /* We can ignore invalid or malformed Prometheus results, for example one which has a hostname\n+                    that could not be matched to any broker, or one where the topic name is null. Such records\n+                    will not be converted to metrics. There are valid use cases where this may occur - for instance,\n+                    when a Prometheus server store metrics from multiple Kafka clusters, in which case the hostname\n+                    may not correspond to any of this cluster's broker hosts.\n+\n+                    This can be really frequent, and hence, it would not make sense to fill the log entries by\n+                    logging this repeatedly.\n+                     */\n+                }\n+            }\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addBrokerMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new BrokerMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addTopicMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new TopicMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int addPartitionMetrics(Cluster cluster, RawMetricType metricType, PrometheusQueryResult queryResult)\n+        throws InvalidPrometheusResultException {\n+        int brokerId = getBrokerId(cluster, queryResult);\n+        String topic = getTopic(queryResult);\n+        int partition = getPartition(queryResult);\n+\n+        int metricsAdded = 0;\n+        for (PrometheusValue value : queryResult.values()) {\n+            addMetricForProcessing(new PartitionMetric(metricType, value.epochSeconds() * MILLIS_IN_SECOND,\n+                                   brokerId, topic, partition, value.value()));\n+            metricsAdded++;\n+        }\n+        return metricsAdded;\n+    }\n+\n+    private int getBrokerId(Cluster cluster, PrometheusQueryResult queryResult) throws\n+        InvalidPrometheusResultException {\n+        String hostPort = queryResult.metric().instance();\n+        if (hostPort == null) {\n+            throw new InvalidPrometheusResultException(\"Instance returned as part of Prometheus API response is null.\");\n+        }\n+        Integer brokerId;\n+\n+        String hostName = hostPort.split(\":\")[0];\n+        brokerId = getBrokerIdForHostName(hostName, cluster);\n+        if (brokerId == null) {\n+            throw new InvalidPrometheusResultException(String.format(\n+                \"Unexpected host %s, does not map to any broker found from Kafka cluster metadata.\", hostName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNzAwOQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 245}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTI4NTI2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusMetric.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDozNTozM1rOHsUudg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwODozMDo1MlrOHskQoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzODk2Ng==", "bodyText": "Based on the way the PrometheusMetric is parsed, the _instance seems to have an expected format -- i.e. <host>:<port>. Is it acceptable to have a null or ill-formed instance? If not, should we add a sanity check to ensure that the input is well-formatted -- then we can drop sanity checks on the reader of instance()?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516238966", "createdAt": "2020-11-02T20:35:33Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusMetric.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+import javax.annotation.Nullable;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusMetric {\n+    @SerializedName(\"__name__\")\n+    final private String _domainName;\n+    @SerializedName(\"instance\")\n+    final private String _instance;\n+    @SerializedName(\"job\")\n+    final private String _job;\n+    @SerializedName(\"name\")\n+    final private String _name;\n+    @SerializedName(\"topic\")\n+    final @Nullable private String _topic;\n+    @SerializedName(\"partition\")\n+    final @Nullable private String _partition;\n+\n+    public PrometheusMetric(\n+        String domainName, String instance, String job, String name,\n+        @Nullable String topic, @Nullable String partition) {\n+        _domainName = domainName;\n+        _instance = instance;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ5MzQ3Mg==", "bodyText": "We are using GSON to deserialize the JSON response returned by Prometheus API. Unfortunately, GSON does not use a non no-args constructor. It simply allocates the object, and then sets the fields using reflection (not using the setters). The only way to force GSON to use a custom constructor is to define a JsonDeserializer class.\nSince this constructor is solely used in unit tests, I have not kept the validation here, but in the reader of instance().", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516493472", "createdAt": "2020-11-03T08:30:52Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusMetric.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+import javax.annotation.Nullable;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusMetric {\n+    @SerializedName(\"__name__\")\n+    final private String _domainName;\n+    @SerializedName(\"instance\")\n+    final private String _instance;\n+    @SerializedName(\"job\")\n+    final private String _job;\n+    @SerializedName(\"name\")\n+    final private String _name;\n+    @SerializedName(\"topic\")\n+    final @Nullable private String _topic;\n+    @SerializedName(\"partition\")\n+    final @Nullable private String _partition;\n+\n+    public PrometheusMetric(\n+        String domainName, String instance, String job, String name,\n+        @Nullable String topic, @Nullable String partition) {\n+        _domainName = domainName;\n+        _instance = instance;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzODk2Ng=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMyMTcxOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo0Njo0MlrOHsVDmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwOTowNjoxMVrOHslc-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NDM3OA==", "bodyText": "This function is never used -- what is its purpose (can we add some JavaDoc to this class to clarify)?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516244378", "createdAt": "2020-11-02T20:46:42Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusData.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusData {\n+    @SerializedName(\"resultType\")\n+    final private String _resultType;\n+    @SerializedName(\"result\")\n+    final private List<PrometheusQueryResult> _result;\n+\n+    public PrometheusData(String resultType, List<PrometheusQueryResult> result) {\n+        _resultType = resultType;\n+        _result = result;\n+    }\n+\n+    public String resultType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUxMzAxNg==", "bodyText": "Removed this. I found out that GSON can handle unknown fields that are not in the POJO without any extra configuration.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516513016", "createdAt": "2020-11-03T09:06:11Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusData.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusData {\n+    @SerializedName(\"resultType\")\n+    final private String _resultType;\n+    @SerializedName(\"result\")\n+    final private List<PrometheusQueryResult> _result;\n+\n+    public PrometheusData(String resultType, List<PrometheusQueryResult> result) {\n+        _resultType = resultType;\n+        _result = result;\n+    }\n+\n+    public String resultType() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NDM3OA=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMyNjE1OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo0ODoxMVrOHsVGZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMTozNzoxOFrOHsWirA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NTA5Mg==", "bodyText": "Nit: (applies to similar uses in other classes in this PR): private final is the preferred style over final private (see source). Should we use the preferred style?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516245092", "createdAt": "2020-11-02T20:48:11Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusData.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusData {\n+    @SerializedName(\"resultType\")\n+    final private String _resultType;\n+    @SerializedName(\"result\")\n+    final private List<PrometheusQueryResult> _result;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2ODcxNg==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516268716", "createdAt": "2020-11-02T21:37:18Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusData.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusData {\n+    @SerializedName(\"resultType\")\n+    final private String _resultType;\n+    @SerializedName(\"result\")\n+    final private List<PrometheusQueryResult> _result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NTA5Mg=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMzNDk2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo1MDo1NlrOHsVLrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwOTowNjoxOVrOHsldRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjQ0Nw==", "bodyText": "Can we add JavaDoc to class to describe its fields?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516246447", "createdAt": "2020-11-02T20:50:56Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusResponse.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusResponse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUxMzA5Mg==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516513092", "createdAt": "2020-11-03T09:06:19Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusResponse.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusResponse {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjQ0Nw=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMzNTUxOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusQueryResult.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo1MTowNVrOHsVL_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwOTowNjoyNFrOHsldcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjUyNw==", "bodyText": "Can we add JavaDoc to class to describe its fields?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516246527", "createdAt": "2020-11-02T20:51:05Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusQueryResult.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusQueryResult {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUxMzEzOA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516513138", "createdAt": "2020-11-03T09:06:24Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusQueryResult.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusQueryResult {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjUyNw=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMzNjA0OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusMetric.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo1MToxNlrOHsVMVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwOTowNjozMVrOHsldrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjYxNQ==", "bodyText": "Can we add JavaDoc to class to describe its fields?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516246615", "createdAt": "2020-11-02T20:51:16Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusMetric.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+import javax.annotation.Nullable;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusMetric {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUxMzE5Ng==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516513196", "createdAt": "2020-11-03T09:06:31Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusMetric.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+import javax.annotation.Nullable;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class PrometheusMetric {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjYxNQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMzNjU2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValue.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo1MToyNlrOHsVMrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwOTowNjozOFrOHsleAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjcwMQ==", "bodyText": "Can we add JavaDoc to class to describe its fields?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516246701", "createdAt": "2020-11-02T20:51:26Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValue.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.JsonAdapter;\n+import com.google.gson.annotations.SerializedName;\n+\n+@JsonAdapter(PrometheusValueDeserializer.class)\n+public class PrometheusValue {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUxMzI4MA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516513280", "createdAt": "2020-11-03T09:06:38Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValue.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.JsonAdapter;\n+import com.google.gson.annotations.SerializedName;\n+\n+@JsonAdapter(PrometheusValueDeserializer.class)\n+public class PrometheusValue {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjcwMQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTMzNzkwOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValue.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo1MTo1NVrOHsVNfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMTozODoxMFrOHsWkDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjkwOQ==", "bodyText": "Nit: this. prefix is redundant.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516246909", "createdAt": "2020-11-02T20:51:55Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValue.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.JsonAdapter;\n+import com.google.gson.annotations.SerializedName;\n+\n+@JsonAdapter(PrometheusValueDeserializer.class)\n+public class PrometheusValue {\n+    @SerializedName(\"epochSeconds\")\n+    private final long _epochSeconds;\n+    @SerializedName(\"value\")\n+    private final double _value;\n+\n+    public PrometheusValue(final long epochSeconds, final double value) {\n+        this._epochSeconds = epochSeconds;\n+        this._value = value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2OTA3MA==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516269070", "createdAt": "2020-11-02T21:38:10Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValue.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.util.Objects;\n+\n+import com.google.gson.annotations.JsonAdapter;\n+import com.google.gson.annotations.SerializedName;\n+\n+@JsonAdapter(PrometheusValueDeserializer.class)\n+public class PrometheusValue {\n+    @SerializedName(\"epochSeconds\")\n+    private final long _epochSeconds;\n+    @SerializedName(\"value\")\n+    private final double _value;\n+\n+    public PrometheusValue(final long epochSeconds, final double value) {\n+        this._epochSeconds = epochSeconds;\n+        this._value = value;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjkwOQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNTM0NTUzOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValueDeserializer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDo1NDoxOFrOHsVSGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMTozODo1MlrOHsWlVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0ODA4OQ==", "bodyText": "Nit: the indentation seems broken -- i.e. no arguments in the first line.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516248089", "createdAt": "2020-11-02T20:54:18Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValueDeserializer.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.lang.reflect.Type;\n+\n+import com.google.gson.JsonArray;\n+import com.google.gson.JsonDeserializationContext;\n+import com.google.gson.JsonDeserializer;\n+import com.google.gson.JsonElement;\n+import com.google.gson.JsonParseException;\n+\n+class PrometheusValueDeserializer implements JsonDeserializer<PrometheusValue> {\n+    @Override\n+    public PrometheusValue deserialize(\n+        JsonElement json, Type typeOfT,\n+        JsonDeserializationContext context) throws JsonParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI2OTM5Ng==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516269396", "createdAt": "2020-11-02T21:38:52Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/model/PrometheusValueDeserializer.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model;\n+\n+import java.lang.reflect.Type;\n+\n+import com.google.gson.JsonArray;\n+import com.google.gson.JsonDeserializationContext;\n+import com.google.gson.JsonDeserializer;\n+import com.google.gson.JsonElement;\n+import com.google.gson.JsonParseException;\n+\n+class PrometheusValueDeserializer implements JsonDeserializer<PrometheusValue> {\n+    @Override\n+    public PrometheusValue deserialize(\n+        JsonElement json, Type typeOfT,\n+        JsonDeserializationContext context) throws JsonParseException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0ODA4OQ=="}, "originalCommit": {"oid": "0d031702bbe2e35b02d89a82b9cf511be7a6cc65"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTg2MjU5OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo1NTowNFrOHtAHCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjowNTo0MlrOHtCIjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0OTc2OQ==", "bodyText": "Nit: Can we use the QUERY_RANGE_API_PATH and SUCCESS from PrometheusAdapter?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516949769", "createdAt": "2020-11-03T20:55:04Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4MjkyNA==", "bodyText": "I have changed and used QUERY_RANGE_API_PATH everywhere. The string \"success\" however is embedded deep inside the JSON strings, and would require a lot of String.format()s to use the SUCCESS from PrometheusAdapter. So I have avoided it. Let me know if you still want me to change it.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516982924", "createdAt": "2020-11-03T22:05:42Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0OTc2OQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTg2MzQ2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo1NToxOVrOHtAHjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTo1MzozNFrOHtBy4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0OTkwMA==", "bodyText": "Nit (applies to other similar uses in this class): Can we use SC_OK?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516949900", "createdAt": "2020-11-03T20:55:19Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3NzM3Nw==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516977377", "createdAt": "2020-11-03T21:53:34Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0OTkwMA=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTg3MDM4OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDo1NzozMlrOHtAL4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTo1NTo0NFrOHtB2sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1MTAwOQ==", "bodyText": "Nit: Can we move the hardcoded query, start, end, step to static variables?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516951009", "createdAt": "2020-11-03T20:57:32Z", "author": {"login": "efeg"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import javax.servlet.http.HttpServletResponse;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.SEC_TO_MS;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final Gson GSON = new Gson();\n+    private static final String QUERY_RANGE_API_PATH = \"/api/v1/query_range\";\n+    private static final String SUCCESS = \"success\";\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final int _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             int samplingIntervalMs) {\n+        if (httpClient == null || prometheusEndpoint == null) {\n+            throw new IllegalArgumentException(\"httpClient or prometheusEndpoint cannot be null.\");\n+        }\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + QUERY_RANGE_API_PATH);\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        /* \"start\" and \"end\" are expected to be unix timestamp in seconds (number of seconds since the Unix epoch).\n+         They accept values with a decimal point (up to 64 bits). The samples returned are inclusive of the \"end\"\n+         timestamp provided.\n+         */\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf((double) startTimeMs / SEC_TO_MS)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf((double) endTimeMs / SEC_TO_MS)));\n+        // step is expected to be in seconds, and accept values with a decimal point (up to 64 bits).\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf((double) _samplingIntervalMs / SEC_TO_MS)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3ODM1Mg==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516978352", "createdAt": "2020-11-03T21:55:44Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapter.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import javax.servlet.http.HttpServletResponse;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.NameValuePair;\n+import org.apache.http.client.entity.UrlEncodedFormEntity;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.message.BasicNameValuePair;\n+import org.apache.http.util.EntityUtils;\n+\n+import com.google.gson.Gson;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusResponse;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.SEC_TO_MS;\n+\n+/**\n+ * This class provides an adapter to make queries to a Prometheus Server to fetch metric values.\n+ */\n+class PrometheusAdapter {\n+    private static final Gson GSON = new Gson();\n+    private static final String QUERY_RANGE_API_PATH = \"/api/v1/query_range\";\n+    private static final String SUCCESS = \"success\";\n+\n+    private final CloseableHttpClient _httpClient;\n+    /* Visible for testing */\n+    final HttpHost _prometheusEndpoint;\n+    /* Visible for testing */\n+    final int _samplingIntervalMs;\n+\n+    public PrometheusAdapter(CloseableHttpClient httpClient,\n+                             HttpHost prometheusEndpoint,\n+                             int samplingIntervalMs) {\n+        if (httpClient == null || prometheusEndpoint == null) {\n+            throw new IllegalArgumentException(\"httpClient or prometheusEndpoint cannot be null.\");\n+        }\n+        _httpClient = httpClient;\n+        _prometheusEndpoint = prometheusEndpoint;\n+        _samplingIntervalMs = samplingIntervalMs;\n+    }\n+\n+    public List<PrometheusQueryResult> queryMetric(String queryString,\n+                                                   long startTimeMs,\n+                                                   long endTimeMs) throws IOException {\n+        URI queryUri = URI.create(_prometheusEndpoint.toURI() + QUERY_RANGE_API_PATH);\n+        HttpPost httpPost = new HttpPost(queryUri);\n+\n+        List<NameValuePair> data = new ArrayList<>();\n+        data.add(new BasicNameValuePair(\"query\", queryString));\n+        /* \"start\" and \"end\" are expected to be unix timestamp in seconds (number of seconds since the Unix epoch).\n+         They accept values with a decimal point (up to 64 bits). The samples returned are inclusive of the \"end\"\n+         timestamp provided.\n+         */\n+        data.add(new BasicNameValuePair(\"start\", String.valueOf((double) startTimeMs / SEC_TO_MS)));\n+        data.add(new BasicNameValuePair(\"end\", String.valueOf((double) endTimeMs / SEC_TO_MS)));\n+        // step is expected to be in seconds, and accept values with a decimal point (up to 64 bits).\n+        data.add(new BasicNameValuePair(\"step\", String.valueOf((double) _samplingIntervalMs / SEC_TO_MS)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1MTAwOQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTg4ODc3OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTowMzozNFrOHtAXIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTo1NTo1N1rOHtB3Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1Mzg5MQ==", "bodyText": "403 -> SC_FORBIDDEN?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516953891", "createdAt": "2020-11-03T21:03:34Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(buildSuccessResponseEntity());\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+        final List<PrometheusQueryResult> prometheusQueryResults = prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+\n+        assertEquals(expectedResults().toString(), prometheusQueryResults.toString());\n+        assertEquals(expectedResults(), prometheusQueryResults);\n+    }\n+\n+    private HttpEntity buildSuccessResponseEntity() {\n+        return new StringEntity(\"{\\n\"\n+            + \"    \\\"status\\\": \\\"success\\\",\\n\"\n+            + \"    \\\"data\\\": {\\n\"\n+            + \"        \\\"resultType\\\": \\\"matrix\\\",\\n\"\n+            + \"        \\\"result\\\": [\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-1.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"1024\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"2048\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            },\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-2.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            }\\n\"\n+            + \"        ]\\n\"\n+            + \"    }\\n\"\n+            + \"}\", StandardCharsets.UTF_8);\n+    }\n+\n+    private List<PrometheusQueryResult> expectedResults() {\n+        return Arrays.asList(\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-1.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 1024),\n+                    new PrometheusValue(1603301430L, 2048)\n+                )\n+            ),\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-2.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 4096),\n+                    new PrometheusValue(1603301430L, 4096)\n+                )\n+            )\n+        );\n+    }\n+\n+    @Test(expected = IOException.class)\n+    public void testFailureResponseWith200Code() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(new StringEntity(\n+                    \"{\\\"status\\\": \\\"failure\\\", \\\"data\\\": {\\\"result\\\": []}}\", StandardCharsets.UTF_8));\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+\n+        prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+    }\n+\n+    @Test(expected = IOException.class)\n+    public void testFailureResponseWith403Code() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(403);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3ODQzOA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516978438", "createdAt": "2020-11-03T21:55:57Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(buildSuccessResponseEntity());\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+        final List<PrometheusQueryResult> prometheusQueryResults = prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+\n+        assertEquals(expectedResults().toString(), prometheusQueryResults.toString());\n+        assertEquals(expectedResults(), prometheusQueryResults);\n+    }\n+\n+    private HttpEntity buildSuccessResponseEntity() {\n+        return new StringEntity(\"{\\n\"\n+            + \"    \\\"status\\\": \\\"success\\\",\\n\"\n+            + \"    \\\"data\\\": {\\n\"\n+            + \"        \\\"resultType\\\": \\\"matrix\\\",\\n\"\n+            + \"        \\\"result\\\": [\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-1.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"1024\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"2048\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            },\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-2.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            }\\n\"\n+            + \"        ]\\n\"\n+            + \"    }\\n\"\n+            + \"}\", StandardCharsets.UTF_8);\n+    }\n+\n+    private List<PrometheusQueryResult> expectedResults() {\n+        return Arrays.asList(\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-1.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 1024),\n+                    new PrometheusValue(1603301430L, 2048)\n+                )\n+            ),\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-2.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 4096),\n+                    new PrometheusValue(1603301430L, 4096)\n+                )\n+            )\n+        );\n+    }\n+\n+    @Test(expected = IOException.class)\n+    public void testFailureResponseWith200Code() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(new StringEntity(\n+                    \"{\\\"status\\\": \\\"failure\\\", \\\"data\\\": {\\\"result\\\": []}}\", StandardCharsets.UTF_8));\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+\n+        prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+    }\n+\n+    @Test(expected = IOException.class)\n+    public void testFailureResponseWith403Code() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(403);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1Mzg5MQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTg5MjQxOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTowNDo0NVrOHtAZXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjowMzoxNlrOHtCEPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NDQ2MA==", "bodyText": "Nit: (applies to similar uses in this class) Can we move the hardcoded 30000 to a static variable?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516954460", "createdAt": "2020-11-03T21:04:45Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(buildSuccessResponseEntity());\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+        final List<PrometheusQueryResult> prometheusQueryResults = prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+\n+        assertEquals(expectedResults().toString(), prometheusQueryResults.toString());\n+        assertEquals(expectedResults(), prometheusQueryResults);\n+    }\n+\n+    private HttpEntity buildSuccessResponseEntity() {\n+        return new StringEntity(\"{\\n\"\n+            + \"    \\\"status\\\": \\\"success\\\",\\n\"\n+            + \"    \\\"data\\\": {\\n\"\n+            + \"        \\\"resultType\\\": \\\"matrix\\\",\\n\"\n+            + \"        \\\"result\\\": [\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-1.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"1024\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"2048\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            },\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-2.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            }\\n\"\n+            + \"        ]\\n\"\n+            + \"    }\\n\"\n+            + \"}\", StandardCharsets.UTF_8);\n+    }\n+\n+    private List<PrometheusQueryResult> expectedResults() {\n+        return Arrays.asList(\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-1.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 1024),\n+                    new PrometheusValue(1603301430L, 2048)\n+                )\n+            ),\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-2.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 4096),\n+                    new PrometheusValue(1603301430L, 4096)\n+                )\n+            )\n+        );\n+    }\n+\n+    @Test(expected = IOException.class)\n+    public void testFailureResponseWith200Code() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(new StringEntity(\n+                    \"{\\\"status\\\": \\\"failure\\\", \\\"data\\\": {\\\"result\\\": []}}\", StandardCharsets.UTF_8));\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4MTgyMw==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516981823", "createdAt": "2020-11-03T22:03:16Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(buildSuccessResponseEntity());\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+        final List<PrometheusQueryResult> prometheusQueryResults = prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+\n+        assertEquals(expectedResults().toString(), prometheusQueryResults.toString());\n+        assertEquals(expectedResults(), prometheusQueryResults);\n+    }\n+\n+    private HttpEntity buildSuccessResponseEntity() {\n+        return new StringEntity(\"{\\n\"\n+            + \"    \\\"status\\\": \\\"success\\\",\\n\"\n+            + \"    \\\"data\\\": {\\n\"\n+            + \"        \\\"resultType\\\": \\\"matrix\\\",\\n\"\n+            + \"        \\\"result\\\": [\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-1.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"1024\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"2048\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            },\\n\"\n+            + \"            {\\n\"\n+            + \"                \\\"metric\\\": {\\n\"\n+            + \"                    \\\"__name__\\\": \\\"kafka_server_BrokerTopicMetrics_OneMinuteRate\\\",\\n\"\n+            + \"                    \\\"instance\\\": \\\"b-2.test-cluster.org:11001\\\",\\n\"\n+            + \"                    \\\"job\\\": \\\"jmx\\\",\\n\"\n+            + \"                    \\\"name\\\": \\\"BytesOutPerSec\\\"\\n\"\n+            + \"                },\\n\"\n+            + \"                \\\"values\\\": [\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301400,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ],\\n\"\n+            + \"                    [\\n\"\n+            + \"                        1603301430,\\n\"\n+            + \"                        \\\"4096\\\"\\n\"\n+            + \"                    ]\\n\"\n+            + \"                ]\\n\"\n+            + \"            }\\n\"\n+            + \"        ]\\n\"\n+            + \"    }\\n\"\n+            + \"}\", StandardCharsets.UTF_8);\n+    }\n+\n+    private List<PrometheusQueryResult> expectedResults() {\n+        return Arrays.asList(\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-1.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 1024),\n+                    new PrometheusValue(1603301430L, 2048)\n+                )\n+            ),\n+            new PrometheusQueryResult(\n+                new PrometheusMetric(\n+                    \"b-2.test-cluster.org:11001\",\n+                    null, null),\n+                Arrays.asList(\n+                    new PrometheusValue(1603301400L, 4096),\n+                    new PrometheusValue(1603301430L, 4096)\n+                )\n+            )\n+        );\n+    }\n+\n+    @Test(expected = IOException.class)\n+    public void testFailureResponseWith200Code() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(new StringEntity(\n+                    \"{\\\"status\\\": \\\"failure\\\", \\\"data\\\": {\\\"result\\\": []}}\", StandardCharsets.UTF_8));\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NDQ2MA=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTg5NjExOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTowNTozOFrOHtAbbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjowODo1NlrOHtCNzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NDk4OQ==", "bodyText": "Should this builder be static?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516954989", "createdAt": "2020-11-03T21:05:38Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(buildSuccessResponseEntity());\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+        final List<PrometheusQueryResult> prometheusQueryResults = prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+\n+        assertEquals(expectedResults().toString(), prometheusQueryResults.toString());\n+        assertEquals(expectedResults(), prometheusQueryResults);\n+    }\n+\n+    private HttpEntity buildSuccessResponseEntity() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NDI3MA==", "bodyText": "Changed this everywhere.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516984270", "createdAt": "2020-11-03T22:08:56Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusAdapterTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpHost;\n+import org.apache.http.HttpRequest;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.localserver.LocalServerTestBase;\n+import org.apache.http.protocol.HttpContext;\n+import org.apache.http.protocol.HttpRequestHandler;\n+import org.junit.Test;\n+\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class PrometheusAdapterTest extends LocalServerTestBase {\n+    private static final long START_TIME_MS = 1603301400000L;\n+    private static final long END_TIME_MS = 1603301459000L;\n+\n+    @Test\n+    public void testSuccessfulResponseDeserialized() throws Exception {\n+        this.serverBootstrap.registerHandler(\"/api/v1/query_range\", new HttpRequestHandler() {\n+            @Override public void handle(HttpRequest request, HttpResponse response, HttpContext context) {\n+                response.setStatusCode(200);\n+                response.setEntity(buildSuccessResponseEntity());\n+            }\n+        });\n+\n+        HttpHost httpHost = this.start();\n+        PrometheusAdapter prometheusAdapter\n+            = new PrometheusAdapter(this.httpclient, httpHost, 30000);\n+        final List<PrometheusQueryResult> prometheusQueryResults = prometheusAdapter.queryMetric(\n+            \"kafka_server_BrokerTopicMetrics_OneMinuteRate{name=\\\"BytesOutPerSec\\\",topic=\\\"\\\"}\",\n+            START_TIME_MS, END_TIME_MS);\n+\n+        assertEquals(expectedResults().toString(), prometheusQueryResults.toString());\n+        assertEquals(expectedResults(), prometheusQueryResults);\n+    }\n+\n+    private HttpEntity buildSuccessResponseEntity() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NDk4OQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTkyNTY2OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMToxNTowN1rOHtAtOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjowNDowMlrOHtCFbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1OTU0Nw==", "bodyText": "Nit: Casting seems redundant.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516959547", "createdAt": "2020-11-03T21:15:07Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4MjEyNg==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516982126", "createdAt": "2020-11-03T22:04:02Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1OTU0Nw=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTk1NjI0OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMToyNToxM1rOHtA_6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjozNTozOFrOHtC6YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NDMyOQ==", "bodyText": "Missing verify(_prometheusAdapter) for the replayed mock.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516964329", "createdAt": "2020-11-03T21:25:13Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk5NTY4MQ==", "bodyText": "Fixed.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516995681", "createdAt": "2020-11-03T22:35:38Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NDMyOQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTk2MjkxOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMToyNzoyNVrOHtBD9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjowOTowNlrOHtCOHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NTM2NA==", "bodyText": "Should these methods be static?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516965364", "createdAt": "2020-11-03T21:27:25Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomMalformedSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"non-number\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomNegativeSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"-2000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsBadHostname() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithBadHostname(),\n+                                             buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullHostPort() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithNullHostPort(),\n+            buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullTopic() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResultsWithNullTopic(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithNullPartition());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsMalformedPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithMalformedPartition());\n+    }\n+\n+    public void testPrometheusQueryReturnsInvalidResults(\n+        List<PrometheusQueryResult> brokerResults,\n+        List<PrometheusQueryResult> topicResults,\n+        List<PrometheusQueryResult> partitionResults) throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, brokerResults,\n+                topicResults, partitionResults);\n+        }\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    private void assertSamplesValid(MetricSampler.Samples samples) {\n+        assertEquals(TOTAL_BROKERS, samples.brokerMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.brokerMetricSamples().stream()\n+                .map(BrokerMetricSample::brokerId)\n+                .collect(Collectors.toSet()));\n+        samples.brokerMetricSamples().forEach(brokerMetricSample -> {\n+            assertEquals(FIXED_VALUE,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.CPU_USAGE),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.LEADER_BYTES_OUT),\n+                DOUBLE_DELTA);\n+        });\n+\n+        assertEquals(TOTAL_BROKERS, samples.partitionMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.partitionMetricSamples().stream()\n+                .map(PartitionMetricSample::entity)\n+                .map(PartitionEntity::tp)\n+                .map(TopicPartition::partition)\n+                .collect(Collectors.toSet()));\n+\n+        samples.partitionMetricSamples().forEach(partitionMetricSample -> {\n+            assertEquals(TEST_TOPIC, partitionMetricSample.entity().tp().topic());\n+            assertEquals(FIXED_VALUE,\n+                         partitionMetricSample.metricValue(\n+                             KafkaMetricDef.commonMetricDefId(KafkaMetricDef.MESSAGE_IN_RATE)),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         partitionMetricSample.metricValue(\n+                         KafkaMetricDef.commonMetricDefId(KafkaMetricDef.LEADER_BYTES_IN)),\n+                DOUBLE_DELTA);\n+        });\n+    }\n+\n+    private void setupPrometheusAdapterMock(RawMetricType metricType,\n+                                            List<PrometheusQueryResult> brokerResults,\n+                                            List<PrometheusQueryResult> topicResults,\n+                                            List<PrometheusQueryResult> partitionResults) throws IOException {\n+        switch (metricType.metricScope()) {\n+            case BROKER:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(brokerResults);\n+                break;\n+            case TOPIC:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(topicResults);\n+                break;\n+            case PARTITION:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(partitionResults);\n+                break;\n+            default:\n+                break;\n+        }\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithBadHostname() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".non-existent-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithNullHostPort() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                null,\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResultsWithNullTopic() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithNullPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithMalformedPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                \"non-number\"\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+                resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                    \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                    TEST_TOPIC,\n+                    String.valueOf(partition)\n+                ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+            }\n+        }\n+        return resultList;\n+    }\n+\n+    private void addCapacityConfig(Map<String, Object> config) throws IOException {\n+        File capacityConfigFile = File.createTempFile(\"capacityConfig\", \"json\");\n+        FileOutputStream fileOutputStream = new FileOutputStream(capacityConfigFile);\n+        try (OutputStreamWriter writer = new OutputStreamWriter(fileOutputStream, StandardCharsets.UTF_8)) {\n+            writer.write(\"{\\n\"\n+                + \"  \\\"brokerCapacities\\\":[\\n\"\n+                + \"    {\\n\"\n+                + \"      \\\"brokerId\\\": \\\"-1\\\",\\n\"\n+                + \"      \\\"capacity\\\": {\\n\"\n+                + \"        \\\"DISK\\\": \\\"100000\\\",\\n\"\n+                + \"        \\\"CPU\\\": {\\\"num.cores\\\": \\\"4\\\"},\\n\"\n+                + \"        \\\"NW_IN\\\": \\\"5000000\\\",\\n\"\n+                + \"        \\\"NW_OUT\\\": \\\"5000000\\\"\\n\"\n+                + \"      }\\n\"\n+                + \"    }\\n\"\n+                + \"  ]\\n\"\n+                + \"}\\n\");\n+        }\n+        config.put(\"capacity.config.file\", capacityConfigFile.getAbsolutePath());\n+        BrokerCapacityConfigResolver brokerCapacityConfigResolver = new BrokerCapacityConfigFileResolver();\n+        config.put(\"broker.capacity.config.resolver.object\", brokerCapacityConfigResolver);\n+        config.put(\"sampling.allow.cpu.capacity.estimation\", true);\n+        brokerCapacityConfigResolver.configure(config);\n+    }\n+\n+    private Set<TopicPartition> generatePartitions() {\n+        Set<TopicPartition> set = new HashSet<>();\n+        for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+            TopicPartition topicPartition = new TopicPartition(TEST_TOPIC, partition);\n+            set.add(topicPartition);\n+        }\n+        return set;\n+    }\n+\n+    private Cluster generateCluster() {\n+        Node[] allNodes = new Node[TOTAL_BROKERS];\n+        Set<PartitionInfo> partitionInfo = new HashSet<>(TOTAL_BROKERS);\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            allNodes[brokerId] = new Node(brokerId, \"broker-\" + brokerId + \".test-cluster.org\", 9092);\n+        }\n+        for (int partitionId = 0; partitionId < TOTAL_PARTITIONS; partitionId++) {\n+            partitionInfo.add(new PartitionInfo(TEST_TOPIC, partitionId, allNodes[partitionId], allNodes, allNodes));\n+        }\n+        return new Cluster(\"cluster_id\", Arrays.asList(allNodes),\n+                           partitionInfo, Collections.emptySet(), Collections.emptySet());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 479}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NDM1MQ==", "bodyText": "Fixed these.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516984351", "createdAt": "2020-11-03T22:09:06Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomMalformedSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"non-number\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomNegativeSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"-2000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsBadHostname() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithBadHostname(),\n+                                             buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullHostPort() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithNullHostPort(),\n+            buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullTopic() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResultsWithNullTopic(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithNullPartition());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsMalformedPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithMalformedPartition());\n+    }\n+\n+    public void testPrometheusQueryReturnsInvalidResults(\n+        List<PrometheusQueryResult> brokerResults,\n+        List<PrometheusQueryResult> topicResults,\n+        List<PrometheusQueryResult> partitionResults) throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, brokerResults,\n+                topicResults, partitionResults);\n+        }\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    private void assertSamplesValid(MetricSampler.Samples samples) {\n+        assertEquals(TOTAL_BROKERS, samples.brokerMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.brokerMetricSamples().stream()\n+                .map(BrokerMetricSample::brokerId)\n+                .collect(Collectors.toSet()));\n+        samples.brokerMetricSamples().forEach(brokerMetricSample -> {\n+            assertEquals(FIXED_VALUE,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.CPU_USAGE),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.LEADER_BYTES_OUT),\n+                DOUBLE_DELTA);\n+        });\n+\n+        assertEquals(TOTAL_BROKERS, samples.partitionMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.partitionMetricSamples().stream()\n+                .map(PartitionMetricSample::entity)\n+                .map(PartitionEntity::tp)\n+                .map(TopicPartition::partition)\n+                .collect(Collectors.toSet()));\n+\n+        samples.partitionMetricSamples().forEach(partitionMetricSample -> {\n+            assertEquals(TEST_TOPIC, partitionMetricSample.entity().tp().topic());\n+            assertEquals(FIXED_VALUE,\n+                         partitionMetricSample.metricValue(\n+                             KafkaMetricDef.commonMetricDefId(KafkaMetricDef.MESSAGE_IN_RATE)),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         partitionMetricSample.metricValue(\n+                         KafkaMetricDef.commonMetricDefId(KafkaMetricDef.LEADER_BYTES_IN)),\n+                DOUBLE_DELTA);\n+        });\n+    }\n+\n+    private void setupPrometheusAdapterMock(RawMetricType metricType,\n+                                            List<PrometheusQueryResult> brokerResults,\n+                                            List<PrometheusQueryResult> topicResults,\n+                                            List<PrometheusQueryResult> partitionResults) throws IOException {\n+        switch (metricType.metricScope()) {\n+            case BROKER:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(brokerResults);\n+                break;\n+            case TOPIC:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(topicResults);\n+                break;\n+            case PARTITION:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(partitionResults);\n+                break;\n+            default:\n+                break;\n+        }\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithBadHostname() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".non-existent-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithNullHostPort() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                null,\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResultsWithNullTopic() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithNullPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithMalformedPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                \"non-number\"\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+                resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                    \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                    TEST_TOPIC,\n+                    String.valueOf(partition)\n+                ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+            }\n+        }\n+        return resultList;\n+    }\n+\n+    private void addCapacityConfig(Map<String, Object> config) throws IOException {\n+        File capacityConfigFile = File.createTempFile(\"capacityConfig\", \"json\");\n+        FileOutputStream fileOutputStream = new FileOutputStream(capacityConfigFile);\n+        try (OutputStreamWriter writer = new OutputStreamWriter(fileOutputStream, StandardCharsets.UTF_8)) {\n+            writer.write(\"{\\n\"\n+                + \"  \\\"brokerCapacities\\\":[\\n\"\n+                + \"    {\\n\"\n+                + \"      \\\"brokerId\\\": \\\"-1\\\",\\n\"\n+                + \"      \\\"capacity\\\": {\\n\"\n+                + \"        \\\"DISK\\\": \\\"100000\\\",\\n\"\n+                + \"        \\\"CPU\\\": {\\\"num.cores\\\": \\\"4\\\"},\\n\"\n+                + \"        \\\"NW_IN\\\": \\\"5000000\\\",\\n\"\n+                + \"        \\\"NW_OUT\\\": \\\"5000000\\\"\\n\"\n+                + \"      }\\n\"\n+                + \"    }\\n\"\n+                + \"  ]\\n\"\n+                + \"}\\n\");\n+        }\n+        config.put(\"capacity.config.file\", capacityConfigFile.getAbsolutePath());\n+        BrokerCapacityConfigResolver brokerCapacityConfigResolver = new BrokerCapacityConfigFileResolver();\n+        config.put(\"broker.capacity.config.resolver.object\", brokerCapacityConfigResolver);\n+        config.put(\"sampling.allow.cpu.capacity.estimation\", true);\n+        brokerCapacityConfigResolver.configure(config);\n+    }\n+\n+    private Set<TopicPartition> generatePartitions() {\n+        Set<TopicPartition> set = new HashSet<>();\n+        for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+            TopicPartition topicPartition = new TopicPartition(TEST_TOPIC, partition);\n+            set.add(topicPartition);\n+        }\n+        return set;\n+    }\n+\n+    private Cluster generateCluster() {\n+        Node[] allNodes = new Node[TOTAL_BROKERS];\n+        Set<PartitionInfo> partitionInfo = new HashSet<>(TOTAL_BROKERS);\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            allNodes[brokerId] = new Node(brokerId, \"broker-\" + brokerId + \".test-cluster.org\", 9092);\n+        }\n+        for (int partitionId = 0; partitionId < TOTAL_PARTITIONS; partitionId++) {\n+            partitionInfo.add(new PartitionInfo(TEST_TOPIC, partitionId, allNodes[partitionId], allNodes, allNodes));\n+        }\n+        return new Cluster(\"cluster_id\", Arrays.asList(allNodes),\n+                           partitionInfo, Collections.emptySet(), Collections.emptySet());\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NTM2NA=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 479}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTk3NzgzOnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTozMjoxOVrOHtBMxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjoyMzoyMVrOHtCmAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NzYyMw==", "bodyText": "Nit: Applies to similar uses above: Should this be Collections.singletonList since it has a single element?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516967623", "createdAt": "2020-11-03T21:32:19Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomMalformedSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"non-number\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomNegativeSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"-2000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsBadHostname() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithBadHostname(),\n+                                             buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullHostPort() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithNullHostPort(),\n+            buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullTopic() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResultsWithNullTopic(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithNullPartition());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsMalformedPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithMalformedPartition());\n+    }\n+\n+    public void testPrometheusQueryReturnsInvalidResults(\n+        List<PrometheusQueryResult> brokerResults,\n+        List<PrometheusQueryResult> topicResults,\n+        List<PrometheusQueryResult> partitionResults) throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, brokerResults,\n+                topicResults, partitionResults);\n+        }\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    private void assertSamplesValid(MetricSampler.Samples samples) {\n+        assertEquals(TOTAL_BROKERS, samples.brokerMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.brokerMetricSamples().stream()\n+                .map(BrokerMetricSample::brokerId)\n+                .collect(Collectors.toSet()));\n+        samples.brokerMetricSamples().forEach(brokerMetricSample -> {\n+            assertEquals(FIXED_VALUE,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.CPU_USAGE),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.LEADER_BYTES_OUT),\n+                DOUBLE_DELTA);\n+        });\n+\n+        assertEquals(TOTAL_BROKERS, samples.partitionMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.partitionMetricSamples().stream()\n+                .map(PartitionMetricSample::entity)\n+                .map(PartitionEntity::tp)\n+                .map(TopicPartition::partition)\n+                .collect(Collectors.toSet()));\n+\n+        samples.partitionMetricSamples().forEach(partitionMetricSample -> {\n+            assertEquals(TEST_TOPIC, partitionMetricSample.entity().tp().topic());\n+            assertEquals(FIXED_VALUE,\n+                         partitionMetricSample.metricValue(\n+                             KafkaMetricDef.commonMetricDefId(KafkaMetricDef.MESSAGE_IN_RATE)),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         partitionMetricSample.metricValue(\n+                         KafkaMetricDef.commonMetricDefId(KafkaMetricDef.LEADER_BYTES_IN)),\n+                DOUBLE_DELTA);\n+        });\n+    }\n+\n+    private void setupPrometheusAdapterMock(RawMetricType metricType,\n+                                            List<PrometheusQueryResult> brokerResults,\n+                                            List<PrometheusQueryResult> topicResults,\n+                                            List<PrometheusQueryResult> partitionResults) throws IOException {\n+        switch (metricType.metricScope()) {\n+            case BROKER:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(brokerResults);\n+                break;\n+            case TOPIC:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(topicResults);\n+                break;\n+            case PARTITION:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(partitionResults);\n+                break;\n+            default:\n+                break;\n+        }\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithBadHostname() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".non-existent-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithNullHostPort() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                null,\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResultsWithNullTopic() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithNullPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithMalformedPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                \"non-number\"\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+                resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                    \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                    TEST_TOPIC,\n+                    String.valueOf(partition)\n+                ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 428}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NTUyMg==", "bodyText": "I have deliberately kept it as Arrays.asList despite some IntelliJ warnings, because there is nothing explicit about the test that requires it to be a single value. I am, however, do not have a strong opinion on this, and can change it to Collections.emptyList().", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516985522", "createdAt": "2020-11-03T22:11:44Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomMalformedSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"non-number\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomNegativeSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"-2000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsBadHostname() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithBadHostname(),\n+                                             buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullHostPort() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithNullHostPort(),\n+            buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullTopic() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResultsWithNullTopic(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithNullPartition());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsMalformedPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithMalformedPartition());\n+    }\n+\n+    public void testPrometheusQueryReturnsInvalidResults(\n+        List<PrometheusQueryResult> brokerResults,\n+        List<PrometheusQueryResult> topicResults,\n+        List<PrometheusQueryResult> partitionResults) throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, brokerResults,\n+                topicResults, partitionResults);\n+        }\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    private void assertSamplesValid(MetricSampler.Samples samples) {\n+        assertEquals(TOTAL_BROKERS, samples.brokerMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.brokerMetricSamples().stream()\n+                .map(BrokerMetricSample::brokerId)\n+                .collect(Collectors.toSet()));\n+        samples.brokerMetricSamples().forEach(brokerMetricSample -> {\n+            assertEquals(FIXED_VALUE,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.CPU_USAGE),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.LEADER_BYTES_OUT),\n+                DOUBLE_DELTA);\n+        });\n+\n+        assertEquals(TOTAL_BROKERS, samples.partitionMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.partitionMetricSamples().stream()\n+                .map(PartitionMetricSample::entity)\n+                .map(PartitionEntity::tp)\n+                .map(TopicPartition::partition)\n+                .collect(Collectors.toSet()));\n+\n+        samples.partitionMetricSamples().forEach(partitionMetricSample -> {\n+            assertEquals(TEST_TOPIC, partitionMetricSample.entity().tp().topic());\n+            assertEquals(FIXED_VALUE,\n+                         partitionMetricSample.metricValue(\n+                             KafkaMetricDef.commonMetricDefId(KafkaMetricDef.MESSAGE_IN_RATE)),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         partitionMetricSample.metricValue(\n+                         KafkaMetricDef.commonMetricDefId(KafkaMetricDef.LEADER_BYTES_IN)),\n+                DOUBLE_DELTA);\n+        });\n+    }\n+\n+    private void setupPrometheusAdapterMock(RawMetricType metricType,\n+                                            List<PrometheusQueryResult> brokerResults,\n+                                            List<PrometheusQueryResult> topicResults,\n+                                            List<PrometheusQueryResult> partitionResults) throws IOException {\n+        switch (metricType.metricScope()) {\n+            case BROKER:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(brokerResults);\n+                break;\n+            case TOPIC:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(topicResults);\n+                break;\n+            case PARTITION:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(partitionResults);\n+                break;\n+            default:\n+                break;\n+        }\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithBadHostname() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".non-existent-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithNullHostPort() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                null,\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResultsWithNullTopic() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithNullPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithMalformedPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                \"non-number\"\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+                resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                    \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                    TEST_TOPIC,\n+                    String.valueOf(partition)\n+                ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NzYyMw=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 428}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk5MDQ2Ng==", "bodyText": "Changed to Collections.emptyList() everywhere.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516990466", "createdAt": "2020-11-03T22:23:21Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {\n+        return new MetricSamplerOptions(\n+            generateCluster(),\n+            generatePartitions(),\n+            START_TIME_MS,\n+            END_TIME_MS,\n+            MetricSampler.SamplingMode.ALL,\n+            KafkaMetricDef.commonMetricDef(),\n+            60000\n+        );\n+    }\n+\n+    @Test\n+    public void testGetSamplesSuccess() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, buildBrokerResults(),\n+                buildTopicResults(), buildPartitionResults());\n+        }\n+\n+        replay(_prometheusAdapter);\n+        MetricSampler.Samples samples = _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        assertSamplesValid(samples);\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test\n+    public void testGetSamplesWithCustomSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"5000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+        assertEquals(5000, (long) _prometheusMetricSampler._prometheusAdapter._samplingIntervalMs);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomMalformedSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"non-number\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesWithCustomNegativeSamplingInterval() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        config.put(PROMETHEUS_QUERY_RESOLUTION_STEP_MS_CONFIG, \"-2000\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsBadHostname() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithBadHostname(),\n+                                             buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullHostPort() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResultsWithNullHostPort(),\n+            buildTopicResults(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullTopic() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResultsWithNullTopic(), buildPartitionResults());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsNullPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithNullPartition());\n+    }\n+\n+    @Test\n+    public void testPrometheusQueryReturnsMalformedPartition() throws Exception {\n+        testPrometheusQueryReturnsInvalidResults(buildBrokerResults(),\n+            buildTopicResults(), buildPartitionResultsWithMalformedPartition());\n+    }\n+\n+    public void testPrometheusQueryReturnsInvalidResults(\n+        List<PrometheusQueryResult> brokerResults,\n+        List<PrometheusQueryResult> topicResults,\n+        List<PrometheusQueryResult> partitionResults) throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        for (RawMetricType rawMetricType : _prometheusQueryMap.keySet()) {\n+            setupPrometheusAdapterMock(rawMetricType, brokerResults,\n+                topicResults, partitionResults);\n+        }\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    private void assertSamplesValid(MetricSampler.Samples samples) {\n+        assertEquals(TOTAL_BROKERS, samples.brokerMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.brokerMetricSamples().stream()\n+                .map(BrokerMetricSample::brokerId)\n+                .collect(Collectors.toSet()));\n+        samples.brokerMetricSamples().forEach(brokerMetricSample -> {\n+            assertEquals(FIXED_VALUE,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.CPU_USAGE),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         brokerMetricSample.metricValue(KafkaMetricDef.LEADER_BYTES_OUT),\n+                DOUBLE_DELTA);\n+        });\n+\n+        assertEquals(TOTAL_BROKERS, samples.partitionMetricSamples().size());\n+        assertEquals(\n+            new HashSet<>(Arrays.asList(0, 1, 2)),\n+            samples.partitionMetricSamples().stream()\n+                .map(PartitionMetricSample::entity)\n+                .map(PartitionEntity::tp)\n+                .map(TopicPartition::partition)\n+                .collect(Collectors.toSet()));\n+\n+        samples.partitionMetricSamples().forEach(partitionMetricSample -> {\n+            assertEquals(TEST_TOPIC, partitionMetricSample.entity().tp().topic());\n+            assertEquals(FIXED_VALUE,\n+                         partitionMetricSample.metricValue(\n+                             KafkaMetricDef.commonMetricDefId(KafkaMetricDef.MESSAGE_IN_RATE)),\n+                DOUBLE_DELTA);\n+            assertEquals(FIXED_VALUE / BYTES_IN_KB,\n+                         partitionMetricSample.metricValue(\n+                         KafkaMetricDef.commonMetricDefId(KafkaMetricDef.LEADER_BYTES_IN)),\n+                DOUBLE_DELTA);\n+        });\n+    }\n+\n+    private void setupPrometheusAdapterMock(RawMetricType metricType,\n+                                            List<PrometheusQueryResult> brokerResults,\n+                                            List<PrometheusQueryResult> topicResults,\n+                                            List<PrometheusQueryResult> partitionResults) throws IOException {\n+        switch (metricType.metricScope()) {\n+            case BROKER:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(brokerResults);\n+                break;\n+            case TOPIC:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(topicResults);\n+                break;\n+            case PARTITION:\n+                expect(_prometheusAdapter.queryMetric(eq(_prometheusQueryMap.get(metricType)), anyLong(), anyLong()))\n+                    .andReturn(partitionResults);\n+                break;\n+            default:\n+                break;\n+        }\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithBadHostname() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".non-existent-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildBrokerResultsWithNullHostPort() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                null,\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildTopicResultsWithNullTopic() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                null,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithNullPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                null\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResultsWithMalformedPartition() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                TEST_TOPIC,\n+                \"non-number\"\n+            ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));\n+        }\n+        return resultList;\n+    }\n+\n+    private List<PrometheusQueryResult> buildPartitionResults() {\n+        List<PrometheusQueryResult> resultList = new ArrayList<>();\n+        for (int brokerId = 0; brokerId < TOTAL_BROKERS; brokerId++) {\n+            for (int partition = 0; partition < TOTAL_PARTITIONS; partition++) {\n+                resultList.add(new PrometheusQueryResult(new PrometheusMetric(\n+                    \"broker-\" + brokerId + \".test-cluster.org:11001\",\n+                    TEST_TOPIC,\n+                    String.valueOf(partition)\n+                ), Arrays.asList(new PrometheusValue(START_EPOCH_SECONDS, FIXED_VALUE))));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NzYyMw=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 428}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTk3OTg5OnYy", "diffSide": "RIGHT", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTozMzowMFrOHtBN-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjowOToxOVrOHtCOjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NzkzMQ==", "bodyText": "Nit: Can we make this builder (and in general other builders in this class) static?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516967931", "createdAt": "2020-11-03T21:33:00Z", "author": {"login": "efeg"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NDQ2Mw==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516984463", "createdAt": "2020-11-03T22:09:19Z", "author": {"login": "wyuka"}, "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/prometheus/PrometheusMetricSamplerTest.java", "diffHunk": "@@ -0,0 +1,491 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import com.linkedin.cruisecontrol.common.config.ConfigException;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigFileResolver;\n+import com.linkedin.kafka.cruisecontrol.config.BrokerCapacityConfigResolver;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.metric.RawMetricType;\n+import com.linkedin.kafka.cruisecontrol.monitor.metricdefinition.KafkaMetricDef;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSampler;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricSamplerOptions;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.BrokerMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionEntity;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.holder.PartitionMetricSample;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusMetric;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusQueryResult;\n+import com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.model.PrometheusValue;\n+\n+import static com.linkedin.kafka.cruisecontrol.monitor.sampling.prometheus.PrometheusMetricSampler.*;\n+import static org.easymock.EasyMock.*;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Unit test for {@link PrometheusMetricSampler} class.\n+ */\n+public class PrometheusMetricSamplerTest {\n+    private static final int MILLIS_IN_SECOND = 1000;\n+    private static final double DOUBLE_DELTA = 0.00000001;\n+    private static final double BYTES_IN_KB = 1024.0;\n+\n+    private static final int FIXED_VALUE = 94;\n+    private static final long START_EPOCH_SECONDS = 1603301400L;\n+    private static final long START_TIME_MS = START_EPOCH_SECONDS * MILLIS_IN_SECOND;\n+    private static final long END_TIME_MS = START_TIME_MS + 59 * MILLIS_IN_SECOND;\n+\n+    private static final int TOTAL_BROKERS = 3;\n+    private static final int TOTAL_PARTITIONS = 3;\n+\n+    private static final String TEST_TOPIC = \"test-topic\";\n+\n+    private PrometheusMetricSampler _prometheusMetricSampler;\n+    private PrometheusAdapter _prometheusAdapter;\n+    private Map<RawMetricType, String> _prometheusQueryMap;\n+\n+    /**\n+     * Set up mocks\n+     */\n+    @Before\n+    public void setUp() {\n+        _prometheusAdapter = mock(PrometheusAdapter.class);\n+        _prometheusMetricSampler = new PrometheusMetricSampler();\n+        _prometheusQueryMap = new DefaultPrometheusQuerySupplier().get();\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNoPortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithPrometheusEndpointNegativePortFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:-20\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test\n+    public void testConfigureWithPrometheusEndpointNoSchemaDoesNotFail() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testConfigureWithNoPrometheusEndpointFails() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = SamplingException.class)\n+    public void testGetSamplesQueryThrowsException() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+        expect(_prometheusAdapter.queryMetric(anyString(), anyLong(), anyLong()))\n+            .andThrow(new IOException(\"Exception in fetching metrics\"));\n+\n+        replay(_prometheusAdapter);\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+    }\n+\n+    @Test\n+    public void testGetSamplesCustomPrometheusQuerySupplier() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, TestQuerySupplier.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+\n+        MetricSamplerOptions metricSamplerOptions = buildMetricSamplerOptions();\n+        _prometheusMetricSampler._prometheusAdapter = _prometheusAdapter;\n+\n+        expect(_prometheusAdapter.queryMetric(eq(TestQuerySupplier.TEST_QUERY), anyLong(), anyLong()))\n+            .andReturn(buildBrokerResults());\n+        replay(_prometheusAdapter);\n+\n+        _prometheusMetricSampler.getSamples(metricSamplerOptions);\n+\n+        verify(_prometheusAdapter);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierUnknownClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, \"com.test.NonExistentClass\");\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void testGetSamplesPrometheusQuerySupplierInvalidClass() throws Exception {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(PROMETHEUS_SERVER_ENDPOINT_CONFIG, \"http://kafka-cluster-1.org:9090\");\n+        addCapacityConfig(config);\n+        config.put(PROMETHEUS_QUERY_SUPPLIER_CONFIG, String.class.getName());\n+        _prometheusMetricSampler.configure(config);\n+    }\n+\n+    private MetricSamplerOptions buildMetricSamplerOptions() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk2NzkzMQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTk5NTA3OnYy", "diffSide": "RIGHT", "path": "docs/wiki/User Guide/Configurations.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTozODoyMFrOHtBXQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjoxMzoyNlrOHtCVhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3MDMwNQ==", "bodyText": "Can we update the table of contents with this new title?", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516970305", "createdAt": "2020-11-03T21:38:20Z", "author": {"login": "efeg"}, "path": "docs/wiki/User Guide/Configurations.md", "diffHunk": "@@ -232,6 +232,12 @@ We are still trying to improve cruise control. And following are some configurat\n | metric.reporter.topic                     | String | N         | \"__CruiseControlMetrics\"                                   | The exact topic name from which the sampler should be consuming the interested metrics from.                                             |\n | metric.reporter.sampler.group.id          | String | N         | 60,000                                                     | The consumer group id to use for the consumers to consume from the Kafka cluster.                                                        |\n \n+### PrometheusMetricSampler configurations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NjI0NA==", "bodyText": "Done.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516986244", "createdAt": "2020-11-03T22:13:26Z", "author": {"login": "wyuka"}, "path": "docs/wiki/User Guide/Configurations.md", "diffHunk": "@@ -232,6 +232,12 @@ We are still trying to improve cruise control. And following are some configurat\n | metric.reporter.topic                     | String | N         | \"__CruiseControlMetrics\"                                   | The exact topic name from which the sampler should be consuming the interested metrics from.                                             |\n | metric.reporter.sampler.group.id          | String | N         | 60,000                                                     | The consumer group id to use for the consumers to consume from the Kafka cluster.                                                        |\n \n+### PrometheusMetricSampler configurations", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3MDMwNQ=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MDAwMDYyOnYy", "diffSide": "RIGHT", "path": "docs/wiki/User Guide/Configurations.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMTo0MDowOFrOHtBaeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMjoxNTozNlrOHtCZcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3MTEyOA==", "bodyText": "Nit: This does not affect the correctness, but the placement of \"|\" in these two rows seem a little off. It would be great if we can align them properly.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516971128", "createdAt": "2020-11-03T21:40:08Z", "author": {"login": "efeg"}, "path": "docs/wiki/User Guide/Configurations.md", "diffHunk": "@@ -232,6 +232,12 @@ We are still trying to improve cruise control. And following are some configurat\n | metric.reporter.topic                     | String | N         | \"__CruiseControlMetrics\"                                   | The exact topic name from which the sampler should be consuming the interested metrics from.                                             |\n | metric.reporter.sampler.group.id          | String | N         | 60,000                                                     | The consumer group id to use for the consumers to consume from the Kafka cluster.                                                        |\n \n+### PrometheusMetricSampler configurations\n+| Name                                 | Type    | Required? | Default Value | Description                                                                                                                                                                                                |\n+|-------------------------------------------|--------|-----------|------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NzI0OA==", "bodyText": "Sorry about this. I should have fixed this to start with. Done now.", "url": "https://github.com/linkedin/cruise-control/pull/1366#discussion_r516987248", "createdAt": "2020-11-03T22:15:36Z", "author": {"login": "wyuka"}, "path": "docs/wiki/User Guide/Configurations.md", "diffHunk": "@@ -232,6 +232,12 @@ We are still trying to improve cruise control. And following are some configurat\n | metric.reporter.topic                     | String | N         | \"__CruiseControlMetrics\"                                   | The exact topic name from which the sampler should be consuming the interested metrics from.                                             |\n | metric.reporter.sampler.group.id          | String | N         | 60,000                                                     | The consumer group id to use for the consumers to consume from the Kafka cluster.                                                        |\n \n+### PrometheusMetricSampler configurations\n+| Name                                 | Type    | Required? | Default Value | Description                                                                                                                                                                                                |\n+|-------------------------------------------|--------|-----------|------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3MTEyOA=="}, "originalCommit": {"oid": "ca1565a1b87ba59171e87895707144a5340c7970"}, "originalPosition": 6}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 715, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}