{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE1MjQyMDU5", "number": 1684, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxODozNzoyOVrOD6vQCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQyMjo0OTowN1rOD66RGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyOTE4MTU1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxODozNzoyOVrOGSvdWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQxNToyNjoyOFrOGS72kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMwNTExMw==", "bodyText": "I have some on context on this test. Did you happen to dig into the root cause of the ConnectionRefused stack trace that you posted (you have the full stack trace with failed RPC name?)?\nIdeally that shouldn't happen right? The waitFor() above means the bootstrap of the master is complete and it should be able to process the shutdown() command, in a normal case. Just want to be sure we are not masking a real bug, especially after Duo moved all the code from RPC to registry.", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422305113", "createdAt": "2020-05-08T18:37:29Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -163,7 +164,16 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       assertNotEquals(\"Timeout waiting for server manager to become available.\",\n         -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n           () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      try {\n+        htu.getConnection().getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a5dfd16764c5606507460ba877d86dc0e112763"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMxNTgxNw==", "bodyText": "This is what I thought that after waitFor(), bootstrap should be over but in 2/50 cases, this stacktrace comes. This test is also present in flaky report and I just realized that flaky results data accessible is 2 days after Duo's commit. Let me check 5-10 older builds before the commit and see if this was not flaky before.", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422315817", "createdAt": "2020-05-08T18:59:06Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -163,7 +164,16 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       assertNotEquals(\"Timeout waiting for server manager to become available.\",\n         -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n           () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      try {\n+        htu.getConnection().getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMwNTExMw=="}, "originalCommit": {"oid": "5a5dfd16764c5606507460ba877d86dc0e112763"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMxODU1MQ==", "bodyText": "Test results before 5th May doesn't seem available and the flakes seem present on the day of commit (need to check timezone diff): https://builds.apache.org/job/HBase-Flaky-Tests/job/master/6184/", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422318551", "createdAt": "2020-05-08T19:04:55Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -163,7 +164,16 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       assertNotEquals(\"Timeout waiting for server manager to become available.\",\n         -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n           () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      try {\n+        htu.getConnection().getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMwNTExMw=="}, "originalCommit": {"oid": "5a5dfd16764c5606507460ba877d86dc0e112763"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMzOTg5OQ==", "bodyText": "Btw the root cause that I have seen for the above Exception:\n2020-05-09 00:58:39,957 ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033] master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\norg.apache.zookeeper.KeeperException$SystemErrorException: KeeperErrorCode = SystemError\n\tat org.apache.hadoop.hbase.zookeeper.ZKWatcher.interruptedException(ZKWatcher.java:626)\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1285)\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1269)\n\tat org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker.setClusterDown(ClusterStatusTracker.java:84)\n\tat org.apache.hadoop.hbase.master.HMaster.shutdown(HMaster.java:2876)\n\tat org.apache.hadoop.hbase.master.MasterRpcServices.shutdown(MasterRpcServices.java:1630)\n\tat org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:395)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:133)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:338)\n\tat org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:318)\nCaused by: java.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:502)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1529)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1512)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:1791)\n\tat org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:171)\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1280)\n\t... 9 more\n2020-05-09 00:58:39,957 DEBUG [Time-limited test-EventThread] zookeeper.ZKWatcher(490): master:53033-0x1000d0785dd0000, quorum=127.0.0.1:60460, baseZNode=/hbase Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/master\n2020-05-09 00:58:39,957 INFO  [M:0;172.20.10.2:53033] regionserver.HRegionServer(1119): stopping server 172.20.10.2,53033,1588966118448; all regions closed.\n2020-05-09 00:58:39,958 INFO  [M:0;172.20.10.2:53033] hbase.ChoreService(329): Chore service for: master/172.20.10.2:0 had [] on shutdown\n2020-05-09 00:58:39,958 DEBUG [M:0;172.20.10.2:53033] master.HMaster(1516): Stopping service threads\n\nDoes not look like issue with moving code to registry.", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422339899", "createdAt": "2020-05-08T19:50:02Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -163,7 +164,16 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       assertNotEquals(\"Timeout waiting for server manager to become available.\",\n         -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n           () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      try {\n+        htu.getConnection().getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMwNTExMw=="}, "originalCommit": {"oid": "5a5dfd16764c5606507460ba877d86dc0e112763"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMjgyMA==", "bodyText": "Ya, doesn't look like it. This is a known problem I think (Root cause is a flaky Zk connection. I can repro it locally once every ~100 runs). The problem here seems to me that shutdown is running inline with the rpc call [1]. Nick actually made the shutdown call async in HBASE-23808 but Stack undid that in HBASE-24052 [2]. Perhaps the fix is to actually make it async in test until we fix HBASE-24070? Thoughts? (@saintstack curious what the rationale was in HBASE-24052).\n[1] https://issues.apache.org/jira/browse/HBASE-24070\n[2] https://issues.apache.org/jira/browse/HBASE-24052?focusedCommentId=17068822&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17068822", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422412820", "createdAt": "2020-05-08T23:03:12Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -163,7 +164,16 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       assertNotEquals(\"Timeout waiting for server manager to become available.\",\n         -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n           () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      try {\n+        htu.getConnection().getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMwNTExMw=="}, "originalCommit": {"oid": "5a5dfd16764c5606507460ba877d86dc0e112763"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjUwODE3OA==", "bodyText": "Thanks @bharathv for the context. IMHO we should make shutdown() async in the test until fixing HBASE-24070.", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422508178", "createdAt": "2020-05-09T15:26:28Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -163,7 +164,16 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       assertNotEquals(\"Timeout waiting for server manager to become available.\",\n         -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n           () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      try {\n+        htu.getConnection().getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMwNTExMw=="}, "originalCommit": {"oid": "5a5dfd16764c5606507460ba877d86dc0e112763"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDk4NDAyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQyMjo0NTo0OFrOGS-v0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNzozNToyOFrOGTGEAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU1NTYwMw==", "bodyText": "we can use htu.getConnection()?", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422555603", "createdAt": "2020-05-09T22:45:48Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,47 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n+          LOG.debug(\"Attempting to establish connection.\");\n+          final CompletableFuture<AsyncConnection> connFuture =\n+            ConnectionFactory.createAsyncConnection(conf);\n+          try (final AsyncConnection conn = connFuture.join()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01178e64782f19ab874cae805fef509be849191f"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU5ODY3MQ==", "bodyText": "since we want to use AsynAdmin with AsyncConnection, htu.getConnection() wouldn't work.", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422598671", "createdAt": "2020-05-10T07:00:15Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,47 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n+          LOG.debug(\"Attempting to establish connection.\");\n+          final CompletableFuture<AsyncConnection> connFuture =\n+            ConnectionFactory.createAsyncConnection(conf);\n+          try (final AsyncConnection conn = connFuture.join()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU1NTYwMw=="}, "originalCommit": {"oid": "01178e64782f19ab874cae805fef509be849191f"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY2OTE1Nw==", "bodyText": "This is already running inside a thread. What purpose does AsyncAdmin serve? Also, you are joining on it right away.", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422669157", "createdAt": "2020-05-10T16:43:20Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,47 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n+          LOG.debug(\"Attempting to establish connection.\");\n+          final CompletableFuture<AsyncConnection> connFuture =\n+            ConnectionFactory.createAsyncConnection(conf);\n+          try (final AsyncConnection conn = connFuture.join()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU1NTYwMw=="}, "originalCommit": {"oid": "01178e64782f19ab874cae805fef509be849191f"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY3NTQ1Ng==", "bodyText": "Hmm since it is already in ForkJoin, this doesn't matter much. I don't have strong opinion, but it's just that we are directly using AsyncConnection and AsyncAdmin rather than via Connection and Admin interfaces. But yes even htu.getConnection() should work after putting ZK recovery configs in htu directly.\nHowever, since it's not much of a diff, do you want Addendum now, or let's wait for some time and see reports and then we can add it maybe in a week?", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422675456", "createdAt": "2020-05-10T17:35:28Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,47 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n+          LOG.debug(\"Attempting to establish connection.\");\n+          final CompletableFuture<AsyncConnection> connFuture =\n+            ConnectionFactory.createAsyncConnection(conf);\n+          try (final AsyncConnection conn = connFuture.join()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU1NTYwMw=="}, "originalCommit": {"oid": "01178e64782f19ab874cae805fef509be849191f"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDk4NjQ5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQyMjo0OTowN1rOGS-w_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzoxNjowMFrOGTBewA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU1NTkwMQ==", "bodyText": "Shouldn't this be before we trigger the shutdown command?", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422555901", "createdAt": "2020-05-09T22:49:07Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,47 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n+          LOG.debug(\"Attempting to establish connection.\");\n+          final CompletableFuture<AsyncConnection> connFuture =\n+            ConnectionFactory.createAsyncConnection(conf);\n+          try (final AsyncConnection conn = connFuture.join()) {\n+            LOG.info(\"Sending shutdown RPC.\");\n+            try {\n+              conn.getAdmin().shutdown().join();\n+              LOG.info(\"Shutdown RPC sent.\");\n+              return true;\n+            } catch (CompletionException e) {\n+              LOG.error(\"Failure sending shutdown RPC.\");\n+            }\n+          } catch (IOException|CompletionException e) {\n+            LOG.error(\"Failed to establish connection.\");\n+          } catch (Throwable e) {\n+            LOG.error(\"Something unexpected happened.\", e);\n+          }\n+          return false;\n+        });\n+        assertNotEquals(\"Failed to issue shutdown RPC after \" + Duration.ofMillis(timeout),\n+          -1, result);\n+      });\n+\n       masterThread.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "01178e64782f19ab874cae805fef509be849191f"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMDM4NA==", "bodyText": "Oh yes, let me update.\nThanks", "url": "https://github.com/apache/hbase/pull/1684#discussion_r422600384", "createdAt": "2020-05-10T07:16:00Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,47 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          final Configuration conf = createResponsiveZkConfig(htu.getConfiguration());\n+          LOG.debug(\"Attempting to establish connection.\");\n+          final CompletableFuture<AsyncConnection> connFuture =\n+            ConnectionFactory.createAsyncConnection(conf);\n+          try (final AsyncConnection conn = connFuture.join()) {\n+            LOG.info(\"Sending shutdown RPC.\");\n+            try {\n+              conn.getAdmin().shutdown().join();\n+              LOG.info(\"Shutdown RPC sent.\");\n+              return true;\n+            } catch (CompletionException e) {\n+              LOG.error(\"Failure sending shutdown RPC.\");\n+            }\n+          } catch (IOException|CompletionException e) {\n+            LOG.error(\"Failed to establish connection.\");\n+          } catch (Throwable e) {\n+            LOG.error(\"Something unexpected happened.\", e);\n+          }\n+          return false;\n+        });\n+        assertNotEquals(\"Failed to issue shutdown RPC after \" + Duration.ofMillis(timeout),\n+          -1, result);\n+      });\n+\n       masterThread.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU1NTkwMQ=="}, "originalCommit": {"oid": "01178e64782f19ab874cae805fef509be849191f"}, "originalPosition": 72}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1763, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}