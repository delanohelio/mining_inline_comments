{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzNjcyNDk2", "number": 2800, "title": "HBASE-25249 Adding StoreContext", "bodyText": "Adding StoreContext which contains the metadata about the HStore. This\nmeta data can be used across the HFileWriter/Readers and other HStore\nconsumers without the need of passing around the complete store and\nexposing its internals.", "createdAt": "2020-12-21T19:10:50Z", "url": "https://github.com/apache/hbase/pull/2800", "merged": true, "mergeCommit": {"oid": "686b72c44e2b72cb8f8817b0bd24d12f25820a8f"}, "closed": true, "closedAt": "2021-01-09T04:16:46Z", "author": {"login": "taklwu"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdocBKWgFqTU1NjY0Mzk1MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdtm-FdAFqTU2MzA3MTI5Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NjQzOTUw", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-556643950", "createdAt": "2020-12-21T20:26:55Z", "commit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDoyNjo1NVrOIJk_Xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDozMDozN1rOIJlE9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNDE0Mg==", "bodyText": "Consider naming this getEncryptionContext (as elsewhere)", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546914142", "createdAt": "2020-12-21T20:26:55Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +501,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getCryptoContext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTMxMw==", "bodyText": "Does DEEP_OVERHEAD and heapSize() need to change? Does TestHeapSize still pass?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546915313", "createdAt": "2020-12-21T20:29:56Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -2559,7 +2572,7 @@ public boolean needsCompaction() {\n    * @return cache configuration for this Store.\n    */\n   public CacheConfig getCacheConfig() {\n-    return this.cacheConf;\n+    return storeContext.getCacheConf();\n   }\n \n   public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 594}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTU3NA==", "bodyText": "Should this implement HeapSize? This information used to be included in Store/HStore heap utilization estimation via HStore#heapSize() and so we should still track it?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546915574", "createdAt": "2020-12-21T20:30:37Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreContext.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public class HStoreContext {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 37}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a", "author": {"user": {"login": "abhishekkhanna01", "name": null}}, "url": "https://github.com/apache/hbase/commit/e5e21fa09a6869fc3de3179ec1dee077e0019a9a", "committedDate": "2020-12-21T19:07:48Z", "message": "HBASE-25249 Adding HStoreContext\n\nAdding HStoreContext which contains the metadata about the HStore. This\nmeta data can be used across the HFileWriter/Readers and other HStore\nconsumers without the need of passing around the complete store and\nexposing its internals."}, "afterCommit": {"oid": "742e7374a1b066b0d45ffadaf2c55a38b6f0513c", "author": {"user": {"login": "abhishekkhanna01", "name": null}}, "url": "https://github.com/apache/hbase/commit/742e7374a1b066b0d45ffadaf2c55a38b6f0513c", "committedDate": "2020-12-21T23:15:40Z", "message": "HBASE-25249 Adding HStoreContext\n\nAdding HStoreContext which contains the metadata about the HStore. This\nmeta data can be used across the HFileWriter/Readers and other HStore\nconsumers without the need of passing around the complete store and\nexposing its internals."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NjU4NzE4", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-557658718", "createdAt": "2020-12-23T06:05:29Z", "commit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwNjowNToyOVrOIKUP4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwNjoxNToxOVrOIKUtaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY4ODQxNw==", "bodyText": "You don't want to do get\nreturn ChecksymType.nameToType(conf.get(HConstants.CHECKSUM_TYPE_NAME, DEFAULT_WHATEVER_IT_IS));\nIs this a candidate for StoreContext? (Perhaps if called frequently). Perhaps StoreContext is not available where this is used?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r547688417", "createdAt": "2020-12-23T06:05:29Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java", "diffHunk": "@@ -136,4 +140,26 @@ public static OptionalLong getMaxSequenceIdInList(Collection<HStoreFile> sfs) {\n     return largestFile.isPresent() ? StoreUtils.getFileSplitPoint(largestFile.get(), comparator)\n         : Optional.empty();\n   }\n+\n+  /**\n+   * Returns the configured checksum algorithm.\n+   * @param conf The configuration\n+   * @return The checksum algorithm that is set in the configuration\n+   */\n+  public static ChecksumType getChecksumType(Configuration conf) {\n+    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ==", "bodyText": "Why introduce an StoreContext? Isn't that what a Store is? Store spans files.\n(We need the 'H' in HStoreContext?)", "url": "https://github.com/apache/hbase/pull/2800#discussion_r547691119", "createdAt": "2020-12-23T06:09:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5NTk3OQ==", "bodyText": "Yeah, just trying to understand why. You want to swap out the HStore implementation or something?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r547695979", "createdAt": "2020-12-23T06:15:19Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69f4aa2a00accbe53341300c6c46de724f8436e8", "author": {"user": {"login": "abhishekkhanna01", "name": null}}, "url": "https://github.com/apache/hbase/commit/69f4aa2a00accbe53341300c6c46de724f8436e8", "committedDate": "2020-12-23T20:09:10Z", "message": "HBASE-25249 Adding HStoreContext\n\nAdding HStoreContext which contains the metadata about the HStore. This\nmeta data can be used across the HFileWriter/Readers and other HStore\nconsumers without the need of passing around the complete store and\nexposing its internals."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2731227aaedbcb904b6a8f6036459ea53a296aa2", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/2731227aaedbcb904b6a8f6036459ea53a296aa2", "committedDate": "2020-12-23T20:09:10Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "baeef9cad915d22678151920ac968bc675a3116f", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/baeef9cad915d22678151920ac968bc675a3116f", "committedDate": "2020-12-23T20:09:10Z", "message": "additional fix checkstyle for HStoreContext"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/7725f5f131525c73a27cd4e5e26eedd8ee8920bd", "committedDate": "2020-12-23T20:09:10Z", "message": "Address comments from stack\n\n- rename HStoreContext to StoreContext\n- rewrite the getChecksumType\n- fix one more style issues"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a", "committedDate": "2020-12-23T02:48:59Z", "message": "additional fix checkstyle for HStoreContext"}, "afterCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/7725f5f131525c73a27cd4e5e26eedd8ee8920bd", "committedDate": "2020-12-23T20:09:10Z", "message": "Address comments from stack\n\n- rename HStoreContext to StoreContext\n- rewrite the getChecksumType\n- fix one more style issues"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4MjA5NTky", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-558209592", "createdAt": "2020-12-23T20:15:38Z", "commit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMDoxNTozOFrOIKzf_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMDoyMDo0NVrOIKzs9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwMDQ0Ng==", "bodyText": "Should this be final? I wouldn't expect you would need to change StoreContext (the reference) after it has been initialized", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548200446", "createdAt": "2020-12-23T20:15:38Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private StoreContext storeContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwMzc2NA==", "bodyText": "Does it simplify things to be able to pass StoreContext directly to the builders?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548203764", "createdAt": "2020-12-23T20:20:45Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1236,18 +1248,19 @@ private HFileContext createFileContext(Compression.Algorithm compression,\n                                 .withIncludesMvcc(includeMVCCReadpoint)\n                                 .withIncludesTags(includesTag)\n                                 .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n+                                .withCompressTags(getColumnFamilyDescriptor().isCompressTags())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "originalPosition": 484}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a9af9fa8f2efcd7bfda6cff4dd4635f1227d810", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/8a9af9fa8f2efcd7bfda6cff4dd4635f1227d810", "committedDate": "2020-12-23T22:17:43Z", "message": "make storecontext final in HStore and fix import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3c9561e16621296589f36554419e28edb67cdf9", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/b3c9561e16621296589f36554419e28edb67cdf9", "committedDate": "2020-12-24T00:14:32Z", "message": "use storecontext with writer generation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzI5MTg1", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-559329185", "createdAt": "2020-12-28T23:06:34Z", "commit": {"oid": "b3c9561e16621296589f36554419e28edb67cdf9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMzowNjozNVrOIMDotA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMzowNjozNVrOIMDotA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMzM5Ng==", "bodyText": "Does it make sense to return the supplier or just a straight getter where internal to this method it calls and returns the result of the .get()?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r549513396", "createdAt": "2020-12-28T23:06:35Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final HFileContext defaultFileContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.defaultFileContext = builder.defaultFileContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public HFileContext getDefaultFileContext() {\n+    return defaultFileContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public Supplier<InetSocketAddress[]> getFavoredNodesSupplier() {\n+    return favoredNodesSupplier;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c9561e16621296589f36554419e28edb67cdf9"}, "originalPosition": 91}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/5e87534477d0f130a10476ae6b1ae2128c21d28c", "committedDate": "2020-12-30T00:18:59Z", "message": "hide supplier from the getter of `FavoredNodes`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMjA2MTI1", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-562206125", "createdAt": "2021-01-05T23:17:27Z", "commit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzoxNzoyN1rOIOrBPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzozNDoyNVrOIOrXyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NTgwNw==", "bodyText": "Whats going on here?\nHFileContext is supposed to be ' Read-only HFile Context Information' but here we are adding a setter. I see there are one or two setters but we also have HFileContextBuilder. Shouldn't we be going via the Builder making HFileContexts? And why is this method not added on the Builder? (Can it be added non-public to encourage users to go via the Builder)?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552255807", "createdAt": "2021-01-05T23:17:27Z", "author": {"login": "saintstack"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "diffHunk": "@@ -138,6 +138,10 @@ public boolean isCompressedOrEncrypted() {\n     return compressAlgo;\n   }\n \n+  public void setCompression(Compression.Algorithm compressAlgo) {\n+    this.compressAlgo = compressAlgo;\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NzU3OQ==", "bodyText": "nit: why bother with this change at all? 'family' is passed on the constructor. Its final. Why bother going via the accessor in the constructor?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552257579", "createdAt": "2021-01-05T23:22:30Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -254,48 +244,47 @@\n   protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       final Configuration confParam, boolean warmup) throws IOException {\n \n-    this.fs = region.getRegionFileSystem();\n-\n-    // Assemble the store's home directory and Ensure it exists.\n-    fs.createStoreDir(family.getNameAsString());\n-    this.region = region;\n-    this.family = family;\n     // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n     // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n     // add global config first, then table and cf overrides, then cf metadata.\n     this.conf = new CompoundConfiguration()\n-      .add(confParam)\n-      .addBytesMap(region.getTableDescriptor().getValues())\n-      .addStringMap(family.getConfiguration())\n-      .addBytesMap(family.getValues());\n-    this.blocksize = family.getBlocksize();\n+        .add(confParam)\n+        .addBytesMap(region.getTableDescriptor().getValues())\n+        .addStringMap(family.getConfiguration())\n+        .addBytesMap(family.getValues());\n+\n+    this.region = region;\n+    this.storeContext = initializeStoreContext(family);\n+\n+    // Assemble the store's home directory and Ensure it exists.\n+    getRegionFileSystem().createStoreDir(getColumnFamilyName());\n+\n+    this.blocksize = getColumnFamilyDescriptor().getBlocksize();\n \n     // set block storage policy for store directory\n-    String policyName = family.getStoragePolicy();\n+    String policyName = getColumnFamilyDescriptor().getStoragePolicy();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1ODc4OQ==", "bodyText": "Could be static?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552258789", "createdAt": "2021-01-05T23:26:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -347,6 +331,48 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n     cacheOnWriteLogged = false;\n   }\n \n+  private StoreContext initializeStoreContext(ColumnFamilyDescriptor family) throws IOException {\n+    return new StoreContext.Builder()\n+        .withBloomType(family.getBloomFilterType())\n+        .withCacheConfig(createCacheConf(family))\n+        .withCellComparator(region.getCellComparator())\n+        .withColumnFamilyDescriptor(family)\n+        .withCompactedFilesSupplier(this::getCompactedFiles)\n+        .withRegionFileSystem(region.getRegionFileSystem())\n+        .withDefaultHFileContext(getDefaultHFileContext(family))\n+        .withFavoredNodesSupplier(this::getFavoredNodes)\n+        .withFamilyStoreDirectoryPath(region.getRegionFileSystem()\n+            .getStoreDir(family.getNameAsString()))\n+        .withRegionCoprocessorHost(region.getCoprocessorHost())\n+        .build();\n+  }\n+\n+  private InetSocketAddress[] getFavoredNodes() {\n+    InetSocketAddress[] favoredNodes = null;\n+    if (region.getRegionServerServices() != null) {\n+      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n+          region.getRegionInfo().getEncodedName());\n+    }\n+    return favoredNodes;\n+  }\n+\n+  private HFileContext getDefaultHFileContext(ColumnFamilyDescriptor family) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1OTM0Ng==", "bodyText": "Has to be public? Has to be on HStore? Can it be on StoreContext? Is there a getStoreContext method on HStore?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552259346", "createdAt": "2021-01-05T23:27:32Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +502,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDA2NQ==", "bodyText": "Why would we do this here and not as methods on the builder?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552260065", "createdAt": "2021-01-05T23:29:48Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1206,53 +1218,34 @@ public StoreFileWriter createWriterInTmp(long maxKeyCount, Compression.Algorithm\n         }\n       }\n     }\n-    InetSocketAddress[] favoredNodes = null;\n-    if (region.getRegionServerServices() != null) {\n-      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n-          region.getRegionInfo().getEncodedName());\n-    }\n-    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag,\n-      cryptoContext);\n-    Path familyTempDir = new Path(fs.getTempDir(), family.getNameAsString());\n-    StoreFileWriter.Builder builder = new StoreFileWriter.Builder(conf, writerCacheConf,\n-        this.getFileSystem())\n-            .withOutputDir(familyTempDir)\n-            .withBloomType(family.getBloomFilterType())\n-            .withMaxKeyCount(maxKeyCount)\n-            .withFavoredNodes(favoredNodes)\n-            .withFileContext(hFileContext)\n-            .withShouldDropCacheBehind(shouldDropBehind)\n-            .withCompactedFilesSupplier(this::getCompactedFiles)\n-            .withFileStoragePolicy(fileStoragePolicy);\n+    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag);\n+    Path familyTempDir = new Path(getRegionFileSystem().getTempDir(), getColumnFamilyName());\n+    StoreFileWriter.Builder builder =\n+      new StoreFileWriter.Builder(conf, writerCacheConf, getFileSystem())\n+        .withOutputDir(familyTempDir)\n+        .withBloomType(storeContext.getBloomFilterType())\n+        .withMaxKeyCount(maxKeyCount)\n+        .withFavoredNodes(storeContext.getFavoredNodes())\n+        .withFileContext(hFileContext)\n+        .withShouldDropCacheBehind(shouldDropBehind)\n+        .withCompactedFilesSupplier(storeContext.getCompactedFilesSupplier())\n+        .withFileStoragePolicy(fileStoragePolicy);\n     return builder.build();\n   }\n \n   private HFileContext createFileContext(Compression.Algorithm compression,\n-      boolean includeMVCCReadpoint, boolean includesTag, Encryption.Context cryptoContext) {\n+    boolean includeMVCCReadpoint, boolean includesTag) {\n     if (compression == null) {\n       compression = HFile.DEFAULT_COMPRESSION_ALGORITHM;\n     }\n-    HFileContext hFileContext = new HFileContextBuilder()\n-                                .withIncludesMvcc(includeMVCCReadpoint)\n-                                .withIncludesTags(includesTag)\n-                                .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n-                                .withBlockSize(blocksize)\n-                                .withHBaseCheckSum(true)\n-                                .withDataBlockEncoding(family.getDataBlockEncoding())\n-                                .withEncryptionContext(cryptoContext)\n-                                .withCreateTime(EnvironmentEdgeManager.currentTime())\n-                                .withColumnFamily(family.getName())\n-                                .withTableName(region.getTableDescriptor()\n-                                    .getTableName().getName())\n-                                .withCellComparator(this.comparator)\n-                                .build();\n-    return hFileContext;\n+    HFileContext fileContext = storeContext.getDefaultFileContext();\n+    fileContext.setIncludesMvcc(includeMVCCReadpoint);\n+    fileContext.setIncludesTags(includesTag);\n+    fileContext.setCompression(compression);\n+    fileContext.setFileCreateTime(EnvironmentEdgeManager.currentTime());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 526}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDU0MQ==", "bodyText": "I would like to know here in class comment if this is read-only/immutable. I think it should be.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552260541", "createdAt": "2021-01-05T23:31:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MTU3OQ==", "bodyText": "Also, just a comment... the HFileContext has this for a comment...\n\nRead-only HFile Context Information. Meta data that is used by HFileWriter/Readers and by\nHFileBlocks. Create one using the {@link HFileContextBuilder} (See HFileInfo and the HFile\nTrailer class).\n\nso it is for readers and writers.... So, StoreContext is probably fine but if you are wondering... here is incidences of Info.java vs Context.java....  If it helps.\nkalashnikov:hbase.apache.git stack$ find src/main/java -name *Info.java\nfind: src/main/java: No such file or directory\nkalashnikov:hbase.apache.git stack$ find hbase-*/src/main/java -name *Info.java\nhbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupTableInfo.java\nhbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java\nhbase-client/src/main/java/org/apache/hadoop/hbase/security/SecurityInfo.java\nhbase-client/src/main/java/org/apache/hadoop/hbase/client/MutableRegionInfo.java\nhbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java\nhbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/mode/DrillDownInfo.java\nhbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/field/FieldInfo.java\nhbase-metrics-api/src/main/java/org/apache/hadoop/hbase/metrics/MetricRegistryInfo.java\nhbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockWithScanInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScanInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallQueueInfo.java\nhbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THRegionInfo.java\nhbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java\n\n\nkalashnikov:hbase.apache.git stack$ find hbase-*/src/main/java -name *Context.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Context.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDecodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/RowPrefixFixedLengthBloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/RowBloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/RowColBloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ReaderContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/CompressionContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NoLimitScannerContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlushContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcCallContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcSchedulerContext.java", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552261579", "createdAt": "2021-01-05T23:34:25Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDU0MQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 35}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2df4758903235ba4feb7508d0986debb35e32a9", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/b2df4758903235ba4feb7508d0986debb35e32a9", "committedDate": "2021-01-06T02:01:56Z", "message": "address comments and remove the defaultFileContext from StoreContext"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMzM5OTky", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-562339992", "createdAt": "2021-01-06T05:48:20Z", "commit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNTo0ODoyMFrOIOywCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNTo0ODoyMFrOIOywCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjM4MjQ3Mg==", "bodyText": "You haven't uploaded new PR so will leave this as unresovled for now.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552382472", "createdAt": "2021-01-06T05:48:20Z", "author": {"login": "saintstack"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "diffHunk": "@@ -138,6 +138,10 @@ public boolean isCompressedOrEncrypted() {\n     return compressAlgo;\n   }\n \n+  public void setCompression(Compression.Algorithm compressAlgo) {\n+    this.compressAlgo = compressAlgo;\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NTgwNw=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 7}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/8a0269fbe6faa6b12edfab1db13c36d923558576", "committedDate": "2021-01-06T06:09:44Z", "message": "removed setter for encryptionContext in HFileContext and fixed indentation in constructor of HStore"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyODg5NTA0", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-562889504", "createdAt": "2021-01-06T17:12:21Z", "commit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNzoxMjoyMlrOIPNgUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNzoxODoyMFrOIPN2Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyMDgxNg==", "bodyText": "You were going to change these back?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552820816", "createdAt": "2021-01-06T17:12:22Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -268,34 +251,37 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       .addBytesMap(region.getTableDescriptor().getValues())\n       .addStringMap(family.getConfiguration())\n       .addBytesMap(family.getValues());\n-    this.blocksize = family.getBlocksize();\n+\n+    this.region = region;\n+    this.storeContext = initializeStoreContext(family);\n+\n+    // Assemble the store's home directory and Ensure it exists.\n+    getRegionFileSystem().createStoreDir(family.getNameAsString());\n \n     // set block storage policy for store directory\n     String policyName = family.getStoragePolicy();\n     if (null == policyName) {\n       policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n     }\n-    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n+    getRegionFileSystem().setStoragePolicy(getColumnFamilyName(), policyName.trim());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyMTA0OQ==", "bodyText": "ditto", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552821049", "createdAt": "2021-01-06T17:12:37Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -308,7 +294,7 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n     }\n \n-    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n+    this.storeEngine = createStoreEngine(this, this.conf, getComparator());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyNjQxOA==", "bodyText": "Is this necessary? Its available on the ColumnFamilyDescriptor. Do we need to add new method on Store?\nWas this change always here or did it just show up in recent amendments to PRs (I don't remember seeing it before but probably just me).\nThanks.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552826418", "createdAt": "2021-01-06T17:18:20Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java", "diffHunk": "@@ -46,6 +46,8 @@\n   int PRIORITY_USER = 1;\n   int NO_PRIORITY = Integer.MIN_VALUE;\n \n+  int getBlockSize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/4b8ab2955ad3d9810256d7f22d7de630c5617364", "committedDate": "2021-01-06T18:24:05Z", "message": "remove getters in constructor of HStore and remove getBlockSize from Store interface"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyOTQzNDA3", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-562943407", "createdAt": "2021-01-06T18:27:28Z", "commit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyOTg5ODE0", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-562989814", "createdAt": "2021-01-06T19:39:15Z", "commit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzMDcxMjk3", "url": "https://github.com/apache/hbase/pull/2800#pullrequestreview-563071297", "createdAt": "2021-01-06T22:07:47Z", "commit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMjowNzo0N1rOIPXl4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMjowNzo0N1rOIPXl4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk4NjA4MQ==", "bodyText": "@apurtell ping again on the heapSize() and implements HeapSize for StoreContext class, could you have another look? I should have fixed it, but will wait for your approval/resolve in 1 day or 2 day before merging it.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552986081", "createdAt": "2021-01-06T22:07:47Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.crypto.Encryption;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the immutable information and references on some of the meta data about the HStore.\n+ * This meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final int blockSize;\n+  private final Encryption.Context encryptionContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.blockSize = builder.blockSize;\n+    this.encryptionContext = builder.encryptionContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public int getBlockSize() {\n+    return blockSize;\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {\n+    return encryptionContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public InetSocketAddress[] getFavoredNodes() {\n+    return favoredNodesSupplier.get();\n+  }\n+\n+  public ColumnFamilyDescriptor getFamily() {\n+    return family;\n+  }\n+\n+  public Path getFamilyStoreDirectoryPath() {\n+    return familyStoreDirectoryPath;\n+  }\n+\n+  public RegionCoprocessorHost getCoprocessorHost() {\n+    return coprocessorHost;\n+  }\n+\n+  public static Builder getBuilder() {\n+    return new Builder();\n+  }\n+\n+  @Override\n+  public long heapSize() {\n+    return FIXED_OVERHEAD;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364"}, "originalPosition": 118}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4874, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}