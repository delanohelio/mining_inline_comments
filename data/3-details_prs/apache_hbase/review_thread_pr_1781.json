{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIzMTM2NTA2", "number": 1781, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMjoyNDo1MVrOD_3e8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0Njo1MlrOD_-Svg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4Mjk1OTIzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMjoyNDo1MVrOGayVwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMjoyNDo1MVrOGayVwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MDkzMA==", "bodyText": "Unrelated changes. Remove", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430740930", "createdAt": "2020-05-26T22:24:51Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "diffHunk": "@@ -592,9 +593,9 @@ public void markRegionsRecovering(ServerName server, Set<HRegionInfo> userRegion\n    * @return whether log is replaying\n    */\n   public boolean isLogReplaying() {\n-    if (server.getCoordinatedStateManager() == null) return false;\n-    return ((BaseCoordinatedStateManager) server.getCoordinatedStateManager())\n-        .getSplitLogManagerCoordination().isReplaying();\n+    CoordinatedStateManager m = server.getCoordinatedStateManager();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4Mjk2MTExOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMjoyNTo0MVrOGayW8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDozMDo1N1rOGa0uKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTIzMg==", "bodyText": "In what Hadoop version were these introduced? If at least 2.7, its definitely fine. If 2.8, probably ok.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430741232", "createdAt": "2020-05-26T22:25:41Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -40,7 +41,9 @@\n import org.apache.hadoop.hbase.io.hfile.CacheStats;\n import org.apache.hadoop.hbase.wal.WALProvider;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.hadoop.hdfs.DFSHedgedReadMetrics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTk0NQ==", "bodyText": "This is from 2.4. https://issues.apache.org/jira/browse/HBASE-7509 Should be good.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430779945", "createdAt": "2020-05-27T00:30:57Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -40,7 +41,9 @@\n import org.apache.hadoop.hbase.io.hfile.CacheStats;\n import org.apache.hadoop.hbase.wal.WALProvider;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.hadoop.hdfs.DFSHedgedReadMetrics;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTIzMg=="}, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE1NzUyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDowNDozMlrOGa0RBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDowNDozMlrOGa0RBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3MjQ4NQ==", "bodyText": "Checkstyle conflicts need to be fixed by adding braces.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430772485", "createdAt": "2020-05-27T00:04:32Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "diffHunk": "@@ -592,9 +593,9 @@ public void markRegionsRecovering(ServerName server, Set<HRegionInfo> userRegion\n    * @return whether log is replaying\n    */\n   public boolean isLogReplaying() {\n-    if (server.getCoordinatedStateManager() == null) return false;\n-    return ((BaseCoordinatedStateManager) server.getCoordinatedStateManager())\n-        .getSplitLogManagerCoordination().isReplaying();\n+    CoordinatedStateManager m = server.getCoordinatedStateManager();\n+    if (m == null) return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE3NDMzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxMzoyOFrOGa0bVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxMzoyOFrOGa0bVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTEyNA==", "bodyText": "Checkstyle conflict. Need braces.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775124", "createdAt": "2020-05-27T00:13:28Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c\n+   * @return The DFSClient DFSHedgedReadMetrics instance or null if can't be found or not on hdfs.\n+   * @throws IOException\n+   */\n+  public static DFSHedgedReadMetrics getDFSHedgedReadMetrics(final Configuration c)\n+      throws IOException {\n+    if (!isHDFS(c)) return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE3NTMxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNDowMlrOGa0b6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNDowMlrOGa0b6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTI3NA==", "bodyText": "can be removed.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775274", "createdAt": "2020-05-27T00:14:02Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE3NTc0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNDoyMFrOGa0cLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNDoyMFrOGa0cLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTM0Mg==", "bodyText": "can be removed", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775342", "createdAt": "2020-05-27T00:14:20Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c\n+   * @return The DFSClient DFSHedgedReadMetrics instance or null if can't be found or not on hdfs.\n+   * @throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE3NzcyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNToyMlrOGa0dYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNToyMlrOGa0dYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTY1MA==", "bodyText": "Not necessary.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775650", "createdAt": "2020-05-27T00:15:22Z", "author": {"login": "clarax"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,146 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   * @throws Exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE5NjQxOnYy", "diffSide": "RIGHT", "path": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoyNTo1MVrOGa0orQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoyNTo1MVrOGa0orQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3ODU0MQ==", "bodyText": "Validated from HBASE15550. LGTM.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430778541", "createdAt": "2020-05-27T00:25:51Z", "author": {"login": "clarax"}, "path": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java", "diffHunk": "@@ -504,6 +504,11 @@ public void getMetrics(MetricsCollector metricsCollector, boolean all) {\n               rsWrap.getCompactedCellsSize())\n           .addCounter(Interns.info(MAJOR_COMPACTED_CELLS_SIZE, MAJOR_COMPACTED_CELLS_SIZE_DESC),\n               rsWrap.getMajorCompactedCellsSize())\n+\n+          .addCounter(Interns.info(HEDGED_READS, HEDGED_READS_DESC), rsWrap.getHedgedReadOps())\n+          .addCounter(Interns.info(HEDGED_READ_WINS, HEDGED_READ_WINS_DESC),\n+              rsWrap.getHedgedReadWins())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzQ5NjU0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzoyNFrOGa3uJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzoyNFrOGa3uJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTA5NA==", "bodyText": "nit, space between 'null?', '0:'", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430829094", "createdAt": "2020-05-27T02:53:24Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadOps();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzQ5Njc1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzozMlrOGa3uSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzozMlrOGa3uSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTEyOQ==", "bodyText": "ditto", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430829129", "createdAt": "2020-05-27T02:53:32Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadOps();\n+  }\n+\n+  @Override\n+  public long getHedgedReadWins() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadWins();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzUyMTkwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxMToxOVrOGa3-zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxMzoyNlrOGa4Aog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzM1OA==", "bodyText": "space between 'null?', still missing.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430833358", "createdAt": "2020-05-27T03:11:19Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0 : this.dfsHedgedReadMetrics.getHedgedReadOps();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzgyNg==", "bodyText": "sorry about that, fixing", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430833826", "createdAt": "2020-05-27T03:13:26Z", "author": {"login": "javierluca"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0 : this.dfsHedgedReadMetrics.getHedgedReadOps();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzM1OA=="}, "originalCommit": {"oid": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4NDA2OTU3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0NToyMlrOGa9RUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0NTo0N1rOGa9SOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDAxNw==", "bodyText": "please pay attention to the space style problems in this unit test.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920017", "createdAt": "2020-05-27T07:45:22Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDI0OQ==", "bodyText": "there're many.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920249", "createdAt": "2020-05-27T07:45:47Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDAxNw=="}, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4NDA3NDg2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0Njo1MlrOGa9Uug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwODozMzozNFrOGa-_Ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA==", "bodyText": "Just remove instead of comments if you don't need following codes", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920890", "createdAt": "2020-05-27T07:46:52Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);\n+    stm.readFully(7*blockSize, actual, 0, 4096);\n+    actual = new byte[3*4096];\n+    stm.readFully(0*blockSize, actual, 0, 3*4096);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 7\");\n+    actual = new byte[8*4096];\n+    stm.readFully(3*blockSize, actual, 0, 8*4096);\n+    checkAndEraseData(actual, 3*blockSize, expected, \"Pread Test 8\");\n+    // read the tail\n+    stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize/2);\n+    IOException res = null;\n+    try { // read beyond the end of the file\n+      stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize);\n+    } catch (IOException e) {\n+      // should throw an exception\n+      res = e;\n+    }\n+    assertTrue(\"Error reading beyond file boundary.\", res != null);\n+\n+    stm.close();\n+  }\n+\n+  private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {\n+    for (int idx = 0; idx < actual.length; idx++) {\n+      assertEquals(message+\" byte \"+(from+idx)+\" differs. expected \"+\n+          expected[from+idx]+\" actual \"+actual[idx],\n+        actual[idx], expected[from+idx]);\n+      actual[idx] = 0;\n+    }\n+  }\n+\n+  private void doPread(FSDataInputStream stm, long position, byte[] buffer,\n+    int offset, int length) throws IOException {\n+    int nread = 0;\n+    // long totalRead = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkzMjUwMA==", "bodyText": "I see. Agree on these kind of comments should go away. However thought it would be better to keep it as similar as branch-2:\n\n  \n    \n      hbase/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java\n    \n    \n        Lines 612 to 641\n      in\n      441935a\n    \n    \n    \n    \n\n        \n          \n             private void doPread(FSDataInputStream stm, long position, byte[] buffer, \n        \n\n        \n          \n                 int offset, int length) throws IOException { \n        \n\n        \n          \n               int nread = 0; \n        \n\n        \n          \n               // long totalRead = 0; \n        \n\n        \n          \n               // DFSInputStream dfstm = null; \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (stm.getWrappedStream() instanceof DFSInputStream) { \n        \n\n        \n          \n                 dfstm = (DFSInputStream) (stm.getWrappedStream()); \n        \n\n        \n          \n                 totalRead = dfstm.getReadStatistics().getTotalBytesRead(); \n        \n\n        \n          \n               } */ \n        \n\n        \n          \n            \n        \n\n        \n          \n               while (nread < length) { \n        \n\n        \n          \n                 int nbytes = \n        \n\n        \n          \n                     stm.read(position + nread, buffer, offset + nread, length - nread); \n        \n\n        \n          \n                 assertTrue(\"Error in pread\", nbytes > 0); \n        \n\n        \n          \n                 nread += nbytes; \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (dfstm != null) { \n        \n\n        \n          \n                 if (isHedgedRead) { \n        \n\n        \n          \n                   assertTrue(\"Expected read statistic to be incremented\", \n        \n\n        \n          \n                     length <= dfstm.getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } else { \n        \n\n        \n          \n                   assertEquals(\"Expected read statistic to be incremented\", length, dfstm \n        \n\n        \n          \n                       .getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } \n        \n\n        \n          \n               }*/ \n        \n\n        \n          \n             } \n        \n    \n  \n\n\nOr master:\n\n  \n    \n      hbase/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java\n    \n    \n        Lines 612 to 641\n      in\n      476cb16\n    \n    \n    \n    \n\n        \n          \n             private void doPread(FSDataInputStream stm, long position, byte[] buffer, \n        \n\n        \n          \n                 int offset, int length) throws IOException { \n        \n\n        \n          \n               int nread = 0; \n        \n\n        \n          \n               // long totalRead = 0; \n        \n\n        \n          \n               // DFSInputStream dfstm = null; \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (stm.getWrappedStream() instanceof DFSInputStream) { \n        \n\n        \n          \n                 dfstm = (DFSInputStream) (stm.getWrappedStream()); \n        \n\n        \n          \n                 totalRead = dfstm.getReadStatistics().getTotalBytesRead(); \n        \n\n        \n          \n               } */ \n        \n\n        \n          \n            \n        \n\n        \n          \n               while (nread < length) { \n        \n\n        \n          \n                 int nbytes = \n        \n\n        \n          \n                     stm.read(position + nread, buffer, offset + nread, length - nread); \n        \n\n        \n          \n                 assertTrue(\"Error in pread\", nbytes > 0); \n        \n\n        \n          \n                 nread += nbytes; \n        \n\n        \n          \n               } \n        \n\n        \n          \n            \n        \n\n        \n          \n               /* Disable. This counts do not add up. Some issue in original hdfs tests? \n        \n\n        \n          \n               if (dfstm != null) { \n        \n\n        \n          \n                 if (isHedgedRead) { \n        \n\n        \n          \n                   assertTrue(\"Expected read statistic to be incremented\", \n        \n\n        \n          \n                     length <= dfstm.getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } else { \n        \n\n        \n          \n                   assertEquals(\"Expected read statistic to be incremented\", length, dfstm \n        \n\n        \n          \n                       .getReadStatistics().getTotalBytesRead() - totalRead); \n        \n\n        \n          \n                 } \n        \n\n        \n          \n               }*/ \n        \n\n        \n          \n             } \n        \n    \n  \n\n\nWas not sure about the policies in this case.\nRemoved for now.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430932500", "createdAt": "2020-05-27T08:07:13Z", "author": {"login": "javierluca"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);\n+    stm.readFully(7*blockSize, actual, 0, 4096);\n+    actual = new byte[3*4096];\n+    stm.readFully(0*blockSize, actual, 0, 3*4096);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 7\");\n+    actual = new byte[8*4096];\n+    stm.readFully(3*blockSize, actual, 0, 8*4096);\n+    checkAndEraseData(actual, 3*blockSize, expected, \"Pread Test 8\");\n+    // read the tail\n+    stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize/2);\n+    IOException res = null;\n+    try { // read beyond the end of the file\n+      stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize);\n+    } catch (IOException e) {\n+      // should throw an exception\n+      res = e;\n+    }\n+    assertTrue(\"Error reading beyond file boundary.\", res != null);\n+\n+    stm.close();\n+  }\n+\n+  private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {\n+    for (int idx = 0; idx < actual.length; idx++) {\n+      assertEquals(message+\" byte \"+(from+idx)+\" differs. expected \"+\n+          expected[from+idx]+\" actual \"+actual[idx],\n+        actual[idx], expected[from+idx]);\n+      actual[idx] = 0;\n+    }\n+  }\n+\n+  private void doPread(FSDataInputStream stm, long position, byte[] buffer,\n+    int offset, int length) throws IOException {\n+    int nread = 0;\n+    // long totalRead = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA=="}, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0ODE4Nw==", "bodyText": "no need to keep as branch-2, there're numerous different already.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430948187", "createdAt": "2020-05-27T08:33:34Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);\n+    stm.readFully(7*blockSize, actual, 0, 4096);\n+    actual = new byte[3*4096];\n+    stm.readFully(0*blockSize, actual, 0, 3*4096);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 7\");\n+    actual = new byte[8*4096];\n+    stm.readFully(3*blockSize, actual, 0, 8*4096);\n+    checkAndEraseData(actual, 3*blockSize, expected, \"Pread Test 8\");\n+    // read the tail\n+    stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize/2);\n+    IOException res = null;\n+    try { // read beyond the end of the file\n+      stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize);\n+    } catch (IOException e) {\n+      // should throw an exception\n+      res = e;\n+    }\n+    assertTrue(\"Error reading beyond file boundary.\", res != null);\n+\n+    stm.close();\n+  }\n+\n+  private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {\n+    for (int idx = 0; idx < actual.length; idx++) {\n+      assertEquals(message+\" byte \"+(from+idx)+\" differs. expected \"+\n+          expected[from+idx]+\" actual \"+actual[idx],\n+        actual[idx], expected[from+idx]);\n+      actual[idx] = 0;\n+    }\n+  }\n+\n+  private void doPread(FSDataInputStream stm, long position, byte[] buffer,\n+    int offset, int length) throws IOException {\n+    int nread = 0;\n+    // long totalRead = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA=="}, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 142}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3049, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}