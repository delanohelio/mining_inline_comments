{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxNTA5NjU0", "number": 1755, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTowODo0MlrOD_2Rgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQyMTozMDowOVrOEBI7KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4Mjc2MDk4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTowODo0MlrOGawYuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QyMDo0MjoyNVrOGbcXnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODkyMA==", "bodyText": "We can race between test and put, potentially undercounting (by ref overwrite). Test if the return of putIfAbsent is not null. If not null, use that instead of the atomic instance you just created. Then increment. If the code that updates failedOpenTracker also has this racy pattern it should be fixed too.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430708920", "createdAt": "2020-05-26T21:08:42Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1972,6 +1975,13 @@ private void unassign(final HRegionInfo region,\n       final RegionState state, final int versionOfClosingNode,\n       final ServerName dest, final boolean transitionInZK,\n       final ServerName src) {\n+    String encodedName = region.getEncodedName();\n+    AtomicInteger failedCloseCount = failedCloseTracker.get(encodedName);\n+    if (failedCloseCount == null) {\n+      failedCloseCount = new AtomicInteger();\n+      failedCloseTracker.put(encodedName, failedCloseCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2NzM0OQ==", "bodyText": "Aren't all the codepaths reaching this point, expected to take an exclusive lock on the region.encodedName()? If so, wondering if we should worry about the non-thread-safe access for this map. I checked all the callers, all except one path in forceRegionStateToOffline() follow this pattern, we should probably fix that.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430767349", "createdAt": "2020-05-26T23:46:41Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1972,6 +1975,13 @@ private void unassign(final HRegionInfo region,\n       final RegionState state, final int versionOfClosingNode,\n       final ServerName dest, final boolean transitionInZK,\n       final ServerName src) {\n+    String encodedName = region.getEncodedName();\n+    AtomicInteger failedCloseCount = failedCloseTracker.get(encodedName);\n+    if (failedCloseCount == null) {\n+      failedCloseCount = new AtomicInteger();\n+      failedCloseTracker.put(encodedName, failedCloseCount);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODkyMA=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc4Mzc1NA==", "bodyText": "I agree, that's what we do even for failedOpenTracker. I can make the change for forceRegionStateToOffline() to take the lock before changing the state", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430783754", "createdAt": "2020-05-27T00:44:22Z", "author": {"login": "sguggilam"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1972,6 +1975,13 @@ private void unassign(final HRegionInfo region,\n       final RegionState state, final int versionOfClosingNode,\n       final ServerName dest, final boolean transitionInZK,\n       final ServerName src) {\n+    String encodedName = region.getEncodedName();\n+    AtomicInteger failedCloseCount = failedCloseTracker.get(encodedName);\n+    if (failedCloseCount == null) {\n+      failedCloseCount = new AtomicInteger();\n+      failedCloseTracker.put(encodedName, failedCloseCount);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODkyMA=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQyOTUzNQ==", "bodyText": "That's fine", "url": "https://github.com/apache/hbase/pull/1755#discussion_r431429535", "createdAt": "2020-05-27T20:42:25Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1972,6 +1975,13 @@ private void unassign(final HRegionInfo region,\n       final RegionState state, final int versionOfClosingNode,\n       final ServerName dest, final boolean transitionInZK,\n       final ServerName src) {\n+    String encodedName = region.getEncodedName();\n+    AtomicInteger failedCloseCount = failedCloseTracker.get(encodedName);\n+    if (failedCloseCount == null) {\n+      failedCloseCount = new AtomicInteger();\n+      failedCloseTracker.put(encodedName, failedCloseCount);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODkyMA=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzEyODIyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMzo0ODoyNVrOGaz-7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDozMzo0NlrOGa0w_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2Nzg1Mw==", "bodyText": "Is there any change in functionality of this section of diff? I think the answer is no and its mostly indents, but I wanted to double check..can you please confirm?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430767853", "createdAt": "2020-05-26T23:48:25Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1997,80 +2007,76 @@ private void unassign(final HRegionInfo region,\n       }\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n       } catch (Throwable t) {\n         long sleepTime = 0;\n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc4MDY3MA==", "bodyText": "Yes,  there is no change in this section", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430780670", "createdAt": "2020-05-27T00:33:46Z", "author": {"login": "sguggilam"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1997,80 +2007,76 @@ private void unassign(final HRegionInfo region,\n       }\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n       } catch (Throwable t) {\n         long sleepTime = 0;\n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2Nzg1Mw=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE4MDc3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoxNzowNlrOGa0fLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMToyMTo1MFrOGcL2qA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjExMQ==", "bodyText": "Pardon my ignorance but I don't fully understand the fix. I think the ask in the jira to spread out the unassigns a bit by using a backoff based approach. To do that why not just fix the sleepTime above in L2075 to use an exponential backoff based approach?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430776111", "createdAt": "2020-05-27T00:17:06Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc4MTQ1OA==", "bodyText": "The idea is to use the exponential backoff configs \"hbase.assignment.retry.sleep.initial\" and \"hbase.assignment.retry.sleep.initial\" for backoff between retries as they can be exhausted pretty fast in case where the server is loaded /busy and cannot really even acknowledge the region close request from the master.  We need to use them to schedule the retry at a later point in a different thread asynchronouly\nThe sleepTime is not really meant for this use case and is not reading any exponential backoff configs", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430781458", "createdAt": "2020-05-27T00:36:47Z", "author": {"login": "sguggilam"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjExMQ=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5MjI3OQ==", "bodyText": "Right, so my point is this. If there is an exception in sendRegionClose() (say ServerTooBusy), we enter the catch() block in L2024, we don't match the if / else if in L2031 (failed server) / 2038 (region not serving) and that leaves us with just looping for \"maximumAttempts\" quickly in succession without a backoff (this is the problem we are trying to address). My point was why not do something like this.\n if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n            || t instanceof RegionServerStoppedException\t\n            || t instanceof ServerNotRunningYetException) {\t        \n......\n} else if ((t instanceof FailedServerException)....) {\n.....\n} else {\n // Handle any other remote exception\n // use your exponential backoff algorithm...\n} \n\nBasically we have the loop for maxAttempts just that it is not using an exponential backoff. Just plug it in an else {} block and we are good? What is advantage of doing it with a delayed callable, doesn't that just complicate the state machine with async requests and book-keeping or is there something that I'm missing?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432192279", "createdAt": "2020-05-29T00:19:08Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjExMQ=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwMzI5MA==", "bodyText": "There are two reasons for this:\n\nIt also deals with the case where the sendRegionClose() itself returns false without getting into the catch block though I am not very sure of the case where it would reach there. the method doc says the method returns false if it doesn't acknowledge the request\nThe exponential backoff is configurable and can even be 5 minutes , so there is no point holding the thread for that amount of time  and rather have it deal asynchronously.\n\nI cannot think of any state machine issue as even if just sleep the current thread , the state would remain the same and the method as such is not returning anything and anyone else would just check the state change made by the method either synchronously or asynchronously. Rather we would delegate it to a new thread and not block the current thread.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432203290", "createdAt": "2020-05-29T01:03:07Z", "author": {"login": "sguggilam"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjExMQ=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwNzUyOA==", "bodyText": "The current code does the sleep in the current thread where it is handling the region already in transition kind of case where there is a max wait time and hence they just sleep for every 100ms and keep retrying before the max ttime ends. In our case we definitely know that we don't want to retry before a configured backoff time, so we can as well schedule it later", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432207528", "createdAt": "2020-05-29T01:21:50Z", "author": {"login": "sguggilam"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjExMQ=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzE5MTM3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoyMjo0OVrOGa0liA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMToyMjo1N1rOGa141w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NzczNg==", "bodyText": "javadoc for the method says \"Send CLOSE RPC if the server is online, otherwise, offline the region.\". With this delayed callables, aren't we violating that? If we hit this block, we will still be in the CLOSING state with the thread pool retrying in the background. Curious if that causes any issues in the statemachine.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430777736", "createdAt": "2020-05-27T00:22:49Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,\n+      getFailedAttempts(encodedName, failedCloseTracker));\n+    if (failedCloseCount.incrementAndGet() <= maximumAttempts && sleepTime > 0) {\n+      if (failedCloseTracker.containsKey(encodedName)) {\n+        // Sleep before trying unassign if this region has failed to close before\n+        scheduledThreadPoolExecutor.schedule(new DelayedUnAssignCallable(this, region, state,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc5OTA2Mw==", "bodyText": "As mentioned, this mainly deals with the case where the RS is even busy to acknowledge the request from Master and tries to have a backoff before next retry to avoid the FAILED_CLOSE state if possible. In the case where we get NotServingRegionException, we still offline the region.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430799063", "createdAt": "2020-05-27T01:22:57Z", "author": {"login": "sguggilam"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,\n+      getFailedAttempts(encodedName, failedCloseTracker));\n+    if (failedCloseCount.incrementAndGet() <= maximumAttempts && sleepTime > 0) {\n+      if (failedCloseTracker.containsKey(encodedName)) {\n+        // Sleep before trying unassign if this region has failed to close before\n+        scheduledThreadPoolExecutor.schedule(new DelayedUnAssignCallable(this, region, state,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NzczNg=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTQ4NjA2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMjo1OTowMVrOGcuemQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMjo1OTowMVrOGcuemQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3NDgwOQ==", "bodyText": "nit: (t instanceof RegionServerAbortedException) is redundant (per static analysis check in the IDE)...", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432774809", "createdAt": "2020-05-29T22:59:01Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTQ5NTMyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowNTozNFrOGcukJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowNTozNFrOGcukJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3NjIzMA==", "bodyText": "merge this logging and the logging in L2074 and log towards the end?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432776230", "createdAt": "2020-05-29T23:05:34Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTQ5ODc3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowNzo1MFrOGcumOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowNzo1MFrOGcumOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3Njc2Mg==", "bodyText": "How about a more readable condition here?\nif (regionCloseFailed || anyOtherExceptionThrown) { // set these flags above\n}\nI think that makes the flow easier to understand.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432776762", "createdAt": "2020-05-29T23:07:50Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n+      // If sleepTime is not set by any of the cases, set it to sleep for\n+      // configured exponential backoff time\n+      if (sleepTime == 0 && i != maximumAttempts) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTUwMDA1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowODozOFrOGcum_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowODozOFrOGcum_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3Njk1OQ==", "bodyText": "do this before propagating the interrupt flag?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432776959", "createdAt": "2020-05-29T23:08:38Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n+      // If sleepTime is not set by any of the cases, set it to sleep for\n+      // configured exponential backoff time\n+      if (sleepTime == 0 && i != maximumAttempts) {\n+        sleepTime = backoffPolicy.getBackoffTime(retryConfig, i);\n+        LOG.info(\"Waiting for \" + sleepTime + \"milliseconds exponential backoff time for \"\n+            + region.getRegionNameAsString() + \" before next retry \" + (i + 1) + \" of \"\n+            + this.maximumAttempts);\n+      }\n+      try {\n+        if (sleepTime > 0 && i != maximumAttempts) {\n+          Thread.sleep(sleepTime);\n+        }\n+      } catch (InterruptedException ie) {\n+        LOG.warn(\"Failed to unassign \" + region.getRegionNameAsString() + \" since interrupted\", ie);\n+        Thread.currentThread().interrupt();\n+        if (state != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTUwMTE4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowOToyM1rOGcunuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzowOToyM1rOGcunuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3NzE0Nw==", "bodyText": "nit: Can you please add a few comments on the code flow so that its easier for other to follow?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432777147", "createdAt": "2020-05-29T23:09:23Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n+      // If sleepTime is not set by any of the cases, set it to sleep for", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTUwOTE0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzoxNToxNlrOGcuslA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzoxNToxNlrOGcuslA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3ODM4OA==", "bodyText": "nit: We use TestTableName.getTableName() that automatically gets the TableName with the running test name.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432778388", "createdAt": "2020-05-29T23:15:16Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "diffHunk": "@@ -598,6 +601,68 @@ public void testCloseFailed() throws Exception {\n     }\n   }\n \n+  /**\n+   * This tests region close with exponential backoff\n+   */\n+  @Test(timeout = 60000)\n+  public void testCloseRegionWithExponentialBackOff() throws Exception {\n+    String table = \"testCloseRegionWithExponentialBackOff\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTUxNzcxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzoyMTo0NlrOGcux2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzoyMTo0NlrOGcux2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3OTczOA==", "bodyText": "Does the test reliably fail without the patch, meaning it ends up in FAILED_CLOSE?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432779738", "createdAt": "2020-05-29T23:21:46Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "diffHunk": "@@ -598,6 +601,68 @@ public void testCloseFailed() throws Exception {\n     }\n   }\n \n+  /**\n+   * This tests region close with exponential backoff\n+   */\n+  @Test(timeout = 60000)\n+  public void testCloseRegionWithExponentialBackOff() throws Exception {\n+    String table = \"testCloseRegionWithExponentialBackOff\";\n+    // Set the backoff time between each retry for failed close\n+    TEST_UTIL.getMiniHBaseCluster().getConf().setLong(\"hbase.assignment.retry.sleep.initial\", 1000);\n+    HMaster activeMaster = TEST_UTIL.getHBaseCluster().getMaster();\n+    TEST_UTIL.getMiniHBaseCluster().stopMaster(activeMaster.getServerName());\n+    TEST_UTIL.getMiniHBaseCluster().startMaster(); // restart the master for conf take into affect\n+\n+    try {\n+      ScheduledThreadPoolExecutor scheduledThreadPoolExecutor =\n+          new ScheduledThreadPoolExecutor(1, Threads.newDaemonThreadFactory(\"ExponentialBackOff\"));\n+\n+      HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(table));\n+      desc.addFamily(new HColumnDescriptor(FAMILY));\n+      admin.createTable(desc);\n+\n+      Table meta = new HTable(conf, TableName.META_TABLE_NAME);\n+      HRegionInfo hri =\n+          new HRegionInfo(desc.getTableName(), Bytes.toBytes(\"A\"), Bytes.toBytes(\"Z\"));\n+      MetaTableAccessor.addRegionToMeta(meta, hri);\n+\n+      HMaster master = TEST_UTIL.getHBaseCluster().getMaster();\n+      AssignmentManager am = master.getAssignmentManager();\n+      assertTrue(TEST_UTIL.assignRegion(hri));\n+      ServerName sn = am.getRegionStates().getRegionServerOfRegion(hri);\n+      TEST_UTIL.assertRegionOnServer(hri, sn, 6000);\n+\n+      MyRegionObserver.preCloseEnabled.set(true);\n+      // Unset the precloseEnabled flag after 1 second for next retry to succeed\n+      scheduledThreadPoolExecutor.schedule(new Runnable() {\n+        @Override\n+        public void run() {\n+          MyRegionObserver.preCloseEnabled.set(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NjMwMjQ5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQyMTozMDowOVrOGc1cdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQyMTozMDowOVrOGc1cdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg4ODk0OQ==", "bodyText": "you already get a \"TableName\", you don't need to convert it to string and back to TableName. Also, TEST_UTIL.deleteTable() takes TableName as argument.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432888949", "createdAt": "2020-05-30T21:30:09Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "diffHunk": "@@ -598,6 +605,68 @@ public void testCloseFailed() throws Exception {\n     }\n   }\n \n+  /**\n+   * This tests region close with exponential backoff\n+   */\n+  @Test(timeout = 60000)\n+  public void testCloseRegionWithExponentialBackOff() throws Exception {\n+    String table = testTableName.getTableName().toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "419a0cb34484f02aa0b2d90102c8ff35d84e97c3"}, "originalPosition": 68}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3029, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}