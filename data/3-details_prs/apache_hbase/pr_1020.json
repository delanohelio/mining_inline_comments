{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxNjUyNjM4", "number": 1020, "title": "HBASE-23653 Expose content of meta table in web ui", "bodyText": "Adds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "createdAt": "2020-01-10T22:50:09Z", "url": "https://github.com/apache/hbase/pull/1020", "merged": true, "mergeCommit": {"oid": "00fc46756abb99de6f833997499505f89c9752e8"}, "closed": true, "closedAt": "2020-01-16T16:46:40Z", "author": {"login": "ndimiduk"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb5H8u2gFqTM0MTQ4NDk5NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABb6yTCXgFqTM0MzY2NTYxOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxNDg0OTk0", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-341484994", "createdAt": "2020-01-11T00:24:16Z", "commit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQwMDoyNDoxNlrOFcjG8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQwMDoyNDoxNlrOFcjG8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3OTY2Ng==", "bodyText": "this should be HConstants.STATE_QUALIFIER", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365479666", "createdAt": "2020-01-11T00:24:16Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 233}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxNDcxNzU4", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-341471758", "createdAt": "2020-01-10T23:21:39Z", "commit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQyMzoyMTozOVrOFcibIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQwMDozMjo0MlrOFcjMYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ2ODQ1MQ==", "bodyText": "Any reason not to use Iterators.limit() from guava?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365468451", "createdAt": "2020-01-10T23:21:39Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+\n+/**\n+ * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NDAzMg==", "bodyText": "Not your change, but creating a new connection every time this page loads seems a little heavy? Why not reuse the async connection created at the master startup? (master.getAsyncConnection()).", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365474032", "createdAt": "2020-01-10T23:50:19Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/resources/hbase-webapps/master/table.jsp", "diffHunk": "@@ -129,6 +151,7 @@\n   pageContext.setAttribute(\"pageTitle\", pageTitle);\n   AsyncConnection connection = ConnectionFactory.createAsyncConnection(master.getConfiguration()).get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw==", "bodyText": "Doesn't ResultScanner support an iterator()? Wrap it with a limit iterator instead of scan all? This way, I guess 10 such requests with max limit can be heavy for the master.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365475813", "createdAt": "2020-01-11T00:00:30Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NjE5OQ==", "bodyText": "Curious why it runs in its own thread. Add a clarifying comment?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365476199", "createdAt": "2020-01-11T00:02:28Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzA1MQ==", "bodyText": "haha , functional style is not as readable :D (my personal opinion)", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365477051", "createdAt": "2020-01-11T00:07:10Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Optional<Filter> buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return Optional.empty();\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    Optional.ofNullable(scanTable)\n+      .map(MetaBrowser::buildTableFilter)\n+      .ifPresent(filters::add);\n+    Optional.ofNullable(scanRegionState)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzE3MA==", "bodyText": "nit: I think this could use a javadoc.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365477170", "createdAt": "2020-01-11T00:07:55Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Optional<Filter> buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return Optional.empty();\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    Optional.ofNullable(scanTable)\n+      .map(MetaBrowser::buildTableFilter)\n+      .ifPresent(filters::add);\n+    Optional.ofNullable(scanRegionState)\n+      .map(MetaBrowser::buildScanRegionStateFilter)\n+      .ifPresent(filters::add);\n+\n+    if (filters.size() == 1) {\n+      return Optional.of(filters.get(0));\n+    }\n+\n+    return Optional.of(new FilterList(FilterList.Operator.MUST_PASS_ALL, filters));\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit(Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT) + 1);\n+    Optional.ofNullable(scanStart)\n+      .ifPresent(startRow -> metaScan.withStartRow(startRow, false));\n+    buildScanFilter().ifPresent(metaScan::setFilter);\n+    return metaScan;\n+  }\n+\n+  private <T> void maybeAddParam(final QueryStringEncoder encoder, final String paramName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3ODcwMg==", "bodyText": "nit: the accessors methods could work directly on regioninfo object ? (less code).", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365478702", "createdAt": "2020-01-11T00:17:59Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final byte[] regionName;\n+  private final byte[] startKey;\n+  private final byte[] endKey;\n+  private final Integer replicaId;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    final Optional<Result> maybeResult = Optional.ofNullable(result);\n+    final Optional<HRegionLocation> maybeLocation = Optional.ofNullable(location);\n+    final Optional<RegionInfo> maybeRegionInfo = maybeLocation.map(HRegionLocation::getRegion);\n+\n+    this.row = maybeResult.map(Result::getRow).orElse(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MDMwNg==", "bodyText": "nice.. for some reason, I thought we already had this somewhere, looked around but didn't find one.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365480306", "createdAt": "2020-01-11T00:28:23Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MDk1Mg==", "bodyText": "May be add a comment that this class doesn't close the connection by the end of it and needs to chained with TestConnectionRule which does it.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365480952", "createdAt": "2020-01-11T00:31:57Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MTA1OQ==", "bodyText": "nit: You'll probably run into checkstyle issues with inline if blocks.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365481059", "createdAt": "2020-01-11T00:32:42Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterRule.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * A {@link Rule} that manages an instance of the {@link MiniHBaseCluster}.\n+ */\n+public class TestClusterRule extends ExternalResource {\n+  private final HBaseTestingUtility testingUtility;\n+  private final StartMiniClusterOption miniClusterOptions;\n+\n+  private MiniHBaseCluster miniCluster;\n+\n+  public TestClusterRule() {\n+    this(StartMiniClusterOption.builder().build());\n+  }\n+\n+  public TestClusterRule(final StartMiniClusterOption miniClusterOptions) {\n+    this.testingUtility = new HBaseTestingUtility();\n+    this.miniClusterOptions = miniClusterOptions;\n+  }\n+\n+  public CompletableFuture<AsyncConnection> createConnection() {\n+    if (miniCluster == null) { throw new IllegalStateException(\"test cluster not initialized\"); }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyMTg1NTU3", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-342185557", "createdAt": "2020-01-13T22:33:16Z", "commit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QyMjozMzoxN1rOFdG03g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QyMjozMzoxN1rOFdG03g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA2NDg2Mg==", "bodyText": "Oops. All this logging should be at TRACE level.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366064862", "createdAt": "2020-01-13T22:33:17Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;\n+  private AsyncAdmin admin;\n+\n+  public ClearUserNamespacesAndTablesRule(final Supplier<AsyncConnection> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    final AsyncConnection connection = Objects.requireNonNull(connectionSupplier.get());\n+    admin = connection.getAdmin();\n+\n+    clearTablesAndNamespaces().join();\n+  }\n+\n+  private CompletableFuture<Void> clearTablesAndNamespaces() {\n+    return deleteUserTables().thenCompose(_void -> deleteUserNamespaces());\n+  }\n+\n+  private CompletableFuture<Void> deleteUserTables() {\n+    return listTableNames()\n+      .thenApply(tableNames -> tableNames.stream()\n+        .map(tableName -> disableIfEnabled(tableName).thenCompose(_void -> deleteTable(tableName)))\n+        .toArray(CompletableFuture[]::new))\n+      .thenCompose(CompletableFuture::allOf);\n+  }\n+\n+  private CompletableFuture<List<TableName>> listTableNames() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing tables\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyMTk3MjM5", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-342197239", "createdAt": "2020-01-13T22:59:42Z", "commit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QyMjo1OTo0MlrOFdHaSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QyMzozMDoyOFrOFdH_8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDQ0Mg==", "bodyText": "Good", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074442", "createdAt": "2020-01-13T22:59:42Z", "author": {"login": "saintstack"}, "path": "hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java", "diffHunk": "@@ -31,7 +34,7 @@\n  * (assuming small number of locations)\n  */\n @InterfaceAudience.Private\n-public class RegionLocations {\n+public class RegionLocations implements Iterable<HRegionLocation> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDYyOA==", "bodyText": "Why?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074628", "createdAt": "2020-01-13T23:00:14Z", "author": {"login": "saintstack"}, "path": "hbase-server/pom.xml", "diffHunk": "@@ -436,6 +436,11 @@\n       <artifactId>hamcrest-core</artifactId>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.hamcrest</groupId>\n+      <artifactId>hamcrest-library</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDk1MA==", "bodyText": "hbase-common? Especially given this is generic.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074950", "createdAt": "2020-01-13T23:01:12Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+\n+/**\n+ * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}\n+ * entries.\n+ * <p>Could just use {@link Iterators#limit(Iterator, int)} except that our consumer needs an API\n+ * to check if the underlying iterator is not yet exhausted.\n+ */\n+@InterfaceAudience.Private\n+public class LimitIterator<T> implements Iterator<T> {\n+\n+  private final Iterator<T> delegate;\n+  private final int limit;\n+  private int count;\n+\n+  LimitIterator(final Iterator<T> delegate, final int limit) {\n+    this.delegate = delegate;\n+    this.limit = limit;\n+    this.count = 0;\n+  }\n+\n+  /**\n+   * @return {@code true} when {@code delegate} has more entries, {@code false} otherwise.\n+   */\n+  public boolean delegateHasMore() {\n+    return delegate.hasNext();\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    if (count < limit) {\n+      return delegate.hasNext();\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public T next() {\n+    if (!hasNext()) { throw new NoSuchElementException(); }\n+    count++;\n+    return delegate.next();\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA==", "bodyText": "Can I page through a 100k hbase:meta ?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080164", "createdAt": "2020-01-13T23:17:33Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDM0NA==", "bodyText": "Above all have to public?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080344", "createdAt": "2020-01-13T23:18:15Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ==", "bodyText": "Scan all each time we page?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080981", "createdAt": "2020-01-13T23:20:24Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MTgwNw==", "bodyText": "MetaTable Visitor no good to you in here?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366081807", "createdAt": "2020-01-13T23:22:59Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ=="}, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA==", "bodyText": "How's this differ from a RegionInfo?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082204", "createdAt": "2020-01-13T23:24:17Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjQxNA==", "bodyText": "It has state and serverName? A location has RI and SN? Should this subclass one of them then?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082414", "createdAt": "2020-01-13T23:25:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA=="}, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjY0NQ==", "bodyText": "These have to be public?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082645", "createdAt": "2020-01-13T23:25:50Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final byte[] regionName;\n+  private final byte[] startKey;\n+  private final byte[] endKey;\n+  private final Integer replicaId;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    final Optional<Result> maybeResult = Optional.ofNullable(result);\n+    final Optional<HRegionLocation> maybeLocation = Optional.ofNullable(location);\n+    final Optional<RegionInfo> maybeRegionInfo = maybeLocation.map(HRegionLocation::getRegion);\n+\n+    this.row = maybeResult.map(Result::getRow).orElse(null);\n+    this.regionInfo = maybeRegionInfo.orElse(null);\n+    this.regionName = maybeRegionInfo.map(RegionInfo::getRegionName).orElse(null);\n+    this.startKey = maybeRegionInfo.map(RegionInfo::getStartKey).orElse(null);\n+    this.endKey = maybeRegionInfo.map(RegionInfo::getEndKey).orElse(null);\n+    this.replicaId = maybeRegionInfo.map(RegionInfo::getReplicaId).orElse(null);\n+    this.regionState = result != null && maybeRegionInfo.isPresent()\n+      ? RegionStateStore.getRegionState(result, maybeRegionInfo.get())\n+      : null;\n+    this.serverName = maybeLocation.map(HRegionLocation::getServerName).orElse(null);\n+  }\n+\n+  public static List<RegionReplicaInfo> from(final Result result) {\n+    if (result == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    final RegionLocations locations = MetaTableAccessor.getRegionLocations(result);\n+    if (locations == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    return StreamSupport.stream(locations.spliterator(), false)\n+      .map(location -> new RegionReplicaInfo(result, location))\n+      .collect(Collectors.toList());\n+  }\n+\n+  public byte[] getRow() {\n+    return row;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public byte[] getRegionName() {\n+    return regionName;\n+  }\n+\n+  public byte[] getStartKey() {\n+    return startKey;\n+  }\n+\n+  public byte[] getEndKey() {\n+    return endKey;\n+  }\n+\n+  public Integer getReplicaId() {\n+    return replicaId;\n+  }\n+\n+  public RegionState.State getRegionState() {\n+    return regionState;\n+  }\n+\n+  public ServerName getServerName() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MzA2Mg==", "bodyText": "Is there a method in the jsp already that makes a line of server name? IIRC?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083062", "createdAt": "2020-01-13T23:27:03Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/resources/hbase-webapps/master/table.jsp", "diffHunk": "@@ -47,46 +52,63 @@\n   import=\"org.apache.hadoop.hbase.client.RegionReplicaUtil\"\n   import=\"org.apache.hadoop.hbase.client.Table\"\n   import=\"org.apache.hadoop.hbase.master.HMaster\"\n-  import=\"org.apache.hadoop.hbase.master.assignment.RegionStates\"\n   import=\"org.apache.hadoop.hbase.master.RegionState\"\n-  import=\"org.apache.hadoop.hbase.quotas.QuotaSettingsFactory\"\n-  import=\"org.apache.hadoop.hbase.quotas.QuotaTableUtil\"\n-  import=\"org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot\"\n-  import=\"org.apache.hadoop.hbase.quotas.ThrottleSettings\"\n-  import=\"org.apache.hadoop.hbase.util.Bytes\"\n-  import=\"org.apache.hadoop.hbase.util.FSUtils\"\n-  import=\"org.apache.hadoop.hbase.zookeeper.MetaTableLocator\"\n-  import=\"org.apache.hadoop.util.StringUtils\"\n-  import=\"org.apache.hbase.thirdparty.com.google.protobuf.ByteString\"%>\n+  import=\"org.apache.hadoop.hbase.master.assignment.RegionStates\"\n+  import=\"org.apache.hadoop.hbase.master.webapp.LimitIterator\"\n+  import=\"org.apache.hadoop.hbase.master.webapp.MetaBrowser\"\n+  import=\"org.apache.hadoop.hbase.master.webapp.RegionReplicaInfo\"\n+  import=\"org.apache.hadoop.hbase.quotas.QuotaSettingsFactory\"%>\n+<%@ page import=\"org.apache.hadoop.hbase.quotas.QuotaTableUtil\" %>\n+<%@ page import=\"org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot\" %>\n+<%@ page import=\"org.apache.hadoop.hbase.quotas.ThrottleSettings\" %>\n+<%@ page import=\"org.apache.hadoop.hbase.util.Bytes\" %>\n+<%@ page import=\"org.apache.hadoop.hbase.util.FSUtils\" %>\n+<%@ page import=\"org.apache.hadoop.hbase.zookeeper.MetaTableLocator\" %>\n+<%@ page import=\"org.apache.hadoop.util.StringUtils\" %>\n+<%@ page import=\"org.apache.hbase.thirdparty.com.google.protobuf.ByteString\" %>\n <%@ page import=\"org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos\" %>\n <%@ page import=\"org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos\" %>\n <%@ page import=\"org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.Quotas\" %>\n <%@ page import=\"org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.SpaceQuota\" %>\n-<%@ page import=\"org.apache.hadoop.hbase.ServerMetrics\" %>\n-<%@ page import=\"org.apache.hadoop.hbase.RegionMetrics\" %>\n-<%@ page import=\"org.apache.hadoop.hbase.Size\" %>\n-<%@ page import=\"org.apache.hadoop.hbase.RegionMetricsBuilder\" %>\n <%!\n   /**\n    * @return An empty region load stamped with the passed in <code>regionInfo</code>\n    * region name.\n    */\n-  private RegionMetrics getEmptyRegionMetrics(final RegionInfo regionInfo) {\n+  private static RegionMetrics getEmptyRegionMetrics(final RegionInfo regionInfo) {\n     return RegionMetricsBuilder.toRegionMetrics(ClusterStatusProtos.RegionLoad.newBuilder().\n             setRegionSpecifier(HBaseProtos.RegionSpecifier.newBuilder().\n                     setType(HBaseProtos.RegionSpecifier.RegionSpecifierType.REGION_NAME).\n                     setValue(ByteString.copyFrom(regionInfo.getRegionName())).build()).build());\n   }\n+\n+  /**\n+   * Given dicey information that may or not be available in meta, render a link to the region on\n+   * its region server.\n+   * @return an anchor tag if one can be built, {@code null} otherwise.\n+   */\n+  private static String buildRegionServerLink(final ServerName serverName, final int rsInfoPort,\n+    final RegionInfo regionInfo, final RegionState.State regionState) {\n+    if (serverName == null || regionInfo == null) { return null; }\n+\n+    if (regionState != RegionState.State.OPEN) {\n+      // region is assigned to RS, but RS knows nothing of it. don't bother with a link.\n+      return serverName.getServerName();\n+    }\n+\n+    final String socketAddress = serverName.getHostname() + \":\" + rsInfoPort;\n+    final String URI = \"//\" + socketAddress + \"/region.jsp\"\n+      + \"?name=\" + regionInfo.getEncodedName();\n+    return \"<a href=\\\"\" + URI + \"\\\">\" + serverName.getServerName() + \"</a>\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4Mzc1MA==", "bodyText": "One thought, why cleanup after a test? What if it takes a bunch of work cleaning up, more than just start fresh?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083750", "createdAt": "2020-01-13T23:29:17Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;\n+  private AsyncAdmin admin;\n+\n+  public ClearUserNamespacesAndTablesRule(final Supplier<AsyncConnection> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    final AsyncConnection connection = Objects.requireNonNull(connectionSupplier.get());\n+    admin = connection.getAdmin();\n+\n+    clearTablesAndNamespaces().join();\n+  }\n+\n+  private CompletableFuture<Void> clearTablesAndNamespaces() {\n+    return deleteUserTables().thenCompose(_void -> deleteUserNamespaces());\n+  }\n+\n+  private CompletableFuture<Void> deleteUserTables() {\n+    return listTableNames()\n+      .thenApply(tableNames -> tableNames.stream()\n+        .map(tableName -> disableIfEnabled(tableName).thenCompose(_void -> deleteTable(tableName)))\n+        .toArray(CompletableFuture[]::new))\n+      .thenCompose(CompletableFuture::allOf);\n+  }\n+\n+  private CompletableFuture<List<TableName>> listTableNames() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing tables\"))\n+      .thenCompose(_void -> admin.listTableNames(false))\n+      .thenApply(tableNames -> {\n+        final StringJoiner joiner = new StringJoiner(\", \", \"[\", \"]\");\n+        tableNames.stream().map(TableName::getNameAsString).forEach(joiner::add);\n+        logger.info(\"found existing tables {}\", joiner.toString());\n+        return tableNames;\n+      });\n+  }\n+\n+  private CompletableFuture<Boolean> isTableEnabled(final TableName tableName) {\n+    return admin.isTableEnabled(tableName)\n+      .thenApply(isEnabled -> {\n+        logger.info(\"table {} is enabled.\", tableName);\n+        return isEnabled;\n+      });\n+  }\n+\n+  private CompletableFuture<Void> disableIfEnabled(final TableName tableName) {\n+    return isTableEnabled(tableName)\n+      .thenCompose(isEnabled -> {\n+        if (isEnabled) { return disableTable(tableName); }\n+        return CompletableFuture.completedFuture(null);\n+      });\n+  }\n+\n+  private CompletableFuture<Void> disableTable(final TableName tableName) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"disabling enabled table {}\", tableName))\n+      .thenCompose(_void -> admin.disableTable(tableName));\n+  }\n+\n+  private CompletableFuture<Void> deleteTable(final TableName tableName) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"deleting disabled table {}\", tableName))\n+      .thenCompose(_void -> admin.deleteTable(tableName));\n+  }\n+\n+  private CompletableFuture<List<String>> listUserNamespaces() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing namespaces\"))\n+      .thenCompose(_void -> admin.listNamespaceDescriptors())\n+      .thenApply(namespaceDescriptors -> {\n+        final StringJoiner joiner = new StringJoiner(\", \", \"[\", \"]\");\n+        final List<String> names = namespaceDescriptors.stream()\n+          .map(NamespaceDescriptor::getName)\n+          .peek(joiner::add)\n+          .collect(Collectors.toList());\n+        logger.info(\"found existing namespaces {}\", joiner.toString());\n+        return names;\n+      })\n+      .thenApply(namespaces -> namespaces.stream()\n+        .filter(namespace -> !Objects.equals(\n+          namespace, NamespaceDescriptor.SYSTEM_NAMESPACE.getName()))\n+        .filter(namespace -> !Objects.equals(\n+          namespace, NamespaceDescriptor.DEFAULT_NAMESPACE.getName()))\n+        .collect(Collectors.toList()));\n+  }\n+\n+  private CompletableFuture<Void> deleteNamespace(final String namespace) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"deleting namespace {}\", namespace))\n+      .thenCompose(_void -> admin.deleteNamespace(namespace));\n+  }\n+\n+  private CompletableFuture<Void> deleteUserNamespaces() {\n+    return listUserNamespaces()\n+      .thenCompose(namespaces -> CompletableFuture.allOf(\n+        namespaces.stream()\n+          .map(this::deleteNamespace)\n+          .toArray(CompletableFuture[]::new)));\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MzgwOA==", "bodyText": "Fun", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083808", "createdAt": "2020-01-13T23:29:33Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * A {@link Rule} that manages an instance of {@link AsyncConnection}.\n+ */\n+public class ConnectionRule extends ExternalResource {\n+\n+  private final Supplier<CompletableFuture<AsyncConnection>> connectionSupplier;\n+  private AsyncConnection connection;\n+\n+  public ConnectionRule(final Supplier<CompletableFuture<AsyncConnection>> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  public AsyncConnection getConnection() {\n+    return connection;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    this.connection = connectionSupplier.get().join();\n+  }\n+\n+  @Override\n+  protected void after() {\n+    if (this.connection != null) {\n+      try {\n+        connection.close();\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw==", "bodyText": "This only reason for import?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366084083", "createdAt": "2020-01-13T23:30:28Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.client.hamcrest;\n+\n+import static org.hamcrest.core.Is.is;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyMjM1OTM2", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-342235936", "createdAt": "2020-01-14T00:56:09Z", "commit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "state": "APPROVED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQwMDo1NjoxMFrOFdJXQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQwMToyNDowNFrOFdJwww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwNjQzMg==", "bodyText": "I think some form of this could be captured in the javadoc class comment. Otherwise, one needs to understand how this class paginates the result by setting the start row intelligently..", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366106432", "createdAt": "2020-01-14T00:56:10Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}, "originalCommit": {"oid": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwODE2Mg==", "bodyText": "nit: If toString() is the only needed call, no need to use generics / templatize it? Just pass an object?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366108162", "createdAt": "2020-01-14T01:03:05Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ==", "bodyText": "nit: return SCAN_LIMIT_DEFAULT here and simplify callers?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366109181", "createdAt": "2020-01-14T01:07:07Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMDY2MQ==", "bodyText": "I was about to suggest something like this.. wrap the output in some auto-closeable implementation that can be used in callers.. neat...", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366110661", "createdAt": "2020-01-14T01:13:53Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,\n+    final T value) {\n+    if (value != null) {\n+      encoder.addParam(paramName, value.toString());\n+    }\n+  }\n+\n+  private QueryStringEncoder buildFirstPageEncoder() {\n+    final QueryStringEncoder encoder =\n+      new QueryStringEncoder(request.getRequestURI());\n+    addParam(encoder, NAME_PARAM, name);\n+    addParam(encoder, SCAN_LIMIT_PARAM, scanLimit);\n+    addParam(encoder, SCAN_REGION_STATE_PARAM, scanRegionState);\n+    addParam(encoder, SCAN_TABLE_PARAM, scanTable);\n+    return encoder;\n+  }\n+\n+  public String buildFirstPageUrl() {\n+    return buildFirstPageEncoder().toString();\n+  }\n+\n+  public static String buildStartParamFrom(final byte[] lastRow) {\n+    if (lastRow == null) {\n+      return null;\n+    }\n+    return urlEncode(Bytes.toStringBinary(lastRow));\n+  }\n+\n+  public String buildNextPageUrl(final byte[] lastRow) {\n+    final QueryStringEncoder encoder = buildFirstPageEncoder();\n+    final String startRow = buildStartParamFrom(lastRow);\n+    addParam(encoder, SCAN_START_PARAM, startRow);\n+    return encoder.toString();\n+  }\n+\n+  private static String urlEncode(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return URLEncoder.encode(val, StandardCharsets.UTF_8.toString());\n+    } catch (UnsupportedEncodingException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static String urlDecode(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return URLDecoder.decode(val, StandardCharsets.UTF_8.toString());\n+    } catch (UnsupportedEncodingException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static Integer tryParseInt(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return Integer.parseInt(val);\n+    } catch (NumberFormatException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz,\n+    final String value) {\n+    if (clazz == null || value == null) {\n+      return null;\n+    }\n+    try {\n+      return T.valueOf(clazz, value);\n+    } catch (IllegalArgumentException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static String buildScanLimitExceededErrorMessage(final int requestValue) {\n+    return String.format(\n+      \"Requested SCAN_LIMIT value %d exceeds maximum value %d.\", requestValue, SCAN_LIMIT_MAX);\n+  }\n+\n+  private static String buildScanLimitMalformedErrorMessage(final String requestValue) {\n+    return String.format(\n+      \"Requested SCAN_LIMIT value '%s' cannot be parsed as an integer.\", requestValue);\n+  }\n+\n+  private static String buildScanLimitLTEZero(final int requestValue) {\n+    return String.format(\"Requested SCAN_LIMIT value %d is <= 0.\", requestValue);\n+  }\n+\n+  private static String buildScanRegionStateMalformedErrorMessage(final String requestValue) {\n+    return String.format(\n+      \"Requested SCAN_REGION_STATE value '%s' cannot be parsed as a RegionState.\", requestValue);\n+  }\n+\n+  /**\n+   * Encapsulates the results produced by this {@link MetaBrowser} instance.\n+   */\n+  public final class Results implements AutoCloseable, Iterable<RegionReplicaInfo> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 346}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMTA0MQ==", "bodyText": "Thanks for the clean up.. this is what I had in mind.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366111041", "createdAt": "2020-01-14T01:15:46Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    this.row = result != null ? result.getRow() : null;\n+    this.regionInfo = location != null ? location.getRegion() : null;\n+    this.regionState = (result != null && regionInfo != null)\n+      ? RegionStateStore.getRegionState(result, regionInfo)\n+      : null;\n+    this.serverName = location != null ? location.getServerName() : null;\n+  }\n+\n+  public static List<RegionReplicaInfo> from(final Result result) {\n+    if (result == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    final RegionLocations locations = MetaTableAccessor.getRegionLocations(result);\n+    if (locations == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    return StreamSupport.stream(locations.spliterator(), false)\n+      .map(location -> new RegionReplicaInfo(result, location))\n+      .collect(Collectors.toList());\n+  }\n+\n+  public byte[] getRow() {\n+    return row;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public byte[] getRegionName() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMjU2Mg==", "bodyText": "I think, the +1 part is pretty subtle.. add a comment that it is needed for pagination hasNext() support?", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366112562", "createdAt": "2020-01-14T01:22:28Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMjk2Mw==", "bodyText": "nice..", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366112963", "createdAt": "2020-01-14T01:24:04Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/resources/hbase-webapps/master/table.jsp", "diffHunk": "@@ -127,8 +147,11 @@\n       pageTitle = \"Table: \" + escaped_fqtn;\n   }\n   pageContext.setAttribute(\"pageTitle\", pageTitle);\n-  AsyncConnection connection = ConnectionFactory.createAsyncConnection(master.getConfiguration()).get();\n-  AsyncAdmin admin = connection.getAdminBuilder().setOperationTimeout(5, TimeUnit.SECONDS).build();\n+  final AsyncConnection connection = master.getAsyncConnection();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyNzIzMDUy", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-342723052", "createdAt": "2020-01-14T17:51:08Z", "commit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNzo1MTowOFrOFdgZeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNzo1MTowOFrOFdgZeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ4MzgzMg==", "bodyText": "this can be package-protected as it's only used internally and within the unit test.", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366483832", "createdAt": "2020-01-14T17:51:08Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,\n+    final T value) {\n+    if (value != null) {\n+      encoder.addParam(paramName, value.toString());\n+    }\n+  }\n+\n+  private QueryStringEncoder buildFirstPageEncoder() {\n+    final QueryStringEncoder encoder =\n+      new QueryStringEncoder(request.getRequestURI());\n+    addParam(encoder, NAME_PARAM, name);\n+    addParam(encoder, SCAN_LIMIT_PARAM, scanLimit);\n+    addParam(encoder, SCAN_REGION_STATE_PARAM, scanRegionState);\n+    addParam(encoder, SCAN_TABLE_PARAM, scanTable);\n+    return encoder;\n+  }\n+\n+  public String buildFirstPageUrl() {\n+    return buildFirstPageEncoder().toString();\n+  }\n+\n+  public static String buildStartParamFrom(final byte[] lastRow) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9"}, "originalPosition": 265}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "committedDate": "2020-01-14T00:41:56Z", "message": "PR feedback"}, "afterCommit": {"oid": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/18d86a4d85b8a70a36131dd02f9e06753b05cad7", "committedDate": "2020-01-14T18:53:18Z", "message": "PR feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ae0beddf4a7b635573e49d7dbc4234cb357c9181", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/ae0beddf4a7b635573e49d7dbc4234cb357c9181", "committedDate": "2020-01-14T19:15:52Z", "message": "add missing ClassRule"}, "afterCommit": {"oid": "f2d794831fa53849c437c27143844a20e338ab92", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/f2d794831fa53849c437c27143844a20e338ab92", "committedDate": "2020-01-15T17:37:11Z", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/a62c8f51c4f84a4eacd1246244ae9e544ed94526", "committedDate": "2020-01-15T17:47:49Z", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f2d794831fa53849c437c27143844a20e338ab92", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/f2d794831fa53849c437c27143844a20e338ab92", "committedDate": "2020-01-15T17:37:11Z", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c."}, "afterCommit": {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/a62c8f51c4f84a4eacd1246244ae9e544ed94526", "committedDate": "2020-01-15T17:47:49Z", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQzNjY1NjE4", "url": "https://github.com/apache/hbase/pull/1020#pullrequestreview-343665618", "createdAt": "2020-01-16T04:18:35Z", "commit": {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3184, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}