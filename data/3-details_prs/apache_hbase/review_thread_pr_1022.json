{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxNzAzNjk0", "number": 1022, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQxNzowNTowN1rODXVpzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QyMzoxNDo0OFrODY9g-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1Nzk4NjA3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQxNzowNTowN1rOFcmRxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQxNzowNzowNVrOFcmSKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTUzMTU5MA==", "bodyText": "We just delete archived files? Why even archive them then? Can we delay the delete at least?", "url": "https://github.com/apache/hbase/pull/1022#discussion_r365531590", "createdAt": "2020-01-11T17:05:07Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +146,30 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private void deleteCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    store.closeAndArchiveCompactedFiles();\n+    // for now we just deleted these HFiles, without moving them to the global archive directory.\n+    // This is because that, our HFiles are on the WAL file system, but the global HFile archive\n+    // directory is on the root(HFile) file system, we can not move between two different file\n+    // systems.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+    if (LOG.isDebugEnabled()) {\n+      FileStatus[] toDelete = fs.listStatus(archiveDir);\n+      if (toDelete != null && toDelete.length > 0) {\n+        LOG.debug(\"Delete all archived HFiles under {}: {}\", archiveDir, Stream.of(toDelete)\n+          .map(s -> s.getPath().getName()).collect(Collectors.joining(\", \", \"[\", \"]\")));\n+      }\n+    }\n+    fs.delete(archiveDir, true);\n+  }\n+\n   private void compact() {\n     try {\n       region.compact(true);\n+      deleteCompactedHFiles();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d2b43013b69176de3b12c7466024bbfd8293a37"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTUzMTY5MQ==", "bodyText": "Why we writing hfiles into WAL fs out of interest?", "url": "https://github.com/apache/hbase/pull/1022#discussion_r365531691", "createdAt": "2020-01-11T17:07:05Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +146,30 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private void deleteCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    store.closeAndArchiveCompactedFiles();\n+    // for now we just deleted these HFiles, without moving them to the global archive directory.\n+    // This is because that, our HFiles are on the WAL file system, but the global HFile archive\n+    // directory is on the root(HFile) file system, we can not move between two different file\n+    // systems.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+    if (LOG.isDebugEnabled()) {\n+      FileStatus[] toDelete = fs.listStatus(archiveDir);\n+      if (toDelete != null && toDelete.length > 0) {\n+        LOG.debug(\"Delete all archived HFiles under {}: {}\", archiveDir, Stream.of(toDelete)\n+          .map(s -> s.getPath().getName()).collect(Collectors.joining(\", \", \"[\", \"]\")));\n+      }\n+    }\n+    fs.delete(archiveDir, true);\n+  }\n+\n   private void compact() {\n     try {\n       region.compact(true);\n+      deleteCompactedHFiles();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTUzMTU5MA=="}, "originalCommit": {"oid": "6d2b43013b69176de3b12c7466024bbfd8293a37"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NDE5OTYyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNzozNDozMFrOFdf6RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQwMDo0Njo1NFrOFeK-CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ3NTg0NQ==", "bodyText": "We should be asking the other cleaner delegates if it's okay to delete these files. I could see it being either the WAL cleaner delegates or the HFile Cleaner delegates, but it needs to be one of them.", "url": "https://github.com/apache/hbase/pull/1022#discussion_r366475845", "createdAt": "2020-01-14T17:34:30Z", "author": {"login": "busbey"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +156,43 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private boolean isHFileDeleteable(FileStatus status) {\n+    long currentTime = EnvironmentEdgeManager.currentTime();\n+    long time = status.getModificationTime();\n+    long life = currentTime - time;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"HFile life: {}ms, ttl: {}ms, current: {}, from: {}\", life, compactedHFileTTLMs,\n+        currentTime, time);\n+    }\n+    if (life < 0) {\n+      LOG.warn(\"Found a hfile ({}) newer than current time ({} < {}), probably a clock skew\",\n+        status.getPath(), currentTime, time);\n+      return false;\n+    }\n+    return life > compactedHFileTTLMs;\n+  }\n+\n+  void cleanupCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    // our HFiles are on the WAL file system, but the global HFile archive directory is on the\n+    // root(HFile) file system, we can not move between two different file systems. So here we have\n+    // to implement our own TTL cleaner.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+\n+    for (FileStatus status : fs.listStatus(archiveDir)) {\n+      if (isHFileDeleteable(status)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbe7d1d16da5246d62acb308a31790bf1501c862"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njg4OTQxMA==", "bodyText": "We are on a different file system comparing to other HFiles so it is not suitable to ask the HFileCleaner, and it seems strange to test a HFile with WALCleaner?", "url": "https://github.com/apache/hbase/pull/1022#discussion_r366889410", "createdAt": "2020-01-15T14:00:33Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +156,43 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private boolean isHFileDeleteable(FileStatus status) {\n+    long currentTime = EnvironmentEdgeManager.currentTime();\n+    long time = status.getModificationTime();\n+    long life = currentTime - time;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"HFile life: {}ms, ttl: {}ms, current: {}, from: {}\", life, compactedHFileTTLMs,\n+        currentTime, time);\n+    }\n+    if (life < 0) {\n+      LOG.warn(\"Found a hfile ({}) newer than current time ({} < {}), probably a clock skew\",\n+        status.getPath(), currentTime, time);\n+      return false;\n+    }\n+    return life > compactedHFileTTLMs;\n+  }\n+\n+  void cleanupCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    // our HFiles are on the WAL file system, but the global HFile archive directory is on the\n+    // root(HFile) file system, we can not move between two different file systems. So here we have\n+    // to implement our own TTL cleaner.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+\n+    for (FileStatus status : fs.listStatus(archiveDir)) {\n+      if (isHFileDeleteable(status)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ3NTg0NQ=="}, "originalCommit": {"oid": "dbe7d1d16da5246d62acb308a31790bf1501c862"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk0NjgwMA==", "bodyText": "Then we should instantiate our own hfile cleaner delegate chain. Defualt it to the TTL delegate and we can skip a custom impl here as well.", "url": "https://github.com/apache/hbase/pull/1022#discussion_r366946800", "createdAt": "2020-01-15T15:40:45Z", "author": {"login": "busbey"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +156,43 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private boolean isHFileDeleteable(FileStatus status) {\n+    long currentTime = EnvironmentEdgeManager.currentTime();\n+    long time = status.getModificationTime();\n+    long life = currentTime - time;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"HFile life: {}ms, ttl: {}ms, current: {}, from: {}\", life, compactedHFileTTLMs,\n+        currentTime, time);\n+    }\n+    if (life < 0) {\n+      LOG.warn(\"Found a hfile ({}) newer than current time ({} < {}), probably a clock skew\",\n+        status.getPath(), currentTime, time);\n+      return false;\n+    }\n+    return life > compactedHFileTTLMs;\n+  }\n+\n+  void cleanupCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    // our HFiles are on the WAL file system, but the global HFile archive directory is on the\n+    // root(HFile) file system, we can not move between two different file systems. So here we have\n+    // to implement our own TTL cleaner.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+\n+    for (FileStatus status : fs.listStatus(archiveDir)) {\n+      if (isHFileDeleteable(status)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ3NTg0NQ=="}, "originalCommit": {"oid": "dbe7d1d16da5246d62acb308a31790bf1501c862"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE3ODM1MQ==", "bodyText": "Maybe a bit overkill? AFAIK, the only suitable HFileCleaner for this region is the TTL one, others like Links or Back Ref are not needed. Just saying. If you think this is the correct way then I could pass the ChoreService in and create a cleaner chain for cleaning the hfiles.", "url": "https://github.com/apache/hbase/pull/1022#discussion_r367178351", "createdAt": "2020-01-16T00:34:49Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +156,43 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private boolean isHFileDeleteable(FileStatus status) {\n+    long currentTime = EnvironmentEdgeManager.currentTime();\n+    long time = status.getModificationTime();\n+    long life = currentTime - time;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"HFile life: {}ms, ttl: {}ms, current: {}, from: {}\", life, compactedHFileTTLMs,\n+        currentTime, time);\n+    }\n+    if (life < 0) {\n+      LOG.warn(\"Found a hfile ({}) newer than current time ({} < {}), probably a clock skew\",\n+        status.getPath(), currentTime, time);\n+      return false;\n+    }\n+    return life > compactedHFileTTLMs;\n+  }\n+\n+  void cleanupCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    // our HFiles are on the WAL file system, but the global HFile archive directory is on the\n+    // root(HFile) file system, we can not move between two different file systems. So here we have\n+    // to implement our own TTL cleaner.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+\n+    for (FileStatus status : fs.listStatus(archiveDir)) {\n+      if (isHFileDeleteable(status)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ3NTg0NQ=="}, "originalCommit": {"oid": "dbe7d1d16da5246d62acb308a31790bf1501c862"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE4MTMyMQ==", "bodyText": "No not overkill. This is the specific mechanism I mentioned needing to feel comfortable with this landing in branches where I will probably have to support deployments.\nI agree that the TTL cleaner is the only one needed by default.\nAt some point something will go wrong where I need to avoid things getting cleaned out of archive. Being able to reuse a cleaner delegate that normally is used with HFiles will make that easier. It's exactly the kind of operational reuse that makes this implementation shift attractive.", "url": "https://github.com/apache/hbase/pull/1022#discussion_r367181321", "createdAt": "2020-01-16T00:46:54Z", "author": {"login": "busbey"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/region/RegionFlusherAndCompactor.java", "diffHunk": "@@ -136,9 +156,43 @@ static void setupConf(Configuration conf) {\n       flushPerChanges, flushIntervalMs);\n   }\n \n+  private boolean isHFileDeleteable(FileStatus status) {\n+    long currentTime = EnvironmentEdgeManager.currentTime();\n+    long time = status.getModificationTime();\n+    long life = currentTime - time;\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"HFile life: {}ms, ttl: {}ms, current: {}, from: {}\", life, compactedHFileTTLMs,\n+        currentTime, time);\n+    }\n+    if (life < 0) {\n+      LOG.warn(\"Found a hfile ({}) newer than current time ({} < {}), probably a clock skew\",\n+        status.getPath(), currentTime, time);\n+      return false;\n+    }\n+    return life > compactedHFileTTLMs;\n+  }\n+\n+  void cleanupCompactedHFiles() throws IOException {\n+    HStore store = Iterables.getOnlyElement(region.getStores());\n+    // our HFiles are on the WAL file system, but the global HFile archive directory is on the\n+    // root(HFile) file system, we can not move between two different file systems. So here we have\n+    // to implement our own TTL cleaner.\n+    Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, region.getRegionInfo(),\n+      store.getColumnFamilyDescriptor().getName());\n+    FileSystem fs = archiveDir.getFileSystem(conf);\n+\n+    for (FileStatus status : fs.listStatus(archiveDir)) {\n+      if (isHFileDeleteable(status)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ3NTg0NQ=="}, "originalCommit": {"oid": "dbe7d1d16da5246d62acb308a31790bf1501c862"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3NTAwMjgzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QyMzoxNDo0OFrOFfHkeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQyMzozMTo1NlrOFfMeeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3NDIwMA==", "bodyText": "Strange place to start cleanerPool down here in the procedure startup given it is used cleaning WALs and hfile...", "url": "https://github.com/apache/hbase/pull/1022#discussion_r368174200", "createdAt": "2020-01-17T23:14:48Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1520,8 +1518,10 @@ protected void stopServiceThreads() {\n \n   private void createProcedureExecutor() throws IOException {\n     MasterProcedureEnv procEnv = new MasterProcedureEnv(this);\n-    procedureStore =\n-      new RegionProcedureStore(this, new MasterProcedureEnv.FsUtilsLeaseRecovery(this));\n+    // Create cleaner thread pool\n+    cleanerPool = new DirScanPool(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee1d497428e68e019bf2c6530a30eff263855dbe"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODIyNDQ4Mg==", "bodyText": "But we have no choice as we need to use it in the next line...", "url": "https://github.com/apache/hbase/pull/1022#discussion_r368224482", "createdAt": "2020-01-18T12:30:29Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1520,8 +1518,10 @@ protected void stopServiceThreads() {\n \n   private void createProcedureExecutor() throws IOException {\n     MasterProcedureEnv procEnv = new MasterProcedureEnv(this);\n-    procedureStore =\n-      new RegionProcedureStore(this, new MasterProcedureEnv.FsUtilsLeaseRecovery(this));\n+    // Create cleaner thread pool\n+    cleanerPool = new DirScanPool(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3NDIwMA=="}, "originalCommit": {"oid": "ee1d497428e68e019bf2c6530a30eff263855dbe"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODIzMTUyOQ==", "bodyText": "I was thinking you'd call it up in the caller's method, in startServiceThreads, rather than down hidden in here in startProcedureExecutor... i.e. move the line up thee lines from where it was.", "url": "https://github.com/apache/hbase/pull/1022#discussion_r368231529", "createdAt": "2020-01-18T15:07:46Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1520,8 +1518,10 @@ protected void stopServiceThreads() {\n \n   private void createProcedureExecutor() throws IOException {\n     MasterProcedureEnv procEnv = new MasterProcedureEnv(this);\n-    procedureStore =\n-      new RegionProcedureStore(this, new MasterProcedureEnv.FsUtilsLeaseRecovery(this));\n+    // Create cleaner thread pool\n+    cleanerPool = new DirScanPool(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3NDIwMA=="}, "originalCommit": {"oid": "ee1d497428e68e019bf2c6530a30eff263855dbe"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI1NDU4Ng==", "bodyText": "The startServiceThreads method is called after createProcedureExecutor, notice that, it is create, not start...", "url": "https://github.com/apache/hbase/pull/1022#discussion_r368254586", "createdAt": "2020-01-18T23:31:56Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1520,8 +1518,10 @@ protected void stopServiceThreads() {\n \n   private void createProcedureExecutor() throws IOException {\n     MasterProcedureEnv procEnv = new MasterProcedureEnv(this);\n-    procedureStore =\n-      new RegionProcedureStore(this, new MasterProcedureEnv.FsUtilsLeaseRecovery(this));\n+    // Create cleaner thread pool\n+    cleanerPool = new DirScanPool(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3NDIwMA=="}, "originalCommit": {"oid": "ee1d497428e68e019bf2c6530a30eff263855dbe"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2233, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}