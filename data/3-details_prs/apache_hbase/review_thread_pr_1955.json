{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4MjQxMjUz", "number": 1955, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzozMzoxNFrOEHzNmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzo1NTozOVrOEHzZ8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjE0NTU0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzozMzoxNFrOGnYDvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTowMjoyNlrOGntlNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MTgyMA==", "bodyText": "Move the log out of the if {} code block?", "url": "https://github.com/apache/hbase/pull/1955#discussion_r443941820", "createdAt": "2020-06-23T03:33:14Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java", "diffHunk": "@@ -2037,8 +2037,7 @@ private void startServices() throws IOException {\n       this.splitLogWorker = new SplitLogWorker(sinkConf, this,\n           this, walFactory);\n       splitLogWorker.start();\n-    } else {\n-      LOG.warn(\"SplitLogWorker Service NOT started; CoordinatedStateManager is null\");\n+      LOG.debug(\"SplitLogWorker started\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NDQ1NA==", "bodyText": "Previous, we used to log when we did NOT start the splitlogworker service. This goes against our usual pattern of only logging started services (logging all the things we did not start would be an endless list -- smile)\nI changed the log here so that it conforms with our usual pattern.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r444294454", "createdAt": "2020-06-23T15:02:26Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java", "diffHunk": "@@ -2037,8 +2037,7 @@ private void startServices() throws IOException {\n       this.splitLogWorker = new SplitLogWorker(sinkConf, this,\n           this, walFactory);\n       splitLogWorker.start();\n-    } else {\n-      LOG.warn(\"SplitLogWorker Service NOT started; CoordinatedStateManager is null\");\n+      LOG.debug(\"SplitLogWorker started\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MTgyMA=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjE0NzIzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RSProcedureHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzozNDoyNlrOGnYEtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTowMzo0N1rOGnto-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MjA2OQ==", "bodyText": "This log is too little?", "url": "https://github.com/apache/hbase/pull/1955#discussion_r443942069", "createdAt": "2020-06-23T03:34:26Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RSProcedureHandler.java", "diffHunk": "@@ -48,7 +48,7 @@ public void process() {\n     try {\n       callable.call();\n     } catch (Throwable t) {\n-      LOG.error(\"Error when call RSProcedureCallable: \", t);\n+      LOG.error(\"pid=\" + this.procId, t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NTQxOQ==", "bodyText": "I changed it so that it logs the pid same way we do it every where else. The thread/class/level already provides what we add here. I just removed redundancy.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r444295419", "createdAt": "2020-06-23T15:03:47Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/RSProcedureHandler.java", "diffHunk": "@@ -48,7 +48,7 @@ public void process() {\n     try {\n       callable.call();\n     } catch (Throwable t) {\n-      LOG.error(\"Error when call RSProcedureCallable: \", t);\n+      LOG.error(\"pid=\" + this.procId, t);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MjA2OQ=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjE0OTUyOnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzozNTo1M1rOGnYGJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjowMzowNVrOGnwMZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MjQzNg==", "bodyText": "The tableDescriptors may be removed from WalSplitter, too?", "url": "https://github.com/apache/hbase/pull/1955#discussion_r443942436", "createdAt": "2020-06-23T03:35:53Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery\n+   *   given hfile has metadata on how it was written.\n+   */\n   private StoreFileWriter createRecoveredHFileWriter(TableName tableName, String regionName,\n       long seqId, String familyName, boolean isMetaTable) throws IOException {\n     Path outputDir = WALSplitUtil.tryCreateRecoveredHFilesDir(walSplitter.rootFS, walSplitter.conf,\n       tableName, regionName, familyName);\n     StoreFileWriter.Builder writerBuilder =\n         new StoreFileWriter.Builder(walSplitter.conf, CacheConfig.DISABLED, walSplitter.rootFS)\n             .withOutputDir(outputDir);\n-\n-    TableDescriptor tableDesc =\n-        tableDescCache.computeIfAbsent(tableName, t -> getTableDescriptor(t));\n-    if (tableDesc == null) {\n-      throw new IOException(\"Failed to get table descriptor for table \" + tableName);\n-    }\n-    ColumnFamilyDescriptor cfd = tableDesc.getColumnFamily(Bytes.toBytesBinary(familyName));\n-    HFileContext hFileContext = createFileContext(cfd, isMetaTable);\n-    return writerBuilder.withFileContext(hFileContext).withBloomType(cfd.getBloomFilterType())\n-        .build();\n-  }\n-\n-  private HFileContext createFileContext(ColumnFamilyDescriptor cfd, boolean isMetaTable)\n-      throws IOException {\n-    return new HFileContextBuilder().withCompression(cfd.getCompressionType())\n-        .withChecksumType(HStore.getChecksumType(walSplitter.conf))\n-        .withBytesPerCheckSum(HStore.getBytesPerChecksum(walSplitter.conf))\n-        .withBlockSize(cfd.getBlocksize()).withCompressTags(cfd.isCompressTags())\n-        .withDataBlockEncoding(cfd.getDataBlockEncoding()).withCellComparator(\n-          isMetaTable ? CellComparatorImpl.META_COMPARATOR : CellComparatorImpl.COMPARATOR)\n-        .build();\n-  }\n-\n-  private TableDescriptor getTableDescriptor(TableName tableName) {\n-    if (walSplitter.rsServices != null) {\n-      try {\n-        return walSplitter.rsServices.getConnection().getAdmin().getDescriptor(tableName);\n-      } catch (IOException e) {\n-        LOG.warn(\"Failed to get table descriptor for {}\", tableName, e);\n-      }\n-    }\n-    LOG.info(\"Failed getting {} table descriptor from master; trying local\", tableName);\n-    try {\n-      return walSplitter.tableDescriptors.get(tableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NzI3NA==", "bodyText": "Good question.\nLooking at it, it usually gets them from the hosting Server, not by RPC to Master so there it should not suffer the issue we see here.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r444297274", "createdAt": "2020-06-23T15:06:23Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery\n+   *   given hfile has metadata on how it was written.\n+   */\n   private StoreFileWriter createRecoveredHFileWriter(TableName tableName, String regionName,\n       long seqId, String familyName, boolean isMetaTable) throws IOException {\n     Path outputDir = WALSplitUtil.tryCreateRecoveredHFilesDir(walSplitter.rootFS, walSplitter.conf,\n       tableName, regionName, familyName);\n     StoreFileWriter.Builder writerBuilder =\n         new StoreFileWriter.Builder(walSplitter.conf, CacheConfig.DISABLED, walSplitter.rootFS)\n             .withOutputDir(outputDir);\n-\n-    TableDescriptor tableDesc =\n-        tableDescCache.computeIfAbsent(tableName, t -> getTableDescriptor(t));\n-    if (tableDesc == null) {\n-      throw new IOException(\"Failed to get table descriptor for table \" + tableName);\n-    }\n-    ColumnFamilyDescriptor cfd = tableDesc.getColumnFamily(Bytes.toBytesBinary(familyName));\n-    HFileContext hFileContext = createFileContext(cfd, isMetaTable);\n-    return writerBuilder.withFileContext(hFileContext).withBloomType(cfd.getBloomFilterType())\n-        .build();\n-  }\n-\n-  private HFileContext createFileContext(ColumnFamilyDescriptor cfd, boolean isMetaTable)\n-      throws IOException {\n-    return new HFileContextBuilder().withCompression(cfd.getCompressionType())\n-        .withChecksumType(HStore.getChecksumType(walSplitter.conf))\n-        .withBytesPerCheckSum(HStore.getBytesPerChecksum(walSplitter.conf))\n-        .withBlockSize(cfd.getBlocksize()).withCompressTags(cfd.isCompressTags())\n-        .withDataBlockEncoding(cfd.getDataBlockEncoding()).withCellComparator(\n-          isMetaTable ? CellComparatorImpl.META_COMPARATOR : CellComparatorImpl.COMPARATOR)\n-        .build();\n-  }\n-\n-  private TableDescriptor getTableDescriptor(TableName tableName) {\n-    if (walSplitter.rsServices != null) {\n-      try {\n-        return walSplitter.rsServices.getConnection().getAdmin().getDescriptor(tableName);\n-      } catch (IOException e) {\n-        LOG.warn(\"Failed to get table descriptor for {}\", tableName, e);\n-      }\n-    }\n-    LOG.info(\"Failed getting {} table descriptor from master; trying local\", tableName);\n-    try {\n-      return walSplitter.tableDescriptors.get(tableName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MjQzNg=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzNzI1Mg==", "bodyText": "Sorry, I get what you are saying now. You added this. Let me undo it.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r444337252", "createdAt": "2020-06-23T16:03:05Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery\n+   *   given hfile has metadata on how it was written.\n+   */\n   private StoreFileWriter createRecoveredHFileWriter(TableName tableName, String regionName,\n       long seqId, String familyName, boolean isMetaTable) throws IOException {\n     Path outputDir = WALSplitUtil.tryCreateRecoveredHFilesDir(walSplitter.rootFS, walSplitter.conf,\n       tableName, regionName, familyName);\n     StoreFileWriter.Builder writerBuilder =\n         new StoreFileWriter.Builder(walSplitter.conf, CacheConfig.DISABLED, walSplitter.rootFS)\n             .withOutputDir(outputDir);\n-\n-    TableDescriptor tableDesc =\n-        tableDescCache.computeIfAbsent(tableName, t -> getTableDescriptor(t));\n-    if (tableDesc == null) {\n-      throw new IOException(\"Failed to get table descriptor for table \" + tableName);\n-    }\n-    ColumnFamilyDescriptor cfd = tableDesc.getColumnFamily(Bytes.toBytesBinary(familyName));\n-    HFileContext hFileContext = createFileContext(cfd, isMetaTable);\n-    return writerBuilder.withFileContext(hFileContext).withBloomType(cfd.getBloomFilterType())\n-        .build();\n-  }\n-\n-  private HFileContext createFileContext(ColumnFamilyDescriptor cfd, boolean isMetaTable)\n-      throws IOException {\n-    return new HFileContextBuilder().withCompression(cfd.getCompressionType())\n-        .withChecksumType(HStore.getChecksumType(walSplitter.conf))\n-        .withBytesPerCheckSum(HStore.getBytesPerChecksum(walSplitter.conf))\n-        .withBlockSize(cfd.getBlocksize()).withCompressTags(cfd.isCompressTags())\n-        .withDataBlockEncoding(cfd.getDataBlockEncoding()).withCellComparator(\n-          isMetaTable ? CellComparatorImpl.META_COMPARATOR : CellComparatorImpl.COMPARATOR)\n-        .build();\n-  }\n-\n-  private TableDescriptor getTableDescriptor(TableName tableName) {\n-    if (walSplitter.rsServices != null) {\n-      try {\n-        return walSplitter.rsServices.getConnection().getAdmin().getDescriptor(tableName);\n-      } catch (IOException e) {\n-        LOG.warn(\"Failed to get table descriptor for {}\", tableName, e);\n-      }\n-    }\n-    LOG.info(\"Failed getting {} table descriptor from master; trying local\", tableName);\n-    try {\n-      return walSplitter.tableDescriptors.get(tableName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0MjQzNg=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjE3NzE1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzo1NTozOVrOGnYW-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxODo0NDoyN1rOGpHX3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0Njc0Nw==", "bodyText": "Should we try to get the TableDescriptor first? If it is not possible, then we fallback to write generic HFiles.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r443946747", "createdAt": "2020-06-23T03:55:39Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk2NzkwNg==", "bodyText": "In Jira, I added a comment about making sure we will compact all these tiny HFiles created as part of WAL split. If we can make sure that part, I would say it ok to create these tiny files with out any table specific things like compression/DBE etc. Anyways we know all these files are going to get compacted and rewritten once we open the region.\nAs of now we are not sure whether or when these tiny files will get compacted. In such case I would +1 ur ask. Do this HFile create with defaults as a fall back only.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r443967906", "createdAt": "2020-06-23T05:21:12Z", "author": {"login": "anoopsjohn"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0Njc0Nw=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMjMxNg==", "bodyText": "Asking the Master for the table descriptor first and if it fails, read the fs is what we had before this patch. Problem was that default configuration had it that the RPC query wasn't timing out before the below happened:\n2020-06-18 19:53:54,175 ERROR [main] master.HMasterCommandLine: Master exiting\njava.lang.RuntimeException: Master not initialized after 200000ms\nI could dick around w/ timings inside here so RPC timed out sooner. It uses the server's Connection. I suppose I could make a new Connection everytime we want to query a Table schema and configure it to try once only but that is a lot of work to do at recovery time.\nOn the compactions, don't we usually pick up the small files first?", "url": "https://github.com/apache/hbase/pull/1955#discussion_r444302316", "createdAt": "2020-06-23T15:13:35Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0Njc0Nw=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxNDU4Nw==", "bodyText": "Actually the timeout can be set per rpc request on the same connection.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r444314587", "createdAt": "2020-06-23T15:30:07Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0Njc0Nw=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2NTU5Nw==", "bodyText": "bq. Actually the timeout can be set per rpc request on the same connection.\nYou are right. Could try this later.", "url": "https://github.com/apache/hbase/pull/1955#discussion_r445765597", "createdAt": "2020-06-25T18:44:27Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -191,50 +186,22 @@ public boolean keepRegionEvent(Entry entry) {\n     return false;\n   }\n \n+  /**\n+   * @return Returns a base HFile without compressions or encodings; good enough for recovery", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk0Njc0Nw=="}, "originalCommit": {"oid": "5b676884cf4b91e040adf30c8abf7629671cb846"}, "originalPosition": 28}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2857, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}