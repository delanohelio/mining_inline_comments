{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkyNzQ0MDgz", "number": 2454, "reviewThreads": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNDo0MjoyNlrOEoHM6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0OToxMFrOErXqUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNDk2NDg4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNDo0MjoyNlrOHZA-6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxOTo1MjowNlrOHZNUVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MjU1NA==", "bodyText": "nit: private Set<E> delegate ?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r495992554", "createdAt": "2020-09-28T14:42:26Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE5NDY0Ng==", "bodyText": "I chose to make the field of a concrete type because we depend on the semantics provided by that type for the semantics defined by this class. I don't want to accidentally construct some other form of Set, for example, down in putAllFirst.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496194646", "createdAt": "2020-09-28T19:52:06Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MjU1NA=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTEwODc3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNTowNDoxNlrOHZCVPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMTo1MDo1MVrOHZpW8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAxNDY1NQ==", "bodyText": "can we directly use (without local var) this.takeLock.lock() and .unlock() ? or assigning to local variable is for any extra level of thread safety purpose?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496014655", "createdAt": "2020-09-28T15:04:16Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE5OTgwMA==", "bodyText": "This is a code-style that I copied from the reference implementation in LinkedBlockingQueue. I asked some jvm experts about this style and they told me it is done this way, particularly for locks, because the jvm doesn't enforce final on non-static fields. Which means that (1) final fields can be monkeyed with via reflection, but an inline variable declaration cannot be, and (2) a final field cannot be optimized by the compiler, while a final inline variable declaration can.\nTIL.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496199800", "createdAt": "2020-09-28T20:01:57Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAxNDY1NQ=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY1NDA2NA==", "bodyText": "Glad to know this!", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496654064", "createdAt": "2020-09-29T11:50:51Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAxNDY1NQ=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTIxODIzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNToxODo0OFrOHZDWeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo0Mjo1OVrOHZ7-xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzMTM1NA==", "bodyText": "wondering if this operation should require putLock.\nEdit: I see we take full lock (take + put) while putting data in front of the queue (LinkedHashSet copy operation in putAllFirst). As long as putAllFirst is the only way to update front elements of the queue, we are good.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496031354", "createdAt": "2020-09-28T15:18:48Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {\n+    takeLock.unlock();\n+    putLock.unlock();\n+  }\n+\n+  /**\n+   * Inserts the specified element at the tail of the queue, if it's not already present.\n+   *\n+   * @param e the element to add\n+   */\n+  public void put(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.add(e);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified element at the head of the queue.\n+   *\n+   * @param e the element to add\n+   */\n+  public void putFirst(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+    putAllFirst(Collections.singleton(e));\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the tail of the queue. Any elements already present in\n+   * the queue are ignored.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAll(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.addAll(c);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the head of the queue.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAllFirst(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    fullyLock();\n+    try {\n+      final LinkedHashSet<E> copy = new LinkedHashSet<>(c.size() + delegate.size());\n+      copy.addAll(c);\n+      copy.addAll(delegate);\n+      delegate = copy;\n+    } finally {\n+      fullyUnlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Retrieves and removes the head of this queue, waiting if necessary\n+   * until an element becomes available.\n+   *\n+   * @return the head of this queue\n+   * @throws InterruptedException if interrupted while waiting\n+   */\n+  public E take() throws InterruptedException {\n+    E x;\n+    takeLock.lockInterruptibly();\n+    try {\n+      while (delegate.isEmpty()) {\n+        notEmpty.await();\n+      }\n+      final Iterator<E> iter = delegate.iterator();\n+      x = iter.next();\n+      iter.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwMjUzMg==", "bodyText": "putAllFirst (and thus putFirst) and clear acquire both locks. putAllFirst is the only method that actually modifies the reference pointed to by delegate... maybe the takeLock isn't strictly needed for clear.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496202532", "createdAt": "2020-09-28T20:07:27Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {\n+    takeLock.unlock();\n+    putLock.unlock();\n+  }\n+\n+  /**\n+   * Inserts the specified element at the tail of the queue, if it's not already present.\n+   *\n+   * @param e the element to add\n+   */\n+  public void put(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.add(e);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified element at the head of the queue.\n+   *\n+   * @param e the element to add\n+   */\n+  public void putFirst(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+    putAllFirst(Collections.singleton(e));\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the tail of the queue. Any elements already present in\n+   * the queue are ignored.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAll(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.addAll(c);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the head of the queue.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAllFirst(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    fullyLock();\n+    try {\n+      final LinkedHashSet<E> copy = new LinkedHashSet<>(c.size() + delegate.size());\n+      copy.addAll(c);\n+      copy.addAll(delegate);\n+      delegate = copy;\n+    } finally {\n+      fullyUnlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Retrieves and removes the head of this queue, waiting if necessary\n+   * until an element becomes available.\n+   *\n+   * @return the head of this queue\n+   * @throws InterruptedException if interrupted while waiting\n+   */\n+  public E take() throws InterruptedException {\n+    E x;\n+    takeLock.lockInterruptibly();\n+    try {\n+      while (delegate.isEmpty()) {\n+        notEmpty.await();\n+      }\n+      final Iterator<E> iter = delegate.iterator();\n+      x = iter.next();\n+      iter.remove();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzMTM1NA=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1OTE3Mw==", "bodyText": "Agree, only putLock should be enough for clear.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496959173", "createdAt": "2020-09-29T18:42:59Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {\n+    takeLock.unlock();\n+    putLock.unlock();\n+  }\n+\n+  /**\n+   * Inserts the specified element at the tail of the queue, if it's not already present.\n+   *\n+   * @param e the element to add\n+   */\n+  public void put(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.add(e);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified element at the head of the queue.\n+   *\n+   * @param e the element to add\n+   */\n+  public void putFirst(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+    putAllFirst(Collections.singleton(e));\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the tail of the queue. Any elements already present in\n+   * the queue are ignored.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAll(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.addAll(c);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the head of the queue.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAllFirst(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    fullyLock();\n+    try {\n+      final LinkedHashSet<E> copy = new LinkedHashSet<>(c.size() + delegate.size());\n+      copy.addAll(c);\n+      copy.addAll(delegate);\n+      delegate = copy;\n+    } finally {\n+      fullyUnlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Retrieves and removes the head of this queue, waiting if necessary\n+   * until an element becomes available.\n+   *\n+   * @return the head of this queue\n+   * @throws InterruptedException if interrupted while waiting\n+   */\n+  public E take() throws InterruptedException {\n+    E x;\n+    takeLock.lockInterruptibly();\n+    try {\n+      while (delegate.isEmpty()) {\n+        notEmpty.await();\n+      }\n+      final Iterator<E> iter = delegate.iterator();\n+      x = iter.next();\n+      iter.remove();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzMTM1NA=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTQ0NDIwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjowNzoyMVrOHZFimg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMDoxMDoyNVrOHZN5FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NzIyNg==", "bodyText": "nit: <= 0MB ?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496067226", "createdAt": "2020-09-28T16:07:21Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNDA1Mw==", "bodyText": "The condition is examining the value in megabytes, while the warning is communicating to the operator in terms of the configuration point, which is in bytes. Thus, \"MB\" is the correct unit, and after division, 1mb is the minimum effective allowable value.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496204053", "createdAt": "2020-09-28T20:10:25Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NzIyNg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTg5ODgyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNDozNzo0OVrOHZwi0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxOToyODowMFrOHZ92Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc3MTc5Mw==", "bodyText": "Looks like reduce(0, Math::addExact) is preferred here over sum() due to long overflow. If it happens for two regions, perhaps we might get their merge request again? (and again.... in subsequent normalizer runs?)", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496771793", "createdAt": "2020-09-29T14:37:49Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk4OTczNA==", "bodyText": "Yes, if this happens, it's reasonable to assume that it would keep happening. However, given that these are region sizes in megabytes, I think it's highly unlikely that overflow would ever happen. I just chose to use Math::addExact so that we'd find out about it instead of silently operating on an overflow value.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496989734", "createdAt": "2020-09-29T19:28:00Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc3MTc5Mw=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTkxMTM1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNDo0MDoxN1rOHZwqqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwMDowODoxNlrOHazYLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc3MzgwMw==", "bodyText": "nit: logging plan also might be helpful", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496773803", "createdAt": "2020-09-29T14:40:17Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2Njc5OA==", "bodyText": "The plan is logged immediately after. It's all coming from the same thread, so I think it's okay to omit plan from this log line.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r497866798", "createdAt": "2020-10-01T00:08:16Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc3MzgwMw=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTk3MjA1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "isResolved": false, "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNDo1MjowNFrOHZxQtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozNzoxMlrOHeGa3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg==", "bodyText": "Is there a foreseeable plan to start merging multiple regions in same plan?\nRegardless, maybe we want to validate list_size() % 2 == 0 in MergeNormalizationPlan constructor which initializes normalizationTargets?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496783542", "createdAt": "2020-09-29T14:52:04Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk4NzE5Nw==", "bodyText": "Yes, merging more than two regions at a time is coming in HBASE-24419. I started working on it yesterday. And yes, this constructor API for MergeNormalizationPlan will likely change because of it.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496987197", "createdAt": "2020-09-29T19:25:17Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMzcwOA==", "bodyText": "This PR is not now posted, #2490.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498413708", "createdAt": "2020-10-01T17:42:42Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MzgwNw==", "bodyText": "Regardless, maybe we want to validate list_size() % 2 == 0 in MergeNormalizationPlan constructor which initializes normalizationTargets?\n\nI still think that this validation is better suitable for this current PR because we are introducing at least the flexibility to submit variable no of target regions for merging.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499443807", "createdAt": "2020-10-05T08:57:56Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg1OTEzNw==", "bodyText": "See the logic of MergeTableRegionsProcedure#checkRegionsToMerge, that's where validation of any merge request is applied. I think it's not the normalizer's job to verify these parameters. And anyway, the user isn't specifying the regions, they only select a table. Normalizer's algorithm generates the region merge list.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499859137", "createdAt": "2020-10-05T20:39:49Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE5OTc0MA==", "bodyText": "Sure this is not normalizer's job to validate but by keeping target regions as list, we do have a probability of keeping odd no of regions around a while just to get them abandoned later by MergeTableRegionsProcedure#checkRegionsToMerge if we keep it unbounded list. Not that SimpleRegionNormalizer will add odd no of regions, this is just to be treated as a probability.\nHow about List<Pair<NormalizerRegionInfo, NormalizerRegionInfo>> as target regions in MergeNormalizationPlan?\nNew POJO NormalizerRegionInfo can just contain regionInfo and regionSize.\nOr List<MergeNormalizerRegionInfo> where MergeNormalizerRegionInfo can keep firstRegion and secondRegion just like how it was before but this time we can have list of that object.\nThis way, at least we indicate a clear way for clients to send list of pair of regions to merge. Thought?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500199740", "createdAt": "2020-10-06T11:26:45Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNjkxOQ==", "bodyText": "I haven't rebased it yet, but have a look at 450c4d5 on #2490 . The limit to pairs is removed by that patch, so I'll just have to undo the pair-wise API you propose.\nI could remove the use of the list from this patch, go back to using the first and second member variables, if you think that aspect is so critical. My push-back is because, in my opinion, it's not the POJO's responsibility to ensure these invariants, it's the job of the merge system. And the merge system already makes these checks, so let's leave that logic written in one place, and leave it close to where it's consumed.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500536919", "createdAt": "2020-10-06T19:15:11Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDg0MjU5NA==", "bodyText": "I don't have strong opinion for API change since merge system is already taking care of validation.\nYou might prefer adding a small javadoc for Builder.addTarget() explaining to add even no of regions with the sequence in which client wants to merge them?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500842594", "createdAt": "2020-10-07T08:47:06Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMwMzUyNw==", "bodyText": "Sure, let me buff up the javadoc on that class. FYI, technically an odd number of regions can be merged, so long a they're more than 2 and in a contiguous sequence.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501303527", "createdAt": "2020-10-07T20:53:46Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNDUwOQ==", "bodyText": "I extended the class-level javadoc to remind callers about the expectations of the mergeRegions method.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501324509", "createdAt": "2020-10-07T21:37:12Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTk5Nzk4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNDo1NzowM1rOHZxg4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNDo1NzowM1rOHZxg4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4NzY4Mg==", "bodyText": "nit: INFO might be better for plan submit failures for both split and merge.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496787682", "createdAt": "2020-09-29T14:57:03Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));\n+\n+    final long pid;\n+    try {\n+      pid = masterServices.mergeRegions(\n+        infos, false, HConstants.NO_NONCE, HConstants.NO_NONCE);\n+    } catch (IOException e) {\n+      LOG.debug(\"failed to submit plan {}.\", plan, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDk2NDUyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestRegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODowOTozN1rOHZ6zVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODowOTozN1rOHZ6zVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkzOTg2MA==", "bodyText": "nit: newSingleThreadExecutor(factory) to keep it similar with worker pool used in source code?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496939860", "createdAt": "2020-09-29T18:09:37Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestRegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import static java.util.Collections.singletonList;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.comparesEqualTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyBoolean;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.when;\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseCommonTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionInfoBuilder;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.hamcrest.Description;\n+import org.hamcrest.Matcher;\n+import org.hamcrest.StringDescription;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+import org.mockito.Answers;\n+import org.mockito.Mock;\n+import org.mockito.MockitoAnnotations;\n+import org.mockito.junit.MockitoJUnit;\n+import org.mockito.junit.MockitoRule;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * A test over {@link RegionNormalizerWorker}. Being a background thread, the only points of\n+ * interaction we have to this class are its input source ({@link RegionNormalizerWorkQueue} and\n+ * its callbacks invoked against {@link RegionNormalizer} and {@link MasterServices}. The work\n+ * queue is simple enough to use directly; for {@link MasterServices}, use a mock because, as of\n+ * now, the worker only invokes 4 methods.\n+ */\n+@Category({ MasterTests.class, SmallTests.class})\n+public class TestRegionNormalizerWorker {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionNormalizerWorker.class);\n+\n+  @Rule\n+  public TestName testName = new TestName();\n+  @Rule\n+  public TableNameTestRule tableName = new TableNameTestRule();\n+\n+  @Rule\n+  public MockitoRule mockitoRule = MockitoJUnit.rule();\n+\n+  @Mock(answer = Answers.RETURNS_DEEP_STUBS)\n+  private MasterServices masterServices;\n+  @Mock\n+  private RegionNormalizer regionNormalizer;\n+\n+  private HBaseCommonTestingUtility testingUtility;\n+  private RegionNormalizerWorkQueue<TableName> queue;\n+  private ExecutorService workerPool;\n+\n+  private final AtomicReference<Throwable> workerThreadThrowable = new AtomicReference<>();\n+\n+  @Before\n+  public void before() throws Exception {\n+    MockitoAnnotations.initMocks(this);\n+    when(masterServices.skipRegionManagementAction(any())).thenReturn(false);\n+    testingUtility = new HBaseCommonTestingUtility();\n+    queue = new RegionNormalizerWorkQueue<>();\n+    workerThreadThrowable.set(null);\n+\n+    final String threadNameFmt =\n+      TestRegionNormalizerWorker.class.getSimpleName() + \"-\" + testName.getMethodName() + \"-%d\";\n+    final ThreadFactory threadFactory = new ThreadFactoryBuilder()\n+      .setNameFormat(threadNameFmt)\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler((t, e) -> workerThreadThrowable.set(e))\n+      .build();\n+    workerPool = Executors.newFixedThreadPool(1, threadFactory);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDIzOTI3OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzoyNzo1NlrOHbURLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzoyNzo1NlrOHbURLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNTY3OA==", "bodyText": "Moved to RegionNormalizerManager.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498405678", "createdAt": "2020-10-01T17:27:56Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -337,9 +330,6 @@ public void run() {\n   // Tracker for split and merge state\n   private SplitOrMergeTracker splitOrMergeTracker;\n \n-  // Tracker for region normalizer state\n-  private RegionNormalizerTracker regionNormalizerTracker;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDI0MTc4OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzoyODozNFrOHbUSpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzoyODozNFrOHbUSpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNjA1NQ==", "bodyText": "Moved to RegionNormalizerWorker.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498406055", "createdAt": "2020-10-01T17:28:34Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -464,9 +451,6 @@ public void run() {\n   // handle table states\n   private TableStateManager tableStateManager;\n \n-  private long splitPlanCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDI0NDg1OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzoyOToyOVrOHbUUpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxNzo0OVrOHb3RGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNjU2NA==", "bodyText": "Table selection code stays here in HMaster.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498406564", "createdAt": "2020-10-01T17:29:29Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3OTA5Nw==", "bodyText": "Thanks, this is very helpful.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498979097", "createdAt": "2020-10-02T18:17:49Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNjU2NA=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDI0ODI2OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzozMDoyNFrOHbUWuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzozMDoyNFrOHbUWuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNzA5Nw==", "bodyText": "All this moves to RegionNormalizerWorker.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498407097", "createdAt": "2020-10-01T17:30:24Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),\n-        ntfp.getNamespace(), ntfp.getRegex(), ntfp.getTableNames(), false)\n-        .stream()\n-        .map(TableDescriptor::getTableName)\n-        .collect(Collectors.toSet());\n-      final Set<TableName> allEnabledTables =\n-        tableStateManager.getTablesInStates(TableState.State.ENABLED);\n-      final List<TableName> targetTables =\n-        new ArrayList<>(Sets.intersection(matchingTables, allEnabledTables));\n-      Collections.shuffle(targetTables);\n-\n-      final List<Long> submittedPlanProcIds = new ArrayList<>();\n-      for (TableName table : targetTables) {\n-        if (table.isSystemTable()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDI2NTU4OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzozNTo0NFrOHbUh8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzozNTo0NFrOHbUh8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwOTk2OA==", "bodyText": "Plan submission is not handled by the worker, the plans are converted into simple POJOs.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498409968", "createdAt": "2020-10-01T17:35:44Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),\n-        ntfp.getNamespace(), ntfp.getRegex(), ntfp.getTableNames(), false)\n-        .stream()\n-        .map(TableDescriptor::getTableName)\n-        .collect(Collectors.toSet());\n-      final Set<TableName> allEnabledTables =\n-        tableStateManager.getTablesInStates(TableState.State.ENABLED);\n-      final List<TableName> targetTables =\n-        new ArrayList<>(Sets.intersection(matchingTables, allEnabledTables));\n-      Collections.shuffle(targetTables);\n-\n-      final List<Long> submittedPlanProcIds = new ArrayList<>();\n-      for (TableName table : targetTables) {\n-        if (table.isSystemTable()) {\n-          continue;\n-        }\n-        final TableDescriptor tblDesc = getTableDescriptors().get(table);\n-        if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n-          LOG.debug(\n-            \"Skipping table {} because normalization is disabled in its table properties.\", table);\n-          continue;\n-        }\n-\n-        // make one last check that the cluster isn't shutting down before proceeding.\n-        if (skipRegionManagementAction(\"region normalizer\")) {\n-          return false;\n-        }\n-\n-        final List<NormalizationPlan> plans = normalizer.computePlansForTable(table);\n-        if (CollectionUtils.isEmpty(plans)) {\n-          LOG.debug(\"No normalization required for table {}.\", table);\n-          continue;\n-        }\n-\n-        affectedTables++;\n-        // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to\n-        // submit task , so there's no artificial rate-\n-        // limiting of merge/split requests due to this serial loop.\n-        for (NormalizationPlan plan : plans) {\n-          long procId = plan.submit(this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 247}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDI3MTYwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzozNzo0M1rOHbUl3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0NToxMFrOHc2tAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMDk3NA==", "bodyText": "I don't love exposing this up here on MasterServices, but imo, it's better to have the interface here than to expose its innards.\nSuggestions welcome.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498410974", "createdAt": "2020-10-01T17:37:43Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkzNTcwNw==", "bodyText": "What is this? Is it substantial enough to be added to this Interface?\n\n@saintstack I asked myself the very same question. It's this or have a MasterServices instance and cast it to HMaster because we need something kind-of secret.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499935707", "createdAt": "2020-10-06T00:02:56Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMDk3NA=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODQzNA==", "bodyText": "ok", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500018434", "createdAt": "2020-10-06T05:45:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMDk3NA=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDI4NDY4OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzo0MToyN1rOHbUt7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzo0MToyN1rOHbUt7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMzAzNw==", "bodyText": "Moved to RegionNormalizerWorker.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498413037", "createdAt": "2020-10-01T17:41:27Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -203,16 +200,6 @@ public void setMasterServices(final MasterServices masterServices) {\n     this.masterServices = masterServices;\n   }\n \n-  @Override\n-  public void planSkipped(final RegionInfo hri, final PlanType type) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzg2NjExOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODo0ODoyMlrOHb4JmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo0Nzo1OVrOHctO9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk5MzU2MQ==", "bodyText": "Nice comments!", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498993561", "createdAt": "2020-10-02T18:48:22Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "diffHunk": "@@ -1953,20 +1952,27 @@ public SetNormalizerRunningResponse setNormalizerRunning(RpcController controlle\n     rpcPreCheck(\"setNormalizerRunning\");\n \n     // Sets normalizer on/off flag in ZK.\n-    boolean prevValue = master.getRegionNormalizerTracker().isNormalizerOn();\n-    boolean newValue = request.getOn();\n-    try {\n-      master.getRegionNormalizerTracker().setNormalizerOn(newValue);\n-    } catch (KeeperException ke) {\n-      LOG.warn(\"Error flipping normalizer switch\", ke);\n-    }\n+    // TODO: this method is totally broken in terms of atomicity of actions and values read.\n+    //  1. The contract has this RPC returning the previous value. There isn't a ZKUtil method\n+    //     that lets us retrieve the previous value as part of setting a new value, so we simply\n+    //     perform a read before issuing the update. Thus we have a data race opportunity, between\n+    //     when the `prevValue` is read and whatever is actually overwritten.\n+    //  2. Down in `setNormalizerOn`, the call to `createAndWatch` inside of the catch clause can\n+    //     itself fail in the event that the znode already exists. Thus, another data race, between\n+    //     when the initial `setData` call is notified of the absence of the target znode and the\n+    //     subsequent `createAndWatch`, with another client creating said node.\n+    //  That said, there's supposed to be only one active master and thus there's supposed to be\n+    //  only one process with the authority to modify the value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MzI4Nw==", "bodyText": "Thanks. Reading through this code surprised and disappointed me. Only after thinking through the macro-architecture did I realize it's probably safe anyway.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499863287", "createdAt": "2020-10-05T20:47:59Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "diffHunk": "@@ -1953,20 +1952,27 @@ public SetNormalizerRunningResponse setNormalizerRunning(RpcController controlle\n     rpcPreCheck(\"setNormalizerRunning\");\n \n     // Sets normalizer on/off flag in ZK.\n-    boolean prevValue = master.getRegionNormalizerTracker().isNormalizerOn();\n-    boolean newValue = request.getOn();\n-    try {\n-      master.getRegionNormalizerTracker().setNormalizerOn(newValue);\n-    } catch (KeeperException ke) {\n-      LOG.warn(\"Error flipping normalizer switch\", ke);\n-    }\n+    // TODO: this method is totally broken in terms of atomicity of actions and values read.\n+    //  1. The contract has this RPC returning the previous value. There isn't a ZKUtil method\n+    //     that lets us retrieve the previous value as part of setting a new value, so we simply\n+    //     perform a read before issuing the update. Thus we have a data race opportunity, between\n+    //     when the `prevValue` is read and whatever is actually overwritten.\n+    //  2. Down in `setNormalizerOn`, the call to `createAndWatch` inside of the catch clause can\n+    //     itself fail in the event that the znode already exists. Thus, another data race, between\n+    //     when the initial `setData` call is notified of the absence of the target znode and the\n+    //     subsequent `createAndWatch`, with another client creating said node.\n+    //  That said, there's supposed to be only one active master and thus there's supposed to be\n+    //  only one process with the authority to modify the value.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk5MzU2MQ=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDUwNzAwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMDowNToyMVrOHb-RdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMDowNToyMVrOHb-RdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA5Mzg3Ng==", "bodyText": "Nit: can it be moved to the first place? In case it needs to skip action, it will save some some time to look up the table  descriptor.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499093876", "createdAt": "2020-10-03T00:05:21Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDY3NzIyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwNDo1MToyNFrOHb_ogQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTowMTowOVrOHctpxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjE2MQ==", "bodyText": "If I understand this correctly, this ratelimiter.acquire() needs to be moved after the mergeRegions() call. mergeRegions() is an asynchronous call, so while it is being run at the background, rateLimiter.acquire() will blocked for some time before it picks up the next action.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499116161", "createdAt": "2020-10-03T04:51:24Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2OTk1MQ==", "bodyText": "I think this is nice one from latency view point but this is all happening behind the scene by a single thread running continuously and hence, there is no actual API level latency coming into picture.\nHowever, it would be definitely better to get blocked on RL.acquite() while Proc is already submitted.\nI am +1 for this.\nAnd one more nice comment with this very reasoning behind keeping RL.acquire() after masterServices.mergeRegions() and masterServices.splitRegion() would be really great!", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499169951", "createdAt": "2020-10-03T18:17:32Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjE2MQ=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTcyNjQ4OQ==", "bodyText": "Yeah, syntax wise, the latency calculated is for the current action, so it needs to block after the current is submitted.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499726489", "createdAt": "2020-10-05T16:31:20Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjE2MQ=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg3MDE0OA==", "bodyText": "So you're suggesting that the thread's blocking could match more closely the way resources are consumed. By checking on the rate limit after submitting the procedure, the thread is held until after the submitted work as been conceptually processed. I think that makes sense.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499870148", "createdAt": "2020-10-05T21:01:09Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjE2MQ=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDY4MjgyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwNTowNDoxN1rOHb_rOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo1MToyMVrOHctV5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjg1Nw==", "bodyText": "Not sure if there is any existing data structure for this part, have to admit that I did not look very closely into this part.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499116857", "createdAt": "2020-10-03T05:04:17Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2NTA2MA==", "bodyText": "I looked but did not find one. I'd be happy to replace this class with something off-the-shelf.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499865060", "createdAt": "2020-10-05T20:51:21Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjg1Nw=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDc3MzY3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0NjowN1rOHc2uJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0MTo1N1rOHeGi9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODcyNQ==", "bodyText": "Used outside this package? If not, remove the public?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500018725", "createdAt": "2020-10-06T05:46:07Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNjU4Mw==", "bodyText": "I went through all classes/interfaces in the package and left only those necessary at public. The comment on the package javadoc also explains this explicitly.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501326583", "createdAt": "2020-10-07T21:41:57Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODcyNQ=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDc3Mzk3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0NjoxNlrOHc2uUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0NjoxNlrOHc2uUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODc3MQ==", "bodyText": "ditto", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500018771", "createdAt": "2020-10-06T05:46:16Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDc3NzM3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0ODowNFrOHc2wZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMDozMTowMlrOHeEYVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTMwMw==", "bodyText": "Should we use this everywhere? ToStringBuilder? Seems ok.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500019303", "createdAt": "2020-10-06T05:48:04Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {\n+    this.regionInfo = regionInfo;\n+    this.regionSizeMb = regionSizeMb;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public long getRegionSizeMb() {\n+    return regionSizeMb;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NormalizationTarget that = (NormalizationTarget) o;\n+\n+    return new EqualsBuilder()\n+      .append(regionSizeMb, that.regionSizeMb)\n+      .append(regionInfo, that.regionInfo)\n+      .isEquals();\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return new HashCodeBuilder(17, 37)\n+      .append(regionInfo)\n+      .append(regionSizeMb)\n+      .toHashCode();\n+  }\n+\n+  @Override public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMTU3Mw==", "bodyText": "I use it everywhere ;) And HashBuilder, and EqualsBuilder. There's no sense in custom versions of this code in different places, unless we're explicitly maintaining backward compatibility is some way... which I find suspect. Would be super cool if we could enforce the use with static analysis.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500531573", "createdAt": "2020-10-06T19:05:17Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {\n+    this.regionInfo = regionInfo;\n+    this.regionSizeMb = regionSizeMb;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public long getRegionSizeMb() {\n+    return regionSizeMb;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NormalizationTarget that = (NormalizationTarget) o;\n+\n+    return new EqualsBuilder()\n+      .append(regionSizeMb, that.regionSizeMb)\n+      .append(regionInfo, that.regionInfo)\n+      .isEquals();\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return new HashCodeBuilder(17, 37)\n+      .append(regionInfo)\n+      .append(regionSizeMb)\n+      .toHashCode();\n+  }\n+\n+  @Override public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTMwMw=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI5MTA5NA==", "bodyText": "Sounds good", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501291094", "createdAt": "2020-10-07T20:31:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {\n+    this.regionInfo = regionInfo;\n+    this.regionSizeMb = regionSizeMb;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public long getRegionSizeMb() {\n+    return regionSizeMb;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NormalizationTarget that = (NormalizationTarget) o;\n+\n+    return new EqualsBuilder()\n+      .append(regionSizeMb, that.regionSizeMb)\n+      .append(regionInfo, that.regionInfo)\n+      .isEquals();\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return new HashCodeBuilder(17, 37)\n+      .append(regionInfo)\n+      .append(regionSizeMb)\n+      .toHashCode();\n+  }\n+\n+  @Override public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTMwMw=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDc3OTg3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0OTowN1rOHc2x2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0MDo1OVrOHeGhQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTY3Mg==", "bodyText": "Good. Remove this check of started that is outside sync block. We don't come here anyways, the lock is narrow... and check of started under synchronized will always be right.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500019672", "createdAt": "2020-10-06T05:49:07Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMjk2Nw==", "bodyText": "I used a double-checked lock out of habit, to avoid the synchronized if we can. I don't have a strong opinion, and will make the change as you suggest.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500532967", "createdAt": "2020-10-06T19:08:03Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTY3Mg=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNjE0Nw==", "bodyText": "Done.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501326147", "createdAt": "2020-10-07T21:40:59Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTY3Mg=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDc4NTMxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo1MTo1NFrOHc21LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0MDo0MlrOHeGg0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA==", "bodyText": "Ditto on these.\nOr want to use a guava Service? That too much?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500020524", "createdAt": "2020-10-06T05:51:54Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMzE0Mw==", "bodyText": "Or want to use a guava Service?\n\nI don't know it. Let me read.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500533143", "createdAt": "2020-10-06T19:08:23Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI2OTkwNg==", "bodyText": "I toyed a bit with guava Service. ServiceExplained helped. I think it could be done, but it's messy. Mapping this code to Guava Service model, theRegionNormalizerManager is itself a ServiceManager that is composed of the RegionNormalizerTracker, which is always run, and the RegionNormalizerWorker, which is run most of the time, but not always. In a purely Guava + Juice application, this might be accomplished by conditionally injecting the RegionNormalizerWorker, based on the configuration.\nAnother difference is that the RegionNormalizerWorker runs in a background thread, but the RegionNormalizerTracker does not. So using Service to manage the latter results in an extra thread we don't need. Likewise, most of the time, the RegionNormalizerManager will spawn the background thread for the worker, but if we're in safe-mode, that's not required. The most natural base class, AbstractExecutionThreadService, will always spawn a background thread, which is where it runs the instance of RegionNormalizerWorker. However, when in safe-mode, we don't need that thread and we cannot keep the Service instance in the RUNNING state without it.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501269906", "createdAt": "2020-10-07T19:51:15Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI5MDgyNQ==", "bodyText": "Here's a WIP to move over to Guava Service. Let me know if you think it's worth the effort. ndimiduk@7d7bdb4", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501290825", "createdAt": "2020-10-07T20:30:33Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI5MjA0Ng==", "bodyText": "ok. Thanks for taking a look.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501292046", "createdAt": "2020-10-07T20:32:49Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNjAzNA==", "bodyText": "I think it's possible to use Guava Service here, but it'll be a little messy because of encapsulation (or not) of the ZooKeeperTracker and because of the safe-mode master state. Let me leave this for a separate PR, perhaps and overhaul of everything in HMaster to be managed thusly.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501326034", "createdAt": "2020-10-07T21:40:42Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDc4Njc3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo1Mjo0MVrOHc22Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo1Mjo0MVrOHc22Fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDc1OA==", "bodyText": "Local to this package? If so, remove the public?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500020758", "createdAt": "2020-10-06T05:52:41Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDgwMTAyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNjowMDowMlrOHc2-9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMDozMzoxN1rOHeEc-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMzAzMQ==", "bodyText": "If I put in something already in the queue, there'll be one instance only.  The new put moves to the head of the queue?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500023031", "createdAt": "2020-10-06T06:00:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDM4ODM0OQ==", "bodyText": "Since LinkedHashSet is used underneath, the order of element should not change if it is re-inserted.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500388349", "createdAt": "2020-10-06T15:26:20Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMzAzMQ=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNDA0Mg==", "bodyText": "@virajjasani has it right -- if the items is already present the addition is effectively ignored and no state is changed. You can intentionally have an existing entry jump to the front of the queue using putFirst/putAllFirst.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500534042", "createdAt": "2020-10-06T19:09:54Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMzAzMQ=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI5MjI4MQ==", "bodyText": "ok", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501292281", "createdAt": "2020-10-07T20:33:17Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMzAzMQ=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDk2ODM0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterChoreScheduled.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNzowNDoyNVrOHc4jwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwODo1MToyM1rOHdpLTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0ODgzMw==", "bodyText": "Darn it, quite a thing I missed \ud83e\udd26", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500048833", "createdAt": "2020-10-06T07:04:25Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterChoreScheduled.java", "diffHunk": "@@ -114,22 +113,27 @@ public void testDefaultScheduledChores() throws Exception {\n     hbckChoreTestChoreField.testIfChoreScheduled(hbckChore);\n   }\n \n-\n+  /**\n+   * Reflect into the {@link HMaster} instance and find by field name a specified instance\n+   * of {@link ScheduledChore}.\n+   */\n   private static class TestChoreField<E extends ScheduledChore> {\n \n-    private E getChoreObj(String fieldName) throws NoSuchFieldException,\n-        IllegalAccessException {\n-      Field masterField = HMaster.class.getDeclaredField(fieldName);\n-      masterField.setAccessible(true);\n-      E choreFieldVal = (E) masterField.get(hMaster);\n-      return choreFieldVal;\n+    @SuppressWarnings(\"unchecked\")\n+    private E getChoreObj(String fieldName) {\n+      try {\n+        Field masterField = HMaster.class.getDeclaredField(fieldName);\n+        masterField.setAccessible(true);\n+        return (E) masterField.get(hMaster);\n+      } catch (Exception e) {\n+        throw new AssertionError(\n+          \"Unable to retrieve field '\" + fieldName + \"' from HMaster instance.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzODQ1Mw==", "bodyText": "You didn't miss it, you let the exception bubble up, which is a common style in this code base. I just updated the test to have a bit of a friendlier error message, something that might help a confused dev to understand a failure, someone who doesn't know the system under test. I happen to like writing test asserts that include an error message, rather than decrypting a stack trace with generic exception messages and a line number. But @saintstack has given me grief about this habit for as long as he's been reviewing my patches ;)", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500538453", "createdAt": "2020-10-06T19:18:01Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterChoreScheduled.java", "diffHunk": "@@ -114,22 +113,27 @@ public void testDefaultScheduledChores() throws Exception {\n     hbckChoreTestChoreField.testIfChoreScheduled(hbckChore);\n   }\n \n-\n+  /**\n+   * Reflect into the {@link HMaster} instance and find by field name a specified instance\n+   * of {@link ScheduledChore}.\n+   */\n   private static class TestChoreField<E extends ScheduledChore> {\n \n-    private E getChoreObj(String fieldName) throws NoSuchFieldException,\n-        IllegalAccessException {\n-      Field masterField = HMaster.class.getDeclaredField(fieldName);\n-      masterField.setAccessible(true);\n-      E choreFieldVal = (E) masterField.get(hMaster);\n-      return choreFieldVal;\n+    @SuppressWarnings(\"unchecked\")\n+    private E getChoreObj(String fieldName) {\n+      try {\n+        Field masterField = HMaster.class.getDeclaredField(fieldName);\n+        masterField.setAccessible(true);\n+        return (E) masterField.get(hMaster);\n+      } catch (Exception e) {\n+        throw new AssertionError(\n+          \"Unable to retrieve field '\" + fieldName + \"' from HMaster instance.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0ODgzMw=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDg0NTM5MQ==", "bodyText": "Given that this test is fragile in the way we use reflection, having this nice message in error clearly points out what the issue is and dev can just focus on refactor part to ensure chore object that is being refactored is somehow covered in this test. Nice error message \ud83d\udc4d", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500845391", "createdAt": "2020-10-07T08:51:23Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterChoreScheduled.java", "diffHunk": "@@ -114,22 +113,27 @@ public void testDefaultScheduledChores() throws Exception {\n     hbckChoreTestChoreField.testIfChoreScheduled(hbckChore);\n   }\n \n-\n+  /**\n+   * Reflect into the {@link HMaster} instance and find by field name a specified instance\n+   * of {@link ScheduledChore}.\n+   */\n   private static class TestChoreField<E extends ScheduledChore> {\n \n-    private E getChoreObj(String fieldName) throws NoSuchFieldException,\n-        IllegalAccessException {\n-      Field masterField = HMaster.class.getDeclaredField(fieldName);\n-      masterField.setAccessible(true);\n-      E choreFieldVal = (E) masterField.get(hMaster);\n-      return choreFieldVal;\n+    @SuppressWarnings(\"unchecked\")\n+    private E getChoreObj(String fieldName) {\n+      try {\n+        Field masterField = HMaster.class.getDeclaredField(fieldName);\n+        masterField.setAccessible(true);\n+        return (E) masterField.get(hMaster);\n+      } catch (Exception e) {\n+        throw new AssertionError(\n+          \"Unable to retrieve field '\" + fieldName + \"' from HMaster instance.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0ODgzMw=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTA4MDkyOnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozNjowNVrOHeGY4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozNjowNVrOHeGY4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNDAwMQ==", "bodyText": "All this doc moved to the package-level.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501324001", "createdAt": "2020-10-07T21:36:05Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -54,29 +53,9 @@\n  *   <li>Otherwise, for the next region in the chain R1, if R0 + R1 is smaller then S, R0 and R1\n  *     are kindly requested to merge.</li>\n  * </ol>\n- * <p>\n- * The following parameters are configurable:\n- * <ol>\n- *   <li>Whether to split a region as part of normalization. Configuration:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzOTExODkwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/package-info.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0OToxMFrOHeGu5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0OToxMFrOHeGu5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyOTYzOQ==", "bodyText": "nice", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501329639", "createdAt": "2020-10-07T21:49:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/package-info.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * The Region Normalizer subsystem is responsible for coaxing all the regions in a table toward\n+ * a \"normal\" size, according to their storefile size. It does this by splitting regions that\n+ * are significantly larger than the norm, and merging regions that are significantly smaller than\n+ * the norm.\n+ * </p>\n+ * The public interface to the Region Normalizer subsystem is limited to the following classes:\n+ * <ul>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerFactory} provides an\n+ *     entry point for creating an instance of the\n+ *     {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager}.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager} encapsulates\n+ *     the whole Region Normalizer subsystem. You'll find one of these hanging off of the\n+ *     {@link org.apache.hadoop.hbase.master.HMaster}, which uses it to delegate API calls. There\n+ *     is usually only a single instance of this class.\n+ *   </li>\n+ *   <li>\n+ *     Various configuration points that share the common prefix of {@code hbase.normalizer}.\n+ *     <ul>\n+ *       <li>Whether to split a region as part of normalization. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#SPLIT_ENABLED_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_SPLIT_ENABLED}.\n+ *       </li>\n+ *       <li>Whether to merge a region as part of normalization. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MERGE_ENABLED_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MERGE_ENABLED}.\n+ *       </li>\n+ *       <li>The minimum number of regions in a table to consider it for merge normalization.\n+ *         Configuration: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MIN_REGION_COUNT_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MIN_REGION_COUNT}.\n+ *       </li>\n+ *       <li>The minimum age for a region to be considered for a merge, in days. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MERGE_MIN_REGION_AGE_DAYS_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MERGE_MIN_REGION_AGE_DAYS}.\n+ *       </li>\n+ *       <li>The minimum size for a region to be considered for a merge, in whole MBs. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MERGE_MIN_REGION_SIZE_MB_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MERGE_MIN_REGION_SIZE_MB}.\n+ *       </li>\n+ *       <li>The limit on total throughput of the Region Normalizer's actions, in whole MBs. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorker#RATE_LIMIT_BYTES_PER_SEC_KEY},\n+ *         default: unlimited.\n+ *       </li>\n+ *     </ul>\n+ *     <p>\n+ *       To see detailed logging of the application of these configuration values, set the log\n+ *       level for this package to `TRACE`.\n+ *     </p>\n+ *   </li>\n+ * </ul>\n+ * The Region Normalizer subsystem is composed of a handful of related classes:\n+ * <ul>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker} provides a system by\n+ *     which the Normalizer can be disabled at runtime. It currently does this by managing a znode,\n+ *     but this is an implementation detail.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorkQueue} is a\n+ *     {@link java.util.Set}-like {@link java.util.Queue} that permits a single copy of a given\n+ *     work item to exist in the queue at one time. It also provides a facility for a producer to\n+ *     add an item to the front of the line. Consumers are blocked waiting for new work.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerChore} wakes up\n+ *     periodically and schedules new normalization work, adding targets to the queue.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorker} runs in a\n+ *     daemon thread, grabbing work off the queue as is it becomes available.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer} implements the\n+ *     logic for calculating target region sizes and emitting a list of corresponding\n+ *     {@link org.apache.hadoop.hbase.master.normalizer.NormalizationPlan} objects.\n+ *   </li>\n+ * </ul>\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65"}, "originalPosition": 100}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2526, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}