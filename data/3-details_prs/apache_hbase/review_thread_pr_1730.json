{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5MzE0MjE5", "number": 1730, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozMToyOVrOD_kxxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Njo0N1rOEJTvew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTg5NDQ0OnYy", "diffSide": "RIGHT", "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozMToyOVrOGaUAaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozMToyOVrOGaUAaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0Mzk0NA==", "bodyText": "Great.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430243944", "createdAt": "2020-05-26T08:31:29Z", "author": {"login": "infraio"}, "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "diffHunk": "@@ -0,0 +1,122 @@\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+\n+# Heterogeneous Storage for Date Tiered Compaction\n+\n+## Objective\n+\n+Support DateTiredCompaction([HBASE-15181](https://issues.apache.org/jira/browse/HBASE-15181))\n+ for cold and hot data separation, support different storage policies for different time periods\n+ of data to get better performance, for example, we can configure the data of last 1 month in SSD,\n+ and 1 month ago data was in HDD.\n+\n++ Date Tiered Compaction (DTCP) is based on date tiering (date-aware), we hope to support\n+  the separation of cold and hot data, heterogeneous storage. Set different storage\n+  policies (in HDFS) for data in different time windows.\n++ DTCP designs different windows, and we can classify the windows according to\n+  the timestamps of the windows. For example: HOT window, WARM window, COLD window.\n++ DTCP divides storefiles into different windows, and performs minor Compaction within\n+  a time window. The storefile generated by Compaction will use the storage strategy of\n+  this window. For example, if a window is a HOT window, the storefile generated by compaction\n+  can be stored on the SSD. There are already WAL and the entire CF support storage policy\n+  (HBASE-12848, HBASE-14061), our goal is to achieve cold and hot separation in one CF or\n+  a region, using different storage policies.\n+\n+## Definition of hot and cold data\n+\n+Usually the data of the last 3 days can be defined as `HOT data`, hot age = 3 days.\n+ If the timestamp of the data is > (timestamp now - hot age), we think the data is hot data.\n+ Warm age, cold age can be defined in the same way. Only one type of data is allowed.\n+ ```\n+  if timestamp >  (now - hot age) , HOT data\n+  else if timestamp >  (now - warm age), WARM data\n+  else if timestamp >  (now - cold age), COLD data\n+  else  default, COLD data\n+```\n+\n+## Time window\n+When given a time now, it is the time when the compaction occurs. Each window and the size of\n+ the window are automatically calculated by DTCP, and the window boundary is rounded according\n+ to the base size.\n+Assuming that the base window size is 1 hour, and each tier has 3 windows, the current time is\n+ between 12:00 and 13:00. We have defined three types of winow (`HOT, WARM, COLD`). The type of\n+ winodw is determined by the timestamp at the beginning of the window and the timestamp now.\n+As shown in the figure 1 below, the type of each window can be determined by the age range\n+ (hot / warm / cold) where (now - window.startTimestamp) falls. Cold age can not need to be set,\n+ the default Long.MAX, meaning that the window with a very early time stamp belongs to the\n+ cold window.\n+![figure 1](https://raw.githubusercontent.com/pengmq1/images/master/F1-HDTCP.png \"figure 1\")\n+\n+## Example configuration\n+\n+| Configuration Key | value | Note |\n+|:---|:---:|:---|\n+|hbase.hstore.compaction.date.tiered.storage.policy.enable|true|if or not use storage policy for window. Default is false|\n+|hbase.hstore.compaction.date.tiered.hot.window.age.millis|3600000|hot data age\n+|hbase.hstore.compaction.date.tiered.hot.window.storage.policy|ALL_SSD|hot data storage policy, Corresponding HDFS storage policy\n+|hbase.hstore.compaction.date.tiered.warm.window.age.millis|20600000||\n+|hbase.hstore.compaction.date.tiered.warm.window.storage.policy|ONE_SSD||\n+|hbase.hstore.compaction.date.tiered.cold.window.age.millis|Long.MAX||\n+|hbase.hstore.compaction.date.tiered.cold.window.storage.policy|HOT||\n+\n+The original date tiered compaction related configuration has the same meaning and maintains\n+ compatibility.\n+If `hbase.hstore.compaction.date.tiered.storage.policy.enable = false`. DTCP still follows the\n+ original logic and has not changed.\n+\n+## Storage strategy\n+HDFS provides the following storage policies, you can refer to\n+ https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html\n+ \n+|Policy ID | Policy Name | Block Placement (3  replicas)|\n+|:---|:---|:---|\n+|15|Lasy_Persist|RAM_DISK: 1, DISK: 2|\n+|12|All_SSD|SSD: 3|\n+|10|One_SSD|SSD: 1, DISK: 2|\n+|7|Hot (default)|DISK: 3|\n+|5|Warm|DISK: 1, ARCHIVE: 2|\n+|2|Cold|ARCHIVE: 3|\n+\n+Date Tiered Compaction (DTCP) supports the output of multiple storefiles. We hope that these\n+ storefiles can be set with different storage policies (in HDFS). \n+ Therefore, through DateTieredMultiFileWriter to generate different StoreFileWriters with\n+  storage policy to achieve the purpose.\n+  \n+## Why use different child tmp dir \n+Before StoreFileWriter writes a storefile, we can create different dirs in the tmp directory\n+ of the region and set the corresponding storage policy for these dirs. This way\n+  StoreFileWriter can write files to different dirs.  \n+Since **HDFS** does not support the create file with the storage policy parameter\n+ (maybe I am wrong, I did not find the relevant interface on hadoop 2.6), and HDFS cannot\n+ set a storage policy for a file / dir path that does not yet exist. When the compaction ends,\n+ the storefile path must exist at this time, and I set the storage policy to Storefile.  \n+But, in HDFS, when the file is written first, and then the storage policy is set.\n+ The actual storage location of the data does not match the storage policy. For example,\n+ write three copies of a file (1 block) in the HDD, then set storage policy is ALL_SSD,\n+ but the data block will not be moved to the SSD immediately.\n+ \u201cHDFS wont move the file content across different block volumes on rename\u201d. Data movement\n+ requires the HDFS mover tool, or use HDFS SPS\n+ (for details, see https://issues.apache.org/jira/browse/HDFS-10285), so in order to\n+ avoid moving data blocks at the HDFS level, we can set the file parent directory to\n+ the storage policy we need before writing data. The new file automatically inherits the\n+ storage policy of the parent directory, and is written according to the correct disk\n+ type when writing. So as to avoid later data movement.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTg5ODUzOnYy", "diffSide": "RIGHT", "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozMjozNVrOGaUDAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1OToxOFrOGa3zzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NDYwOQ==", "bodyText": "What will happen if the CF config storage policy and enable this feature too?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430244609", "createdAt": "2020-05-26T08:32:35Z", "author": {"login": "infraio"}, "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "diffHunk": "@@ -0,0 +1,122 @@\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+\n+# Heterogeneous Storage for Date Tiered Compaction\n+\n+## Objective\n+\n+Support DateTiredCompaction([HBASE-15181](https://issues.apache.org/jira/browse/HBASE-15181))\n+ for cold and hot data separation, support different storage policies for different time periods\n+ of data to get better performance, for example, we can configure the data of last 1 month in SSD,\n+ and 1 month ago data was in HDD.\n+\n++ Date Tiered Compaction (DTCP) is based on date tiering (date-aware), we hope to support\n+  the separation of cold and hot data, heterogeneous storage. Set different storage\n+  policies (in HDFS) for data in different time windows.\n++ DTCP designs different windows, and we can classify the windows according to\n+  the timestamps of the windows. For example: HOT window, WARM window, COLD window.\n++ DTCP divides storefiles into different windows, and performs minor Compaction within\n+  a time window. The storefile generated by Compaction will use the storage strategy of\n+  this window. For example, if a window is a HOT window, the storefile generated by compaction\n+  can be stored on the SSD. There are already WAL and the entire CF support storage policy\n+  (HBASE-12848, HBASE-14061), our goal is to achieve cold and hot separation in one CF or\n+  a region, using different storage policies.\n+\n+## Definition of hot and cold data\n+\n+Usually the data of the last 3 days can be defined as `HOT data`, hot age = 3 days.\n+ If the timestamp of the data is > (timestamp now - hot age), we think the data is hot data.\n+ Warm age, cold age can be defined in the same way. Only one type of data is allowed.\n+ ```\n+  if timestamp >  (now - hot age) , HOT data\n+  else if timestamp >  (now - warm age), WARM data\n+  else if timestamp >  (now - cold age), COLD data\n+  else  default, COLD data\n+```\n+\n+## Time window\n+When given a time now, it is the time when the compaction occurs. Each window and the size of\n+ the window are automatically calculated by DTCP, and the window boundary is rounded according\n+ to the base size.\n+Assuming that the base window size is 1 hour, and each tier has 3 windows, the current time is\n+ between 12:00 and 13:00. We have defined three types of winow (`HOT, WARM, COLD`). The type of\n+ winodw is determined by the timestamp at the beginning of the window and the timestamp now.\n+As shown in the figure 1 below, the type of each window can be determined by the age range\n+ (hot / warm / cold) where (now - window.startTimestamp) falls. Cold age can not need to be set,\n+ the default Long.MAX, meaning that the window with a very early time stamp belongs to the\n+ cold window.\n+![figure 1](https://raw.githubusercontent.com/pengmq1/images/master/F1-HDTCP.png \"figure 1\")\n+\n+## Example configuration\n+\n+| Configuration Key | value | Note |\n+|:---|:---:|:---|\n+|hbase.hstore.compaction.date.tiered.storage.policy.enable|true|if or not use storage policy for window. Default is false|\n+|hbase.hstore.compaction.date.tiered.hot.window.age.millis|3600000|hot data age\n+|hbase.hstore.compaction.date.tiered.hot.window.storage.policy|ALL_SSD|hot data storage policy, Corresponding HDFS storage policy\n+|hbase.hstore.compaction.date.tiered.warm.window.age.millis|20600000||\n+|hbase.hstore.compaction.date.tiered.warm.window.storage.policy|ONE_SSD||\n+|hbase.hstore.compaction.date.tiered.cold.window.age.millis|Long.MAX||\n+|hbase.hstore.compaction.date.tiered.cold.window.storage.policy|HOT||\n+\n+The original date tiered compaction related configuration has the same meaning and maintains", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMDU0Mg==", "bodyText": "If hbase.hstore.compaction.date.tiered.storage.policy.enable is true, this will override CF config storage policy, and hbase.hstore.block.storage.policy does not work. Because storefile must belong to one window and will use window storage policy", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430830542", "createdAt": "2020-05-27T02:59:18Z", "author": {"login": "pengmq1"}, "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "diffHunk": "@@ -0,0 +1,122 @@\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+\n+# Heterogeneous Storage for Date Tiered Compaction\n+\n+## Objective\n+\n+Support DateTiredCompaction([HBASE-15181](https://issues.apache.org/jira/browse/HBASE-15181))\n+ for cold and hot data separation, support different storage policies for different time periods\n+ of data to get better performance, for example, we can configure the data of last 1 month in SSD,\n+ and 1 month ago data was in HDD.\n+\n++ Date Tiered Compaction (DTCP) is based on date tiering (date-aware), we hope to support\n+  the separation of cold and hot data, heterogeneous storage. Set different storage\n+  policies (in HDFS) for data in different time windows.\n++ DTCP designs different windows, and we can classify the windows according to\n+  the timestamps of the windows. For example: HOT window, WARM window, COLD window.\n++ DTCP divides storefiles into different windows, and performs minor Compaction within\n+  a time window. The storefile generated by Compaction will use the storage strategy of\n+  this window. For example, if a window is a HOT window, the storefile generated by compaction\n+  can be stored on the SSD. There are already WAL and the entire CF support storage policy\n+  (HBASE-12848, HBASE-14061), our goal is to achieve cold and hot separation in one CF or\n+  a region, using different storage policies.\n+\n+## Definition of hot and cold data\n+\n+Usually the data of the last 3 days can be defined as `HOT data`, hot age = 3 days.\n+ If the timestamp of the data is > (timestamp now - hot age), we think the data is hot data.\n+ Warm age, cold age can be defined in the same way. Only one type of data is allowed.\n+ ```\n+  if timestamp >  (now - hot age) , HOT data\n+  else if timestamp >  (now - warm age), WARM data\n+  else if timestamp >  (now - cold age), COLD data\n+  else  default, COLD data\n+```\n+\n+## Time window\n+When given a time now, it is the time when the compaction occurs. Each window and the size of\n+ the window are automatically calculated by DTCP, and the window boundary is rounded according\n+ to the base size.\n+Assuming that the base window size is 1 hour, and each tier has 3 windows, the current time is\n+ between 12:00 and 13:00. We have defined three types of winow (`HOT, WARM, COLD`). The type of\n+ winodw is determined by the timestamp at the beginning of the window and the timestamp now.\n+As shown in the figure 1 below, the type of each window can be determined by the age range\n+ (hot / warm / cold) where (now - window.startTimestamp) falls. Cold age can not need to be set,\n+ the default Long.MAX, meaning that the window with a very early time stamp belongs to the\n+ cold window.\n+![figure 1](https://raw.githubusercontent.com/pengmq1/images/master/F1-HDTCP.png \"figure 1\")\n+\n+## Example configuration\n+\n+| Configuration Key | value | Note |\n+|:---|:---:|:---|\n+|hbase.hstore.compaction.date.tiered.storage.policy.enable|true|if or not use storage policy for window. Default is false|\n+|hbase.hstore.compaction.date.tiered.hot.window.age.millis|3600000|hot data age\n+|hbase.hstore.compaction.date.tiered.hot.window.storage.policy|ALL_SSD|hot data storage policy, Corresponding HDFS storage policy\n+|hbase.hstore.compaction.date.tiered.warm.window.age.millis|20600000||\n+|hbase.hstore.compaction.date.tiered.warm.window.storage.policy|ONE_SSD||\n+|hbase.hstore.compaction.date.tiered.cold.window.age.millis|Long.MAX||\n+|hbase.hstore.compaction.date.tiered.cold.window.storage.policy|HOT||\n+\n+The original date tiered compaction related configuration has the same meaning and maintains", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NDYwOQ=="}, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTg5OTg0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozMzowMFrOGaUD1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowMDowNFrOGa30eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NDgyMg==", "bodyText": "Can be removed.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430244822", "createdAt": "2020-05-26T08:33:00Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "diffHunk": "@@ -38,23 +38,34 @@\n \n   private final boolean needEmptyFile;\n \n+  private final Map<Long, String> lowerBoundariesPolicies;\n+\n   /**\n+   * @param lowerBoundariesPolicies each window to storage policy map.\n    * @param needEmptyFile whether need to create an empty store file if we haven't written out\n    *          anything.\n    */\n-  public DateTieredMultiFileWriter(List<Long> lowerBoundaries, boolean needEmptyFile) {\n+  public DateTieredMultiFileWriter(List<Long> lowerBoundaries,\n+      Map<Long, String> lowerBoundariesPolicies, boolean needEmptyFile) {\n     for (Long lowerBoundary : lowerBoundaries) {\n       lowerBoundary2Writer.put(lowerBoundary, null);\n     }\n     this.needEmptyFile = needEmptyFile;\n+    this.lowerBoundariesPolicies = lowerBoundariesPolicies;\n   }\n \n   @Override\n   public void append(Cell cell) throws IOException {\n     Map.Entry<Long, StoreFileWriter> entry = lowerBoundary2Writer.floorEntry(cell.getTimestamp());\n     StoreFileWriter writer = entry.getValue();\n     if (writer == null) {\n-      writer = writerFactory.createWriter();\n+      String lowerBoundaryStoragePolicy = lowerBoundariesPolicies.get(entry.getKey());\n+      if (lowerBoundaryStoragePolicy != null) {\n+        writer = writerFactory.createWriterWithStoragePolicy(lowerBoundaryStoragePolicy);\n+      } else {\n+        writer = writerFactory.createWriter();\n+      }\n+      //writer = writerFactory.createWriter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMDcxMw==", "bodyText": "Will remove later.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430830713", "createdAt": "2020-05-27T03:00:04Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "diffHunk": "@@ -38,23 +38,34 @@\n \n   private final boolean needEmptyFile;\n \n+  private final Map<Long, String> lowerBoundariesPolicies;\n+\n   /**\n+   * @param lowerBoundariesPolicies each window to storage policy map.\n    * @param needEmptyFile whether need to create an empty store file if we haven't written out\n    *          anything.\n    */\n-  public DateTieredMultiFileWriter(List<Long> lowerBoundaries, boolean needEmptyFile) {\n+  public DateTieredMultiFileWriter(List<Long> lowerBoundaries,\n+      Map<Long, String> lowerBoundariesPolicies, boolean needEmptyFile) {\n     for (Long lowerBoundary : lowerBoundaries) {\n       lowerBoundary2Writer.put(lowerBoundary, null);\n     }\n     this.needEmptyFile = needEmptyFile;\n+    this.lowerBoundariesPolicies = lowerBoundariesPolicies;\n   }\n \n   @Override\n   public void append(Cell cell) throws IOException {\n     Map.Entry<Long, StoreFileWriter> entry = lowerBoundary2Writer.floorEntry(cell.getTimestamp());\n     StoreFileWriter writer = entry.getValue();\n     if (writer == null) {\n-      writer = writerFactory.createWriter();\n+      String lowerBoundaryStoragePolicy = lowerBoundariesPolicies.get(entry.getKey());\n+      if (lowerBoundaryStoragePolicy != null) {\n+        writer = writerFactory.createWriterWithStoragePolicy(lowerBoundaryStoragePolicy);\n+      } else {\n+        writer = writerFactory.createWriter();\n+      }\n+      //writer = writerFactory.createWriter();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NDgyMg=="}, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTkwNDUwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozNDoxNlrOGaUGxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowODowOFrOGa38IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NTU3NQ==", "bodyText": "Only log once when region created? If so, can use info log.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430245575", "createdAt": "2020-05-26T08:34:16Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "diffHunk": "@@ -547,6 +553,22 @@ public StoreFileWriter build() throws IOException {\n       CommonFSUtils.setStoragePolicy(this.fs, dir, policyName);\n \n       if (filePath == null) {\n+        // The stored file and related blocks will used the directory based StoragePolicy.\n+        // Because HDFS DistributedFileSystem does not support create files with storage policy\n+        // before version 3.3.0 (See HDFS-13209). Use child dir here is to make stored files\n+        // satisfy the specific storage policy when writing. So as to avoid later data movement.\n+        // We don't want to change whole temp dir to 'fileStoragePolicy'.\n+        if (fileStoragePolicy != null && !fileStoragePolicy.isEmpty()) {\n+          dir = new Path(dir, HConstants.STORAGE_POLICY_PREFIX + fileStoragePolicy);\n+          if (!fs.exists(dir)) {\n+            HRegionFileSystem.mkdirs(fs, conf, dir);\n+          }\n+          CommonFSUtils.setStoragePolicy(this.fs, dir, fileStoragePolicy);\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMjY3Mg==", "bodyText": "yes, I think each type storage policy will create tmp dir once.  CommonFSUtils.setStoragePolicy(this.fs, dir, fileStoragePolicy); should follow HRegionFileSystem.mkdirs?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430832672", "createdAt": "2020-05-27T03:08:08Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "diffHunk": "@@ -547,6 +553,22 @@ public StoreFileWriter build() throws IOException {\n       CommonFSUtils.setStoragePolicy(this.fs, dir, policyName);\n \n       if (filePath == null) {\n+        // The stored file and related blocks will used the directory based StoragePolicy.\n+        // Because HDFS DistributedFileSystem does not support create files with storage policy\n+        // before version 3.3.0 (See HDFS-13209). Use child dir here is to make stored files\n+        // satisfy the specific storage policy when writing. So as to avoid later data movement.\n+        // We don't want to change whole temp dir to 'fileStoragePolicy'.\n+        if (fileStoragePolicy != null && !fileStoragePolicy.isEmpty()) {\n+          dir = new Path(dir, HConstants.STORAGE_POLICY_PREFIX + fileStoragePolicy);\n+          if (!fs.exists(dir)) {\n+            HRegionFileSystem.mkdirs(fs, conf, dir);\n+          }\n+          CommonFSUtils.setStoragePolicy(this.fs, dir, fileStoragePolicy);\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NTU3NQ=="}, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTQxNjM4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQwOTo0MDowNVrOGmq3zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwODowNjozNVrOGm2C7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTQ4NQ==", "bodyText": "ImmutableMap?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443201485", "createdAt": "2020-06-21T09:40:05Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "diffHunk": "@@ -38,23 +38,33 @@\n \n   private final boolean needEmptyFile;\n \n+  private final Map<Long, String> lowerBoundariesPolicies;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzM4NDU1Nw==", "bodyText": "lowerBoundariesPolicies is HashMap", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443384557", "createdAt": "2020-06-22T08:06:35Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "diffHunk": "@@ -38,23 +38,33 @@\n \n   private final boolean needEmptyFile;\n \n+  private final Map<Long, String> lowerBoundariesPolicies;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTQ4NQ=="}, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTQxNzAxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQwOTo0MToxOFrOGmq4LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo0NDoyOVrOGpyqDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTU4MQ==", "bodyText": "Cast it to DateTieredCompactionRequest with a locl variable and then make use of the casted instance?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443201581", "createdAt": "2020-06-21T09:41:18Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine.java", "diffHunk": "@@ -94,6 +94,7 @@ public void forceSelect(CompactionRequestImpl request) {\n         throws IOException {\n       if (request instanceof DateTieredCompactionRequest) {\n         return compactor.compact(request, ((DateTieredCompactionRequest) request).getBoundaries(),\n+          ((DateTieredCompactionRequest) request).getBoundariesPolicies(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzM4Nzc1Nw==", "bodyText": "Yeah... I follow the use of ((DateTieredCompactionRequest) request).getBoundaries() previous line of code.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443387757", "createdAt": "2020-06-22T08:12:57Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine.java", "diffHunk": "@@ -94,6 +94,7 @@ public void forceSelect(CompactionRequestImpl request) {\n         throws IOException {\n       if (request instanceof DateTieredCompactionRequest) {\n         return compactor.compact(request, ((DateTieredCompactionRequest) request).getBoundaries(),\n+          ((DateTieredCompactionRequest) request).getBoundariesPolicies(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTU4MQ=="}, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NDc2NQ==", "bodyText": "Use a local variable and no need to cast twice...", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446474765", "createdAt": "2020-06-27T02:44:29Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine.java", "diffHunk": "@@ -94,6 +94,7 @@ public void forceSelect(CompactionRequestImpl request) {\n         throws IOException {\n       if (request instanceof DateTieredCompactionRequest) {\n         return compactor.compact(request, ((DateTieredCompactionRequest) request).getBoundaries(),\n+          ((DateTieredCompactionRequest) request).getBoundariesPolicies(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTU4MQ=="}, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTQxNzk4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQwOTo0MzowMFrOGmq4uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwODoxMzoxMFrOGm2P3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTcyMQ==", "bodyText": "Just use Strings.isNullOrEmpty in guava.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443201721", "createdAt": "2020-06-21T09:43:00Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "diffHunk": "@@ -547,6 +553,20 @@ public StoreFileWriter build() throws IOException {\n       CommonFSUtils.setStoragePolicy(this.fs, dir, policyName);\n \n       if (filePath == null) {\n+        // The stored file and related blocks will used the directory based StoragePolicy.\n+        // Because HDFS DistributedFileSystem does not support create files with storage policy\n+        // before version 3.3.0 (See HDFS-13209). Use child dir here is to make stored files\n+        // satisfy the specific storage policy when writing. So as to avoid later data movement.\n+        // We don't want to change whole temp dir to 'fileStoragePolicy'.\n+        if (fileStoragePolicy != null && !fileStoragePolicy.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzM4Nzg3MA==", "bodyText": "OK", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443387870", "createdAt": "2020-06-22T08:13:10Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "diffHunk": "@@ -547,6 +553,20 @@ public StoreFileWriter build() throws IOException {\n       CommonFSUtils.setStoragePolicy(this.fs, dir, policyName);\n \n       if (filePath == null) {\n+        // The stored file and related blocks will used the directory based StoragePolicy.\n+        // Because HDFS DistributedFileSystem does not support create files with storage policy\n+        // before version 3.3.0 (See HDFS-13209). Use child dir here is to make stored files\n+        // satisfy the specific storage policy when writing. So as to avoid later data movement.\n+        // We don't want to change whole temp dir to 'fileStoragePolicy'.\n+        if (fileStoragePolicy != null && !fileStoragePolicy.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTcyMQ=="}, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTk1NjExOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1MDo0N1rOGpyr7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1MDo0N1rOGpyr7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTI0Nw==", "bodyText": "Method name start with \"test\"? incomingWindowHot => testIncomingWindowHot", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446475247", "createdAt": "2020-06-27T02:50:47Z", "author": {"login": "infraio"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration;\n+import org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category({ RegionServerTests.class, SmallTests.class })\n+public class TestDateTieredCompactionPolicyHeterogeneousStorage\n+    extends AbstractTestDateTieredCompactionPolicy {\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestDateTieredCompactionPolicyHeterogeneousStorage.class);\n+  public static final String HOT_WINDOW_SP = \"ALL_SSD\";\n+  public static final String WARM_WINDOW_SP = \"ONE_SSD\";\n+  public static final String COLD_WINDOW_SP = \"HOT\";\n+\n+  @Override\n+  protected void config() {\n+    super.config();\n+\n+    // Set up policy\n+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY,\n+      \"org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine\");\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_MAX_AGE_MILLIS_KEY, 100);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_INCOMING_WINDOW_MIN_KEY, 3);\n+    conf.setLong(ExponentialCompactionWindowFactory.BASE_WINDOW_MILLIS_KEY, 6);\n+    conf.setInt(ExponentialCompactionWindowFactory.WINDOWS_PER_TIER_KEY, 4);\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_SINGLE_OUTPUT_FOR_MINOR_COMPACTION_KEY,\n+      false);\n+\n+    // Special settings for compaction policy per window\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MIN_KEY, 2);\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MAX_KEY, 12);\n+    this.conf.setFloat(CompactionConfiguration.HBASE_HSTORE_COMPACTION_RATIO_KEY, 1.2F);\n+\n+    conf.setInt(HStore.BLOCKING_STOREFILES_KEY, 20);\n+    conf.setLong(HConstants.MAJOR_COMPACTION_PERIOD, 5);\n+\n+    // Set Storage Policy for different type window\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_STORAGE_POLICY_ENABLE_KEY, true);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_AGE_MILLIS_KEY, 6);\n+    conf.set(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_STORAGE_POLICY_KEY, HOT_WINDOW_SP);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_AGE_MILLIS_KEY, 12);\n+    conf.set(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_STORAGE_POLICY_KEY, WARM_WINDOW_SP);\n+    conf.set(CompactionConfiguration.DATE_TIERED_COLD_WINDOW_STORAGE_POLICY_KEY, COLD_WINDOW_SP);\n+  }\n+\n+  /**\n+   * Test for incoming window and is HOT window\n+   * window start >= now - hot age\n+   * @throws IOException with error\n+   */\n+  @Test\n+  public void incomingWindowHot() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTk1Nzk3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Mjo1NFrOGpysww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwNzoxOToyN1rOGp7DAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTQ1OQ==", "bodyText": "Set storage policy for a file? This is not work before hdfs 3.3.0?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446475459", "createdAt": "2020-06-27T02:52:54Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1564,6 +1568,18 @@ public void deleteChangedReaderObserver(ChangedReadersObserver o) {\n     return sfs;\n   }\n \n+  // Set correct storage policy from the file name of DTCP.\n+  // Rename file will not change the storage policy.\n+  private void setStoragePolicyFromFileName(List<Path> newFiles) throws IOException {\n+    String prefix = HConstants.STORAGE_POLICY_PREFIX;\n+    for (Path newFile : newFiles) {\n+      if (newFile.getParent().getName().startsWith(prefix)) {\n+        CommonFSUtils.setStoragePolicy(fs.getFileSystem(), newFile,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxMjIyNg==", "bodyText": "newFiles\u662f\u5df2\u7ecfcompact\u7ed3\u675f\u5c06\u8981\u88abrename\u5230region\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\uff0c\u4f46\u5b83\u4eec\u7684storage policy\u8fd8\u672a\u6307\u5b9a\uff0c\u6587\u4ef6storage policy\u5c5e\u6027\u4fdd\u5b58\u5728INode\u4e2d\u3002rename\u4e4b\u540e\uff0c\u672a\u6307\u5b9astorage policy\u7684\u6587\u4ef6\u81ea\u52a8\u7ee7\u627f\u65b0\u7684\u7236\u76ee\u5f55\u3002\u5728rename\u4e4b\u524d\u8c03\u7528setStoragePolicy()\u662f\u4e3a\u4e86\u4f7f\u6570\u636e\u548cstorage policy\u4fdd\u6301\u4e00\u81f4\uff0c\u907f\u514dstorage policy\u56e0\u4e3a\u7ee7\u627f\u53d1\u751f\u53d8\u5316\u3002HDFS\u5141\u8bb8\u5bf9\u5df2\u7ecfclose\u7684\u6587\u4ef6\u8bbe\u7f6estorage policy, \u5e76\u4e0d\u4f1a\u53d1\u751f\u6570\u636e\u79fb\u52a8\u3002\u800c\u4e14newFiles\u7684\u6570\u636e\u548c\u4e34\u65f6\u7236\u76ee\u5f55\u7684storage policy\u5df2\u7ecf\u662f\u4e00\u81f4\u7684\u4e86(\u56e0\u4e3a\u5728compact\u4e4b\u524dtmp\u4e0b\u5df2\u521b\u5efa\u4e86\u4e0d\u540c\u5b58\u50a8\u7b56\u7565\u7684\u4e34\u65f6\u76ee\u5f55)\nnewFiles are files that have been compacted and will be renamed to the region directory, but their storage policy has not been specified. The file storage policy attribute is saved in INode. After renaming, files that do not specify a storage policy automatically inherit the new parent directory. Before renaming call to setStoragePolicy() here is to keep the data consistent with the storage policy and avoid the storage policy changing due to inheritance. HDFS allows storage policy to be set on files that have been closed, and no data movement will occur. Moreover, the data of newFiles and the storage policy of tmp parent dir are already consistent (because tmp directories of different storage strategies have been created under tmp dir before compact).", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446612226", "createdAt": "2020-06-28T07:19:27Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1564,6 +1568,18 @@ public void deleteChangedReaderObserver(ChangedReadersObserver o) {\n     return sfs;\n   }\n \n+  // Set correct storage policy from the file name of DTCP.\n+  // Rename file will not change the storage policy.\n+  private void setStoragePolicyFromFileName(List<Path> newFiles) throws IOException {\n+    String prefix = HConstants.STORAGE_POLICY_PREFIX;\n+    for (Path newFile : newFiles) {\n+      if (newFile.getParent().getName().startsWith(prefix)) {\n+        CommonFSUtils.setStoragePolicy(fs.getFileSystem(), newFile,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTQ1OQ=="}, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTk2MDkxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Njo0N1rOGpyuFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwODowNDoxOVrOGp7TwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTc5OQ==", "bodyText": "Explain more about the unit test? What is the different between this and other tests? And there are some duplicate code in these test methods?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446475799", "createdAt": "2020-06-27T02:56:47Z", "author": {"login": "infraio"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration;\n+import org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category({ RegionServerTests.class, SmallTests.class })\n+public class TestDateTieredCompactionPolicyHeterogeneousStorage\n+    extends AbstractTestDateTieredCompactionPolicy {\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestDateTieredCompactionPolicyHeterogeneousStorage.class);\n+  public static final String HOT_WINDOW_SP = \"ALL_SSD\";\n+  public static final String WARM_WINDOW_SP = \"ONE_SSD\";\n+  public static final String COLD_WINDOW_SP = \"HOT\";\n+\n+  @Override\n+  protected void config() {\n+    super.config();\n+\n+    // Set up policy\n+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY,\n+      \"org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine\");\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_MAX_AGE_MILLIS_KEY, 100);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_INCOMING_WINDOW_MIN_KEY, 3);\n+    conf.setLong(ExponentialCompactionWindowFactory.BASE_WINDOW_MILLIS_KEY, 6);\n+    conf.setInt(ExponentialCompactionWindowFactory.WINDOWS_PER_TIER_KEY, 4);\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_SINGLE_OUTPUT_FOR_MINOR_COMPACTION_KEY,\n+      false);\n+\n+    // Special settings for compaction policy per window\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MIN_KEY, 2);\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MAX_KEY, 12);\n+    this.conf.setFloat(CompactionConfiguration.HBASE_HSTORE_COMPACTION_RATIO_KEY, 1.2F);\n+\n+    conf.setInt(HStore.BLOCKING_STOREFILES_KEY, 20);\n+    conf.setLong(HConstants.MAJOR_COMPACTION_PERIOD, 5);\n+\n+    // Set Storage Policy for different type window\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_STORAGE_POLICY_ENABLE_KEY, true);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_AGE_MILLIS_KEY, 6);\n+    conf.set(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_STORAGE_POLICY_KEY, HOT_WINDOW_SP);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_AGE_MILLIS_KEY, 12);\n+    conf.set(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_STORAGE_POLICY_KEY, WARM_WINDOW_SP);\n+    conf.set(CompactionConfiguration.DATE_TIERED_COLD_WINDOW_STORAGE_POLICY_KEY, COLD_WINDOW_SP);\n+  }\n+\n+  /**\n+   * Test for incoming window and is HOT window\n+   * window start >= now - hot age\n+   * @throws IOException with error\n+   */\n+  @Test\n+  public void incomingWindowHot() throws IOException {\n+    long[] minTimestamps = new long[] { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n+    long[] maxTimestamps = new long[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };\n+    long[] sizes = new long[] { 30, 31, 32, 33, 34, 20, 21, 22, 23, 24, 25, 10, 11, 12, 13 };\n+    Map<Long, String> expected = new HashMap<>();\n+    // boundaries = { Long.MIN_VALUE, 12 }\n+    expected.put(12L, HOT_WINDOW_SP);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNjUxMg==", "bodyText": "fix duplicate code", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446616512", "createdAt": "2020-06-28T08:04:19Z", "author": {"login": "pengmq1"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration;\n+import org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category({ RegionServerTests.class, SmallTests.class })\n+public class TestDateTieredCompactionPolicyHeterogeneousStorage\n+    extends AbstractTestDateTieredCompactionPolicy {\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestDateTieredCompactionPolicyHeterogeneousStorage.class);\n+  public static final String HOT_WINDOW_SP = \"ALL_SSD\";\n+  public static final String WARM_WINDOW_SP = \"ONE_SSD\";\n+  public static final String COLD_WINDOW_SP = \"HOT\";\n+\n+  @Override\n+  protected void config() {\n+    super.config();\n+\n+    // Set up policy\n+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY,\n+      \"org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine\");\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_MAX_AGE_MILLIS_KEY, 100);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_INCOMING_WINDOW_MIN_KEY, 3);\n+    conf.setLong(ExponentialCompactionWindowFactory.BASE_WINDOW_MILLIS_KEY, 6);\n+    conf.setInt(ExponentialCompactionWindowFactory.WINDOWS_PER_TIER_KEY, 4);\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_SINGLE_OUTPUT_FOR_MINOR_COMPACTION_KEY,\n+      false);\n+\n+    // Special settings for compaction policy per window\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MIN_KEY, 2);\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MAX_KEY, 12);\n+    this.conf.setFloat(CompactionConfiguration.HBASE_HSTORE_COMPACTION_RATIO_KEY, 1.2F);\n+\n+    conf.setInt(HStore.BLOCKING_STOREFILES_KEY, 20);\n+    conf.setLong(HConstants.MAJOR_COMPACTION_PERIOD, 5);\n+\n+    // Set Storage Policy for different type window\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_STORAGE_POLICY_ENABLE_KEY, true);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_AGE_MILLIS_KEY, 6);\n+    conf.set(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_STORAGE_POLICY_KEY, HOT_WINDOW_SP);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_AGE_MILLIS_KEY, 12);\n+    conf.set(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_STORAGE_POLICY_KEY, WARM_WINDOW_SP);\n+    conf.set(CompactionConfiguration.DATE_TIERED_COLD_WINDOW_STORAGE_POLICY_KEY, COLD_WINDOW_SP);\n+  }\n+\n+  /**\n+   * Test for incoming window and is HOT window\n+   * window start >= now - hot age\n+   * @throws IOException with error\n+   */\n+  @Test\n+  public void incomingWindowHot() throws IOException {\n+    long[] minTimestamps = new long[] { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n+    long[] maxTimestamps = new long[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };\n+    long[] sizes = new long[] { 30, 31, 32, 33, 34, 20, 21, 22, 23, 24, 25, 10, 11, 12, 13 };\n+    Map<Long, String> expected = new HashMap<>();\n+    // boundaries = { Long.MIN_VALUE, 12 }\n+    expected.put(12L, HOT_WINDOW_SP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTc5OQ=="}, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 86}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2990, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}