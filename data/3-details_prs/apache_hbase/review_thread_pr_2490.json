{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2NDE2NjU4", "number": 2490, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODoyNzowNlrOEqbs3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQyMDowODozOFrOEwdMzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTI5NTAyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODoyNzowNlrOHcoxAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODoyNzowNlrOHcoxAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc5MDA4MQ==", "bodyText": "Good. Normalizer is growing-up!", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499790081", "createdAt": "2020-10-05T18:27:06Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -406,11 +396,8 @@ public void run() {\n   private final LockManager lockManager = new LockManager(this);\n \n   private RSGroupBasedLoadBalancer balancer;\n-  // a lock to prevent concurrent normalization actions.\n-  private final ReentrantLock normalizationInProgressLock = new ReentrantLock();\n-  private RegionNormalizer normalizer;\n   private BalancerChore balancerChore;\n-  private RegionNormalizerChore normalizerChore;\n+  private RegionNormalizerManager regionNormalizerManager;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTMwMDY2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODoyODo1M1rOHco0hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjo0NjowNFrOHcwPfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc5MDk4Mg==", "bodyText": "These things have to be public on HMaster?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499790982", "createdAt": "2020-10-05T18:28:53Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -3003,18 +2933,20 @@ public double getAverageLoad() {\n     return regionStates.getAverageLoad();\n   }\n \n-  /*\n-   * @return the count of region split plans executed\n+  /**\n+   * Exposed here for metrics.\n+   * @see RegionNormalizerManager#getSplitPlanCount()\n    */\n   public long getSplitPlanCount() {\n-    return splitPlanCount;\n+    return regionNormalizerManager.getSplitPlanCount();\n   }\n \n-  /*\n-   * @return the count of region merge plans executed\n+  /**\n+   * Exposed here for metrics.\n+   * @see RegionNormalizerManager#getMergePlanCount()\n    */\n   public long getMergePlanCount() {\n-    return mergePlanCount;\n+    return regionNormalizerManager.getMergePlanCount();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 301}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkxMjU3Mw==", "bodyText": "Yeah these are exposed as metrics, all of which hang off of the HMaster instance. I think it's a larger refactor (and probably backward incompatible) to move these various metrics down a level in the metric name dotted-path.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499912573", "createdAt": "2020-10-05T22:46:04Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -3003,18 +2933,20 @@ public double getAverageLoad() {\n     return regionStates.getAverageLoad();\n   }\n \n-  /*\n-   * @return the count of region split plans executed\n+  /**\n+   * Exposed here for metrics.\n+   * @see RegionNormalizerManager#getSplitPlanCount()\n    */\n   public long getSplitPlanCount() {\n-    return splitPlanCount;\n+    return regionNormalizerManager.getSplitPlanCount();\n   }\n \n-  /*\n-   * @return the count of region merge plans executed\n+  /**\n+   * Exposed here for metrics.\n+   * @see RegionNormalizerManager#getMergePlanCount()\n    */\n   public long getMergePlanCount() {\n-    return mergePlanCount;\n+    return regionNormalizerManager.getMergePlanCount();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc5MDk4Mg=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 301}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTM1MDE3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODo0NDoyM1rOHcpT-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjo0Njo0MFrOHcwQMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc5OTAzMw==", "bodyText": "Changing signature but this should be fine since this is IA.Private.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499799033", "createdAt": "2020-10-05T18:44:23Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "diffHunk": "@@ -1953,20 +1952,27 @@ public SetNormalizerRunningResponse setNormalizerRunning(RpcController controlle\n     rpcPreCheck(\"setNormalizerRunning\");\n \n     // Sets normalizer on/off flag in ZK.\n-    boolean prevValue = master.getRegionNormalizerTracker().isNormalizerOn();\n-    boolean newValue = request.getOn();\n-    try {\n-      master.getRegionNormalizerTracker().setNormalizerOn(newValue);\n-    } catch (KeeperException ke) {\n-      LOG.warn(\"Error flipping normalizer switch\", ke);\n-    }\n+    // TODO: this method is totally broken in terms of atomicity of actions and values read.\n+    //  1. The contract has this RPC returning the previous value. There isn't a ZKUtil method\n+    //     that lets us retrieve the previous value as part of setting a new value, so we simply\n+    //     perform a read before issuing the update. Thus we have a data race opportunity, between\n+    //     when the `prevValue` is read and whatever is actually overwritten.\n+    //  2. Down in `setNormalizerOn`, the call to `createAndWatch` inside of the catch clause can\n+    //     itself fail in the event that the znode already exists. Thus, another data race, between\n+    //     when the initial `setData` call is notified of the absence of the target znode and the\n+    //     subsequent `createAndWatch`, with another client creating said node.\n+    //  That said, there's supposed to be only one active master and thus there's supposed to be\n+    //  only one process with the authority to modify the value.\n+    final boolean prevValue = master.getRegionNormalizerManager().isNormalizerOn();\n+    final boolean newValue = request.getOn();\n+    master.getRegionNormalizerManager().setNormalizerOn(newValue);\n     LOG.info(\"{} set normalizerSwitch={}\", master.getClientIdAuditPrefix(), newValue);\n     return SetNormalizerRunningResponse.newBuilder().setPrevNormalizerValue(prevValue).build();\n   }\n \n   @Override\n   public IsNormalizerEnabledResponse isNormalizerEnabled(RpcController controller,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkxMjc1NQ==", "bodyText": "Yes there are a number of changed signatures on methods and classes, all IA.Private.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499912755", "createdAt": "2020-10-05T22:46:40Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "diffHunk": "@@ -1953,20 +1952,27 @@ public SetNormalizerRunningResponse setNormalizerRunning(RpcController controlle\n     rpcPreCheck(\"setNormalizerRunning\");\n \n     // Sets normalizer on/off flag in ZK.\n-    boolean prevValue = master.getRegionNormalizerTracker().isNormalizerOn();\n-    boolean newValue = request.getOn();\n-    try {\n-      master.getRegionNormalizerTracker().setNormalizerOn(newValue);\n-    } catch (KeeperException ke) {\n-      LOG.warn(\"Error flipping normalizer switch\", ke);\n-    }\n+    // TODO: this method is totally broken in terms of atomicity of actions and values read.\n+    //  1. The contract has this RPC returning the previous value. There isn't a ZKUtil method\n+    //     that lets us retrieve the previous value as part of setting a new value, so we simply\n+    //     perform a read before issuing the update. Thus we have a data race opportunity, between\n+    //     when the `prevValue` is read and whatever is actually overwritten.\n+    //  2. Down in `setNormalizerOn`, the call to `createAndWatch` inside of the catch clause can\n+    //     itself fail in the event that the znode already exists. Thus, another data race, between\n+    //     when the initial `setData` call is notified of the absence of the target znode and the\n+    //     subsequent `createAndWatch`, with another client creating said node.\n+    //  That said, there's supposed to be only one active master and thus there's supposed to be\n+    //  only one process with the authority to modify the value.\n+    final boolean prevValue = master.getRegionNormalizerManager().isNormalizerOn();\n+    final boolean newValue = request.getOn();\n+    master.getRegionNormalizerManager().setNormalizerOn(newValue);\n     LOG.info(\"{} set normalizerSwitch={}\", master.getClientIdAuditPrefix(), newValue);\n     return SetNormalizerRunningResponse.newBuilder().setPrevNormalizerValue(prevValue).build();\n   }\n \n   @Override\n   public IsNormalizerEnabledResponse isNormalizerEnabled(RpcController controller,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc5OTAzMw=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTM1MzgxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODo0NToyOFrOHcpWQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODo0NToyOFrOHcpWQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc5OTYxNw==", "bodyText": "What is this?  Is it substantial enough to be added to this Interface?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499799617", "createdAt": "2020-10-05T18:45:28Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTM2MTgwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODo0Nzo0M1rOHcpbLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjo0Nzo0OVrOHcwRhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwMDg3Nw==", "bodyText": "merge? You mean split?\ngit blame doesn't explain why this is here?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499800877", "createdAt": "2020-10-05T18:47:43Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java", "diffHunk": "@@ -570,8 +569,10 @@ private void preSplitRegion(final MasterProcedureEnv env)\n     try {\n       env.getMasterServices().getMasterQuotaManager().onRegionSplit(this.getParentRegion());\n     } catch (QuotaExceededException e) {\n-      env.getMasterServices().getRegionNormalizer().planSkipped(this.getParentRegion(),\n-          NormalizationPlan.PlanType.SPLIT);\n+      // TODO: why is this here? merge requests can be submitted by actors other than the normalizer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkxMzA5Mg==", "bodyText": "Sorry, this is a copy-paste error in the comment.\nI added it because these callbacks seem to me to be only partially implemented.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499913092", "createdAt": "2020-10-05T22:47:49Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java", "diffHunk": "@@ -570,8 +569,10 @@ private void preSplitRegion(final MasterProcedureEnv env)\n     try {\n       env.getMasterServices().getMasterQuotaManager().onRegionSplit(this.getParentRegion());\n     } catch (QuotaExceededException e) {\n-      env.getMasterServices().getRegionNormalizer().planSkipped(this.getParentRegion(),\n-          NormalizationPlan.PlanType.SPLIT);\n+      // TODO: why is this here? merge requests can be submitted by actors other than the normalizer", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwMDg3Nw=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTM2OTMxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODo0OTo1N1rOHcpf0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODo0OTo1N1rOHcpf0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwMjA2Nw==", "bodyText": "method is createNormalizeService but it returns a 'manager'.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499802067", "createdAt": "2020-10-05T18:49:57Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerFactory.java", "diffHunk": "@@ -32,13 +35,30 @@\n   private RegionNormalizerFactory() {\n   }\n \n+  public static RegionNormalizerManager createNormalizerService(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTQzMzY5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTowOTowM1rOHcqIbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMzo1MzoxNlrOHcxevw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjQ2MQ==", "bodyText": "These ok?\nFindBugs\u2122 is licenced under the LGPL. Copyright \u00a9 2006 University of Maryland.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499812461", "createdAt": "2020-10-05T19:09:03Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkxMzM5OQ==", "bodyText": "It's on our class path... if it's LGPL, why is it there?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499913399", "createdAt": "2020-10-05T22:48:44Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjQ2MQ=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkyMDY1NA==", "bodyText": "These classes are on our classpath and come from the clean-room implementation of FindBugs, https://github.com/stephenc/findbugs-annotations, via the com.github.stephenc.findbugs:findbugs-annotations:1.3.9-1 dependency. We're okay here.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499920654", "createdAt": "2020-10-05T23:12:01Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjQ2MQ=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkzMjg2Mw==", "bodyText": "good", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499932863", "createdAt": "2020-10-05T23:53:16Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjQ2MQ=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTQzNjQ4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTowOTo1NVrOHcqKLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDowODozOVrOHcxv8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjkxMA==", "bodyText": "Check it already started or that silly?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499812910", "createdAt": "2020-10-05T19:09:55Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    regionNormalizerTracker.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkzNzI2NQ==", "bodyText": "Now guarding this on a \"startStopLock\".", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499937265", "createdAt": "2020-10-06T00:08:39Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    regionNormalizerTracker.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMjkxMA=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTQzNzQxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOToxMDoxM1rOHcqKwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDowODo0NFrOHcxwBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMzA1OA==", "bodyText": "Ok if already stopped? idempotent?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499813058", "createdAt": "2020-10-05T19:10:13Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    regionNormalizerTracker.start();\n+    if (worker != null) {\n+      // worker will be null when master is in maintenance mode.\n+      pool.submit(worker);\n+    }\n+  }\n+\n+  public void stop() {\n+    pool.shutdownNow(); // shutdownNow to interrupt the worker thread sitting on `take()`", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkzNzI4NQ==", "bodyText": "Now guarding this on a \"startStopLock\".", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499937285", "createdAt": "2020-10-06T00:08:44Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    regionNormalizerTracker.start();\n+    if (worker != null) {\n+      // worker will be null when master is in maintenance mode.\n+      pool.submit(worker);\n+    }\n+  }\n+\n+  public void stop() {\n+    pool.shutdownNow(); // shutdownNow to interrupt the worker thread sitting on `take()`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxMzA1OA=="}, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTQ0MzM1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOToxMjowMVrOHcqOcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOToxMjowMVrOHcqOcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgxNDAwMg==", "bodyText": "Got this far. Will be back to review more.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r499814002", "createdAt": "2020-10-05T19:12:01Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    regionNormalizerTracker.start();\n+    if (worker != null) {\n+      // worker will be null when master is in maintenance mode.\n+      pool.submit(worker);\n+    }\n+  }\n+\n+  public void stop() {\n+    pool.shutdownNow(); // shutdownNow to interrupt the worker thread sitting on `take()`\n+    regionNormalizerTracker.stop();\n+  }\n+\n+  public RegionNormalizerChore getRegionNormalizerChore() {\n+    return regionNormalizerChore;\n+  }\n+\n+  /**\n+   * Return {@code true} if region normalizer is on, {@code false} otherwise\n+   */\n+  public boolean isNormalizerOn() {\n+    return regionNormalizerTracker.isNormalizerOn();\n+  }\n+\n+  /**\n+   * Set region normalizer on/off\n+   * @param normalizerOn whether normalizer should be on or off\n+   */\n+  public void setNormalizerOn(boolean normalizerOn) {\n+    try {\n+      regionNormalizerTracker.setNormalizerOn(normalizerOn);\n+    } catch (KeeperException e) {\n+      LOG.warn(\"Error flipping normalizer switch\", e);\n+    }\n+  }\n+\n+  /**\n+   * Call-back for the case where plan couldn't be executed due to constraint violation,\n+   * such as namespace quota.\n+   * @param type type of plan that was skipped.\n+   */\n+  public void planSkipped(NormalizationPlan.PlanType type) {\n+    // TODO: this appears to be used only for testing.\n+    if (worker != null) {\n+      worker.planSkipped(type);\n+    }\n+  }\n+\n+  /**\n+   * Retrieve a count of the number of times plans of type {@code type} were submitted but skipped.\n+   * @param type type of plan for which skipped count is to be returned\n+   */\n+  public long getSkippedCount(NormalizationPlan.PlanType type) {\n+    // TODO: this appears to be used only for testing.\n+    return worker == null ? 0 : worker.getSkippedCount(type);\n+  }\n+\n+  /**\n+   * Return the number of times a {@link SplitNormalizationPlan} has been submitted.\n+   */\n+  public long getSplitPlanCount() {\n+    return worker == null ? 0 : worker.getSplitPlanCount();\n+  }\n+\n+  /**\n+   * Return the number of times a {@link MergeNormalizationPlan} has been submitted.\n+   */\n+  public long getMergePlanCount() {\n+    return worker == null ? 0 : worker.getMergePlanCount();\n+  }\n+\n+  /**\n+   * Submit tables for normalization.\n+   * @param tables   a list of tables to submit.\n+   * @param priority {@code true} when these requested tables should skip to the front of the queue.\n+   * @return {@code true} when work was queued, {@code false} otherwise.\n+   */\n+  public boolean normalizeRegions(List<TableName> tables, boolean priority) {\n+    if (workQueue == null) {\n+      return false;\n+    }\n+    if (priority) {\n+      workQueue.putAllFirst(tables);\n+    } else {\n+      workQueue.putAll(tables);\n+    }\n+    return true;\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "450c4d5e1865577f85af1a9751567cc121d55d00"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0NzA4NDc4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNjo1OTo1OFrOHfR2Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNjo1OTo1OFrOHfR2Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2MDMzMA==", "bodyText": "Upgraded this datatype to a long because this is the type returned by the region size info we get from AM.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r502560330", "createdAt": "2020-10-09T16:59:58Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -124,10 +126,10 @@ private static Period parseMergeMinRegionAge(final Configuration conf) {\n     return Period.ofDays(settledValue);\n   }\n \n-  private static int parseMergeMinRegionSizeMb(final Configuration conf) {\n-    final int parsedValue =\n-      conf.getInt(MERGE_MIN_REGION_SIZE_MB_KEY, DEFAULT_MERGE_MIN_REGION_SIZE_MB);\n-    final int settledValue = Math.max(0, parsedValue);\n+  private static long parseMergeMinRegionSizeMb(final Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa59808c63f092ee0a60d6e02625d6168a22f828"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0NzA4NzI5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNzowMDozOFrOHfR3uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNzowMDozOFrOHfR3uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2MDY5OQ==", "bodyText": "The meat of this patch is this new nested loop.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r502560699", "createdAt": "2020-10-09T17:00:38Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -315,35 +317,61 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n    * towards target average or target region count.\n    */\n   private List<NormalizationPlan> computeMergeNormalizationPlans(final NormalizeContext ctx) {\n-    if (ctx.getTableRegions().size() < minRegionCount) {\n+    if (isEmpty(ctx.getTableRegions()) || ctx.getTableRegions().size() < minRegionCount) {\n       LOG.debug(\"Table {} has {} regions, required min number of regions for normalizer to run\"\n         + \" is {}, not computing merge plans.\", ctx.getTableName(), ctx.getTableRegions().size(),\n         minRegionCount);\n       return Collections.emptyList();\n     }\n \n-    final double avgRegionSizeMb = ctx.getAverageRegionSizeMb();\n+    final long avgRegionSizeMb = (long) ctx.getAverageRegionSizeMb();\n+    if (avgRegionSizeMb < mergeMinRegionSizeMb) {\n+      return Collections.emptyList();\n+    }\n     LOG.debug(\"Computing normalization plan for table {}. average region size: {}, number of\"\n       + \" regions: {}.\", ctx.getTableName(), avgRegionSizeMb, ctx.getTableRegions().size());\n \n-    final List<NormalizationPlan> plans = new ArrayList<>();\n-    for (int candidateIdx = 0; candidateIdx < ctx.getTableRegions().size() - 1; candidateIdx++) {\n-      final RegionInfo current = ctx.getTableRegions().get(candidateIdx);\n-      final RegionInfo next = ctx.getTableRegions().get(candidateIdx + 1);\n-      if (skipForMerge(ctx.getRegionStates(), current)\n-        || skipForMerge(ctx.getRegionStates(), next)) {\n-        continue;\n+    // this nested loop walks the table's region chain once, looking for contiguous sequences of\n+    // regions that meet the criteria for merge. The outer loop tracks the starting point of the\n+    // next sequence, the inner loop looks for the end of that sequence. A single sequence becomes\n+    // an instance of MergeNormalizationPlan.\n+\n+    final List<NormalizationPlan> plans = new LinkedList<>();\n+    final List<NormalizationTarget> rangeMembers = new LinkedList<>();\n+    long sumRangeMembersSizeMb;\n+    int current = 0;\n+    for (int rangeStart = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa59808c63f092ee0a60d6e02625d6168a22f828"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0NzA5NTU3OnYy", "diffSide": "RIGHT", "path": "hbase-common/src/test/java/org/apache/hadoop/hbase/MatcherPredicate.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNzowMzoxNlrOHfR9Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNzowMzoxNlrOHfR9Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2MjA2Ng==", "bodyText": "I realized all my waiting in tests (1) use conditions that are shipped in the hamcrest library (2) have failure messages that look like failed Matcher explanations.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r502562066", "createdAt": "2020-10-09T17:03:16Z", "author": {"login": "ndimiduk"}, "path": "hbase-common/src/test/java/org/apache/hadoop/hbase/MatcherPredicate.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.function.Supplier;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.hamcrest.Description;\n+import org.hamcrest.Matcher;\n+import org.hamcrest.StringDescription;\n+\n+/**\n+ * An implementation of {@link Waiter.ExplainingPredicate} that uses Hamcrest {@link Matcher} for both", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa59808c63f092ee0a60d6e02625d6168a22f828"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MjQ1NTE5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQyMDowODozOFrOHmCOcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQyMDo1OTo1MVrOHmFIzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NDQwMA==", "bodyText": "In this case, shall we check if there is already something in rangeMembers and add it to the plan?", "url": "https://github.com/apache/hbase/pull/2490#discussion_r509644400", "createdAt": "2020-10-21T20:08:38Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -315,35 +316,60 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n    * towards target average or target region count.\n    */\n   private List<NormalizationPlan> computeMergeNormalizationPlans(final NormalizeContext ctx) {\n-    if (ctx.getTableRegions().size() < minRegionCount) {\n+    if (isEmpty(ctx.getTableRegions()) || ctx.getTableRegions().size() < minRegionCount) {\n       LOG.debug(\"Table {} has {} regions, required min number of regions for normalizer to run\"\n         + \" is {}, not computing merge plans.\", ctx.getTableName(), ctx.getTableRegions().size(),\n         minRegionCount);\n       return Collections.emptyList();\n     }\n \n-    final double avgRegionSizeMb = ctx.getAverageRegionSizeMb();\n+    final long avgRegionSizeMb = (long) ctx.getAverageRegionSizeMb();\n+    if (avgRegionSizeMb < mergeMinRegionSizeMb) {\n+      return Collections.emptyList();\n+    }\n     LOG.debug(\"Computing normalization plan for table {}. average region size: {}, number of\"\n       + \" regions: {}.\", ctx.getTableName(), avgRegionSizeMb, ctx.getTableRegions().size());\n \n-    final List<NormalizationPlan> plans = new ArrayList<>();\n-    for (int candidateIdx = 0; candidateIdx < ctx.getTableRegions().size() - 1; candidateIdx++) {\n-      final RegionInfo current = ctx.getTableRegions().get(candidateIdx);\n-      final RegionInfo next = ctx.getTableRegions().get(candidateIdx + 1);\n-      if (skipForMerge(ctx.getRegionStates(), current)\n-        || skipForMerge(ctx.getRegionStates(), next)) {\n-        continue;\n+    // this nested loop walks the table's region chain once, looking for contiguous sequences of\n+    // regions that meet the criteria for merge. The outer loop tracks the starting point of the\n+    // next sequence, the inner loop looks for the end of that sequence. A single sequence becomes\n+    // an instance of MergeNormalizationPlan.\n+\n+    final List<NormalizationPlan> plans = new LinkedList<>();\n+    final List<NormalizationTarget> rangeMembers = new LinkedList<>();\n+    long sumRangeMembersSizeMb;\n+    int current = 0;\n+    for (int rangeStart = 0;\n+         rangeStart < ctx.getTableRegions().size() - 1 && current < ctx.getTableRegions().size();) {\n+      // walk the region chain looking for contiguous sequences of regions that can be merged.\n+      rangeMembers.clear();\n+      sumRangeMembersSizeMb = 0;\n+      for (current = rangeStart; current < ctx.getTableRegions().size(); current++) {\n+        final RegionInfo regionInfo = ctx.getTableRegions().get(current);\n+        final long regionSizeMb = getRegionSizeMB(regionInfo);\n+        if (skipForMerge(ctx.getRegionStates(), regionInfo)) {\n+          // this region cannot participate in a range. resume the outer loop.\n+          rangeStart = Math.max(current, rangeStart + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b936da6332b69f0f629811b579635bf85dbbe624"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0ODA5MQ==", "bodyText": "Meant for the case that  region 1, 2, 3, 4, 5\nLet say 1 and 2 are in rangeMembers, but 3 needs to be skipped. In this case, 1,2 needs to be added to plans.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r509648091", "createdAt": "2020-10-21T20:11:16Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -315,35 +316,60 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n    * towards target average or target region count.\n    */\n   private List<NormalizationPlan> computeMergeNormalizationPlans(final NormalizeContext ctx) {\n-    if (ctx.getTableRegions().size() < minRegionCount) {\n+    if (isEmpty(ctx.getTableRegions()) || ctx.getTableRegions().size() < minRegionCount) {\n       LOG.debug(\"Table {} has {} regions, required min number of regions for normalizer to run\"\n         + \" is {}, not computing merge plans.\", ctx.getTableName(), ctx.getTableRegions().size(),\n         minRegionCount);\n       return Collections.emptyList();\n     }\n \n-    final double avgRegionSizeMb = ctx.getAverageRegionSizeMb();\n+    final long avgRegionSizeMb = (long) ctx.getAverageRegionSizeMb();\n+    if (avgRegionSizeMb < mergeMinRegionSizeMb) {\n+      return Collections.emptyList();\n+    }\n     LOG.debug(\"Computing normalization plan for table {}. average region size: {}, number of\"\n       + \" regions: {}.\", ctx.getTableName(), avgRegionSizeMb, ctx.getTableRegions().size());\n \n-    final List<NormalizationPlan> plans = new ArrayList<>();\n-    for (int candidateIdx = 0; candidateIdx < ctx.getTableRegions().size() - 1; candidateIdx++) {\n-      final RegionInfo current = ctx.getTableRegions().get(candidateIdx);\n-      final RegionInfo next = ctx.getTableRegions().get(candidateIdx + 1);\n-      if (skipForMerge(ctx.getRegionStates(), current)\n-        || skipForMerge(ctx.getRegionStates(), next)) {\n-        continue;\n+    // this nested loop walks the table's region chain once, looking for contiguous sequences of\n+    // regions that meet the criteria for merge. The outer loop tracks the starting point of the\n+    // next sequence, the inner loop looks for the end of that sequence. A single sequence becomes\n+    // an instance of MergeNormalizationPlan.\n+\n+    final List<NormalizationPlan> plans = new LinkedList<>();\n+    final List<NormalizationTarget> rangeMembers = new LinkedList<>();\n+    long sumRangeMembersSizeMb;\n+    int current = 0;\n+    for (int rangeStart = 0;\n+         rangeStart < ctx.getTableRegions().size() - 1 && current < ctx.getTableRegions().size();) {\n+      // walk the region chain looking for contiguous sequences of regions that can be merged.\n+      rangeMembers.clear();\n+      sumRangeMembersSizeMb = 0;\n+      for (current = rangeStart; current < ctx.getTableRegions().size(); current++) {\n+        final RegionInfo regionInfo = ctx.getTableRegions().get(current);\n+        final long regionSizeMb = getRegionSizeMB(regionInfo);\n+        if (skipForMerge(ctx.getRegionStates(), regionInfo)) {\n+          // this region cannot participate in a range. resume the outer loop.\n+          rangeStart = Math.max(current, rangeStart + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NDQwMA=="}, "originalCommit": {"oid": "b936da6332b69f0f629811b579635bf85dbbe624"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0OTk5MA==", "bodyText": "Oh, nvm, it is taken care of, after the break, there is a check to see if there are enough members which deserves a plan.", "url": "https://github.com/apache/hbase/pull/2490#discussion_r509649990", "createdAt": "2020-10-21T20:12:26Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -315,35 +316,60 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n    * towards target average or target region count.\n    */\n   private List<NormalizationPlan> computeMergeNormalizationPlans(final NormalizeContext ctx) {\n-    if (ctx.getTableRegions().size() < minRegionCount) {\n+    if (isEmpty(ctx.getTableRegions()) || ctx.getTableRegions().size() < minRegionCount) {\n       LOG.debug(\"Table {} has {} regions, required min number of regions for normalizer to run\"\n         + \" is {}, not computing merge plans.\", ctx.getTableName(), ctx.getTableRegions().size(),\n         minRegionCount);\n       return Collections.emptyList();\n     }\n \n-    final double avgRegionSizeMb = ctx.getAverageRegionSizeMb();\n+    final long avgRegionSizeMb = (long) ctx.getAverageRegionSizeMb();\n+    if (avgRegionSizeMb < mergeMinRegionSizeMb) {\n+      return Collections.emptyList();\n+    }\n     LOG.debug(\"Computing normalization plan for table {}. average region size: {}, number of\"\n       + \" regions: {}.\", ctx.getTableName(), avgRegionSizeMb, ctx.getTableRegions().size());\n \n-    final List<NormalizationPlan> plans = new ArrayList<>();\n-    for (int candidateIdx = 0; candidateIdx < ctx.getTableRegions().size() - 1; candidateIdx++) {\n-      final RegionInfo current = ctx.getTableRegions().get(candidateIdx);\n-      final RegionInfo next = ctx.getTableRegions().get(candidateIdx + 1);\n-      if (skipForMerge(ctx.getRegionStates(), current)\n-        || skipForMerge(ctx.getRegionStates(), next)) {\n-        continue;\n+    // this nested loop walks the table's region chain once, looking for contiguous sequences of\n+    // regions that meet the criteria for merge. The outer loop tracks the starting point of the\n+    // next sequence, the inner loop looks for the end of that sequence. A single sequence becomes\n+    // an instance of MergeNormalizationPlan.\n+\n+    final List<NormalizationPlan> plans = new LinkedList<>();\n+    final List<NormalizationTarget> rangeMembers = new LinkedList<>();\n+    long sumRangeMembersSizeMb;\n+    int current = 0;\n+    for (int rangeStart = 0;\n+         rangeStart < ctx.getTableRegions().size() - 1 && current < ctx.getTableRegions().size();) {\n+      // walk the region chain looking for contiguous sequences of regions that can be merged.\n+      rangeMembers.clear();\n+      sumRangeMembersSizeMb = 0;\n+      for (current = rangeStart; current < ctx.getTableRegions().size(); current++) {\n+        final RegionInfo regionInfo = ctx.getTableRegions().get(current);\n+        final long regionSizeMb = getRegionSizeMB(regionInfo);\n+        if (skipForMerge(ctx.getRegionStates(), regionInfo)) {\n+          // this region cannot participate in a range. resume the outer loop.\n+          rangeStart = Math.max(current, rangeStart + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NDQwMA=="}, "originalCommit": {"oid": "b936da6332b69f0f629811b579635bf85dbbe624"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY5MjExMQ==", "bodyText": "This line of questions convinces me you really read the code. Thank you for the thoughtful review!", "url": "https://github.com/apache/hbase/pull/2490#discussion_r509692111", "createdAt": "2020-10-21T20:59:51Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -315,35 +316,60 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n    * towards target average or target region count.\n    */\n   private List<NormalizationPlan> computeMergeNormalizationPlans(final NormalizeContext ctx) {\n-    if (ctx.getTableRegions().size() < minRegionCount) {\n+    if (isEmpty(ctx.getTableRegions()) || ctx.getTableRegions().size() < minRegionCount) {\n       LOG.debug(\"Table {} has {} regions, required min number of regions for normalizer to run\"\n         + \" is {}, not computing merge plans.\", ctx.getTableName(), ctx.getTableRegions().size(),\n         minRegionCount);\n       return Collections.emptyList();\n     }\n \n-    final double avgRegionSizeMb = ctx.getAverageRegionSizeMb();\n+    final long avgRegionSizeMb = (long) ctx.getAverageRegionSizeMb();\n+    if (avgRegionSizeMb < mergeMinRegionSizeMb) {\n+      return Collections.emptyList();\n+    }\n     LOG.debug(\"Computing normalization plan for table {}. average region size: {}, number of\"\n       + \" regions: {}.\", ctx.getTableName(), avgRegionSizeMb, ctx.getTableRegions().size());\n \n-    final List<NormalizationPlan> plans = new ArrayList<>();\n-    for (int candidateIdx = 0; candidateIdx < ctx.getTableRegions().size() - 1; candidateIdx++) {\n-      final RegionInfo current = ctx.getTableRegions().get(candidateIdx);\n-      final RegionInfo next = ctx.getTableRegions().get(candidateIdx + 1);\n-      if (skipForMerge(ctx.getRegionStates(), current)\n-        || skipForMerge(ctx.getRegionStates(), next)) {\n-        continue;\n+    // this nested loop walks the table's region chain once, looking for contiguous sequences of\n+    // regions that meet the criteria for merge. The outer loop tracks the starting point of the\n+    // next sequence, the inner loop looks for the end of that sequence. A single sequence becomes\n+    // an instance of MergeNormalizationPlan.\n+\n+    final List<NormalizationPlan> plans = new LinkedList<>();\n+    final List<NormalizationTarget> rangeMembers = new LinkedList<>();\n+    long sumRangeMembersSizeMb;\n+    int current = 0;\n+    for (int rangeStart = 0;\n+         rangeStart < ctx.getTableRegions().size() - 1 && current < ctx.getTableRegions().size();) {\n+      // walk the region chain looking for contiguous sequences of regions that can be merged.\n+      rangeMembers.clear();\n+      sumRangeMembersSizeMb = 0;\n+      for (current = rangeStart; current < ctx.getTableRegions().size(); current++) {\n+        final RegionInfo regionInfo = ctx.getTableRegions().get(current);\n+        final long regionSizeMb = getRegionSizeMB(regionInfo);\n+        if (skipForMerge(ctx.getRegionStates(), regionInfo)) {\n+          // this region cannot participate in a range. resume the outer loop.\n+          rangeStart = Math.max(current, rangeStart + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NDQwMA=="}, "originalCommit": {"oid": "b936da6332b69f0f629811b579635bf85dbbe624"}, "originalPosition": 119}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2545, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}