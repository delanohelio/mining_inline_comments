{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkyNzQ0MDgz", "number": 2454, "title": "HBASE-24628 Region normalizer now respects a rate limit", "bodyText": "Implement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava RateLimiter to perform the\nresource accounting. RateLimiter works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided package-info.java for\nan overview of this new structure.\nIntroduces a new configuration,\nhbase.normalizer.throughput.max_bytes_per_sec, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued 1_000_000. Supports values configured using the\nhuman-readable suffixes honored by Configuration.getLongBytes", "createdAt": "2020-09-24T23:10:02Z", "url": "https://github.com/apache/hbase/pull/2454", "merged": true, "mergeCommit": {"oid": "78ae1f176d4215dcc34067ed25d786a4fcd4d888"}, "closed": true, "closedAt": "2020-10-08T22:45:09Z", "author": {"login": "ndimiduk"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdMKgzAABqjM4MDU1MzAzNTY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQpilBABqjM4NTc2NTYyOTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ce8f9e8e14168a4d57ae42e24db714bdb5befd56", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/ce8f9e8e14168a4d57ae42e24db714bdb5befd56", "committedDate": "2020-09-24T23:08:05Z", "message": "HBASE-24628 WIP Normalizer needs a throttle\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure."}, "afterCommit": {"oid": "5e7a9aff0b6d4641ab7e18f253fabdfdb03d97bc", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/5e7a9aff0b6d4641ab7e18f253fabdfdb03d97bc", "committedDate": "2020-09-25T00:18:40Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5e7a9aff0b6d4641ab7e18f253fabdfdb03d97bc", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/5e7a9aff0b6d4641ab7e18f253fabdfdb03d97bc", "committedDate": "2020-09-25T00:18:40Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}, "afterCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/6863e334b907f1835b7166ecd27c64ff832a9d64", "committedDate": "2020-09-25T22:14:14Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3NTkxMDc0", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-497591074", "createdAt": "2020-09-28T14:42:26Z", "commit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNDo0MjoyNlrOHZA-6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjowNzoyMVrOHZFimg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MjU1NA==", "bodyText": "nit: private Set<E> delegate ?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r495992554", "createdAt": "2020-09-28T14:42:26Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAxNDY1NQ==", "bodyText": "can we directly use (without local var) this.takeLock.lock() and .unlock() ? or assigning to local variable is for any extra level of thread safety purpose?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496014655", "createdAt": "2020-09-28T15:04:16Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzMTM1NA==", "bodyText": "wondering if this operation should require putLock.\nEdit: I see we take full lock (take + put) while putting data in front of the queue (LinkedHashSet copy operation in putAllFirst). As long as putAllFirst is the only way to update front elements of the queue, we are good.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496031354", "createdAt": "2020-09-28T15:18:48Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {\n+    takeLock.unlock();\n+    putLock.unlock();\n+  }\n+\n+  /**\n+   * Inserts the specified element at the tail of the queue, if it's not already present.\n+   *\n+   * @param e the element to add\n+   */\n+  public void put(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.add(e);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified element at the head of the queue.\n+   *\n+   * @param e the element to add\n+   */\n+  public void putFirst(E e) {\n+    if (e == null) {\n+      throw new NullPointerException();\n+    }\n+    putAllFirst(Collections.singleton(e));\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the tail of the queue. Any elements already present in\n+   * the queue are ignored.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAll(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    putLock.lock();\n+    try {\n+      delegate.addAll(c);\n+    } finally {\n+      putLock.unlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Inserts the specified elements at the head of the queue.\n+   *\n+   * @param c the elements to add\n+   */\n+  public void putAllFirst(Collection<? extends E> c) {\n+    if (c == null) {\n+      throw new NullPointerException();\n+    }\n+\n+    fullyLock();\n+    try {\n+      final LinkedHashSet<E> copy = new LinkedHashSet<>(c.size() + delegate.size());\n+      copy.addAll(c);\n+      copy.addAll(delegate);\n+      delegate = copy;\n+    } finally {\n+      fullyUnlock();\n+    }\n+\n+    if (!delegate.isEmpty()) {\n+      signalNotEmpty();\n+    }\n+  }\n+\n+  /**\n+   * Retrieves and removes the head of this queue, waiting if necessary\n+   * until an element becomes available.\n+   *\n+   * @return the head of this queue\n+   * @throws InterruptedException if interrupted while waiting\n+   */\n+  public E take() throws InterruptedException {\n+    E x;\n+    takeLock.lockInterruptibly();\n+    try {\n+      while (delegate.isEmpty()) {\n+        notEmpty.await();\n+      }\n+      final Iterator<E> iter = delegate.iterator();\n+      x = iter.next();\n+      iter.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NzIyNg==", "bodyText": "nit: <= 0MB ?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496067226", "createdAt": "2020-09-28T16:07:21Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4NTQyNzAy", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-498542702", "createdAt": "2020-09-29T14:37:49Z", "commit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNDozNzo0OVrOHZwi0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODowOTozN1rOHZ6zVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc3MTc5Mw==", "bodyText": "Looks like reduce(0, Math::addExact) is preferred here over sum() due to long overflow. If it happens for two regions, perhaps we might get their merge request again? (and again.... in subsequent normalizer runs?)", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496771793", "createdAt": "2020-09-29T14:37:49Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc3MzgwMw==", "bodyText": "nit: logging plan also might be helpful", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496773803", "createdAt": "2020-09-29T14:40:17Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg==", "bodyText": "Is there a foreseeable plan to start merging multiple regions in same plan?\nRegardless, maybe we want to validate list_size() % 2 == 0 in MergeNormalizationPlan constructor which initializes normalizationTargets?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496783542", "createdAt": "2020-09-29T14:52:04Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4NzY4Mg==", "bodyText": "nit: INFO might be better for plan submit failures for both split and merge.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496787682", "createdAt": "2020-09-29T14:57:03Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));\n+\n+    final long pid;\n+    try {\n+      pid = masterServices.mergeRegions(\n+        infos, false, HConstants.NO_NONCE, HConstants.NO_NONCE);\n+    } catch (IOException e) {\n+      LOG.debug(\"failed to submit plan {}.\", plan, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkzOTg2MA==", "bodyText": "nit: newSingleThreadExecutor(factory) to keep it similar with worker pool used in source code?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r496939860", "createdAt": "2020-09-29T18:09:37Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/normalizer/TestRegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import static java.util.Collections.singletonList;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.comparesEqualTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyBoolean;\n+import static org.mockito.ArgumentMatchers.anyLong;\n+import static org.mockito.Mockito.when;\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseCommonTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionInfoBuilder;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.hamcrest.Description;\n+import org.hamcrest.Matcher;\n+import org.hamcrest.StringDescription;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+import org.mockito.Answers;\n+import org.mockito.Mock;\n+import org.mockito.MockitoAnnotations;\n+import org.mockito.junit.MockitoJUnit;\n+import org.mockito.junit.MockitoRule;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * A test over {@link RegionNormalizerWorker}. Being a background thread, the only points of\n+ * interaction we have to this class are its input source ({@link RegionNormalizerWorkQueue} and\n+ * its callbacks invoked against {@link RegionNormalizer} and {@link MasterServices}. The work\n+ * queue is simple enough to use directly; for {@link MasterServices}, use a mock because, as of\n+ * now, the worker only invokes 4 methods.\n+ */\n+@Category({ MasterTests.class, SmallTests.class})\n+public class TestRegionNormalizerWorker {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionNormalizerWorker.class);\n+\n+  @Rule\n+  public TestName testName = new TestName();\n+  @Rule\n+  public TableNameTestRule tableName = new TableNameTestRule();\n+\n+  @Rule\n+  public MockitoRule mockitoRule = MockitoJUnit.rule();\n+\n+  @Mock(answer = Answers.RETURNS_DEEP_STUBS)\n+  private MasterServices masterServices;\n+  @Mock\n+  private RegionNormalizer regionNormalizer;\n+\n+  private HBaseCommonTestingUtility testingUtility;\n+  private RegionNormalizerWorkQueue<TableName> queue;\n+  private ExecutorService workerPool;\n+\n+  private final AtomicReference<Throwable> workerThreadThrowable = new AtomicReference<>();\n+\n+  @Before\n+  public void before() throws Exception {\n+    MockitoAnnotations.initMocks(this);\n+    when(masterServices.skipRegionManagementAction(any())).thenReturn(false);\n+    testingUtility = new HBaseCommonTestingUtility();\n+    queue = new RegionNormalizerWorkQueue<>();\n+    workerThreadThrowable.set(null);\n+\n+    final String threadNameFmt =\n+      TestRegionNormalizerWorker.class.getSimpleName() + \"-\" + testName.getMethodName() + \"-%d\";\n+    final ThreadFactory threadFactory = new ThreadFactoryBuilder()\n+      .setNameFormat(threadNameFmt)\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler((t, e) -> workerThreadThrowable.set(e))\n+      .build();\n+    workerPool = Executors.newFixedThreadPool(1, threadFactory);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 117}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/6863e334b907f1835b7166ecd27c64ff832a9d64", "committedDate": "2020-09-25T22:14:14Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}, "afterCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/1f7dd568616e14c65f734c55b7e9c0ed821635a4", "committedDate": "2020-10-01T00:10:43Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNTY3NjQy", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-500567642", "createdAt": "2020-10-01T17:27:56Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzoyNzo1NlrOHbURLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzo0Mjo0MlrOHbUwjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNTY3OA==", "bodyText": "Moved to RegionNormalizerManager.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498405678", "createdAt": "2020-10-01T17:27:56Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -337,9 +330,6 @@ public void run() {\n   // Tracker for split and merge state\n   private SplitOrMergeTracker splitOrMergeTracker;\n \n-  // Tracker for region normalizer state\n-  private RegionNormalizerTracker regionNormalizerTracker;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNjA1NQ==", "bodyText": "Moved to RegionNormalizerWorker.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498406055", "createdAt": "2020-10-01T17:28:34Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -464,9 +451,6 @@ public void run() {\n   // handle table states\n   private TableStateManager tableStateManager;\n \n-  private long splitPlanCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNjU2NA==", "bodyText": "Table selection code stays here in HMaster.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498406564", "createdAt": "2020-10-01T17:29:29Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwNzA5Nw==", "bodyText": "All this moves to RegionNormalizerWorker.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498407097", "createdAt": "2020-10-01T17:30:24Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),\n-        ntfp.getNamespace(), ntfp.getRegex(), ntfp.getTableNames(), false)\n-        .stream()\n-        .map(TableDescriptor::getTableName)\n-        .collect(Collectors.toSet());\n-      final Set<TableName> allEnabledTables =\n-        tableStateManager.getTablesInStates(TableState.State.ENABLED);\n-      final List<TableName> targetTables =\n-        new ArrayList<>(Sets.intersection(matchingTables, allEnabledTables));\n-      Collections.shuffle(targetTables);\n-\n-      final List<Long> submittedPlanProcIds = new ArrayList<>();\n-      for (TableName table : targetTables) {\n-        if (table.isSystemTable()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwOTk2OA==", "bodyText": "Plan submission is not handled by the worker, the plans are converted into simple POJOs.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498409968", "createdAt": "2020-10-01T17:35:44Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -1924,70 +1907,17 @@ public boolean normalizeRegions(final NormalizeTableFilterParams ntfp) throws IO\n       return false;\n     }\n \n-    if (!normalizationInProgressLock.tryLock()) {\n-      // Don't run the normalizer concurrently\n-      LOG.info(\"Normalization already in progress. Skipping request.\");\n-      return true;\n-    }\n-\n-    int affectedTables = 0;\n-    try {\n-      final Set<TableName> matchingTables = getTableDescriptors(new LinkedList<>(),\n-        ntfp.getNamespace(), ntfp.getRegex(), ntfp.getTableNames(), false)\n-        .stream()\n-        .map(TableDescriptor::getTableName)\n-        .collect(Collectors.toSet());\n-      final Set<TableName> allEnabledTables =\n-        tableStateManager.getTablesInStates(TableState.State.ENABLED);\n-      final List<TableName> targetTables =\n-        new ArrayList<>(Sets.intersection(matchingTables, allEnabledTables));\n-      Collections.shuffle(targetTables);\n-\n-      final List<Long> submittedPlanProcIds = new ArrayList<>();\n-      for (TableName table : targetTables) {\n-        if (table.isSystemTable()) {\n-          continue;\n-        }\n-        final TableDescriptor tblDesc = getTableDescriptors().get(table);\n-        if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n-          LOG.debug(\n-            \"Skipping table {} because normalization is disabled in its table properties.\", table);\n-          continue;\n-        }\n-\n-        // make one last check that the cluster isn't shutting down before proceeding.\n-        if (skipRegionManagementAction(\"region normalizer\")) {\n-          return false;\n-        }\n-\n-        final List<NormalizationPlan> plans = normalizer.computePlansForTable(table);\n-        if (CollectionUtils.isEmpty(plans)) {\n-          LOG.debug(\"No normalization required for table {}.\", table);\n-          continue;\n-        }\n-\n-        affectedTables++;\n-        // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to\n-        // submit task , so there's no artificial rate-\n-        // limiting of merge/split requests due to this serial loop.\n-        for (NormalizationPlan plan : plans) {\n-          long procId = plan.submit(this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMDk3NA==", "bodyText": "I don't love exposing this up here on MasterServices, but imo, it's better to have the interface here than to expose its innards.\nSuggestions welcome.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498410974", "createdAt": "2020-10-01T17:37:43Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMzAzNw==", "bodyText": "Moved to RegionNormalizerWorker.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498413037", "createdAt": "2020-10-01T17:41:27Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -203,16 +200,6 @@ public void setMasterServices(final MasterServices masterServices) {\n     this.masterServices = masterServices;\n   }\n \n-  @Override\n-  public void planSkipped(final RegionInfo hri, final PlanType type) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMzcwOA==", "bodyText": "This PR is not now posted, #2490.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498413708", "createdAt": "2020-10-01T17:42:42Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwOTE0Njcw", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-500914670", "createdAt": "2020-10-02T07:33:15Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMzcwMjQ5", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-501370249", "createdAt": "2020-10-02T18:48:22Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODo0ODoyMlrOHb4JmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODo0ODoyMlrOHb4JmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk5MzU2MQ==", "bodyText": "Nice comments!", "url": "https://github.com/apache/hbase/pull/2454#discussion_r498993561", "createdAt": "2020-10-02T18:48:22Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java", "diffHunk": "@@ -1953,20 +1952,27 @@ public SetNormalizerRunningResponse setNormalizerRunning(RpcController controlle\n     rpcPreCheck(\"setNormalizerRunning\");\n \n     // Sets normalizer on/off flag in ZK.\n-    boolean prevValue = master.getRegionNormalizerTracker().isNormalizerOn();\n-    boolean newValue = request.getOn();\n-    try {\n-      master.getRegionNormalizerTracker().setNormalizerOn(newValue);\n-    } catch (KeeperException ke) {\n-      LOG.warn(\"Error flipping normalizer switch\", ke);\n-    }\n+    // TODO: this method is totally broken in terms of atomicity of actions and values read.\n+    //  1. The contract has this RPC returning the previous value. There isn't a ZKUtil method\n+    //     that lets us retrieve the previous value as part of setting a new value, so we simply\n+    //     perform a read before issuing the update. Thus we have a data race opportunity, between\n+    //     when the `prevValue` is read and whatever is actually overwritten.\n+    //  2. Down in `setNormalizerOn`, the call to `createAndWatch` inside of the catch clause can\n+    //     itself fail in the event that the znode already exists. Thus, another data race, between\n+    //     when the initial `setData` call is notified of the absence of the target znode and the\n+    //     subsequent `createAndWatch`, with another client creating said node.\n+    //  That said, there's supposed to be only one active master and thus there's supposed to be\n+    //  only one process with the authority to modify the value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTAwNTcx", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-501500571", "createdAt": "2020-10-03T00:05:21Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMDowNToyMVrOHb-RdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwMDowNToyMVrOHb-RdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA5Mzg3Ng==", "bodyText": "Nit: can it be moved to the first place? In case it needs to skip action, it will save some some time to look up the table  descriptor.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499093876", "createdAt": "2020-10-03T00:05:21Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 154}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTIwODcz", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-501520873", "createdAt": "2020-10-03T04:51:24Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwNDo1MToyNFrOHb_ogQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwNDo1MToyNFrOHb_ogQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjE2MQ==", "bodyText": "If I understand this correctly, this ratelimiter.acquire() needs to be moved after the mergeRegions() call. mergeRegions() is an asynchronous call, so while it is being run at the background, rateLimiter.acquire() will blocked for some time before it picks up the next action.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499116161", "createdAt": "2020-10-03T04:51:24Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.RateLimiter;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n+\n+/**\n+ * Consumes normalization request targets ({@link TableName}s) off the\n+ * {@link RegionNormalizerWorkQueue}, dispatches them to the {@link RegionNormalizer},\n+ * and executes the resulting {@link NormalizationPlan}s.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorker implements Runnable {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerWorker.class);\n+  private static final String RATE_LIMIT_BYTES_PER_SEC_KEY =\n+    \"hbase.normalizer.throughput.max_bytes_per_sec\";\n+  private static final long RATE_UNLIMITED_BYTES = 1_000_000_000_000L; // 1TB/sec\n+\n+  private final MasterServices masterServices;\n+  private final RegionNormalizer regionNormalizer;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RateLimiter rateLimiter;\n+\n+  private final long[] skippedCount;\n+  private long splitPlanCount;\n+  private long mergePlanCount;\n+\n+  public RegionNormalizerWorker(\n+    final Configuration configuration,\n+    final MasterServices masterServices,\n+    final RegionNormalizer regionNormalizer,\n+    final RegionNormalizerWorkQueue<TableName> workQueue\n+  ) {\n+    this.masterServices = masterServices;\n+    this.regionNormalizer = regionNormalizer;\n+    this.workQueue = workQueue;\n+    this.skippedCount = new long[NormalizationPlan.PlanType.values().length];\n+    this.splitPlanCount = 0;\n+    this.mergePlanCount = 0;\n+    this.rateLimiter = loadRateLimiter(configuration);\n+  }\n+\n+  private static RateLimiter loadRateLimiter(final Configuration configuration) {\n+    long rateLimitBytes =\n+      configuration.getLongBytes(RATE_LIMIT_BYTES_PER_SEC_KEY, RATE_UNLIMITED_BYTES);\n+    long rateLimitMbs = rateLimitBytes / 1_000_000L;\n+    if (rateLimitMbs <= 0) {\n+      LOG.warn(\"Configured value {}={} is <= 1MB. Falling back to default.\",\n+        RATE_LIMIT_BYTES_PER_SEC_KEY, rateLimitBytes);\n+      rateLimitBytes = RATE_UNLIMITED_BYTES;\n+      rateLimitMbs = RATE_UNLIMITED_BYTES / 1_000_000L;\n+    }\n+    LOG.info(\"Normalizer rate limit set to {}\",\n+      rateLimitBytes == RATE_UNLIMITED_BYTES ? \"unlimited\" : rateLimitMbs + \" MB/sec\");\n+    return RateLimiter.create(rateLimitMbs);\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#planSkipped(NormalizationPlan.PlanType)\n+   */\n+  void planSkipped(NormalizationPlan.PlanType type) {\n+    synchronized (skippedCount) {\n+      // updates come here via procedure threads, so synchronize access to this counter.\n+      skippedCount[type.ordinal()]++;\n+    }\n+  }\n+\n+  /**\n+   * @see RegionNormalizerManager#getSkippedCount(NormalizationPlan.PlanType)\n+   */\n+  long getSkippedCount(NormalizationPlan.PlanType type) {\n+    return skippedCount[type.ordinal()];\n+  }\n+\n+  /**\n+   * @see HMaster#getSplitPlanCount()\n+   */\n+  long getSplitPlanCount() {\n+    return splitPlanCount;\n+  }\n+\n+  /**\n+   * @see HMaster#getMergePlanCount()\n+   */\n+  long getMergePlanCount() {\n+    return mergePlanCount;\n+  }\n+\n+  @Override\n+  public void run() {\n+    while (true) {\n+      if (Thread.interrupted()) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+      final TableName tableName;\n+      try {\n+        tableName = workQueue.take();\n+      } catch (InterruptedException e) {\n+        LOG.debug(\"interrupt detected. terminating.\");\n+        break;\n+      }\n+\n+      final List<NormalizationPlan> plans = calculatePlans(tableName);\n+      submitPlans(plans);\n+    }\n+  }\n+\n+  private List<NormalizationPlan> calculatePlans(final TableName tableName) {\n+    try {\n+      final TableDescriptor tblDesc = masterServices.getTableDescriptors().get(tableName);\n+      if (tblDesc != null && !tblDesc.isNormalizationEnabled()) {\n+        LOG.debug(\"Skipping table {} because normalization is disabled in its table properties.\",\n+          tableName);\n+        return Collections.emptyList();\n+      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Skipping table {} because unable to access its table descriptor.\", tableName, e);\n+      return Collections.emptyList();\n+    }\n+\n+    if (masterServices.skipRegionManagementAction(\"region normalizer\")) {\n+      return Collections.emptyList();\n+    }\n+\n+    final List<NormalizationPlan> plans = regionNormalizer.computePlansForTable(tableName);\n+    if (CollectionUtils.isEmpty(plans)) {\n+      LOG.debug(\"No normalization required for table {}.\", tableName);\n+      return Collections.emptyList();\n+    }\n+    return plans;\n+  }\n+\n+  private void submitPlans(final List<NormalizationPlan> plans) {\n+    // as of this writing, `plan.submit()` is non-blocking and uses Async Admin APIs to submit\n+    // task, so there's no artificial rate-limiting of merge/split requests due to this serial loop.\n+    for (NormalizationPlan plan : plans) {\n+      switch (plan.getType()) {\n+        case MERGE: {\n+          submitMergePlan((MergeNormalizationPlan) plan);\n+          break;\n+        }\n+        case SPLIT: {\n+          submitSplitPlan((SplitNormalizationPlan) plan);\n+          break;\n+        }\n+        case NONE:\n+          LOG.debug(\"Nothing to do for {} with PlanType=NONE. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+        default:\n+          LOG.warn(\"Plan {} is of an unrecognized PlanType. Ignoring.\", plan);\n+          planSkipped(plan.getType());\n+          break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Interacts with {@link MasterServices} in order to execute a plan.\n+   */\n+  private void submitMergePlan(final MergeNormalizationPlan plan) {\n+    final int totalSizeMb;\n+    try {\n+      final long totalSizeMbLong = plan.getNormalizationTargets()\n+        .stream()\n+        .mapToLong(NormalizationTarget::getRegionSizeMb)\n+        .reduce(0, Math::addExact);\n+      totalSizeMb = Math.toIntExact(totalSizeMbLong);\n+    } catch (ArithmeticException e) {\n+      LOG.debug(\"Sum of merge request size overflows rate limiter data type. {}\", plan);\n+      planSkipped(plan.getType());\n+      return;\n+    }\n+\n+    final RegionInfo[] infos = plan.getNormalizationTargets()\n+      .stream()\n+      .map(NormalizationTarget::getRegionInfo)\n+      .toArray(RegionInfo[]::new);\n+    final long rateLimitedSecs = Math.round(rateLimiter.acquire(Math.max(1, totalSizeMb)));\n+    LOG.debug(\"Rate limiting delayed this operation by {}\", Duration.ofSeconds(rateLimitedSecs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 211}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTIxMzg4", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-501521388", "createdAt": "2020-10-03T05:04:17Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwNTowNDoxN1rOHb_rOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QwNTowNDoxN1rOHb_rOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTExNjg1Nw==", "bodyText": "Not sure if there is any existing data structure for this part, have to admit that I did not look very closely into this part.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499116857", "createdAt": "2020-10-03T05:04:17Z", "author": {"login": "huaxiangsun"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {\n+    delegate = new LinkedHashSet<>();\n+    takeLock = new ReentrantLock();\n+    notEmpty = takeLock.newCondition();\n+    putLock = new ReentrantLock();\n+  }\n+\n+  /**\n+   * Signals a waiting take. Called only from put/offer (which do not\n+   * otherwise ordinarily lock takeLock.)\n+   */\n+  private void signalNotEmpty() {\n+    final ReentrantLock takeLock = this.takeLock;\n+    takeLock.lock();\n+    try {\n+      notEmpty.signal();\n+    } finally {\n+      takeLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Locks to prevent both puts and takes.\n+   */\n+  private void fullyLock() {\n+    putLock.lock();\n+    takeLock.lock();\n+  }\n+\n+  /**\n+   * Unlocks to allow both puts and takes.\n+   */\n+  private void fullyUnlock() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 95}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTIxNDA4", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-501521408", "createdAt": "2020-10-03T05:04:37Z", "commit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/1f7dd568616e14c65f734c55b7e9c0ed821635a4", "committedDate": "2020-10-01T00:10:43Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}, "afterCommit": {"oid": "bcecc6018db2b136eb3a9f9882018c1c8c8a41e7", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/bcecc6018db2b136eb3a9f9882018c1c8c8a41e7", "committedDate": "2020-10-05T21:59:38Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyNDk5Njc5", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-502499679", "createdAt": "2020-10-06T00:02:56Z", "commit": {"oid": "bcecc6018db2b136eb3a9f9882018c1c8c8a41e7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDowMjo1NlrOHcxp2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDowMjo1NlrOHcxp2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkzNTcwNw==", "bodyText": "What is this? Is it substantial enough to be added to this Interface?\n\n@saintstack I asked myself the very same question. It's this or have a MasterServices instance and cast it to HMaster because we need something kind-of secret.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r499935707", "createdAt": "2020-10-06T00:02:56Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMDk3NA=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyNTk4ODMz", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-502598833", "createdAt": "2020-10-06T05:45:10Z", "commit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNTo0NToxMFrOHc2tAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNjowMDowMlrOHc2-9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODQzNA==", "bodyText": "ok", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500018434", "createdAt": "2020-10-06T05:45:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -354,6 +353,13 @@ long splitRegion(\n    */\n   boolean isInMaintenanceMode();\n \n+  /**\n+   * Checks master state before initiating action over region topology.\n+   * @param action the name of the action under consideration, for logging.\n+   * @return {@code true} when the caller should exit early, {@code false} otherwise.\n+   */\n+  boolean skipRegionManagementAction(final String action);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxMDk3NA=="}, "originalCommit": {"oid": "1f7dd568616e14c65f734c55b7e9c0ed821635a4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODcyNQ==", "bodyText": "Used outside this package? If not, remove the public?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500018725", "createdAt": "2020-10-06T05:46:07Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxODc3MQ==", "bodyText": "ditto", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500018771", "createdAt": "2020-10-06T05:46:16Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTMwMw==", "bodyText": "Should we use this everywhere? ToStringBuilder? Seems ok.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500019303", "createdAt": "2020-10-06T05:48:04Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {\n+    this.regionInfo = regionInfo;\n+    this.regionSizeMb = regionSizeMb;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public long getRegionSizeMb() {\n+    return regionSizeMb;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NormalizationTarget that = (NormalizationTarget) o;\n+\n+    return new EqualsBuilder()\n+      .append(regionSizeMb, that.regionSizeMb)\n+      .append(regionInfo, that.regionInfo)\n+      .isEquals();\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return new HashCodeBuilder(17, 37)\n+      .append(regionInfo)\n+      .append(regionSizeMb)\n+      .toHashCode();\n+  }\n+\n+  @Override public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTY3Mg==", "bodyText": "Good. Remove this check of started that is outside sync block. We don't come here anyways, the lock is narrow... and check of started under synchronized will always be right.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500019672", "createdAt": "2020-10-06T05:49:07Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA==", "bodyText": "Ditto on these.\nOr want to use a guava Service? That too much?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500020524", "createdAt": "2020-10-06T05:51:54Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDc1OA==", "bodyText": "Local to this package? If so, remove the public?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500020758", "createdAt": "2020-10-06T05:52:41Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>\n+ *   <li>Work retrieval blocks the calling thread until new work is available, as a\n+ *     {@link BlockingQueue}.</li>\n+ *   <li>Allows a producer to insert an item at the head of the queue, if desired.</li>\n+ * </ul>\n+ * Assumes low-frequency and low-parallelism concurrent access, so protects state using a\n+ * simplistic synchronization strategy.\n+ */\n+@InterfaceAudience.Private\n+class RegionNormalizerWorkQueue<E> {\n+\n+  /** Underlying storage structure that gives us the Set behavior and FIFO retrieval policy. */\n+  private LinkedHashSet<E> delegate;\n+\n+  // the locking structure used here follows the example found in LinkedBlockingQueue. The\n+  // difference is that our locks guard access to `delegate` rather than the head node.\n+\n+  /** Lock held by take, poll, etc */\n+  private final ReentrantLock takeLock;\n+\n+  /** Wait queue for waiting takes */\n+  private final Condition notEmpty;\n+\n+  /** Lock held by put, offer, etc */\n+  private final ReentrantLock putLock;\n+\n+  public RegionNormalizerWorkQueue() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMzAzMQ==", "bodyText": "If I put in something already in the queue, there'll be one instance only.  The new put moves to the head of the queue?", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500023031", "createdAt": "2020-10-06T06:00:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyNjM3NDk1", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-502637495", "createdAt": "2020-10-06T07:04:25Z", "commit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNzowNDoyNVrOHc4jwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNzowNDoyNVrOHc4jwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0ODgzMw==", "bodyText": "Darn it, quite a thing I missed \ud83e\udd26", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500048833", "createdAt": "2020-10-06T07:04:25Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterChoreScheduled.java", "diffHunk": "@@ -114,22 +113,27 @@ public void testDefaultScheduledChores() throws Exception {\n     hbckChoreTestChoreField.testIfChoreScheduled(hbckChore);\n   }\n \n-\n+  /**\n+   * Reflect into the {@link HMaster} instance and find by field name a specified instance\n+   * of {@link ScheduledChore}.\n+   */\n   private static class TestChoreField<E extends ScheduledChore> {\n \n-    private E getChoreObj(String fieldName) throws NoSuchFieldException,\n-        IllegalAccessException {\n-      Field masterField = HMaster.class.getDeclaredField(fieldName);\n-      masterField.setAccessible(true);\n-      E choreFieldVal = (E) masterField.get(hMaster);\n-      return choreFieldVal;\n+    @SuppressWarnings(\"unchecked\")\n+    private E getChoreObj(String fieldName) {\n+      try {\n+        Field masterField = HMaster.class.getDeclaredField(fieldName);\n+        masterField.setAccessible(true);\n+        return (E) masterField.get(hMaster);\n+      } catch (Exception e) {\n+        throw new AssertionError(\n+          \"Unable to retrieve field '\" + fieldName + \"' from HMaster instance.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzMjc0NDQ2", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-503274446", "createdAt": "2020-10-06T19:05:17Z", "commit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxOTowNToxN1rOHdWBdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxOToxODowMVrOHdWcVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMTU3Mw==", "bodyText": "I use it everywhere ;) And HashBuilder, and EqualsBuilder. There's no sense in custom versions of this code in different places, unless we're explicitly maintaining backward compatibility is some way... which I find suspect. Would be super cool if we could enforce the use with static analysis.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500531573", "createdAt": "2020-10-06T19:05:17Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that caries details about a region selected for normalization through the pipeline.\n+ */\n+@InterfaceAudience.Private\n+public class NormalizationTarget {\n+  private final RegionInfo regionInfo;\n+  private final long regionSizeMb;\n+\n+  public NormalizationTarget(final RegionInfo regionInfo, final long regionSizeMb) {\n+    this.regionInfo = regionInfo;\n+    this.regionSizeMb = regionSizeMb;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public long getRegionSizeMb() {\n+    return regionSizeMb;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NormalizationTarget that = (NormalizationTarget) o;\n+\n+    return new EqualsBuilder()\n+      .append(regionSizeMb, that.regionSizeMb)\n+      .append(regionInfo, that.regionInfo)\n+      .isEquals();\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return new HashCodeBuilder(17, 37)\n+      .append(regionInfo)\n+      .append(regionSizeMb)\n+      .toHashCode();\n+  }\n+\n+  @Override public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTMwMw=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMjk2Nw==", "bodyText": "I used a double-checked lock out of habit, to avoid the synchronized if we can. I don't have a strong opinion, and will make the change as you suggest.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500532967", "createdAt": "2020-10-06T19:08:03Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAxOTY3Mg=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMzE0Mw==", "bodyText": "Or want to use a guava Service?\n\nI don't know it. Let me read.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500533143", "createdAt": "2020-10-06T19:08:23Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import edu.umd.cs.findbugs.annotations.NonNull;\n+import edu.umd.cs.findbugs.annotations.Nullable;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+\n+/**\n+ * This class encapsulates the details of the {@link RegionNormalizer} subsystem.\n+ */\n+@InterfaceAudience.Private\n+public class RegionNormalizerManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(RegionNormalizerManager.class);\n+\n+  private final RegionNormalizerTracker regionNormalizerTracker;\n+  private final RegionNormalizerChore regionNormalizerChore;\n+  private final RegionNormalizerWorkQueue<TableName> workQueue;\n+  private final RegionNormalizerWorker worker;\n+  private final ExecutorService pool;\n+\n+  private final Object startStopLock = new Object();\n+  private boolean started = false;\n+  private boolean stopped = false;\n+\n+  public RegionNormalizerManager(\n+    @NonNull  final RegionNormalizerTracker regionNormalizerTracker,\n+    @Nullable final RegionNormalizerChore regionNormalizerChore,\n+    @Nullable final RegionNormalizerWorkQueue<TableName> workQueue,\n+    @Nullable final RegionNormalizerWorker worker\n+  ) {\n+    this.regionNormalizerTracker = regionNormalizerTracker;\n+    this.regionNormalizerChore = regionNormalizerChore;\n+    this.workQueue = workQueue;\n+    this.worker = worker;\n+    this.pool = Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setDaemon(true)\n+      .setNameFormat(\"normalizer-worker-%d\")\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) ->\n+          LOG.error(\"Uncaught exception, worker thread likely terminated.\", throwable))\n+      .build());\n+  }\n+\n+  public void start() {\n+    if (started) {\n+      return;\n+    }\n+    synchronized (startStopLock) {\n+      if (started) {\n+        return;\n+      }\n+      regionNormalizerTracker.start();\n+      if (worker != null) {\n+        // worker will be null when master is in maintenance mode.\n+        pool.submit(worker);\n+      }\n+      started = true;\n+    }\n+  }\n+\n+  public void stop() {\n+    if (!started) {\n+      throw new IllegalStateException(\"calling `stop` without first calling `start`.\");\n+    }\n+    if (stopped) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMDUyNA=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNDA0Mg==", "bodyText": "@virajjasani has it right -- if the items is already present the addition is effectively ignored and no state is changed. You can intentionally have an existing entry jump to the front of the queue using putFirst/putAllFirst.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500534042", "createdAt": "2020-10-06T19:09:54Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A specialized collection that holds pending work for the {@link RegionNormalizerWorker}. It is\n+ * an ordered collection class that has the following properties:\n+ * <ul>\n+ *   <li>Guarantees uniqueness of elements, as a {@link Set}.</li>\n+ *   <li>Consumers retrieve objects from the head, as a {@link Queue}, via {@link #take()}.</li>\n+ *   <li>Work is retrieved on a FIFO policy.</li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDAyMzAzMQ=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzNjkxOQ==", "bodyText": "I haven't rebased it yet, but have a look at 450c4d5 on #2490 . The limit to pairs is removed by that patch, so I'll just have to undo the pair-wise API you propose.\nI could remove the use of the list from this patch, go back to using the first and second member variables, if you think that aspect is so critical. My push-back is because, in my opinion, it's not the POJO's responsibility to ensure these invariants, it's the job of the merge system. And the merge system already makes these checks, so let's leave that logic written in one place, and leave it close to where it's consumed.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500536919", "createdAt": "2020-10-06T19:15:11Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -371,7 +358,11 @@ private boolean skipForMerge(final RegionStates regionStates, final RegionInfo r\n       final long nextSizeMb = getRegionSizeMB(next);\n       // always merge away empty regions when they present themselves.\n       if (currentSizeMb == 0 || nextSizeMb == 0 || currentSizeMb + nextSizeMb < avgRegionSizeMb) {\n-        plans.add(new MergeNormalizationPlan(current, next));\n+        final MergeNormalizationPlan plan = new MergeNormalizationPlan.Builder()\n+          .addTarget(current, currentSizeMb)\n+          .addTarget(next, nextSizeMb)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc4MzU0Mg=="}, "originalCommit": {"oid": "6863e334b907f1835b7166ecd27c64ff832a9d64"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzODQ1Mw==", "bodyText": "You didn't miss it, you let the exception bubble up, which is a common style in this code base. I just updated the test to have a bit of a friendlier error message, something that might help a confused dev to understand a failure, someone who doesn't know the system under test. I happen to like writing test asserts that include an error message, rather than decrypting a stack trace with generic exception messages and a line number. But @saintstack has given me grief about this habit for as long as he's been reviewing my patches ;)", "url": "https://github.com/apache/hbase/pull/2454#discussion_r500538453", "createdAt": "2020-10-06T19:18:01Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterChoreScheduled.java", "diffHunk": "@@ -114,22 +113,27 @@ public void testDefaultScheduledChores() throws Exception {\n     hbckChoreTestChoreField.testIfChoreScheduled(hbckChore);\n   }\n \n-\n+  /**\n+   * Reflect into the {@link HMaster} instance and find by field name a specified instance\n+   * of {@link ScheduledChore}.\n+   */\n   private static class TestChoreField<E extends ScheduledChore> {\n \n-    private E getChoreObj(String fieldName) throws NoSuchFieldException,\n-        IllegalAccessException {\n-      Field masterField = HMaster.class.getDeclaredField(fieldName);\n-      masterField.setAccessible(true);\n-      E choreFieldVal = (E) masterField.get(hMaster);\n-      return choreFieldVal;\n+    @SuppressWarnings(\"unchecked\")\n+    private E getChoreObj(String fieldName) {\n+      try {\n+        Field masterField = HMaster.class.getDeclaredField(fieldName);\n+        masterField.setAccessible(true);\n+        return (E) masterField.get(hMaster);\n+      } catch (Exception e) {\n+        throw new AssertionError(\n+          \"Unable to retrieve field '\" + fieldName + \"' from HMaster instance.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0ODgzMw=="}, "originalCommit": {"oid": "802814cfc2cd9196c64fd8af4aa6743d293f89bc"}, "originalPosition": 52}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2ff0e64d567a203be229a917bf656ea733804917", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/2ff0e64d567a203be229a917bf656ea733804917", "committedDate": "2020-10-07T21:31:10Z", "message": "More PR feedback and cleanup\n\n* lock down all classes and interfaces in the `normalizer` package\n  that don't need to be `public`.\n* add more details to the package-level documentation, including\n  moving docs for all configuration points honored throughout the\n  subsystem.\n* remove use of `HMaster` where possible, instead dispatching to\n  `RegionNormalizerManager` or `MasterServices` where possible.\n* Slightly simplify `RegionNormalizerManager` start/stop locking"}, "afterCommit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65", "committedDate": "2020-10-07T21:31:23Z", "message": "More PR feedback and cleanup\n\n* lock down all classes and interfaces in the `normalizer` package\n  that don't need to be `public`.\n* add more details to the package-level documentation, including\n  moving docs for all configuration points honored throughout the\n  subsystem.\n* remove use of `HMaster` where possible, instead dispatching to\n  `RegionNormalizerManager` or `MasterServices` where possible.\n* Slightly simplify `RegionNormalizerManager` start/stop locking"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0Mjg1MjM2", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-504285236", "createdAt": "2020-10-07T21:36:05Z", "commit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozNjowNVrOHeGY4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTozNjowNVrOHeGY4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyNDAwMQ==", "bodyText": "All this doc moved to the package-level.", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501324001", "createdAt": "2020-10-07T21:36:05Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java", "diffHunk": "@@ -54,29 +53,9 @@\n  *   <li>Otherwise, for the next region in the chain R1, if R0 + R1 is smaller then S, R0 and R1\n  *     are kindly requested to merge.</li>\n  * </ol>\n- * <p>\n- * The following parameters are configurable:\n- * <ol>\n- *   <li>Whether to split a region as part of normalization. Configuration:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MjkyMDg1", "url": "https://github.com/apache/hbase/pull/2454#pullrequestreview-504292085", "createdAt": "2020-10-07T21:49:10Z", "commit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0OToxMFrOHeGu5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMTo0OToxMFrOHeGu5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMyOTYzOQ==", "bodyText": "nice", "url": "https://github.com/apache/hbase/pull/2454#discussion_r501329639", "createdAt": "2020-10-07T21:49:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/package-info.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * The Region Normalizer subsystem is responsible for coaxing all the regions in a table toward\n+ * a \"normal\" size, according to their storefile size. It does this by splitting regions that\n+ * are significantly larger than the norm, and merging regions that are significantly smaller than\n+ * the norm.\n+ * </p>\n+ * The public interface to the Region Normalizer subsystem is limited to the following classes:\n+ * <ul>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerFactory} provides an\n+ *     entry point for creating an instance of the\n+ *     {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager}.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager} encapsulates\n+ *     the whole Region Normalizer subsystem. You'll find one of these hanging off of the\n+ *     {@link org.apache.hadoop.hbase.master.HMaster}, which uses it to delegate API calls. There\n+ *     is usually only a single instance of this class.\n+ *   </li>\n+ *   <li>\n+ *     Various configuration points that share the common prefix of {@code hbase.normalizer}.\n+ *     <ul>\n+ *       <li>Whether to split a region as part of normalization. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#SPLIT_ENABLED_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_SPLIT_ENABLED}.\n+ *       </li>\n+ *       <li>Whether to merge a region as part of normalization. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MERGE_ENABLED_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MERGE_ENABLED}.\n+ *       </li>\n+ *       <li>The minimum number of regions in a table to consider it for merge normalization.\n+ *         Configuration: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MIN_REGION_COUNT_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MIN_REGION_COUNT}.\n+ *       </li>\n+ *       <li>The minimum age for a region to be considered for a merge, in days. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MERGE_MIN_REGION_AGE_DAYS_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MERGE_MIN_REGION_AGE_DAYS}.\n+ *       </li>\n+ *       <li>The minimum size for a region to be considered for a merge, in whole MBs. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#MERGE_MIN_REGION_SIZE_MB_KEY},\n+ *         default: {@value org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer#DEFAULT_MERGE_MIN_REGION_SIZE_MB}.\n+ *       </li>\n+ *       <li>The limit on total throughput of the Region Normalizer's actions, in whole MBs. Configuration:\n+ *         {@value org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorker#RATE_LIMIT_BYTES_PER_SEC_KEY},\n+ *         default: unlimited.\n+ *       </li>\n+ *     </ul>\n+ *     <p>\n+ *       To see detailed logging of the application of these configuration values, set the log\n+ *       level for this package to `TRACE`.\n+ *     </p>\n+ *   </li>\n+ * </ul>\n+ * The Region Normalizer subsystem is composed of a handful of related classes:\n+ * <ul>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.zookeeper.RegionNormalizerTracker} provides a system by\n+ *     which the Normalizer can be disabled at runtime. It currently does this by managing a znode,\n+ *     but this is an implementation detail.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorkQueue} is a\n+ *     {@link java.util.Set}-like {@link java.util.Queue} that permits a single copy of a given\n+ *     work item to exist in the queue at one time. It also provides a facility for a producer to\n+ *     add an item to the front of the line. Consumers are blocked waiting for new work.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerChore} wakes up\n+ *     periodically and schedules new normalization work, adding targets to the queue.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorker} runs in a\n+ *     daemon thread, grabbing work off the queue as is it becomes available.\n+ *   </li>\n+ *   <li>\n+ *     The {@link org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer} implements the\n+ *     logic for calculating target region sizes and emitting a list of corresponding\n+ *     {@link org.apache.hadoop.hbase.master.normalizer.NormalizationPlan} objects.\n+ *   </li>\n+ * </ul>\n+ */\n+package org.apache.hadoop.hbase.master.normalizer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65"}, "originalPosition": 100}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55d3ae19a4be25a1a193ee4fb5965dde50247c54", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/55d3ae19a4be25a1a193ee4fb5965dde50247c54", "committedDate": "2020-10-08T22:41:36Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`\n\nSigned-off-by: Viraj Jasani <vjasani@apache.org>\nSigned-off-by: Huaxiang Sun <huaxiangsun@apache.com>\nSigned-off-by: Michael Stack <stack@apache.org>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/61f5b68a00e988f03eb8bd1dd488c3ac5e74ba65", "committedDate": "2020-10-07T21:31:23Z", "message": "More PR feedback and cleanup\n\n* lock down all classes and interfaces in the `normalizer` package\n  that don't need to be `public`.\n* add more details to the package-level documentation, including\n  moving docs for all configuration points honored throughout the\n  subsystem.\n* remove use of `HMaster` where possible, instead dispatching to\n  `RegionNormalizerManager` or `MasterServices` where possible.\n* Slightly simplify `RegionNormalizerManager` start/stop locking"}, "afterCommit": {"oid": "55d3ae19a4be25a1a193ee4fb5965dde50247c54", "author": {"user": {"login": "ndimiduk", "name": "Nick Dimiduk"}}, "url": "https://github.com/apache/hbase/commit/55d3ae19a4be25a1a193ee4fb5965dde50247c54", "committedDate": "2020-10-08T22:41:36Z", "message": "HBASE-24628 Region normalizer now respects a rate limit\n\nImplement a rate limiter for the normalizer. Implemented in terms of\nMB/sec of affacted region size (the same metrics used to make\nnormalization decisions). Uses Guava `RateLimiter` to perform the\nresource accounting. `RateLimiter` works by blocking (uninterruptible\n\ud83d\ude16) the calling thread. Thus, the whole construction of the normalizer\nsubsystem needed refactoring. See the provided `package-info.java` for\nan overview of this new structure.\n\nIntroduces a new configuration,\n`hbase.normalizer.throughput.max_bytes_per_sec`, for specifying a\nlimit on the throughput of actions executed by the normalizer. Note\nthat while this configuration value is in bytes, the minimum honored\nvalued `1_000_000`. Supports values configured using the\nhuman-readable suffixes honored by `Configuration.getLongBytes`\n\nSigned-off-by: Viraj Jasani <vjasani@apache.org>\nSigned-off-by: Huaxiang Sun <huaxiangsun@apache.com>\nSigned-off-by: Michael Stack <stack@apache.org>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4590, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}