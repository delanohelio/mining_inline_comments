{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIzMTM2NTA2", "number": 1781, "title": "HBASE-24435 Add hedgedReads and hedgedReadWins count metrics", "bodyText": "https://issues.apache.org/jira/browse/HBASE-24435\nCherry-picked 71ed703 with just a few adjustments to adapt it to current branch-1.", "createdAt": "2020-05-26T11:04:04Z", "url": "https://github.com/apache/hbase/pull/1781", "merged": true, "mergeCommit": {"oid": "6aa2286733f8ca224c03c1a61c17077fa29410a3"}, "closed": true, "closedAt": "2020-05-28T02:11:11Z", "author": {"login": "javierluca"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABclCw9ZAH2gAyNDIzMTM2NTA2OjI0YzczN2Y4ODNiNzExZGY4ZDU1NWUwY2Y1ZTQ5MzI0NTc1Y2M1ZTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABclbdFUgFqTQxOTM3NzA5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "24c737f883b711df8d555e0cf5e49324575cc5e3", "author": {"user": {"login": "saintstack", "name": "Michael Stack"}}, "url": "https://github.com/apache/hbase/commit/24c737f883b711df8d555e0cf5e49324575cc5e3", "committedDate": "2020-05-26T11:14:34Z", "message": "HBASE-24435 Add hedgedReads and hedgedReadWins count metrics\n\nConflicts:\n\thbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzIwNjU0", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418720654", "createdAt": "2020-05-26T22:24:50Z", "commit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMjoyNDo1MVrOGayVwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMjoyNTo0MVrOGayW8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MDkzMA==", "bodyText": "Unrelated changes. Remove", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430740930", "createdAt": "2020-05-26T22:24:51Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "diffHunk": "@@ -592,9 +593,9 @@ public void markRegionsRecovering(ServerName server, Set<HRegionInfo> userRegion\n    * @return whether log is replaying\n    */\n   public boolean isLogReplaying() {\n-    if (server.getCoordinatedStateManager() == null) return false;\n-    return ((BaseCoordinatedStateManager) server.getCoordinatedStateManager())\n-        .getSplitLogManagerCoordination().isReplaying();\n+    CoordinatedStateManager m = server.getCoordinatedStateManager();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTIzMg==", "bodyText": "In what Hadoop version were these introduced? If at least 2.7, its definitely fine. If 2.8, probably ok.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430741232", "createdAt": "2020-05-26T22:25:41Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -40,7 +41,9 @@\n import org.apache.hadoop.hbase.io.hfile.CacheStats;\n import org.apache.hadoop.hbase.wal.WALProvider;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.hadoop.hdfs.DFSHedgedReadMetrics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzU3OTQ1", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418757945", "createdAt": "2020-05-27T00:04:32Z", "commit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDowNDozMlrOGa0RBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDozMDo1N1rOGa0uKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3MjQ4NQ==", "bodyText": "Checkstyle conflicts need to be fixed by adding braces.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430772485", "createdAt": "2020-05-27T00:04:32Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java", "diffHunk": "@@ -592,9 +593,9 @@ public void markRegionsRecovering(ServerName server, Set<HRegionInfo> userRegion\n    * @return whether log is replaying\n    */\n   public boolean isLogReplaying() {\n-    if (server.getCoordinatedStateManager() == null) return false;\n-    return ((BaseCoordinatedStateManager) server.getCoordinatedStateManager())\n-        .getSplitLogManagerCoordination().isReplaying();\n+    CoordinatedStateManager m = server.getCoordinatedStateManager();\n+    if (m == null) return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTEyNA==", "bodyText": "Checkstyle conflict. Need braces.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775124", "createdAt": "2020-05-27T00:13:28Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c\n+   * @return The DFSClient DFSHedgedReadMetrics instance or null if can't be found or not on hdfs.\n+   * @throws IOException\n+   */\n+  public static DFSHedgedReadMetrics getDFSHedgedReadMetrics(final Configuration c)\n+      throws IOException {\n+    if (!isHDFS(c)) return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTI3NA==", "bodyText": "can be removed.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775274", "createdAt": "2020-05-27T00:14:02Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTM0Mg==", "bodyText": "can be removed", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775342", "createdAt": "2020-05-27T00:14:20Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java", "diffHunk": "@@ -1780,4 +1782,47 @@ public static void checkShortCircuitReadBufferSize(final Configuration conf) {\n     int hbaseSize = conf.getInt(\"hbase.\" + dfsKey, defaultSize);\n     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));\n   }\n+\n+  /**\n+   * @param c\n+   * @return The DFSClient DFSHedgedReadMetrics instance or null if can't be found or not on hdfs.\n+   * @throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTY1MA==", "bodyText": "Not necessary.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430775650", "createdAt": "2020-05-27T00:15:22Z", "author": {"login": "clarax"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,146 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   * @throws Exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3ODU0MQ==", "bodyText": "Validated from HBASE15550. LGTM.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430778541", "createdAt": "2020-05-27T00:25:51Z", "author": {"login": "clarax"}, "path": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java", "diffHunk": "@@ -504,6 +504,11 @@ public void getMetrics(MetricsCollector metricsCollector, boolean all) {\n               rsWrap.getCompactedCellsSize())\n           .addCounter(Interns.info(MAJOR_COMPACTED_CELLS_SIZE, MAJOR_COMPACTED_CELLS_SIZE_DESC),\n               rsWrap.getMajorCompactedCellsSize())\n+\n+          .addCounter(Interns.info(HEDGED_READS, HEDGED_READS_DESC), rsWrap.getHedgedReadOps())\n+          .addCounter(Interns.info(HEDGED_READ_WINS, HEDGED_READ_WINS_DESC),\n+              rsWrap.getHedgedReadWins())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTk0NQ==", "bodyText": "This is from 2.4. https://issues.apache.org/jira/browse/HBASE-7509 Should be good.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430779945", "createdAt": "2020-05-27T00:30:57Z", "author": {"login": "clarax"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -40,7 +41,9 @@\n import org.apache.hadoop.hbase.io.hfile.CacheStats;\n import org.apache.hadoop.hbase.wal.WALProvider;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n+import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.hadoop.hdfs.DFSHedgedReadMetrics;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTIzMg=="}, "originalCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e"}, "originalPosition": 14}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22b3531fe074bc1722ce655d893be3ff80c78abd", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/22b3531fe074bc1722ce655d893be3ff80c78abd", "committedDate": "2020-05-27T02:27:19Z", "message": "revert SplitLogManager unrelated changes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4e9bf66bea31b151ffa43729cdc577f85786509e", "author": {"user": {"login": "saintstack", "name": "Michael Stack"}}, "url": "https://github.com/apache/hbase/commit/4e9bf66bea31b151ffa43729cdc577f85786509e", "committedDate": "2020-05-26T10:56:27Z", "message": "HBASE-24435 Add hedgedReads and hedgedReadWins count metrics\n\nConflicts:\n\thbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java"}, "afterCommit": {"oid": "22b3531fe074bc1722ce655d893be3ff80c78abd", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/22b3531fe074bc1722ce655d893be3ff80c78abd", "committedDate": "2020-05-27T02:27:19Z", "message": "revert SplitLogManager unrelated changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "517a0f8574f7fab212a447b925694f15a0d3b061", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/517a0f8574f7fab212a447b925694f15a0d3b061", "committedDate": "2020-05-27T02:31:22Z", "message": "remove not needed @param and @throws in headers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/3416a3297b7c07fa15b454bdea5f62bd209184d5", "committedDate": "2020-05-27T02:31:54Z", "message": "add braces (style)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODIzMDAy", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418823002", "createdAt": "2020-05-27T02:53:24Z", "commit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzoyNFrOGa3uJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzoyNFrOGa3uJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTA5NA==", "bodyText": "nit, space between 'null?', '0:'", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430829094", "createdAt": "2020-05-27T02:53:24Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadOps();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODIzMDQz", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418823043", "createdAt": "2020-05-27T02:53:32Z", "commit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzozMlrOGa3uSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMjo1MzozMlrOGa3uSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyOTEyOQ==", "bodyText": "ditto", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430829129", "createdAt": "2020-05-27T02:53:32Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadOps();\n+  }\n+\n+  @Override\n+  public long getHedgedReadWins() {\n+    return this.dfsHedgedReadMetrics == null? 0: this.dfsHedgedReadMetrics.getHedgedReadWins();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODIzNTU5", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418823559", "createdAt": "2020-05-27T02:55:16Z", "commit": {"oid": "3416a3297b7c07fa15b454bdea5f62bd209184d5"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2", "committedDate": "2020-05-27T03:04:38Z", "message": "spacing for style"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4ODI4MTk5", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418828199", "createdAt": "2020-05-27T03:11:19Z", "commit": {"oid": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxMToxOVrOGa3-zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxMToxOVrOGa3-zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMzM1OA==", "bodyText": "space between 'null?', still missing.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430833358", "createdAt": "2020-05-27T03:11:19Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java", "diffHunk": "@@ -819,6 +832,16 @@ public long getZeroCopyBytesRead() {\n     return FSDataInputStreamWrapper.getZeroCopyBytesRead();\n   }\n \n+  @Override\n+  public long getHedgedReadOps() {\n+    return this.dfsHedgedReadMetrics == null? 0 : this.dfsHedgedReadMetrics.getHedgedReadOps();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b1976d6664598bf0a2aeaaef2ebcfe3033e96f2"}, "originalPosition": 48}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/a865e0d51bb6901c8af7980d6434a3cd8f205512", "committedDate": "2020-05-27T03:13:04Z", "message": "spacing for style (2)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4OTM1ODAx", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418935801", "createdAt": "2020-05-27T07:45:22Z", "commit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0NToyMlrOGa9RUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0NToyMlrOGa9RUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDAxNw==", "bodyText": "please pay attention to the space style problems in this unit test.", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920017", "createdAt": "2020-05-27T07:45:22Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4OTM2ODk5", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-418936899", "createdAt": "2020-05-27T07:46:52Z", "commit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0Njo1MlrOGa9Uug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwNzo0Njo1MlrOGa9Uug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkyMDg5MA==", "bodyText": "Just remove instead of comments if you don't need following codes", "url": "https://github.com/apache/hbase/pull/1781#discussion_r430920890", "createdAt": "2020-05-27T07:46:52Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java", "diffHunk": "@@ -435,4 +440,145 @@ public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {\n     }\n   }\n \n+  /**\n+   * Ugly test that ensures we can get at the hedged read counters in dfsclient.\n+   * Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.\n+   */\n+  @Test public void testDFSHedgedReadMetrics() throws Exception {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    // Enable hedged reads and set it so the threshold is really low.\n+    // Most of this test is taken from HDFS, from TestPread.\n+    Configuration conf = htu.getConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE, 5);\n+    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS, 0);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 4096);\n+    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY, 4096);\n+    // Set short retry timeouts so this test runs faster\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n+    conf.setBoolean(\"dfs.datanode.transferTo.allowed\", false);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    // Get the metrics.  Should be empty.\n+    DFSHedgedReadMetrics metrics = FSUtils.getDFSHedgedReadMetrics(conf);\n+    assertEquals(0, metrics.getHedgedReadOps());\n+    FileSystem fileSys = cluster.getFileSystem();\n+    try {\n+      Path p = new Path(\"preadtest.dat\");\n+      // We need > 1 blocks to test out the hedged reads.\n+      DFSTestUtil.createFile(fileSys, p, 12 * blockSize, 12 * blockSize,\n+        blockSize, (short) 3, seed);\n+      pReadFile(fileSys, p);\n+      cleanupFile(fileSys, p);\n+      assertTrue(metrics.getHedgedReadOps() > 0);\n+    } finally {\n+      fileSys.close();\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  // Below is taken from TestPread over in HDFS.\n+  static final int blockSize = 4096;\n+  static final long seed = 0xDEADBEEFL;\n+\n+  private void pReadFile(FileSystem fileSys, Path name) throws IOException {\n+    FSDataInputStream stm = fileSys.open(name);\n+    byte[] expected = new byte[12 * blockSize];\n+    Random rand = new Random(seed);\n+    rand.nextBytes(expected);\n+    // do a sanity check. Read first 4K bytes\n+    byte[] actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 0, expected, \"Read Sanity Test\");\n+    // now do a pread for the first 8K bytes\n+    actual = new byte[8192];\n+    doPread(stm, 0L, actual, 0, 8192);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 1\");\n+    // Now check to see if the normal read returns 4K-8K byte range\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 4096, expected, \"Pread Test 2\");\n+    // Now see if we can cross a single block boundary successfully\n+    // read 4K bytes from blockSize - 2K offset\n+    stm.readFully(blockSize - 2048, actual, 0, 4096);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 3\");\n+    // now see if we can cross two block boundaries successfully\n+    // read blockSize + 4K bytes from blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(blockSize - 2048, actual);\n+    checkAndEraseData(actual, (blockSize - 2048), expected, \"Pread Test 4\");\n+    // now see if we can cross two block boundaries that are not cached\n+    // read blockSize + 4K bytes from 10*blockSize - 2K offset\n+    actual = new byte[blockSize + 4096];\n+    stm.readFully(10 * blockSize - 2048, actual);\n+    checkAndEraseData(actual, (10 * blockSize - 2048), expected, \"Pread Test 5\");\n+    // now check that even after all these preads, we can still read\n+    // bytes 8K-12K\n+    actual = new byte[4096];\n+    stm.readFully(actual);\n+    checkAndEraseData(actual, 8192, expected, \"Pread Test 6\");\n+    // done\n+    stm.close();\n+    // check block location caching\n+    stm = fileSys.open(name);\n+    stm.readFully(1, actual, 0, 4096);\n+    stm.readFully(4*blockSize, actual, 0, 4096);\n+    stm.readFully(7*blockSize, actual, 0, 4096);\n+    actual = new byte[3*4096];\n+    stm.readFully(0*blockSize, actual, 0, 3*4096);\n+    checkAndEraseData(actual, 0, expected, \"Pread Test 7\");\n+    actual = new byte[8*4096];\n+    stm.readFully(3*blockSize, actual, 0, 8*4096);\n+    checkAndEraseData(actual, 3*blockSize, expected, \"Pread Test 8\");\n+    // read the tail\n+    stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize/2);\n+    IOException res = null;\n+    try { // read beyond the end of the file\n+      stm.readFully(11*blockSize+blockSize/2, actual, 0, blockSize);\n+    } catch (IOException e) {\n+      // should throw an exception\n+      res = e;\n+    }\n+    assertTrue(\"Error reading beyond file boundary.\", res != null);\n+\n+    stm.close();\n+  }\n+\n+  private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {\n+    for (int idx = 0; idx < actual.length; idx++) {\n+      assertEquals(message+\" byte \"+(from+idx)+\" differs. expected \"+\n+          expected[from+idx]+\" actual \"+actual[idx],\n+        actual[idx], expected[from+idx]);\n+      actual[idx] = 0;\n+    }\n+  }\n+\n+  private void doPread(FSDataInputStream stm, long position, byte[] buffer,\n+    int offset, int length) throws IOException {\n+    int nread = 0;\n+    // long totalRead = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a865e0d51bb6901c8af7980d6434a3cd8f205512"}, "originalPosition": 142}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6640f763f8328d1d8f5b0c1f3f1f01c62c62dcfc", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/6640f763f8328d1d8f5b0c1f3f1f01c62c62dcfc", "committedDate": "2020-05-27T08:02:21Z", "message": "more spacing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "58652d4d39d5bf92d429f357e371e36e23baf73f", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/58652d4d39d5bf92d429f357e371e36e23baf73f", "committedDate": "2020-05-27T08:03:26Z", "message": "remove commented code"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5MDQ0MTU4", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-419044158", "createdAt": "2020-05-27T10:05:01Z", "commit": {"oid": "58652d4d39d5bf92d429f357e371e36e23baf73f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5Mzc3MDk2", "url": "https://github.com/apache/hbase/pull/1781#pullrequestreview-419377096", "createdAt": "2020-05-27T16:00:29Z", "commit": {"oid": "58652d4d39d5bf92d429f357e371e36e23baf73f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4752, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}