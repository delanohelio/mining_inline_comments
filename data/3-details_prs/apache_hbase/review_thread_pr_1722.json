{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE4NDc0MTIx", "number": 1722, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwOTo1NDowNlrOD9JylA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMTowMDo0MlrOEB950A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDUwMTMyOnYy", "diffSide": "LEFT", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwOTo1NDowNlrOGWfrqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwOTo1NDowNlrOGWfrqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI0MDkzOA==", "bodyText": "As HConstants is marked as InterfaceAudience.Public this one should be deprecated first.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426240938", "createdAt": "2020-05-17T09:54:06Z", "author": {"login": "HorizonNet"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java", "diffHunk": "@@ -1306,10 +1306,6 @@\n   public static final String REPLICATION_SOURCE_MAXTHREADS_KEY =\n       \"hbase.replication.source.maxthreads\";\n \n-  /** Drop edits for tables that been deleted from the replication source and target */\n-  public static final String REPLICATION_DROP_ON_DELETED_TABLE_KEY =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f30584b6fa102ee18a32b105fce899494ec7d778"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDUwNjMxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDroppedTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QxMDowMDoyMVrOGWfuOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QxMDowMDoyMVrOGWfuOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI0MTU5NA==", "bodyText": "As it is deprecated, please use TableDescriptorBuilder instead.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426241594", "createdAt": "2020-05-17T10:00:21Z", "author": {"login": "HorizonNet"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDroppedTable.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.NamespaceDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDroppedTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(TestReplicationEditsDroppedWithDroppedTable.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final String namespace = \"NS\";\n+  private static final TableName NORMAL_TABLE = TableName.valueOf(\"normal-table\");\n+  private static final TableName DROPPED_TABLE = TableName.valueOf(\"dropped-table\");\n+  private static final TableName DROPPED_NS_TABLE = TableName.valueOf(\"NS:dropped-table\");\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] FAMILY = Bytes.toBytes(\"f\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_TABLE_KEY, true);\n+    conf1.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+\n+    NamespaceDescriptor nsDesc = NamespaceDescriptor.create(namespace).build();\n+    admin1.createNamespace(nsDesc);\n+    admin2.createNamespace(nsDesc);\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = new ReplicationPeerConfig();\n+    rpc.setClusterKey(utility2.getClusterKey()).setReplicateAllUserTables(true);\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable(NORMAL_TABLE);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();\n+    Thread.sleep(SLEEP_TIME);\n+    // Drop table\n+    admin1.disableTable(NORMAL_TABLE);\n+    admin1.deleteTable(NORMAL_TABLE);\n+    admin2.disableTable(NORMAL_TABLE);\n+    admin2.deleteTable(NORMAL_TABLE);\n+  }\n+\n+  private void createTable(TableName tableName) throws Exception {\n+    HTableDescriptor desc = new HTableDescriptor(tableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f30584b6fa102ee18a32b105fce899494ec7d778"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDUwNzI1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDroppedTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QxMDowMjowNlrOGWfuxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QxMDowMjowNlrOGWfuxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI0MTczNQ==", "bodyText": "As it is deprecated, please use the appropriate builder.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426241735", "createdAt": "2020-05-17T10:02:06Z", "author": {"login": "HorizonNet"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDroppedTable.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.NamespaceDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDroppedTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(TestReplicationEditsDroppedWithDroppedTable.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final String namespace = \"NS\";\n+  private static final TableName NORMAL_TABLE = TableName.valueOf(\"normal-table\");\n+  private static final TableName DROPPED_TABLE = TableName.valueOf(\"dropped-table\");\n+  private static final TableName DROPPED_NS_TABLE = TableName.valueOf(\"NS:dropped-table\");\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] FAMILY = Bytes.toBytes(\"f\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_TABLE_KEY, true);\n+    conf1.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+\n+    NamespaceDescriptor nsDesc = NamespaceDescriptor.create(namespace).build();\n+    admin1.createNamespace(nsDesc);\n+    admin2.createNamespace(nsDesc);\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = new ReplicationPeerConfig();\n+    rpc.setClusterKey(utility2.getClusterKey()).setReplicateAllUserTables(true);\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable(NORMAL_TABLE);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();\n+    Thread.sleep(SLEEP_TIME);\n+    // Drop table\n+    admin1.disableTable(NORMAL_TABLE);\n+    admin1.deleteTable(NORMAL_TABLE);\n+    admin2.disableTable(NORMAL_TABLE);\n+    admin2.deleteTable(NORMAL_TABLE);\n+  }\n+\n+  private void createTable(TableName tableName) throws Exception {\n+    HTableDescriptor desc = new HTableDescriptor(tableName);\n+    HColumnDescriptor familyDesc = new HColumnDescriptor(FAMILY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f30584b6fa102ee18a32b105fce899494ec7d778"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzQ3MzczOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTowODo0OFrOGW7gtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOToxMjoyM1rOGXXOog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5Njg4Nw==", "bodyText": "Do we need these two checks? Sounds like these are redundant.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426696887", "createdAt": "2020-05-18T15:08:48Z", "author": {"login": "wchevreuil"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,146 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE1MTAxMA==", "bodyText": "Thanks for reviewing. What I was thinking is that two checks just in case, it is not easy to ensure that the format of exceptions thrown everywhere is uniform", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427151010", "createdAt": "2020-05-19T09:12:23Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,146 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5Njg4Nw=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzQ3NTM2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTowOToxM1rOGW7h4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOToxNDoyN1rOGXXT_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NzE4NA==", "bodyText": "Same as previous comment.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426697184", "createdAt": "2020-05-18T15:09:13Z", "author": {"login": "wchevreuil"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,146 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;\n       }\n     }\n-    return null;\n+    return false;\n   }\n \n-  // Filter a set of batches by TableName\n-  private List<List<Entry>> filterBatches(final List<List<Entry>> oldEntryList, TableName table) {\n-    return oldEntryList\n-        .stream().map(entries -> entries.stream()\n-            .filter(e -> !e.getKey().getTableName().equals(table)).collect(Collectors.toList()))\n-        .collect(Collectors.toList());\n+  /**\n+   * Check if there's an {@link NoSuchColumnFamilyException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isNoSuchColumnFamilyException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"NoSuchColumnFamilyException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof NoSuchColumnFamilyException) {\n+        return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE1MjM4MQ==", "bodyText": "Same as above.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427152381", "createdAt": "2020-05-19T09:14:27Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,146 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;\n       }\n     }\n-    return null;\n+    return false;\n   }\n \n-  // Filter a set of batches by TableName\n-  private List<List<Entry>> filterBatches(final List<List<Entry>> oldEntryList, TableName table) {\n-    return oldEntryList\n-        .stream().map(entries -> entries.stream()\n-            .filter(e -> !e.getKey().getTableName().equals(table)).collect(Collectors.toList()))\n-        .collect(Collectors.toList());\n+  /**\n+   * Check if there's an {@link NoSuchColumnFamilyException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isNoSuchColumnFamilyException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"NoSuchColumnFamilyException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof NoSuchColumnFamilyException) {\n+        return true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NzE4NA=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzY3OTM5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo1NjoxOVrOGW9h6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOToyMjo1OFrOGXXo1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyOTk2MQ==", "bodyText": "Do you think we need two separate configuration properties? If someone is OK dropping data in this scenario, don't you think that they would want to default into dropping data for all cases?", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426729961", "createdAt": "2020-05-18T15:56:19Z", "author": {"login": "joshelser"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -168,7 +183,9 @@ public void init(Context context) throws IOException {\n     this.replicationRpcLimit = (int)(0.95 * conf.getLong(RpcServer.MAX_REQUEST_SIZE,\n       RpcServer.DEFAULT_MAX_REQUEST_SIZE));\n     this.dropOnDeletedTables =\n-        this.conf.getBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n+        this.conf.getBoolean(REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n+    this.dropOnDeletedColumnFamilies = this.conf\n+        .getBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE1NzcxOA==", "bodyText": "Thanks for reviewing. The first property is an existing one, which is moved from HConstants, so two separate properties are retained in the end.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427157718", "createdAt": "2020-05-19T09:22:58Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -168,7 +183,9 @@ public void init(Context context) throws IOException {\n     this.replicationRpcLimit = (int)(0.95 * conf.getLong(RpcServer.MAX_REQUEST_SIZE,\n       RpcServer.DEFAULT_MAX_REQUEST_SIZE));\n     this.dropOnDeletedTables =\n-        this.conf.getBoolean(HConstants.REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n+        this.conf.getBoolean(REPLICATION_DROP_ON_DELETED_TABLE_KEY, false);\n+    this.dropOnDeletedColumnFamilies = this.conf\n+        .getBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyOTk2MQ=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzY5NjY4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTo1OTozM1rOGW9siQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwOTozOTozM1rOGXYRMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczMjY4MQ==", "bodyText": "Should we assume the table exist's when we can't check for it?", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426732681", "createdAt": "2020-05-18T15:59:33Z", "author": {"login": "joshelser"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,146 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;\n       }\n     }\n-    return null;\n+    return false;\n   }\n \n-  // Filter a set of batches by TableName\n-  private List<List<Entry>> filterBatches(final List<List<Entry>> oldEntryList, TableName table) {\n-    return oldEntryList\n-        .stream().map(entries -> entries.stream()\n-            .filter(e -> !e.getKey().getTableName().equals(table)).collect(Collectors.toList()))\n-        .collect(Collectors.toList());\n+  /**\n+   * Check if there's an {@link NoSuchColumnFamilyException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isNoSuchColumnFamilyException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"NoSuchColumnFamilyException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof NoSuchColumnFamilyException) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @VisibleForTesting\n+  List<List<Entry>> filterNotExistTableEdits(final List<List<Entry>> oldEntryList) {\n+    List<List<Entry>> entryList = new ArrayList<>();\n+    Map<TableName, Boolean> existMap = new HashMap<>();\n+    try (Connection localConn = ConnectionFactory.createConnection(ctx.getLocalConfiguration());\n+         Admin localAdmin = localConn.getAdmin()) {\n+      for (List<Entry> oldEntries : oldEntryList) {\n+        List<Entry> entries = new ArrayList<>();\n+        for (Entry e : oldEntries) {\n+          TableName tableName = e.getKey().getTableName();\n+          boolean exist = true;\n+          if (existMap.containsKey(tableName)) {\n+            exist = existMap.get(tableName);\n+          } else {\n+            try {\n+              exist = localAdmin.tableExists(tableName);\n+              existMap.put(tableName, exist);\n+            } catch (IOException iox) {\n+              LOG.warn(\"Exception checking for local table \" + tableName, iox);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE2ODA0OA==", "bodyText": "Yes, we can't drop edits without full assurance. The variable exist is true by default, which ensure this. Let me add some comments to explain it.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427168048", "createdAt": "2020-05-19T09:39:33Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,146 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;\n       }\n     }\n-    return null;\n+    return false;\n   }\n \n-  // Filter a set of batches by TableName\n-  private List<List<Entry>> filterBatches(final List<List<Entry>> oldEntryList, TableName table) {\n-    return oldEntryList\n-        .stream().map(entries -> entries.stream()\n-            .filter(e -> !e.getKey().getTableName().equals(table)).collect(Collectors.toList()))\n-        .collect(Collectors.toList());\n+  /**\n+   * Check if there's an {@link NoSuchColumnFamilyException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isNoSuchColumnFamilyException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"NoSuchColumnFamilyException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof NoSuchColumnFamilyException) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @VisibleForTesting\n+  List<List<Entry>> filterNotExistTableEdits(final List<List<Entry>> oldEntryList) {\n+    List<List<Entry>> entryList = new ArrayList<>();\n+    Map<TableName, Boolean> existMap = new HashMap<>();\n+    try (Connection localConn = ConnectionFactory.createConnection(ctx.getLocalConfiguration());\n+         Admin localAdmin = localConn.getAdmin()) {\n+      for (List<Entry> oldEntries : oldEntryList) {\n+        List<Entry> entries = new ArrayList<>();\n+        for (Entry e : oldEntries) {\n+          TableName tableName = e.getKey().getTableName();\n+          boolean exist = true;\n+          if (existMap.containsKey(tableName)) {\n+            exist = existMap.get(tableName);\n+          } else {\n+            try {\n+              exist = localAdmin.tableExists(tableName);\n+              existMap.put(tableName, exist);\n+            } catch (IOException iox) {\n+              LOG.warn(\"Exception checking for local table \" + tableName, iox);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczMjY4MQ=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzcxNTgzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjowNDoxMVrOGW94Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoyMDozMFrOGXZw7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczNTcxOA==", "bodyText": "Drop this?", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426735718", "createdAt": "2020-05-18T16:04:11Z", "author": {"login": "joshelser"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_GLOBAL;\n+import static org.apache.hadoop.hbase.HConstants.ZOOKEEPER_ZNODE_PARENT;\n+import static org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDeletedTableCFs {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final TableName TABLE = TableName.valueOf(\"table\");\n+  private static final byte[] NORMAL_CF = Bytes.toBytes(\"normal_cf\");\n+  private static final byte[] DROPPED_CF = Bytes.toBytes(\"dropped_cf\");\n+\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, true);\n+    conf1.set(ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();\n+    Thread.sleep(SLEEP_TIME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5MjU1OQ==", "bodyText": "Done", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427192559", "createdAt": "2020-05-19T10:20:30Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_GLOBAL;\n+import static org.apache.hadoop.hbase.HConstants.ZOOKEEPER_ZNODE_PARENT;\n+import static org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDeletedTableCFs {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final TableName TABLE = TableName.valueOf(\"table\");\n+  private static final byte[] NORMAL_CF = Bytes.toBytes(\"normal_cf\");\n+  private static final byte[] DROPPED_CF = Bytes.toBytes(\"dropped_cf\");\n+\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, true);\n+    conf1.set(ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();\n+    Thread.sleep(SLEEP_TIME);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczNTcxOA=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzcyMzAxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjowNTo1OFrOGW988A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoyMDoxNlrOGXZwdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczNjg4MA==", "bodyText": "HBaseTestingUtility has some nice waitFor() methods which simplify this kind of logic", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426736880", "createdAt": "2020-05-18T16:05:58Z", "author": {"login": "joshelser"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_GLOBAL;\n+import static org.apache.hadoop.hbase.HConstants.ZOOKEEPER_ZNODE_PARENT;\n+import static org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDeletedTableCFs {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final TableName TABLE = TableName.valueOf(\"table\");\n+  private static final byte[] NORMAL_CF = Bytes.toBytes(\"normal_cf\");\n+  private static final byte[] DROPPED_CF = Bytes.toBytes(\"dropped_cf\");\n+\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, true);\n+    conf1.set(ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();\n+    Thread.sleep(SLEEP_TIME);\n+    // Drop table\n+    admin1.disableTable(TABLE);\n+    admin1.deleteTable(TABLE);\n+    admin2.disableTable(TABLE);\n+    admin2.deleteTable(TABLE);\n+  }\n+\n+  private void createTable() throws Exception {\n+    TableDescriptor desc = createTableDescriptor(NORMAL_CF, DROPPED_CF);\n+    admin1.createTable(desc);\n+    admin2.createTable(desc);\n+    utility1.waitUntilAllRegionsAssigned(desc.getTableName());\n+    utility2.waitUntilAllRegionsAssigned(desc.getTableName());\n+  }\n+\n+  @Test\n+  public void testEditsDroppedWithDeleteCF() throws Exception {\n+    admin1.disableReplicationPeer(PEER_ID);\n+\n+    try (Table table = utility1.getConnection().getTable(TABLE)) {\n+      Put put = new Put(ROW);\n+      put.addColumn(DROPPED_CF, QUALIFIER, VALUE);\n+      table.put(put);\n+    }\n+\n+    deleteCf(admin1);\n+\n+    admin1.enableReplicationPeer(PEER_ID);\n+\n+    verifyReplicationProceeded();\n+  }\n+\n+  @Test\n+  public void testEditsBehindDeleteCFTiming() throws Exception {\n+    admin1.disableReplicationPeer(PEER_ID);\n+\n+    try (Table table = utility1.getConnection().getTable(TABLE)) {\n+      Put put = new Put(ROW);\n+      put.addColumn(DROPPED_CF, QUALIFIER, VALUE);\n+      table.put(put);\n+    }\n+\n+    // Only delete cf from peer cluster\n+    deleteCf(admin2);\n+\n+    admin1.enableReplicationPeer(PEER_ID);\n+\n+    // the source table's cf still exists, replication should be stalled\n+    verifyReplicationStuck();\n+    deleteCf(admin1);\n+    // now the source table's cf is gone, replication should proceed, the\n+    // offending edits be dropped\n+    verifyReplicationProceeded();\n+  }\n+\n+  private void verifyReplicationProceeded() throws Exception {\n+    try (Table table = utility1.getConnection().getTable(TABLE)) {\n+      Put put = new Put(ROW);\n+      put.addColumn(NORMAL_CF, QUALIFIER, VALUE);\n+      table.put(put);\n+    }\n+    try (Table peerTable = utility2.getConnection().getTable(TABLE)) {\n+      for (int i = 0; i < NB_RETRIES; i++) {\n+        if (i == NB_RETRIES - 1) {\n+          fail(\"Waited too much time for put replication\");\n+        }\n+        Result result = peerTable.get(new Get(ROW).addColumn(NORMAL_CF, QUALIFIER));\n+        if (result == null || result.isEmpty()) {\n+          LOG.info(\"Row not available in peer cluster\");\n+          Thread.sleep(SLEEP_TIME);\n+        } else {\n+          assertArrayEquals(VALUE, result.getValue(NORMAL_CF, QUALIFIER));\n+          break;\n+        }\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5MjQzOA==", "bodyText": "Thanks for tips, I've fixed it.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427192438", "createdAt": "2020-05-19T10:20:16Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_GLOBAL;\n+import static org.apache.hadoop.hbase.HConstants.ZOOKEEPER_ZNODE_PARENT;\n+import static org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDeletedTableCFs {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final TableName TABLE = TableName.valueOf(\"table\");\n+  private static final byte[] NORMAL_CF = Bytes.toBytes(\"normal_cf\");\n+  private static final byte[] DROPPED_CF = Bytes.toBytes(\"dropped_cf\");\n+\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, true);\n+    conf1.set(ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();\n+    Thread.sleep(SLEEP_TIME);\n+    // Drop table\n+    admin1.disableTable(TABLE);\n+    admin1.deleteTable(TABLE);\n+    admin2.disableTable(TABLE);\n+    admin2.deleteTable(TABLE);\n+  }\n+\n+  private void createTable() throws Exception {\n+    TableDescriptor desc = createTableDescriptor(NORMAL_CF, DROPPED_CF);\n+    admin1.createTable(desc);\n+    admin2.createTable(desc);\n+    utility1.waitUntilAllRegionsAssigned(desc.getTableName());\n+    utility2.waitUntilAllRegionsAssigned(desc.getTableName());\n+  }\n+\n+  @Test\n+  public void testEditsDroppedWithDeleteCF() throws Exception {\n+    admin1.disableReplicationPeer(PEER_ID);\n+\n+    try (Table table = utility1.getConnection().getTable(TABLE)) {\n+      Put put = new Put(ROW);\n+      put.addColumn(DROPPED_CF, QUALIFIER, VALUE);\n+      table.put(put);\n+    }\n+\n+    deleteCf(admin1);\n+\n+    admin1.enableReplicationPeer(PEER_ID);\n+\n+    verifyReplicationProceeded();\n+  }\n+\n+  @Test\n+  public void testEditsBehindDeleteCFTiming() throws Exception {\n+    admin1.disableReplicationPeer(PEER_ID);\n+\n+    try (Table table = utility1.getConnection().getTable(TABLE)) {\n+      Put put = new Put(ROW);\n+      put.addColumn(DROPPED_CF, QUALIFIER, VALUE);\n+      table.put(put);\n+    }\n+\n+    // Only delete cf from peer cluster\n+    deleteCf(admin2);\n+\n+    admin1.enableReplicationPeer(PEER_ID);\n+\n+    // the source table's cf still exists, replication should be stalled\n+    verifyReplicationStuck();\n+    deleteCf(admin1);\n+    // now the source table's cf is gone, replication should proceed, the\n+    // offending edits be dropped\n+    verifyReplicationProceeded();\n+  }\n+\n+  private void verifyReplicationProceeded() throws Exception {\n+    try (Table table = utility1.getConnection().getTable(TABLE)) {\n+      Put put = new Put(ROW);\n+      put.addColumn(NORMAL_CF, QUALIFIER, VALUE);\n+      table.put(put);\n+    }\n+    try (Table peerTable = utility2.getConnection().getTable(TABLE)) {\n+      for (int i = 0; i < NB_RETRIES; i++) {\n+        if (i == NB_RETRIES - 1) {\n+          fail(\"Waited too much time for put replication\");\n+        }\n+        Result result = peerTable.get(new Get(ROW).addColumn(NORMAL_CF, QUALIFIER));\n+        if (result == null || result.isEmpty()) {\n+          LOG.info(\"Row not available in peer cluster\");\n+          Thread.sleep(SLEEP_TIME);\n+        } else {\n+          assertArrayEquals(VALUE, result.getValue(NORMAL_CF, QUALIFIER));\n+          break;\n+        }\n+      }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczNjg4MA=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NzcyNjY4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDroppedTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjowNjo1MlrOGW9_Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjowNjo1MlrOGW9_Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczNzQ5NA==", "bodyText": "Two sleeps intentional? If so, make it SLEEP_TIME * 2?", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426737494", "createdAt": "2020-05-18T16:06:52Z", "author": {"login": "joshelser"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDroppedTable.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NamespaceDescriptor;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDroppedTable {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDroppedTable.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDroppedTable.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final String namespace = \"NS\";\n+  private static final TableName NORMAL_TABLE = TableName.valueOf(\"normal-table\");\n+  private static final TableName DROPPED_TABLE = TableName.valueOf(\"dropped-table\");\n+  private static final TableName DROPPED_NS_TABLE = TableName.valueOf(\"NS:dropped-table\");\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] FAMILY = Bytes.toBytes(\"f\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_TABLE_KEY, true);\n+    conf1.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+\n+    NamespaceDescriptor nsDesc = NamespaceDescriptor.create(namespace).build();\n+    admin1.createNamespace(nsDesc);\n+    admin2.createNamespace(nsDesc);\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable(NORMAL_TABLE);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+    Thread.sleep(SLEEP_TIME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1ODMzODM5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTowNzowMFrOGXEB8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMDoxOToxMVrOGXZuHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzNjQ2Nw==", "bodyText": "Remove this commented out line.", "url": "https://github.com/apache/hbase/pull/1722#discussion_r426836467", "createdAt": "2020-05-18T19:07:00Z", "author": {"login": "HorizonNet"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_GLOBAL;\n+import static org.apache.hadoop.hbase.HConstants.ZOOKEEPER_ZNODE_PARENT;\n+import static org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDeletedTableCFs {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final TableName TABLE = TableName.valueOf(\"table\");\n+  private static final byte[] NORMAL_CF = Bytes.toBytes(\"normal_cf\");\n+  private static final byte[] DROPPED_CF = Bytes.toBytes(\"dropped_cf\");\n+\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, true);\n+    conf1.set(ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzE5MTgzNw==", "bodyText": "Done", "url": "https://github.com/apache/hbase/pull/1722#discussion_r427191837", "createdAt": "2020-05-19T10:19:11Z", "author": {"login": "ddupg"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEditsDroppedWithDeletedTableCFs.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_GLOBAL;\n+import static org.apache.hadoop.hbase.HConstants.ZOOKEEPER_ZNODE_PARENT;\n+import static org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ LargeTests.class })\n+public class TestReplicationEditsDroppedWithDeletedTableCFs {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestReplicationEditsDroppedWithDeletedTableCFs.class);\n+\n+  private static Configuration conf1 = HBaseConfiguration.create();\n+  private static Configuration conf2 = HBaseConfiguration.create();\n+\n+  protected static HBaseTestingUtility utility1;\n+  protected static HBaseTestingUtility utility2;\n+\n+  private static Admin admin1;\n+  private static Admin admin2;\n+\n+  private static final TableName TABLE = TableName.valueOf(\"table\");\n+  private static final byte[] NORMAL_CF = Bytes.toBytes(\"normal_cf\");\n+  private static final byte[] DROPPED_CF = Bytes.toBytes(\"dropped_cf\");\n+\n+  private static final byte[] ROW = Bytes.toBytes(\"row\");\n+  private static final byte[] QUALIFIER = Bytes.toBytes(\"q\");\n+  private static final byte[] VALUE = Bytes.toBytes(\"value\");\n+\n+  private static final String PEER_ID = \"1\";\n+  private static final long SLEEP_TIME = 1000;\n+  private static final int NB_RETRIES = 10;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // Set true to filter replication edits for dropped table\n+    conf1.setBoolean(REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY, true);\n+    conf1.set(ZOOKEEPER_ZNODE_PARENT, \"/1\");\n+    conf1.setInt(\"replication.source.nb.capacity\", 1);\n+    utility1 = new HBaseTestingUtility(conf1);\n+    utility1.startMiniZKCluster();\n+    MiniZooKeeperCluster miniZK = utility1.getZkCluster();\n+    conf1 = utility1.getConfiguration();\n+\n+    conf2 = HBaseConfiguration.create(conf1);\n+    conf2.set(ZOOKEEPER_ZNODE_PARENT, \"/2\");\n+    utility2 = new HBaseTestingUtility(conf2);\n+    utility2.setZkCluster(miniZK);\n+\n+    utility1.startMiniCluster(1);\n+    utility2.startMiniCluster(1);\n+\n+    admin1 = utility1.getAdmin();\n+    admin2 = utility2.getAdmin();\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility2.shutdownMiniCluster();\n+    utility1.shutdownMiniCluster();\n+  }\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // Roll log\n+    for (JVMClusterUtil.RegionServerThread r : utility1.getHBaseCluster()\n+        .getRegionServerThreads()) {\n+      utility1.getAdmin().rollWALWriter(r.getRegionServer().getServerName());\n+    }\n+    // add peer\n+    ReplicationPeerConfig rpc = ReplicationPeerConfig.newBuilder()\n+        .setClusterKey(utility2.getClusterKey())\n+        .setReplicateAllUserTables(true).build();\n+    admin1.addReplicationPeer(PEER_ID, rpc);\n+    // create table\n+    createTable();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    // Remove peer\n+    admin1.removeReplicationPeer(PEER_ID);\n+    Thread.sleep(SLEEP_TIME);\n+//    utility1.getMiniHBaseCluster().getMaster().getReplicationZKNodeCleanerChore().choreForTesting();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzNjQ2Nw=="}, "originalCommit": {"oid": "4e6e930cd0a14dbcbeb73e28b909df8b0654396b"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDk4MjU2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMTowMDo0MlrOGeIunA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMTowMDo0MlrOGeIunA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI1MzQ2OA==", "bodyText": "Nit: add whitespace before \"filtering\". And seems miss a ' for columnfamily {}?", "url": "https://github.com/apache/hbase/pull/1722#discussion_r434253468", "createdAt": "2020-06-03T01:00:42Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java", "diffHunk": "@@ -279,28 +296,148 @@ private int getEstimatedEntrySize(Entry e) {\n     }\n   }\n \n-  private TableName parseTable(String msg) {\n-    // ... TableNotFoundException: '<table>'/n...\n-    Pattern p = Pattern.compile(\"TableNotFoundException: '([\\\\S]*)'\");\n-    Matcher m = p.matcher(msg);\n-    if (m.find()) {\n-      String table = m.group(1);\n-      try {\n-        // double check that table is a valid table name\n-        TableName.valueOf(TableName.isLegalFullyQualifiedTableName(Bytes.toBytes(table)));\n-        return TableName.valueOf(table);\n-      } catch (IllegalArgumentException ignore) {\n+  /**\n+   * Check if there's an {@link TableNotFoundException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isTableNotFoundException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"TableNotFoundException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof TableNotFoundException) {\n+        return true;\n       }\n     }\n-    return null;\n+    return false;\n   }\n \n-  // Filter a set of batches by TableName\n-  private List<List<Entry>> filterBatches(final List<List<Entry>> oldEntryList, TableName table) {\n-    return oldEntryList\n-        .stream().map(entries -> entries.stream()\n-            .filter(e -> !e.getKey().getTableName().equals(table)).collect(Collectors.toList()))\n-        .collect(Collectors.toList());\n+  /**\n+   * Check if there's an {@link NoSuchColumnFamilyException} in the caused by stacktrace.\n+   */\n+  @VisibleForTesting\n+  public static boolean isNoSuchColumnFamilyException(Throwable io) {\n+    if (io instanceof RemoteException) {\n+      io = ((RemoteException) io).unwrapRemoteException();\n+    }\n+    if (io != null && io.getMessage().contains(\"NoSuchColumnFamilyException\")) {\n+      return true;\n+    }\n+    for (; io != null; io = io.getCause()) {\n+      if (io instanceof NoSuchColumnFamilyException) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @VisibleForTesting\n+  List<List<Entry>> filterNotExistTableEdits(final List<List<Entry>> oldEntryList) {\n+    List<List<Entry>> entryList = new ArrayList<>();\n+    Map<TableName, Boolean> existMap = new HashMap<>();\n+    try (Connection localConn = ConnectionFactory.createConnection(ctx.getLocalConfiguration());\n+         Admin localAdmin = localConn.getAdmin()) {\n+      for (List<Entry> oldEntries : oldEntryList) {\n+        List<Entry> entries = new ArrayList<>();\n+        for (Entry e : oldEntries) {\n+          TableName tableName = e.getKey().getTableName();\n+          boolean exist = true;\n+          if (existMap.containsKey(tableName)) {\n+            exist = existMap.get(tableName);\n+          } else {\n+            try {\n+              exist = localAdmin.tableExists(tableName);\n+              existMap.put(tableName, exist);\n+            } catch (IOException iox) {\n+              LOG.warn(\"Exception checking for local table \" + tableName, iox);\n+              // we can't drop edits without full assurance, so we assume table exists.\n+              exist = true;\n+            }\n+          }\n+          if (exist) {\n+            entries.add(e);\n+          } else {\n+            // Would potentially be better to retry in one of the outer loops\n+            // and add a table filter there; but that would break the encapsulation,\n+            // so we're doing the filtering here.\n+            LOG.warn(\"Missing table detected at sink, local table also does not exist, \"\n+                + \"filtering edits for table '{}'\", tableName);\n+          }\n+        }\n+        if (!entries.isEmpty()) {\n+          entryList.add(entries);\n+        }\n+      }\n+    } catch (IOException iox) {\n+      LOG.warn(\"Exception when creating connection to check local table\", iox);\n+      return oldEntryList;\n+    }\n+    return entryList;\n+  }\n+\n+  @VisibleForTesting\n+  List<List<Entry>> filterNotExistColumnFamilyEdits(final List<List<Entry>> oldEntryList) {\n+    List<List<Entry>> entryList = new ArrayList<>();\n+    Map<TableName, Set<String>> existColumnFamilyMap = new HashMap<>();\n+    try (Connection localConn = ConnectionFactory.createConnection(ctx.getLocalConfiguration());\n+         Admin localAdmin = localConn.getAdmin()) {\n+      for (List<Entry> oldEntries : oldEntryList) {\n+        List<Entry> entries = new ArrayList<>();\n+        for (Entry e : oldEntries) {\n+          TableName tableName = e.getKey().getTableName();\n+          if (!existColumnFamilyMap.containsKey(tableName)) {\n+            try {\n+              Set<String> cfs = localAdmin.getDescriptor(tableName).getColumnFamilyNames().stream()\n+                  .map(Bytes::toString).collect(Collectors.toSet());\n+              existColumnFamilyMap.put(tableName, cfs);\n+            } catch (Exception ex) {\n+              LOG.warn(\"Exception getting cf names for local table {}\", tableName, ex);\n+              // if catch any exception, we are not sure about table's description,\n+              // so replicate raw entry\n+              entries.add(e);\n+              continue;\n+            }\n+          }\n+\n+          Set<String> existColumnFamilies = existColumnFamilyMap.get(tableName);\n+          Set<String> missingCFs = new HashSet<>();\n+          WALEdit walEdit = new WALEdit();\n+          walEdit.getCells().addAll(e.getEdit().getCells());\n+          WALUtil.filterCells(walEdit, cell -> {\n+            String cf = Bytes.toString(CellUtil.cloneFamily(cell));\n+            if (existColumnFamilies.contains(cf)) {\n+              return cell;\n+            } else {\n+              missingCFs.add(cf);\n+              return null;\n+            }\n+          });\n+          if (!walEdit.isEmpty()) {\n+            Entry newEntry = new Entry(e.getKey(), walEdit);\n+            entries.add(newEntry);\n+          }\n+\n+          if (!missingCFs.isEmpty()) {\n+            // Would potentially be better to retry in one of the outer loops\n+            // and add a table filter there; but that would break the encapsulation,\n+            // so we're doing the filtering here.\n+            LOG.warn(\n+                \"Missing column family detected at sink, local column family also does not exist,\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883b53597fbd3a7b7d27a4c3d4a465c4a792fa63"}, "originalPosition": 235}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1782, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}