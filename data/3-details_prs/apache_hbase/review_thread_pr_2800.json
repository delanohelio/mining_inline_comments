{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzNjcyNDk2", "number": 2800, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDoyNjo1NVrOFH7Ftw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMjowNzo0N1rOFL6p_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODUyNDcxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDoyNjo1NVrOIJk_Xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDo1NjoyMVrOIOGb2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNDE0Mg==", "bodyText": "Consider naming this getEncryptionContext (as elsewhere)", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546914142", "createdAt": "2020-12-21T20:26:55Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +501,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getCryptoContext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1MTc5MA==", "bodyText": "sure, and thanks for pointing it out", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546951790", "createdAt": "2020-12-21T22:02:45Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +501,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getCryptoContext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNDE0Mg=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY1NjQwOA==", "bodyText": "I have changed that in the latest update, please see if you have more comments, and I marked it as resolved first", "url": "https://github.com/apache/hbase/pull/2800#discussion_r551656408", "createdAt": "2021-01-05T00:56:21Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +501,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getCryptoContext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNDE0Mg=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 303}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODUzMjA3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDoyOTo1NlrOIJlD8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDo1Njo1M1rOIOGcjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTMxMw==", "bodyText": "Does DEEP_OVERHEAD and heapSize() need to change? Does TestHeapSize still pass?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546915313", "createdAt": "2020-12-21T20:29:56Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -2559,7 +2572,7 @@ public boolean needsCompaction() {\n    * @return cache configuration for this Store.\n    */\n   public CacheConfig getCacheConfig() {\n-    return this.cacheConf;\n+    return storeContext.getCacheConf();\n   }\n \n   public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 594}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1MTgwOA==", "bodyText": "TestHeapSize still passed when I checked it locally.\nfrom what I understood from the FIXED_OVERHEAD were being calculated by ClassSize.estimateBase, it will automatically calculate the our newly added fields and reference included the change of HStoreContext storeContext.\nSo for the DEEP_OVERHEAD that takes new calculated FIXED_OVERHEAD to come up the heap size, it should be done already. Or did I miss something here?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546951808", "createdAt": "2020-12-21T22:02:48Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -2559,7 +2572,7 @@ public boolean needsCompaction() {\n    * @return cache configuration for this Store.\n    */\n   public CacheConfig getCacheConfig() {\n-    return this.cacheConf;\n+    return storeContext.getCacheConf();\n   }\n \n   public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTMxMw=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 594}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY1NjU5MA==", "bodyText": "this has been change as well, can you have another look ?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r551656590", "createdAt": "2021-01-05T00:56:53Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -2559,7 +2572,7 @@ public boolean needsCompaction() {\n    * @return cache configuration for this Store.\n    */\n   public CacheConfig getCacheConfig() {\n-    return this.cacheConf;\n+    return storeContext.getCacheConf();\n   }\n \n   public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTMxMw=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 594}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODUzMzYyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreContext.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDozMDozN1rOIJlE9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDo1NzoxOVrOIOGc-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTU3NA==", "bodyText": "Should this implement HeapSize? This information used to be included in Store/HStore heap utilization estimation via HStore#heapSize() and so we should still track it?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546915574", "createdAt": "2020-12-21T20:30:37Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreContext.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public class HStoreContext {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1MTgzMQ==", "bodyText": "as I mentioned above about FIXED_OVERHEAD were being calculated by ClassSize.estimateBase, it should be already covered", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546951831", "createdAt": "2020-12-21T22:02:52Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreContext.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public class HStoreContext {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTU3NA=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk2NTUyOA==", "bodyText": "after a second look, you're right that this HStoreContext should have implemented HeapSize, I will change it for the next diff", "url": "https://github.com/apache/hbase/pull/2800#discussion_r546965528", "createdAt": "2020-12-21T22:41:52Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreContext.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public class HStoreContext {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTU3NA=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY1NjY5OQ==", "bodyText": "fixed and implemented heapSize()", "url": "https://github.com/apache/hbase/pull/2800#discussion_r551656699", "createdAt": "2021-01-05T00:57:19Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreContext.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public class HStoreContext {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxNTU3NA=="}, "originalCommit": {"oid": "e5e21fa09a6869fc3de3179ec1dee077e0019a9a"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MzY4MzcwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwNjowNToyOVrOIKUP4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMToyMzo0MVrOIOn0QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY4ODQxNw==", "bodyText": "You don't want to do get\nreturn ChecksymType.nameToType(conf.get(HConstants.CHECKSUM_TYPE_NAME, DEFAULT_WHATEVER_IT_IS));\nIs this a candidate for StoreContext? (Perhaps if called frequently). Perhaps StoreContext is not available where this is used?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r547688417", "createdAt": "2020-12-23T06:05:29Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java", "diffHunk": "@@ -136,4 +140,26 @@ public static OptionalLong getMaxSequenceIdInList(Collection<HStoreFile> sfs) {\n     return largestFile.isPresent() ? StoreUtils.getFileSplitPoint(largestFile.get(), comparator)\n         : Optional.empty();\n   }\n+\n+  /**\n+   * Returns the configured checksum algorithm.\n+   * @param conf The configuration\n+   * @return The checksum algorithm that is set in the configuration\n+   */\n+  public static ChecksumType getChecksumType(Configuration conf) {\n+    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE2Mjc5Mg==", "bodyText": "will change in next commit. we put it as it's a static helper method, IMO we're writing it in the right Utils class?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548162792", "createdAt": "2020-12-23T19:25:45Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java", "diffHunk": "@@ -136,4 +140,26 @@ public static OptionalLong getMaxSequenceIdInList(Collection<HStoreFile> sfs) {\n     return largestFile.isPresent() ? StoreUtils.getFileSplitPoint(largestFile.get(), comparator)\n         : Optional.empty();\n   }\n+\n+  /**\n+   * Returns the configured checksum algorithm.\n+   * @param conf The configuration\n+   * @return The checksum algorithm that is set in the configuration\n+   */\n+  public static ChecksumType getChecksumType(Configuration conf) {\n+    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY4ODQxNw=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIwMzMyOQ==", "bodyText": "@saintstack  we have updated it in the last commit, do you mind to have a second look?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552203329", "createdAt": "2021-01-05T21:23:41Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreUtils.java", "diffHunk": "@@ -136,4 +140,26 @@ public static OptionalLong getMaxSequenceIdInList(Collection<HStoreFile> sfs) {\n     return largestFile.isPresent() ? StoreUtils.getFileSplitPoint(largestFile.get(), comparator)\n         : Optional.empty();\n   }\n+\n+  /**\n+   * Returns the configured checksum algorithm.\n+   * @param conf The configuration\n+   * @return The checksum algorithm that is set in the configuration\n+   */\n+  public static ChecksumType getChecksumType(Configuration conf) {\n+    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY4ODQxNw=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MzcwMDE5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": true, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwNjowOTowMlrOIKUabw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMjowNToxMlrOIOvIQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ==", "bodyText": "Why introduce an StoreContext? Isn't that what a Store is? Store spans files.\n(We need the 'H' in HStoreContext?)", "url": "https://github.com/apache/hbase/pull/2800#discussion_r547691119", "createdAt": "2020-12-23T06:09:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5NTk3OQ==", "bodyText": "Yeah, just trying to understand why. You want to swap out the HStore implementation or something?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r547695979", "createdAt": "2020-12-23T06:15:19Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEyNjA0OA==", "bodyText": "we introduced this context for a reason. HStore is a big informative object passed as a input to many file operation classes e.g. StoreFlusher to get access for related information in reference, e.g. columnFamilyDescriptor and regionFileSystem, we're trying to group those read-only reference such that we don't have to always use HStore object in the lower classes.\nAlthough this refactoring is the base for the upcoming patches in HBASE-24749 that we will introduce new StoreFileWriterFactory and StoreFileCommitter, we think this could be a good wrapper to group and limit access for internal reference as well.\n\n(We need the 'H' in HStoreContext?)\n\nwe can remove the H from naming, will do in the next commit", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548126048", "createdAt": "2020-12-23T18:38:18Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwOTk0OA==", "bodyText": "Yeah the idea is that HStore has way more scope than most consumers of HStore actually need (1. The information about the store and 2. the functions to mutate things - A lot of callers just care about the info, not the actual functions) so it made sense to separate out the info to a POJO to reduce the scope of what is being exposed.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548209948", "createdAt": "2020-12-23T20:28:55Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIxMzQ0Ng==", "bodyText": "Ok. Stick that on the head of the context class justifying why it exists.\nWhere do you draw the line on what is in context and what is in Store? Thanks.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548213446", "createdAt": "2020-12-23T20:33:33Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMxNzg0NA==", "bodyText": "IMO those informative accessor/reference and final/read-only primitives should ideally be in the context, although we're focusing on writer (StoreFileWriter) related reference and may have missed few of them (e.g. scanInfo) in this commit.\nlet's try to clarify your suggestion\n\nif you see the StoreContext is general to be applied on most cases, are those missing fields (e.g. scanInfo and  final primitives) what you're trying to point out ? if so, we can revisit and filter/add more into the StoreContext\nThe scope of this Context is more related to Writer(StoreFileWriter)/Committer(will be added), should we rename it to StoreWriterContext/StoreWriteContext  that used by those operators?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548317844", "createdAt": "2020-12-23T23:26:27Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjIwNTc4NQ==", "bodyText": "@saintstack , do you have other comments about the scope of this StoreContext ? do we need to rename it ? or do you think we could move forward and introduce this context object with the proposed set of informative accessor/reference ?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552205785", "createdAt": "2021-01-05T21:26:22Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NDEzMA==", "bodyText": "@taklwu\nSorry for late reply (Thanks for pinging).\n\nlet's try to clarify your suggestion\n\nI was asking a question, not making a suggestion -- smile.\nOne thing I notice is that you and @z-york talk of the Store 'info' or 'information'. So, would StoreInfo make more sense than StoreContext? It would align with ScanInfo (Yeah, StoreInfo looks like it should have ScanInfo since it a superset or should subsume ScanInfo).\nAbove you say this... \"...we're trying to group those read-only reference such that we don't have to always use HStore object in the lower classes.\"  Seems like you could use a version of this to answer my question (ScanInfo is 'Immutable information for scans over a store'... so StoreInfo could be Store immutable info?).\nBack to your comments...\nI was trying to figure when to pass Store and when I'd pass StoreInfo/StoreContext only.\nLets just have a StoreContext/StoreInfo. Lets NOT have a StoreWriterContext; i.e. a context for write-side only.\nOne sec.... let me look at the PR here.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552254130", "createdAt": "2021-01-05T23:12:48Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMzEzNw==", "bodyText": "thanks, I should have provide a followup in the latest diff and proposed to keep with the StoreContext naming.\nmarked this conversation as resolved.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552323137", "createdAt": "2021-01-06T02:05:12Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private HStoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY5MTExOQ=="}, "originalCommit": {"oid": "e3a02b9daa452be8b3ce3b3cc6ce480537cbe13a"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjkwMjI5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMDoxNTozOFrOIKzf_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMjowNzo1MlrOIK4d8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwMDQ0Ng==", "bodyText": "Should this be final? I wouldn't expect you would need to change StoreContext (the reference) after it has been initialized", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548200446", "createdAt": "2020-12-23T20:15:38Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private StoreContext storeContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODI4MTg0Mw==", "bodyText": "you're right, it could be final, will do it in next commit", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548281843", "createdAt": "2020-12-23T22:07:52Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -246,6 +234,8 @@\n   private AtomicLong compactedCellsSize = new AtomicLong();\n   private AtomicLong majorCompactedCellsSize = new AtomicLong();\n \n+  private StoreContext storeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwMDQ0Ng=="}, "originalCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjkyMjc3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMDoyMDo0NVrOIKzs9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMjoxNDo1MlrOIK4vuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwMzc2NA==", "bodyText": "Does it simplify things to be able to pass StoreContext directly to the builders?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548203764", "createdAt": "2020-12-23T20:20:45Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1236,18 +1248,19 @@ private HFileContext createFileContext(Compression.Algorithm compression,\n                                 .withIncludesMvcc(includeMVCCReadpoint)\n                                 .withIncludesTags(includesTag)\n                                 .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n+                                .withCompressTags(getColumnFamilyDescriptor().isCompressTags())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "originalPosition": 484}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODI4NjM5NQ==", "bodyText": "I'd like to keep the current way that passing required fields to the creation of HFileContext.  Mainly HFileContext is also a member of the StoreContext which if we pass into StoreContext to the creation of HFileContext, it will be a loop and is very strange.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r548286395", "createdAt": "2020-12-23T22:14:52Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1236,18 +1248,19 @@ private HFileContext createFileContext(Compression.Algorithm compression,\n                                 .withIncludesMvcc(includeMVCCReadpoint)\n                                 .withIncludesTags(includesTag)\n                                 .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n+                                .withCompressTags(getColumnFamilyDescriptor().isCompressTags())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODIwMzc2NA=="}, "originalCommit": {"oid": "7725f5f131525c73a27cd4e5e26eedd8ee8920bd"}, "originalPosition": 484}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjUyMDY3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMzowNjozNVrOIMDotA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQwMDoyMDoyNVrOIMa7Tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMzM5Ng==", "bodyText": "Does it make sense to return the supplier or just a straight getter where internal to this method it calls and returns the result of the .get()?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r549513396", "createdAt": "2020-12-28T23:06:35Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final HFileContext defaultFileContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.defaultFileContext = builder.defaultFileContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public HFileContext getDefaultFileContext() {\n+    return defaultFileContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public Supplier<InetSocketAddress[]> getFavoredNodesSupplier() {\n+    return favoredNodesSupplier;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c9561e16621296589f36554419e28edb67cdf9"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxNDE3OA==", "bodyText": "I guess the answer to this will depend on whether we expect the contents to change... I see it is passed into the HFileContext... is it evaluated right away or only used when it is needed?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r549514178", "createdAt": "2020-12-28T23:10:40Z", "author": {"login": "z-york"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final HFileContext defaultFileContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.defaultFileContext = builder.defaultFileContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public HFileContext getDefaultFileContext() {\n+    return defaultFileContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public Supplier<InetSocketAddress[]> getFavoredNodesSupplier() {\n+    return favoredNodesSupplier;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMzM5Ng=="}, "originalCommit": {"oid": "b3c9561e16621296589f36554419e28edb67cdf9"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg5NDk5MQ==", "bodyText": "For the getCompactedFilesSupplier(), we should keep it as using supplier because the actual values at that time will be used when StoreFileWriter.java#appendMetadata is being called for writing COMPACTION_EVENT_KEY. unless we change the builder of StoreFileWriter, I would propose to keep the supplier like this.\nfor the getFavoredNodesSupplier(), I think you may be right that withFavoredNodes could be using the values directly per writer creation, will update in next change.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r549894991", "createdAt": "2020-12-30T00:20:25Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final HFileContext defaultFileContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.defaultFileContext = builder.defaultFileContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public HFileContext getDefaultFileContext() {\n+    return defaultFileContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public Supplier<InetSocketAddress[]> getFavoredNodesSupplier() {\n+    return favoredNodesSupplier;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMzM5Ng=="}, "originalCommit": {"oid": "b3c9561e16621296589f36554419e28edb67cdf9"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTc5MTk5OnYy", "diffSide": "RIGHT", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzoxNzoyN1rOIOrBPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNjoxMjo1OFrOIOzJrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NTgwNw==", "bodyText": "Whats going on here?\nHFileContext is supposed to be ' Read-only HFile Context Information' but here we are adding a setter. I see there are one or two setters but we also have HFileContextBuilder. Shouldn't we be going via the Builder making HFileContexts? And why is this method not added on the Builder? (Can it be added non-public to encourage users to go via the Builder)?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552255807", "createdAt": "2021-01-05T23:17:27Z", "author": {"login": "saintstack"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "diffHunk": "@@ -138,6 +138,10 @@ public boolean isCompressedOrEncrypted() {\n     return compressAlgo;\n   }\n \n+  public void setCompression(Compression.Algorithm compressAlgo) {\n+    this.compressAlgo = compressAlgo;\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMTY1Ng==", "bodyText": "fixed, and reverted back to builder pattern", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552321656", "createdAt": "2021-01-06T01:59:40Z", "author": {"login": "taklwu"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "diffHunk": "@@ -138,6 +138,10 @@ public boolean isCompressedOrEncrypted() {\n     return compressAlgo;\n   }\n \n+  public void setCompression(Compression.Algorithm compressAlgo) {\n+    this.compressAlgo = compressAlgo;\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NTgwNw=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjM4MjQ3Mg==", "bodyText": "You haven't uploaded new PR so will leave this as unresovled for now.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552382472", "createdAt": "2021-01-06T05:48:20Z", "author": {"login": "saintstack"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "diffHunk": "@@ -138,6 +138,10 @@ public boolean isCompressedOrEncrypted() {\n     return compressAlgo;\n   }\n \n+  public void setCompression(Compression.Algorithm compressAlgo) {\n+    this.compressAlgo = compressAlgo;\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NTgwNw=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjM4OTAzOQ==", "bodyText": "oops, I missed this change. and now it should have updated/removed.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552389039", "createdAt": "2021-01-06T06:12:58Z", "author": {"login": "taklwu"}, "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java", "diffHunk": "@@ -138,6 +138,10 @@ public boolean isCompressedOrEncrypted() {\n     return compressAlgo;\n   }\n \n+  public void setCompression(Compression.Algorithm compressAlgo) {\n+    this.compressAlgo = compressAlgo;\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NTgwNw=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTgwMzc3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzoyMjozMFrOIOrIKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMTo1OTowOVrOIOvB9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NzU3OQ==", "bodyText": "nit: why bother with this change at all? 'family' is passed on the constructor. Its final. Why bother going via the accessor in the constructor?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552257579", "createdAt": "2021-01-05T23:22:30Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -254,48 +244,47 @@\n   protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       final Configuration confParam, boolean warmup) throws IOException {\n \n-    this.fs = region.getRegionFileSystem();\n-\n-    // Assemble the store's home directory and Ensure it exists.\n-    fs.createStoreDir(family.getNameAsString());\n-    this.region = region;\n-    this.family = family;\n     // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n     // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n     // add global config first, then table and cf overrides, then cf metadata.\n     this.conf = new CompoundConfiguration()\n-      .add(confParam)\n-      .addBytesMap(region.getTableDescriptor().getValues())\n-      .addStringMap(family.getConfiguration())\n-      .addBytesMap(family.getValues());\n-    this.blocksize = family.getBlocksize();\n+        .add(confParam)\n+        .addBytesMap(region.getTableDescriptor().getValues())\n+        .addStringMap(family.getConfiguration())\n+        .addBytesMap(family.getValues());\n+\n+    this.region = region;\n+    this.storeContext = initializeStoreContext(family);\n+\n+    // Assemble the store's home directory and Ensure it exists.\n+    getRegionFileSystem().createStoreDir(getColumnFamilyName());\n+\n+    this.blocksize = getColumnFamilyDescriptor().getBlocksize();\n \n     // set block storage policy for store directory\n-    String policyName = family.getStoragePolicy();\n+    String policyName = getColumnFamilyDescriptor().getStoragePolicy();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMTUyNg==", "bodyText": "fixed.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552321526", "createdAt": "2021-01-06T01:59:09Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -254,48 +244,47 @@\n   protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       final Configuration confParam, boolean warmup) throws IOException {\n \n-    this.fs = region.getRegionFileSystem();\n-\n-    // Assemble the store's home directory and Ensure it exists.\n-    fs.createStoreDir(family.getNameAsString());\n-    this.region = region;\n-    this.family = family;\n     // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n     // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n     // add global config first, then table and cf overrides, then cf metadata.\n     this.conf = new CompoundConfiguration()\n-      .add(confParam)\n-      .addBytesMap(region.getTableDescriptor().getValues())\n-      .addStringMap(family.getConfiguration())\n-      .addBytesMap(family.getValues());\n-    this.blocksize = family.getBlocksize();\n+        .add(confParam)\n+        .addBytesMap(region.getTableDescriptor().getValues())\n+        .addStringMap(family.getConfiguration())\n+        .addBytesMap(family.getValues());\n+\n+    this.region = region;\n+    this.storeContext = initializeStoreContext(family);\n+\n+    // Assemble the store's home directory and Ensure it exists.\n+    getRegionFileSystem().createStoreDir(getColumnFamilyName());\n+\n+    this.blocksize = getColumnFamilyDescriptor().getBlocksize();\n \n     // set block storage policy for store directory\n-    String policyName = family.getStoragePolicy();\n+    String policyName = getColumnFamilyDescriptor().getStoragePolicy();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1NzU3OQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTgxMjIwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzoyNjowMlrOIOrM5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMTo1ODo1OVrOIOvBxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1ODc4OQ==", "bodyText": "Could be static?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552258789", "createdAt": "2021-01-05T23:26:02Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -347,6 +331,48 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n     cacheOnWriteLogged = false;\n   }\n \n+  private StoreContext initializeStoreContext(ColumnFamilyDescriptor family) throws IOException {\n+    return new StoreContext.Builder()\n+        .withBloomType(family.getBloomFilterType())\n+        .withCacheConfig(createCacheConf(family))\n+        .withCellComparator(region.getCellComparator())\n+        .withColumnFamilyDescriptor(family)\n+        .withCompactedFilesSupplier(this::getCompactedFiles)\n+        .withRegionFileSystem(region.getRegionFileSystem())\n+        .withDefaultHFileContext(getDefaultHFileContext(family))\n+        .withFavoredNodesSupplier(this::getFavoredNodes)\n+        .withFamilyStoreDirectoryPath(region.getRegionFileSystem()\n+            .getStoreDir(family.getNameAsString()))\n+        .withRegionCoprocessorHost(region.getCoprocessorHost())\n+        .build();\n+  }\n+\n+  private InetSocketAddress[] getFavoredNodes() {\n+    InetSocketAddress[] favoredNodes = null;\n+    if (region.getRegionServerServices() != null) {\n+      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n+          region.getRegionInfo().getEncodedName());\n+    }\n+    return favoredNodes;\n+  }\n+\n+  private HFileContext getDefaultHFileContext(ColumnFamilyDescriptor family) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMTQ3OA==", "bodyText": "the conf and region used within this function was blocking it to be static, but thinking it again, having this big defaultHFileContext within StoreFileContext is strange and the original thought was to reuse reference if we can.\nI will remove this getDefaultHFileContext and defaultHFileContext in StoreContext", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552321478", "createdAt": "2021-01-06T01:58:59Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -347,6 +331,48 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n     cacheOnWriteLogged = false;\n   }\n \n+  private StoreContext initializeStoreContext(ColumnFamilyDescriptor family) throws IOException {\n+    return new StoreContext.Builder()\n+        .withBloomType(family.getBloomFilterType())\n+        .withCacheConfig(createCacheConf(family))\n+        .withCellComparator(region.getCellComparator())\n+        .withColumnFamilyDescriptor(family)\n+        .withCompactedFilesSupplier(this::getCompactedFiles)\n+        .withRegionFileSystem(region.getRegionFileSystem())\n+        .withDefaultHFileContext(getDefaultHFileContext(family))\n+        .withFavoredNodesSupplier(this::getFavoredNodes)\n+        .withFamilyStoreDirectoryPath(region.getRegionFileSystem()\n+            .getStoreDir(family.getNameAsString()))\n+        .withRegionCoprocessorHost(region.getCoprocessorHost())\n+        .build();\n+  }\n+\n+  private InetSocketAddress[] getFavoredNodes() {\n+    InetSocketAddress[] favoredNodes = null;\n+    if (region.getRegionServerServices() != null) {\n+      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n+          region.getRegionInfo().getEncodedName());\n+    }\n+    return favoredNodes;\n+  }\n+\n+  private HFileContext getDefaultHFileContext(ColumnFamilyDescriptor family) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1ODc4OQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTgxNTk5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzoyNzozMlrOIOrPEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNTo0NjozMFrOIOyt1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1OTM0Ng==", "bodyText": "Has to be public? Has to be on HStore? Can it be on StoreContext? Is there a getStoreContext method on HStore?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552259346", "createdAt": "2021-01-05T23:27:32Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +502,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMTA2OA==", "bodyText": "getEncryptionContext was used by HMobStore such we set it to public , it could be package-private.\ngetStoreContext was planned to add in a followup PR when used, tho you're right that if we have a getStoreContext then this getEncryptionContext could be removed", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552321068", "createdAt": "2021-01-06T01:57:34Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +502,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1OTM0Ng=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjM4MTkxMQ==", "bodyText": "Package private sounds good if only used locally.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552381911", "createdAt": "2021-01-06T05:46:30Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -474,33 +502,14 @@ public long getBlockingFileCount() {\n   }\n   /* End implementation of StoreConfigInformation */\n \n-  /**\n-   * Returns the configured bytesPerChecksum value.\n-   * @param conf The configuration\n-   * @return The bytesPerChecksum that is set in the configuration\n-   */\n-  public static int getBytesPerChecksum(Configuration conf) {\n-    return conf.getInt(HConstants.BYTES_PER_CHECKSUM,\n-                       HFile.DEFAULT_BYTES_PER_CHECKSUM);\n-  }\n-\n-  /**\n-   * Returns the configured checksum algorithm.\n-   * @param conf The configuration\n-   * @return The checksum algorithm that is set in the configuration\n-   */\n-  public static ChecksumType getChecksumType(Configuration conf) {\n-    String checksumName = conf.get(HConstants.CHECKSUM_TYPE_NAME);\n-    if (checksumName == null) {\n-      return ChecksumType.getDefaultChecksumType();\n-    } else {\n-      return ChecksumType.nameToType(checksumName);\n-    }\n-  }\n \n   @Override\n   public ColumnFamilyDescriptor getColumnFamilyDescriptor() {\n-    return this.family;\n+    return this.storeContext.getFamily();\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI1OTM0Ng=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 304}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTgyMTE1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzoyOTo0OFrOIOrR4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNTo0MzoxMFrOIOyqSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDA2NQ==", "bodyText": "Why would we do this here and not as methods on the builder?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552260065", "createdAt": "2021-01-05T23:29:48Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1206,53 +1218,34 @@ public StoreFileWriter createWriterInTmp(long maxKeyCount, Compression.Algorithm\n         }\n       }\n     }\n-    InetSocketAddress[] favoredNodes = null;\n-    if (region.getRegionServerServices() != null) {\n-      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n-          region.getRegionInfo().getEncodedName());\n-    }\n-    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag,\n-      cryptoContext);\n-    Path familyTempDir = new Path(fs.getTempDir(), family.getNameAsString());\n-    StoreFileWriter.Builder builder = new StoreFileWriter.Builder(conf, writerCacheConf,\n-        this.getFileSystem())\n-            .withOutputDir(familyTempDir)\n-            .withBloomType(family.getBloomFilterType())\n-            .withMaxKeyCount(maxKeyCount)\n-            .withFavoredNodes(favoredNodes)\n-            .withFileContext(hFileContext)\n-            .withShouldDropCacheBehind(shouldDropBehind)\n-            .withCompactedFilesSupplier(this::getCompactedFiles)\n-            .withFileStoragePolicy(fileStoragePolicy);\n+    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag);\n+    Path familyTempDir = new Path(getRegionFileSystem().getTempDir(), getColumnFamilyName());\n+    StoreFileWriter.Builder builder =\n+      new StoreFileWriter.Builder(conf, writerCacheConf, getFileSystem())\n+        .withOutputDir(familyTempDir)\n+        .withBloomType(storeContext.getBloomFilterType())\n+        .withMaxKeyCount(maxKeyCount)\n+        .withFavoredNodes(storeContext.getFavoredNodes())\n+        .withFileContext(hFileContext)\n+        .withShouldDropCacheBehind(shouldDropBehind)\n+        .withCompactedFilesSupplier(storeContext.getCompactedFilesSupplier())\n+        .withFileStoragePolicy(fileStoragePolicy);\n     return builder.build();\n   }\n \n   private HFileContext createFileContext(Compression.Algorithm compression,\n-      boolean includeMVCCReadpoint, boolean includesTag, Encryption.Context cryptoContext) {\n+    boolean includeMVCCReadpoint, boolean includesTag) {\n     if (compression == null) {\n       compression = HFile.DEFAULT_COMPRESSION_ALGORITHM;\n     }\n-    HFileContext hFileContext = new HFileContextBuilder()\n-                                .withIncludesMvcc(includeMVCCReadpoint)\n-                                .withIncludesTags(includesTag)\n-                                .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n-                                .withBlockSize(blocksize)\n-                                .withHBaseCheckSum(true)\n-                                .withDataBlockEncoding(family.getDataBlockEncoding())\n-                                .withEncryptionContext(cryptoContext)\n-                                .withCreateTime(EnvironmentEdgeManager.currentTime())\n-                                .withColumnFamily(family.getName())\n-                                .withTableName(region.getTableDescriptor()\n-                                    .getTableName().getName())\n-                                .withCellComparator(this.comparator)\n-                                .build();\n-    return hFileContext;\n+    HFileContext fileContext = storeContext.getDefaultFileContext();\n+    fileContext.setIncludesMvcc(includeMVCCReadpoint);\n+    fileContext.setIncludesTags(includesTag);\n+    fileContext.setCompression(compression);\n+    fileContext.setFileCreateTime(EnvironmentEdgeManager.currentTime());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 526}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMDcwOQ==", "bodyText": "it was trying to reuse the defaultFileContext built with initializeStoreContext, but after rethinking on your comment, I'm going to remove the defaultFileContext and go back with the original builder pattern.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552320709", "createdAt": "2021-01-06T01:56:28Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1206,53 +1218,34 @@ public StoreFileWriter createWriterInTmp(long maxKeyCount, Compression.Algorithm\n         }\n       }\n     }\n-    InetSocketAddress[] favoredNodes = null;\n-    if (region.getRegionServerServices() != null) {\n-      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n-          region.getRegionInfo().getEncodedName());\n-    }\n-    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag,\n-      cryptoContext);\n-    Path familyTempDir = new Path(fs.getTempDir(), family.getNameAsString());\n-    StoreFileWriter.Builder builder = new StoreFileWriter.Builder(conf, writerCacheConf,\n-        this.getFileSystem())\n-            .withOutputDir(familyTempDir)\n-            .withBloomType(family.getBloomFilterType())\n-            .withMaxKeyCount(maxKeyCount)\n-            .withFavoredNodes(favoredNodes)\n-            .withFileContext(hFileContext)\n-            .withShouldDropCacheBehind(shouldDropBehind)\n-            .withCompactedFilesSupplier(this::getCompactedFiles)\n-            .withFileStoragePolicy(fileStoragePolicy);\n+    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag);\n+    Path familyTempDir = new Path(getRegionFileSystem().getTempDir(), getColumnFamilyName());\n+    StoreFileWriter.Builder builder =\n+      new StoreFileWriter.Builder(conf, writerCacheConf, getFileSystem())\n+        .withOutputDir(familyTempDir)\n+        .withBloomType(storeContext.getBloomFilterType())\n+        .withMaxKeyCount(maxKeyCount)\n+        .withFavoredNodes(storeContext.getFavoredNodes())\n+        .withFileContext(hFileContext)\n+        .withShouldDropCacheBehind(shouldDropBehind)\n+        .withCompactedFilesSupplier(storeContext.getCompactedFilesSupplier())\n+        .withFileStoragePolicy(fileStoragePolicy);\n     return builder.build();\n   }\n \n   private HFileContext createFileContext(Compression.Algorithm compression,\n-      boolean includeMVCCReadpoint, boolean includesTag, Encryption.Context cryptoContext) {\n+    boolean includeMVCCReadpoint, boolean includesTag) {\n     if (compression == null) {\n       compression = HFile.DEFAULT_COMPRESSION_ALGORITHM;\n     }\n-    HFileContext hFileContext = new HFileContextBuilder()\n-                                .withIncludesMvcc(includeMVCCReadpoint)\n-                                .withIncludesTags(includesTag)\n-                                .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n-                                .withBlockSize(blocksize)\n-                                .withHBaseCheckSum(true)\n-                                .withDataBlockEncoding(family.getDataBlockEncoding())\n-                                .withEncryptionContext(cryptoContext)\n-                                .withCreateTime(EnvironmentEdgeManager.currentTime())\n-                                .withColumnFamily(family.getName())\n-                                .withTableName(region.getTableDescriptor()\n-                                    .getTableName().getName())\n-                                .withCellComparator(this.comparator)\n-                                .build();\n-    return hFileContext;\n+    HFileContext fileContext = storeContext.getDefaultFileContext();\n+    fileContext.setIncludesMvcc(includeMVCCReadpoint);\n+    fileContext.setIncludesTags(includesTag);\n+    fileContext.setCompression(compression);\n+    fileContext.setFileCreateTime(EnvironmentEdgeManager.currentTime());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDA2NQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 526}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjM4MTAwMg==", "bodyText": "That'd be better. If a builder, lets use it everywhere (can add a constructor on builder that allows a bit of shortcutting auto-filling most fields....)", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552381002", "createdAt": "2021-01-06T05:43:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1206,53 +1218,34 @@ public StoreFileWriter createWriterInTmp(long maxKeyCount, Compression.Algorithm\n         }\n       }\n     }\n-    InetSocketAddress[] favoredNodes = null;\n-    if (region.getRegionServerServices() != null) {\n-      favoredNodes = region.getRegionServerServices().getFavoredNodesForRegion(\n-          region.getRegionInfo().getEncodedName());\n-    }\n-    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag,\n-      cryptoContext);\n-    Path familyTempDir = new Path(fs.getTempDir(), family.getNameAsString());\n-    StoreFileWriter.Builder builder = new StoreFileWriter.Builder(conf, writerCacheConf,\n-        this.getFileSystem())\n-            .withOutputDir(familyTempDir)\n-            .withBloomType(family.getBloomFilterType())\n-            .withMaxKeyCount(maxKeyCount)\n-            .withFavoredNodes(favoredNodes)\n-            .withFileContext(hFileContext)\n-            .withShouldDropCacheBehind(shouldDropBehind)\n-            .withCompactedFilesSupplier(this::getCompactedFiles)\n-            .withFileStoragePolicy(fileStoragePolicy);\n+    HFileContext hFileContext = createFileContext(compression, includeMVCCReadpoint, includesTag);\n+    Path familyTempDir = new Path(getRegionFileSystem().getTempDir(), getColumnFamilyName());\n+    StoreFileWriter.Builder builder =\n+      new StoreFileWriter.Builder(conf, writerCacheConf, getFileSystem())\n+        .withOutputDir(familyTempDir)\n+        .withBloomType(storeContext.getBloomFilterType())\n+        .withMaxKeyCount(maxKeyCount)\n+        .withFavoredNodes(storeContext.getFavoredNodes())\n+        .withFileContext(hFileContext)\n+        .withShouldDropCacheBehind(shouldDropBehind)\n+        .withCompactedFilesSupplier(storeContext.getCompactedFilesSupplier())\n+        .withFileStoragePolicy(fileStoragePolicy);\n     return builder.build();\n   }\n \n   private HFileContext createFileContext(Compression.Algorithm compression,\n-      boolean includeMVCCReadpoint, boolean includesTag, Encryption.Context cryptoContext) {\n+    boolean includeMVCCReadpoint, boolean includesTag) {\n     if (compression == null) {\n       compression = HFile.DEFAULT_COMPRESSION_ALGORITHM;\n     }\n-    HFileContext hFileContext = new HFileContextBuilder()\n-                                .withIncludesMvcc(includeMVCCReadpoint)\n-                                .withIncludesTags(includesTag)\n-                                .withCompression(compression)\n-                                .withCompressTags(family.isCompressTags())\n-                                .withChecksumType(checksumType)\n-                                .withBytesPerCheckSum(bytesPerChecksum)\n-                                .withBlockSize(blocksize)\n-                                .withHBaseCheckSum(true)\n-                                .withDataBlockEncoding(family.getDataBlockEncoding())\n-                                .withEncryptionContext(cryptoContext)\n-                                .withCreateTime(EnvironmentEdgeManager.currentTime())\n-                                .withColumnFamily(family.getName())\n-                                .withTableName(region.getTableDescriptor()\n-                                    .getTableName().getName())\n-                                .withCellComparator(this.comparator)\n-                                .build();\n-    return hFileContext;\n+    HFileContext fileContext = storeContext.getDefaultFileContext();\n+    fileContext.setIncludesMvcc(includeMVCCReadpoint);\n+    fileContext.setIncludesTags(includesTag);\n+    fileContext.setCompression(compression);\n+    fileContext.setFileCreateTime(EnvironmentEdgeManager.currentTime());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDA2NQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 526}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3NTgyNDE2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQyMzozMToxMFrOIOrTvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwNTo0MjoxMVrOIOypfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDU0MQ==", "bodyText": "I would like to know here in class comment if this is read-only/immutable. I think it should be.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552260541", "createdAt": "2021-01-05T23:31:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MTU3OQ==", "bodyText": "Also, just a comment... the HFileContext has this for a comment...\n\nRead-only HFile Context Information. Meta data that is used by HFileWriter/Readers and by\nHFileBlocks. Create one using the {@link HFileContextBuilder} (See HFileInfo and the HFile\nTrailer class).\n\nso it is for readers and writers.... So, StoreContext is probably fine but if you are wondering... here is incidences of Info.java vs Context.java....  If it helps.\nkalashnikov:hbase.apache.git stack$ find src/main/java -name *Info.java\nfind: src/main/java: No such file or directory\nkalashnikov:hbase.apache.git stack$ find hbase-*/src/main/java -name *Info.java\nhbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupTableInfo.java\nhbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupInfo.java\nhbase-client/src/main/java/org/apache/hadoop/hbase/security/SecurityInfo.java\nhbase-client/src/main/java/org/apache/hadoop/hbase/client/MutableRegionInfo.java\nhbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionInfo.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfo.java\nhbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/mode/DrillDownInfo.java\nhbase-hbtop/src/main/java/org/apache/hadoop/hbase/hbtop/field/FieldInfo.java\nhbase-metrics-api/src/main/java/org/apache/hadoop/hbase/metrics/MetricRegistryInfo.java\nhbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckTableInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/HbckRegionInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockWithScanInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScanInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/ipc/CallQueueInfo.java\nhbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/generated/THRegionInfo.java\nhbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java\n\n\nkalashnikov:hbase.apache.git stack$ find hbase-*/src/main/java -name *Context.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/Context.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDecodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileContext.java\nhbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/RowPrefixFixedLengthBloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/RowBloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/RowColBloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/util/BloomContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/ReaderContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/CompressionContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ScannerContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/NoLimitScannerContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlushContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcCallContext.java\nhbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcSchedulerContext.java", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552261579", "createdAt": "2021-01-05T23:34:25Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDU0MQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyMDY3OA==", "bodyText": "thanks for sharing a list of *Info and *Context, I browsed few of them and seems Context should be still okie. so let's keep it with StoreContext and made this change simple, and I add the immutable keyword in the block of the class comment.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552320678", "createdAt": "2021-01-06T01:56:22Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDU0MQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjM4MDc5Nw==", "bodyText": "Sounds good.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552380797", "createdAt": "2021-01-06T05:42:11Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the information on some of the meta data about the HStore. This\n+ * meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI2MDU0MQ=="}, "originalCommit": {"oid": "5e87534477d0f130a10476ae6b1ae2128c21d28c"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3OTM2MzY2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNzoxMjoyMlrOIPNgUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxODoyNjowN1rOIPRlVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyMDgxNg==", "bodyText": "You were going to change these back?", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552820816", "createdAt": "2021-01-06T17:12:22Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -268,34 +251,37 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       .addBytesMap(region.getTableDescriptor().getValues())\n       .addStringMap(family.getConfiguration())\n       .addBytesMap(family.getValues());\n-    this.blocksize = family.getBlocksize();\n+\n+    this.region = region;\n+    this.storeContext = initializeStoreContext(family);\n+\n+    // Assemble the store's home directory and Ensure it exists.\n+    getRegionFileSystem().createStoreDir(family.getNameAsString());\n \n     // set block storage policy for store directory\n     String policyName = family.getStoragePolicy();\n     if (null == policyName) {\n       policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n     }\n-    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n+    getRegionFileSystem().setStoragePolicy(getColumnFamilyName(), policyName.trim());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4NzYzOQ==", "bodyText": "changed back to use family and region.getRegionFileSystem() directly.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552887639", "createdAt": "2021-01-06T18:26:07Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -268,34 +251,37 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       .addBytesMap(region.getTableDescriptor().getValues())\n       .addStringMap(family.getConfiguration())\n       .addBytesMap(family.getValues());\n-    this.blocksize = family.getBlocksize();\n+\n+    this.region = region;\n+    this.storeContext = initializeStoreContext(family);\n+\n+    // Assemble the store's home directory and Ensure it exists.\n+    getRegionFileSystem().createStoreDir(family.getNameAsString());\n \n     // set block storage policy for store directory\n     String policyName = family.getStoragePolicy();\n     if (null == policyName) {\n       policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n     }\n-    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n+    getRegionFileSystem().setStoragePolicy(getColumnFamilyName(), policyName.trim());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyMDgxNg=="}, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3OTM2NDk4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNzoxMjozN1rOIPNhOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxODoyNjoyOFrOIPRmKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyMTA0OQ==", "bodyText": "ditto", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552821049", "createdAt": "2021-01-06T17:12:37Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -308,7 +294,7 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n     }\n \n-    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n+    this.storeEngine = createStoreEngine(this, this.conf, getComparator());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4Nzg1MQ==", "bodyText": "changed to use region.getCellComparator().", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552887851", "createdAt": "2021-01-06T18:26:28Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -308,7 +294,7 @@ protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n       this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n     }\n \n-    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n+    this.storeEngine = createStoreEngine(this, this.conf, getComparator());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyMTA0OQ=="}, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3OTM5Nzg2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNzoxODoyMFrOIPN2Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxODoyMTowMVrOIPRZXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyNjQxOA==", "bodyText": "Is this necessary? Its available on the ColumnFamilyDescriptor. Do we need to add new method on Store?\nWas this change always here or did it just show up in recent amendments to PRs (I don't remember seeing it before but probably just me).\nThanks.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552826418", "createdAt": "2021-01-06T17:18:20Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java", "diffHunk": "@@ -46,6 +46,8 @@\n   int PRIORITY_USER = 1;\n   int NO_PRIORITY = Integer.MIN_VALUE;\n \n+  int getBlockSize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg3NDg3Mw==", "bodyText": "this was new change in the latest commit of this PR, mainly it's used for HMobStore when calling createWriterInTmp. but let me change it using  getStoreContext().getBlockSize() to avoid adding a new interface.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552874873", "createdAt": "2021-01-06T18:10:38Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java", "diffHunk": "@@ -46,6 +46,8 @@\n   int PRIORITY_USER = 1;\n   int NO_PRIORITY = Integer.MIN_VALUE;\n \n+  int getBlockSize();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyNjQxOA=="}, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4NDU3Mw==", "bodyText": "Yes. Please. Then I'll +1 this nice PR. Thanks.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552884573", "createdAt": "2021-01-06T18:21:01Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java", "diffHunk": "@@ -46,6 +46,8 @@\n   int PRIORITY_USER = 1;\n   int NO_PRIORITY = Integer.MIN_VALUE;\n \n+  int getBlockSize();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjgyNjQxOA=="}, "originalCommit": {"oid": "8a0269fbe6faa6b12edfab1db13c36d923558576"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ4MDM5Njc2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQyMjowNzo0N1rOIPXl4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQxOTozMDo0MFrOIQegKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk4NjA4MQ==", "bodyText": "@apurtell ping again on the heapSize() and implements HeapSize for StoreContext class, could you have another look? I should have fixed it, but will wait for your approval/resolve in 1 day or 2 day before merging it.", "url": "https://github.com/apache/hbase/pull/2800#discussion_r552986081", "createdAt": "2021-01-06T22:07:47Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.crypto.Encryption;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the immutable information and references on some of the meta data about the HStore.\n+ * This meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final int blockSize;\n+  private final Encryption.Context encryptionContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.blockSize = builder.blockSize;\n+    this.encryptionContext = builder.encryptionContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public int getBlockSize() {\n+    return blockSize;\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {\n+    return encryptionContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public InetSocketAddress[] getFavoredNodes() {\n+    return favoredNodesSupplier.get();\n+  }\n+\n+  public ColumnFamilyDescriptor getFamily() {\n+    return family;\n+  }\n+\n+  public Path getFamilyStoreDirectoryPath() {\n+    return familyStoreDirectoryPath;\n+  }\n+\n+  public RegionCoprocessorHost getCoprocessorHost() {\n+    return coprocessorHost;\n+  }\n+\n+  public static Builder getBuilder() {\n+    return new Builder();\n+  }\n+\n+  @Override\n+  public long heapSize() {\n+    return FIXED_OVERHEAD;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDE0Nzg4MA==", "bodyText": "@apurtell sorry for pinging again, I will merge this evening if I don't hear from you", "url": "https://github.com/apache/hbase/pull/2800#discussion_r554147880", "createdAt": "2021-01-08T19:30:40Z", "author": {"login": "taklwu"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreContext.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.net.InetSocketAddress;\n+import java.util.Collection;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.io.HeapSize;\n+import org.apache.hadoop.hbase.io.crypto.Encryption;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.util.ClassSize;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * This carries the immutable information and references on some of the meta data about the HStore.\n+ * This meta data can be used across the HFileWriter/Readers and other HStore consumers without the\n+ * need of passing around the complete store.\n+ */\n+@InterfaceAudience.Private\n+public final class StoreContext implements HeapSize {\n+  public static final long FIXED_OVERHEAD = ClassSize.estimateBase(HStore.class, false);\n+\n+  private final int blockSize;\n+  private final Encryption.Context encryptionContext;\n+  private final CacheConfig cacheConf;\n+  private final HRegionFileSystem regionFileSystem;\n+  private final CellComparator comparator;\n+  private final BloomType bloomFilterType;\n+  private final Supplier<Collection<HStoreFile>> compactedFilesSupplier;\n+  private final Supplier<InetSocketAddress[]> favoredNodesSupplier;\n+  private final ColumnFamilyDescriptor family;\n+  private final Path familyStoreDirectoryPath;\n+  private final RegionCoprocessorHost coprocessorHost;\n+\n+  private StoreContext(Builder builder) {\n+    this.blockSize = builder.blockSize;\n+    this.encryptionContext = builder.encryptionContext;\n+    this.cacheConf = builder.cacheConf;\n+    this.regionFileSystem = builder.regionFileSystem;\n+    this.comparator = builder.comparator;\n+    this.bloomFilterType = builder.bloomFilterType;\n+    this.compactedFilesSupplier = builder.compactedFilesSupplier;\n+    this.favoredNodesSupplier = builder.favoredNodesSupplier;\n+    this.family = builder.family;\n+    this.familyStoreDirectoryPath = builder.familyStoreDirectoryPath;\n+    this.coprocessorHost = builder.coprocessorHost;\n+  }\n+\n+  public int getBlockSize() {\n+    return blockSize;\n+  }\n+\n+  public Encryption.Context getEncryptionContext() {\n+    return encryptionContext;\n+  }\n+\n+  public CacheConfig getCacheConf() {\n+    return cacheConf;\n+  }\n+\n+  public HRegionFileSystem getRegionFileSystem() {\n+    return regionFileSystem;\n+  }\n+\n+  public CellComparator getComparator() {\n+    return comparator;\n+  }\n+\n+  public BloomType getBloomFilterType() {\n+    return bloomFilterType;\n+  }\n+\n+  public Supplier<Collection<HStoreFile>> getCompactedFilesSupplier() {\n+    return compactedFilesSupplier;\n+  }\n+\n+  public InetSocketAddress[] getFavoredNodes() {\n+    return favoredNodesSupplier.get();\n+  }\n+\n+  public ColumnFamilyDescriptor getFamily() {\n+    return family;\n+  }\n+\n+  public Path getFamilyStoreDirectoryPath() {\n+    return familyStoreDirectoryPath;\n+  }\n+\n+  public RegionCoprocessorHost getCoprocessorHost() {\n+    return coprocessorHost;\n+  }\n+\n+  public static Builder getBuilder() {\n+    return new Builder();\n+  }\n+\n+  @Override\n+  public long heapSize() {\n+    return FIXED_OVERHEAD;\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk4NjA4MQ=="}, "originalCommit": {"oid": "4b8ab2955ad3d9810256d7f22d7de630c5617364"}, "originalPosition": 118}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2256, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}