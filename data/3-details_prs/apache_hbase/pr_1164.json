{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc0MDYxMzI4", "number": 1164, "title": "HBASE-23331: Documentation for HBASE-18095", "bodyText": "", "createdAt": "2020-02-12T02:35:24Z", "url": "https://github.com/apache/hbase/pull/1164", "merged": true, "mergeCommit": {"oid": "985ac149b0898df6f84629b2c1c4f6710d1e0e88"}, "closed": true, "closedAt": "2020-02-13T01:04:52Z", "author": {"login": "bharathv"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcDgAdRgFqTM1NzIwMzYyMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcDxtNXgFqTM1NzkxNjc1OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MjAzNjIw", "url": "https://github.com/apache/hbase/pull/1164#pullrequestreview-357203620", "createdAt": "2020-02-12T05:54:14Z", "commit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "state": "APPROVED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwNTo1NDoxNFrOFoifMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwNjowNDoxN1rOFoioBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1MjQwMQ==", "bodyText": "s/source of truth/store/ ?", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378052401", "createdAt": "2020-02-12T05:54:14Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1MjU0OQ==", "bodyText": "Good", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378052549", "createdAt": "2020-02-12T05:55:02Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1Mzg1OA==", "bodyText": "s/Goal/Description/", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378053858", "createdAt": "2020-02-12T06:00:35Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDA0Ng==", "bodyText": "s/We would like to remove this/This feature removes the/", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378054046", "createdAt": "2020-02-12T06:01:27Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDE0NQ==", "bodyText": "This is the refguide. Audience is operators. Also feature is 'done' so s/Proposed//", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378054145", "createdAt": "2020-02-12T06:01:58Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDMxMQ==", "bodyText": "Good", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378054311", "createdAt": "2020-02-12T06:02:49Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.\n+* There is a higher connection load on the masters than before.\n+* More state synchronization traffic (see below)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDQ1MQ==", "bodyText": "s/they should/they/", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378054451", "createdAt": "2020-02-12T06:03:23Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.\n+* There is a higher connection load on the masters than before.\n+* More state synchronization traffic (see below)\n+\n+End users should factor these requirements into their cluster deployment if they intend to use this feature.\n+\n+=== Server side changes\n+\n+Now that the master end points are considered as source of truth for clients, they should track the latest meta information for clusterID, active master and meta table locations. Since the clients can connect to any master end point (details below), all the masters (active/standby) now track all the relevant meta information. The idea is to implement an in-memory cache local to all the masters and it should keep up with changes to this metadata. This is tracked in the following jiras.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDU4Mw==", "bodyText": "s/is to implement/is that we implement/", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378054583", "createdAt": "2020-02-12T06:03:56Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.\n+* There is a higher connection load on the masters than before.\n+* More state synchronization traffic (see below)\n+\n+End users should factor these requirements into their cluster deployment if they intend to use this feature.\n+\n+=== Server side changes\n+\n+Now that the master end points are considered as source of truth for clients, they should track the latest meta information for clusterID, active master and meta table locations. Since the clients can connect to any master end point (details below), all the masters (active/standby) now track all the relevant meta information. The idea is to implement an in-memory cache local to all the masters and it should keep up with changes to this metadata. This is tracked in the following jiras.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDQ1MQ=="}, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA1NDY2Mw==", "bodyText": "TMI No need of these JIRA listings.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378054663", "createdAt": "2020-02-12T06:04:17Z", "author": {"login": "saintstack"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.\n+* There is a higher connection load on the masters than before.\n+* More state synchronization traffic (see below)\n+\n+End users should factor these requirements into their cluster deployment if they intend to use this feature.\n+\n+=== Server side changes\n+\n+Now that the master end points are considered as source of truth for clients, they should track the latest meta information for clusterID, active master and meta table locations. Since the clients can connect to any master end point (details below), all the masters (active/standby) now track all the relevant meta information. The idea is to implement an in-memory cache local to all the masters and it should keep up with changes to this metadata. This is tracked in the following jiras.\n+\n+* Clusterid tracking - https://issues.apache.org/jira/browse/HBASE-23257[HBASE-23257]\n+* Active master tracking - https://issues.apache.org/jira/browse/HBASE-23275[HBASE-23275]\n+* Meta location tracking - https://issues.apache.org/jira/browse/HBASE-23281[HBASE-23281]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MjE0NzA2", "url": "https://github.com/apache/hbase/pull/1164#pullrequestreview-357214706", "createdAt": "2020-02-12T06:32:30Z", "commit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3NzA0ODA1", "url": "https://github.com/apache/hbase/pull/1164#pullrequestreview-357704805", "createdAt": "2020-02-12T18:56:48Z", "commit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c", "author": {"user": {"login": "bharathv", "name": "Bharath Vissapragada"}}, "url": "https://github.com/apache/hbase/commit/51d59f5c2147de397474525ca3d4918e36b5389c", "committedDate": "2020-02-13T00:59:35Z", "message": "HBASE-23331: Documentation for HBASE-18095"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1b929ab64308eba56d00fcb524cada650fd9ad25", "author": {"user": {"login": "bharathv", "name": "Bharath Vissapragada"}}, "url": "https://github.com/apache/hbase/commit/1b929ab64308eba56d00fcb524cada650fd9ad25", "committedDate": "2020-02-12T02:32:35Z", "message": "HBASE-23331: Documentation for HBASE-18095"}, "afterCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c", "author": {"user": {"login": "bharathv", "name": "Bharath Vissapragada"}}, "url": "https://github.com/apache/hbase/commit/51d59f5c2147de397474525ca3d4918e36b5389c", "committedDate": "2020-02-13T00:59:35Z", "message": "HBASE-23331: Documentation for HBASE-18095"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3OTA3MTE5", "url": "https://github.com/apache/hbase/pull/1164#pullrequestreview-357907119", "createdAt": "2020-02-13T01:44:17Z", "commit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMTo0NDoxN1rOFpEjwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMTo1Mjo0MlrOFpErOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMDYyNg==", "bodyText": "I know we nit-picked over this language earlier. With this feature enabled, this change places the masters into the client lifecycle under all scenarios. It's simply the lifecycle. That involvement mirrors the current ZK involvement, which is to say that the masters are introduced into the path of service discovery that a client undertakes in order to locate the data it seeks.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378610626", "createdAt": "2020-02-13T01:44:17Z", "author": {"login": "ndimiduk"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTIwMQ==", "bodyText": "Is there an issue filed for this? Might as well link it here.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378611201", "createdAt": "2020-02-13T01:46:48Z", "author": {"login": "ndimiduk"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.\n+* There is a higher connection load on the masters than before.\n+* More state synchronization traffic (see below)\n+\n+End users should factor these requirements into their cluster deployment if they intend to use this feature.\n+\n+=== Server side changes\n+\n+Now that the master end points are considered as source of truth for clients, they should track the latest meta information for clusterID, active master and meta table locations. Since the clients can connect to any master end point (details below), all the masters (active/standby) now track all the relevant meta information. The idea is to implement an in-memory cache local to all the masters and it should keep up with changes to this metadata. This is tracked in the following jiras.\n+\n+* Clusterid tracking - https://issues.apache.org/jira/browse/HBASE-23257[HBASE-23257]\n+* Active master tracking - https://issues.apache.org/jira/browse/HBASE-23275[HBASE-23275]\n+* Meta location tracking - https://issues.apache.org/jira/browse/HBASE-23281[HBASE-23281]\n+\n+Masters and region servers (all cluster internal processes) still use ZK for cluster state coordination. Masters additionally cache this information in-memory and rely on ZK listeners and watchers to track the changes to them. New RPC endpoints are added to serve this information to the clients. Having an in-memory cache can speed up client lookups rather than fetching from ZK synchronously with the client lookup RPCs.\n+\n+=== Client side changes\n+\n+The proposal is to implement a new AsyncRegistry (https://issues.apache.org/jira/browse/HBASE-23305[HBASE-23305]) based on the master RPC endpoints discussed above. Few interesting optimizations on the client side as follows.\n+\n+* Each client randomly picks a master from the list of host:ports passed in the configuration. This avoids hotspotting on a single master.\n+* Client can also do hedged lookups, meaning a single RPC to fetch meta information (say active master) can be sent to multiple masters and which ever returns first can be passed along to the caller. This can be done under a config flag since it comes with an additional RPC load. The default behavior is to randomly probe masters until the list is exhausted.\n+* Callers are expected to cache the meta information in higher levels and only probe again if the cached information is stale (which is quite possible).\n+\n+One user noted that there are some clients that rely on cluster ID for delegation token based auth. So there is a proposal to expose it on an auth-less end point for delegation auth support..", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTQyNQ==", "bodyText": "New as of 2.3.0, right? I think you'll want to update this throughout the doc.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378611425", "createdAt": "2020-02-13T01:47:53Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -260,6 +260,73 @@ For region name, we only accept `byte[]` as the parameter type and it may be a f\n \n Information on non-Java clients and custom protocols is covered in <<external_apis>>\n \n+[[client.masterregistry]]\n+=== Master registry (new as of release 3.0.0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTcyMg==", "bodyText": "s/beased/based/", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378611722", "createdAt": "2020-02-13T01:49:10Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -260,6 +260,73 @@ For region name, we only accept `byte[]` as the parameter type and it may be a f\n \n Information on non-Java clients and custom protocols is covered in <<external_apis>>\n \n+[[client.masterregistry]]\n+=== Master registry (new as of release 3.0.0)\n+\n+Client internally works with a _connection registry_ to fetch the metadata needed by connections.\n+This connection registry implementation is responsible for fetching the following metadata.\n+\n+* Active master address\n+* Current meta region(s) locations\n+* Cluster ID (unique to this cluster)\n+\n+This information is needed as a part of various client operations like connection set up, scans,\n+gets etc. Up until releases 2.x.y, the default connection registry is based on ZooKeeper as the\n+source of truth and the the clients fetched the metadata from zookeeper znodes. As of release 3.0.0,\n+the default implementation for connection registry has been switched  to a master based\n+implementation. With this change, the clients now fetch the required metadata from master RPC end\n+points directly. This change was done for the following reasons.\n+\n+* Reduce load on ZooKeeper since that is critical for cluster operation.\n+* Holistic client timeout and retry configurations since the new registry brings all the client\n+operations under HBase rpc framework.\n+* Remove the ZooKeeper client dependency on HBase client library.\n+\n+This means that\n+\n+* At least a single active or stand by master is needed for cluster connection setup. Refer to\n+<<master.runtime>> for more details.\n+* Master can be in a critical path of read/write operations, especially if the client metadata cache\n+is empty or stale.\n+* There is higher connection load on the masters that before since the clients talk directly to\n+HMasters instead of ZooKeeper ensemble`\n+\n+To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed\n+service to fetch the connection metadata. This lets the client connect to any master (not just active).\n+\n+==== RPC hedging\n+\n+This feature also implements an new RPC channel that can hedge requests to multiple masters. This\n+lets the client make the same request to multiple servers and which ever responds first is returned\n+back to the client and the other other in-flight requests are canceled. This improves the\n+performance, especially when a subset of servers are under load. The hedging fan out size is\n+configurable, meaning the number of requests that are hedged in a single attempt, using the\n+configuration key _hbase.rpc.hedged.fanout_ in the client configuration. It defaults to 2. With this\n+default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not\n+adapt to any sort of live rpc performance metrics.\n+\n+==== Additional Notes\n+\n+* Clients hedge the requests in a randomized order to avoid hot-spotting a single server.\n+* Cluster internal connections (master<->regionservers) still use ZooKeeper based connection\n+registry.\n+* Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same\n+as before.\n+* Inter cluster replication still uses ZooKeeper beased connection registry to simplify configuration", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTg2Nw==", "bodyText": "It's good that this is gated by a single config change.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378611867", "createdAt": "2020-02-13T01:49:48Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -260,6 +260,73 @@ For region name, we only accept `byte[]` as the parameter type and it may be a f\n \n Information on non-Java clients and custom protocols is covered in <<external_apis>>\n \n+[[client.masterregistry]]\n+=== Master registry (new as of release 3.0.0)\n+\n+Client internally works with a _connection registry_ to fetch the metadata needed by connections.\n+This connection registry implementation is responsible for fetching the following metadata.\n+\n+* Active master address\n+* Current meta region(s) locations\n+* Cluster ID (unique to this cluster)\n+\n+This information is needed as a part of various client operations like connection set up, scans,\n+gets etc. Up until releases 2.x.y, the default connection registry is based on ZooKeeper as the\n+source of truth and the the clients fetched the metadata from zookeeper znodes. As of release 3.0.0,\n+the default implementation for connection registry has been switched  to a master based\n+implementation. With this change, the clients now fetch the required metadata from master RPC end\n+points directly. This change was done for the following reasons.\n+\n+* Reduce load on ZooKeeper since that is critical for cluster operation.\n+* Holistic client timeout and retry configurations since the new registry brings all the client\n+operations under HBase rpc framework.\n+* Remove the ZooKeeper client dependency on HBase client library.\n+\n+This means that\n+\n+* At least a single active or stand by master is needed for cluster connection setup. Refer to\n+<<master.runtime>> for more details.\n+* Master can be in a critical path of read/write operations, especially if the client metadata cache\n+is empty or stale.\n+* There is higher connection load on the masters that before since the clients talk directly to\n+HMasters instead of ZooKeeper ensemble`\n+\n+To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed\n+service to fetch the connection metadata. This lets the client connect to any master (not just active).\n+\n+==== RPC hedging\n+\n+This feature also implements an new RPC channel that can hedge requests to multiple masters. This\n+lets the client make the same request to multiple servers and which ever responds first is returned\n+back to the client and the other other in-flight requests are canceled. This improves the\n+performance, especially when a subset of servers are under load. The hedging fan out size is\n+configurable, meaning the number of requests that are hedged in a single attempt, using the\n+configuration key _hbase.rpc.hedged.fanout_ in the client configuration. It defaults to 2. With this\n+default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not\n+adapt to any sort of live rpc performance metrics.\n+\n+==== Additional Notes\n+\n+* Clients hedge the requests in a randomized order to avoid hot-spotting a single server.\n+* Cluster internal connections (master<->regionservers) still use ZooKeeper based connection\n+registry.\n+* Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same\n+as before.\n+* Inter cluster replication still uses ZooKeeper beased connection registry to simplify configuration\n+management.\n+\n+For more implementation details, please refer to the https://github.com/apache/hbase/tree/master/dev-support/design-docs[design doc] and\n+https://issues.apache.org/jira/browse/HBASE-18095[HBASE-18095].\n+\n+'''\n+NOTE: (Advanced) In case of any issues with the master based registry, use the following\n+configuration to fallback to the ZooKeeper based connection registry implementation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMjI5Mg==", "bodyText": "a/now/know/", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378612292", "createdAt": "2020-02-13T01:51:37Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/configuration.adoc", "diffHunk": "@@ -563,38 +563,63 @@ Changes here will require a cluster restart for HBase to notice the change thoug\n \n If you are running HBase in standalone mode, you don't need to configure anything for your client to work provided that they are all on the same machine.\n \n-Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for current critical locations.\n-ZooKeeper is where all these values are kept.\n-Thus clients require the location of the ZooKeeper ensemble before they can do anything else.\n-Usually this ensemble location is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.\n+Starting release 3.0.0, the default connection registry has been switched to a master based implementation. Refer to <<client.masterregistry>> for more details about\n+what a connection registry is and implications of this change. Depending on your HBase version, following is the expected minimal client configuration.\n \n-If you are configuring an IDE to run an HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).\n+==== Up until 2.x.y releases\n+In 2.x.y releases, the default connection registry was based on ZooKeeper as the source of truth. This means that the clients always looked up ZooKeeper znodes to fetch\n+the required metadata. For example, if an active master crashed and the a new master is elected, clients looked up the master znode to fetch\n+the active master address (similarly for meta locations). This meant that the clients needed to have access to ZooKeeper and need to now", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMjUzOQ==", "bodyText": "nit: is dependency information relevant in the configuration section?", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378612539", "createdAt": "2020-02-13T01:52:42Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/configuration.adoc", "diffHunk": "@@ -563,38 +563,63 @@ Changes here will require a cluster restart for HBase to notice the change thoug\n \n If you are running HBase in standalone mode, you don't need to configure anything for your client to work provided that they are all on the same machine.\n \n-Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for current critical locations.\n-ZooKeeper is where all these values are kept.\n-Thus clients require the location of the ZooKeeper ensemble before they can do anything else.\n-Usually this ensemble location is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.\n+Starting release 3.0.0, the default connection registry has been switched to a master based implementation. Refer to <<client.masterregistry>> for more details about\n+what a connection registry is and implications of this change. Depending on your HBase version, following is the expected minimal client configuration.\n \n-If you are configuring an IDE to run an HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).\n+==== Up until 2.x.y releases\n+In 2.x.y releases, the default connection registry was based on ZooKeeper as the source of truth. This means that the clients always looked up ZooKeeper znodes to fetch\n+the required metadata. For example, if an active master crashed and the a new master is elected, clients looked up the master znode to fetch\n+the active master address (similarly for meta locations). This meant that the clients needed to have access to ZooKeeper and need to now\n+the ZooKeeper ensemble information before they can do anything. This can be configured in the client configuration xml as follows:\n \n-For Java applications using Maven, including the hbase-shaded-client module is the recommended dependency when connecting to a cluster:\n [source,xml]\n ----\n-<dependency>\n-  <groupId>org.apache.hbase</groupId>\n-  <artifactId>hbase-shaded-client</artifactId>\n-  <version>2.0.0</version>\n-</dependency>\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+<configuration>\n+  <property>\n+    <name>hbase.zookeeper.quorum</name>\n+    <value>example1,example2,example3</value>\n+    <description> Zookeeper ensemble information</description>\n+  </property>\n+</configuration>\n ----\n \n-A basic example _hbase-site.xml_ for client only may look as follows:\n+==== Starting 3.0.0 release\n+\n+The default implementation was switched to a master based connection registry. With this implementation, clients always contact the active or\n+stand-by master RPC end points to fetch the the connection registry information. This means that the clients should have access to the list of active and master\n+end points before they can do anything. This can be configured in the client configuration xml as follows:\n+\n [source,xml]\n ----\n <?xml version=\"1.0\"?>\n <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n <configuration>\n   <property>\n-    <name>hbase.zookeeper.quorum</name>\n+    <name>hbase.masters</name>\n     <value>example1,example2,example3</value>\n-    <description>The directory shared by region servers.\n-    </description>\n+    <description>List of master rpc end points for the hbase cluster.</description>\n   </property>\n </configuration>\n ----\n \n+The configuration value for _hbase.masters_ is a comma separated list of _host:port_ values. If no port value is specified, the default of _16000_ is assumed.\n+\n+Usually this configuration is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.\n+\n+If you are configuring an IDE to run an HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).\n+\n+For Java applications using Maven, including the hbase-shaded-client module is the recommended dependency when connecting to a cluster:\n+[source,xml]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3OTE2NzU4", "url": "https://github.com/apache/hbase/pull/1164#pullrequestreview-357916758", "createdAt": "2020-02-13T02:19:36Z", "commit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMjoxOTozNlrOFpFDow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QwMjo0MjowMlrOFpFXqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxODc4Nw==", "bodyText": "HBASE-23330, yes that was fixed too.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378618787", "createdAt": "2020-02-13T02:19:36Z", "author": {"login": "bharathv"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.\n+* There is a higher connection load on the masters than before.\n+* More state synchronization traffic (see below)\n+\n+End users should factor these requirements into their cluster deployment if they intend to use this feature.\n+\n+=== Server side changes\n+\n+Now that the master end points are considered as source of truth for clients, they should track the latest meta information for clusterID, active master and meta table locations. Since the clients can connect to any master end point (details below), all the masters (active/standby) now track all the relevant meta information. The idea is to implement an in-memory cache local to all the masters and it should keep up with changes to this metadata. This is tracked in the following jiras.\n+\n+* Clusterid tracking - https://issues.apache.org/jira/browse/HBASE-23257[HBASE-23257]\n+* Active master tracking - https://issues.apache.org/jira/browse/HBASE-23275[HBASE-23275]\n+* Meta location tracking - https://issues.apache.org/jira/browse/HBASE-23281[HBASE-23281]\n+\n+Masters and region servers (all cluster internal processes) still use ZK for cluster state coordination. Masters additionally cache this information in-memory and rely on ZK listeners and watchers to track the changes to them. New RPC endpoints are added to serve this information to the clients. Having an in-memory cache can speed up client lookups rather than fetching from ZK synchronously with the client lookup RPCs.\n+\n+=== Client side changes\n+\n+The proposal is to implement a new AsyncRegistry (https://issues.apache.org/jira/browse/HBASE-23305[HBASE-23305]) based on the master RPC endpoints discussed above. Few interesting optimizations on the client side as follows.\n+\n+* Each client randomly picks a master from the list of host:ports passed in the configuration. This avoids hotspotting on a single master.\n+* Client can also do hedged lookups, meaning a single RPC to fetch meta information (say active master) can be sent to multiple masters and which ever returns first can be passed along to the caller. This can be done under a config flag since it comes with an additional RPC load. The default behavior is to randomly probe masters until the list is exhausted.\n+* Callers are expected to cache the meta information in higher levels and only probe again if the cached information is stale (which is quite possible).\n+\n+One user noted that there are some clients that rely on cluster ID for delegation token based auth. So there is a proposal to expose it on an auth-less end point for delegation auth support..", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTIwMQ=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxODk4OA==", "bodyText": "I wanted to update the docs once it actually makes it to the 2.3.0...that ok? (didn't want to do that without committing to the branch).", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378618988", "createdAt": "2020-02-13T02:20:29Z", "author": {"login": "bharathv"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -260,6 +260,73 @@ For region name, we only accept `byte[]` as the parameter type and it may be a f\n \n Information on non-Java clients and custom protocols is covered in <<external_apis>>\n \n+[[client.masterregistry]]\n+=== Master registry (new as of release 3.0.0)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTQyNQ=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxOTA3Mw==", "bodyText": "Done.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378619073", "createdAt": "2020-02-13T02:20:51Z", "author": {"login": "bharathv"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -260,6 +260,73 @@ For region name, we only accept `byte[]` as the parameter type and it may be a f\n \n Information on non-Java clients and custom protocols is covered in <<external_apis>>\n \n+[[client.masterregistry]]\n+=== Master registry (new as of release 3.0.0)\n+\n+Client internally works with a _connection registry_ to fetch the metadata needed by connections.\n+This connection registry implementation is responsible for fetching the following metadata.\n+\n+* Active master address\n+* Current meta region(s) locations\n+* Cluster ID (unique to this cluster)\n+\n+This information is needed as a part of various client operations like connection set up, scans,\n+gets etc. Up until releases 2.x.y, the default connection registry is based on ZooKeeper as the\n+source of truth and the the clients fetched the metadata from zookeeper znodes. As of release 3.0.0,\n+the default implementation for connection registry has been switched  to a master based\n+implementation. With this change, the clients now fetch the required metadata from master RPC end\n+points directly. This change was done for the following reasons.\n+\n+* Reduce load on ZooKeeper since that is critical for cluster operation.\n+* Holistic client timeout and retry configurations since the new registry brings all the client\n+operations under HBase rpc framework.\n+* Remove the ZooKeeper client dependency on HBase client library.\n+\n+This means that\n+\n+* At least a single active or stand by master is needed for cluster connection setup. Refer to\n+<<master.runtime>> for more details.\n+* Master can be in a critical path of read/write operations, especially if the client metadata cache\n+is empty or stale.\n+* There is higher connection load on the masters that before since the clients talk directly to\n+HMasters instead of ZooKeeper ensemble`\n+\n+To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed\n+service to fetch the connection metadata. This lets the client connect to any master (not just active).\n+\n+==== RPC hedging\n+\n+This feature also implements an new RPC channel that can hedge requests to multiple masters. This\n+lets the client make the same request to multiple servers and which ever responds first is returned\n+back to the client and the other other in-flight requests are canceled. This improves the\n+performance, especially when a subset of servers are under load. The hedging fan out size is\n+configurable, meaning the number of requests that are hedged in a single attempt, using the\n+configuration key _hbase.rpc.hedged.fanout_ in the client configuration. It defaults to 2. With this\n+default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not\n+adapt to any sort of live rpc performance metrics.\n+\n+==== Additional Notes\n+\n+* Clients hedge the requests in a randomized order to avoid hot-spotting a single server.\n+* Cluster internal connections (master<->regionservers) still use ZooKeeper based connection\n+registry.\n+* Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same\n+as before.\n+* Inter cluster replication still uses ZooKeeper beased connection registry to simplify configuration", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTcyMg=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxOTEyMQ==", "bodyText": "Yep.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378619121", "createdAt": "2020-02-13T02:21:00Z", "author": {"login": "bharathv"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -260,6 +260,73 @@ For region name, we only accept `byte[]` as the parameter type and it may be a f\n \n Information on non-Java clients and custom protocols is covered in <<external_apis>>\n \n+[[client.masterregistry]]\n+=== Master registry (new as of release 3.0.0)\n+\n+Client internally works with a _connection registry_ to fetch the metadata needed by connections.\n+This connection registry implementation is responsible for fetching the following metadata.\n+\n+* Active master address\n+* Current meta region(s) locations\n+* Cluster ID (unique to this cluster)\n+\n+This information is needed as a part of various client operations like connection set up, scans,\n+gets etc. Up until releases 2.x.y, the default connection registry is based on ZooKeeper as the\n+source of truth and the the clients fetched the metadata from zookeeper znodes. As of release 3.0.0,\n+the default implementation for connection registry has been switched  to a master based\n+implementation. With this change, the clients now fetch the required metadata from master RPC end\n+points directly. This change was done for the following reasons.\n+\n+* Reduce load on ZooKeeper since that is critical for cluster operation.\n+* Holistic client timeout and retry configurations since the new registry brings all the client\n+operations under HBase rpc framework.\n+* Remove the ZooKeeper client dependency on HBase client library.\n+\n+This means that\n+\n+* At least a single active or stand by master is needed for cluster connection setup. Refer to\n+<<master.runtime>> for more details.\n+* Master can be in a critical path of read/write operations, especially if the client metadata cache\n+is empty or stale.\n+* There is higher connection load on the masters that before since the clients talk directly to\n+HMasters instead of ZooKeeper ensemble`\n+\n+To reduce hot-spotting on a single master, all the masters (active & stand-by) expose the needed\n+service to fetch the connection metadata. This lets the client connect to any master (not just active).\n+\n+==== RPC hedging\n+\n+This feature also implements an new RPC channel that can hedge requests to multiple masters. This\n+lets the client make the same request to multiple servers and which ever responds first is returned\n+back to the client and the other other in-flight requests are canceled. This improves the\n+performance, especially when a subset of servers are under load. The hedging fan out size is\n+configurable, meaning the number of requests that are hedged in a single attempt, using the\n+configuration key _hbase.rpc.hedged.fanout_ in the client configuration. It defaults to 2. With this\n+default, the RPCs are tried in batches of 2. The hedging policy is still primitive and does not\n+adapt to any sort of live rpc performance metrics.\n+\n+==== Additional Notes\n+\n+* Clients hedge the requests in a randomized order to avoid hot-spotting a single server.\n+* Cluster internal connections (master<->regionservers) still use ZooKeeper based connection\n+registry.\n+* Cluster internal state is still tracked in Zookeeper, hence ZK availability requirements are same\n+as before.\n+* Inter cluster replication still uses ZooKeeper beased connection registry to simplify configuration\n+management.\n+\n+For more implementation details, please refer to the https://github.com/apache/hbase/tree/master/dev-support/design-docs[design doc] and\n+https://issues.apache.org/jira/browse/HBASE-18095[HBASE-18095].\n+\n+'''\n+NOTE: (Advanced) In case of any issues with the master based registry, use the following\n+configuration to fallback to the ZooKeeper based connection registry implementation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMTg2Nw=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxOTI2Mw==", "bodyText": "Done.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378619263", "createdAt": "2020-02-13T02:21:35Z", "author": {"login": "bharathv"}, "path": "src/main/asciidoc/_chapters/configuration.adoc", "diffHunk": "@@ -563,38 +563,63 @@ Changes here will require a cluster restart for HBase to notice the change thoug\n \n If you are running HBase in standalone mode, you don't need to configure anything for your client to work provided that they are all on the same machine.\n \n-Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for current critical locations.\n-ZooKeeper is where all these values are kept.\n-Thus clients require the location of the ZooKeeper ensemble before they can do anything else.\n-Usually this ensemble location is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.\n+Starting release 3.0.0, the default connection registry has been switched to a master based implementation. Refer to <<client.masterregistry>> for more details about\n+what a connection registry is and implications of this change. Depending on your HBase version, following is the expected minimal client configuration.\n \n-If you are configuring an IDE to run an HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).\n+==== Up until 2.x.y releases\n+In 2.x.y releases, the default connection registry was based on ZooKeeper as the source of truth. This means that the clients always looked up ZooKeeper znodes to fetch\n+the required metadata. For example, if an active master crashed and the a new master is elected, clients looked up the master znode to fetch\n+the active master address (similarly for meta locations). This meant that the clients needed to have access to ZooKeeper and need to now", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMjI5Mg=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYyMDc1OA==", "bodyText": "ya seems out of place but the section is named \"Client configuration and dependencies connecting to an HBase cluster\".. so probably was intentional.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378620758", "createdAt": "2020-02-13T02:28:04Z", "author": {"login": "bharathv"}, "path": "src/main/asciidoc/_chapters/configuration.adoc", "diffHunk": "@@ -563,38 +563,63 @@ Changes here will require a cluster restart for HBase to notice the change thoug\n \n If you are running HBase in standalone mode, you don't need to configure anything for your client to work provided that they are all on the same machine.\n \n-Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for current critical locations.\n-ZooKeeper is where all these values are kept.\n-Thus clients require the location of the ZooKeeper ensemble before they can do anything else.\n-Usually this ensemble location is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.\n+Starting release 3.0.0, the default connection registry has been switched to a master based implementation. Refer to <<client.masterregistry>> for more details about\n+what a connection registry is and implications of this change. Depending on your HBase version, following is the expected minimal client configuration.\n \n-If you are configuring an IDE to run an HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).\n+==== Up until 2.x.y releases\n+In 2.x.y releases, the default connection registry was based on ZooKeeper as the source of truth. This means that the clients always looked up ZooKeeper znodes to fetch\n+the required metadata. For example, if an active master crashed and the a new master is elected, clients looked up the master znode to fetch\n+the active master address (similarly for meta locations). This meant that the clients needed to have access to ZooKeeper and need to now\n+the ZooKeeper ensemble information before they can do anything. This can be configured in the client configuration xml as follows:\n \n-For Java applications using Maven, including the hbase-shaded-client module is the recommended dependency when connecting to a cluster:\n [source,xml]\n ----\n-<dependency>\n-  <groupId>org.apache.hbase</groupId>\n-  <artifactId>hbase-shaded-client</artifactId>\n-  <version>2.0.0</version>\n-</dependency>\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+<configuration>\n+  <property>\n+    <name>hbase.zookeeper.quorum</name>\n+    <value>example1,example2,example3</value>\n+    <description> Zookeeper ensemble information</description>\n+  </property>\n+</configuration>\n ----\n \n-A basic example _hbase-site.xml_ for client only may look as follows:\n+==== Starting 3.0.0 release\n+\n+The default implementation was switched to a master based connection registry. With this implementation, clients always contact the active or\n+stand-by master RPC end points to fetch the the connection registry information. This means that the clients should have access to the list of active and master\n+end points before they can do anything. This can be configured in the client configuration xml as follows:\n+\n [source,xml]\n ----\n <?xml version=\"1.0\"?>\n <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n <configuration>\n   <property>\n-    <name>hbase.zookeeper.quorum</name>\n+    <name>hbase.masters</name>\n     <value>example1,example2,example3</value>\n-    <description>The directory shared by region servers.\n-    </description>\n+    <description>List of master rpc end points for the hbase cluster.</description>\n   </property>\n </configuration>\n ----\n \n+The configuration value for _hbase.masters_ is a comma separated list of _host:port_ values. If no port value is specified, the default of _16000_ is assumed.\n+\n+Usually this configuration is kept out in the _hbase-site.xml_ and is picked up by the client from the `CLASSPATH`.\n+\n+If you are configuring an IDE to run an HBase client, you should include the _conf/_ directory on your classpath so _hbase-site.xml_ settings can be found (or add _src/test/resources_ to pick up the hbase-site.xml used by tests).\n+\n+For Java applications using Maven, including the hbase-shaded-client module is the recommended dependency when connecting to a cluster:\n+[source,xml]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMjUzOQ=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYyMzkxMg==", "bodyText": "Fixed.", "url": "https://github.com/apache/hbase/pull/1164#discussion_r378623912", "createdAt": "2020-02-13T02:42:02Z", "author": {"login": "bharathv"}, "path": "dev-support/design-docs/HBASE-18095-Zookeeper-less-client-connection-design.adoc", "diffHunk": "@@ -0,0 +1,112 @@\n+////\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+////\n+\n+= HBASE-18095: Zookeeper-less client connection\n+\n+\n+== Context\n+Currently, Zookeeper (ZK) lies in the critical code path of connection init. To set up a connection to a given HBase cluster, client relies on the zookeeper quorum configured in the client hbase-site.xml and attempts to fetch the following information.\n+\n+* ClusterID\n+* Active HMaster server name\n+* Meta table region locations\n+\n+ZK is deemed the source of truth since other processes that maintain the cluster state persist the changes to this data into ZK. So it is an obvious place to look at for clients to fetch the latest cluster state.  However this comes with it\u2019s own set of problems, some of them are below.\n+\n+* Timeouts and retry logic for ZK clients are managed separately from HBase configuration. This is more administration overhead for end users (example: multiple timeouts are to be configured for different types of RPCs. client->master, client->ZK etc.). This prevents HBase from having a single holistic timeout configuration that applies to any RPCs.\n+* If there is any issue with ZK (like connection overload / timeouts), the entire HBase service appears frozen and there is little visibility into it.\n+* Exposing zookeeper to all the clients can be risky since it can potentially be abused to DDOS.\n+* Embedded ZK client is bundled with hbase client jar as a dependency (with it\u2019s log spew :-]).\n+\n+== Goal\n+\n+We would like to remove this ZK dependency in the HBase client and instead have the clients query a preconfigured list of active and standby master host:port addresses. This brings all the client interactions with HBase under the same RPC framework that is holistically controlled by a set of hbase client configuration parameters. This also alleviates the pressure on ZK cluster which is critical from an operational standpoint as some core processes like replication, log splitting, master election etc. depend on it. The next section describes the kind of changes needed on both server and client side to support this behavior.\n+\n+== Proposed design\n+\n+As mentioned above, clients now get a pre configured list active and standby master addresses that they can query to fetch the meta information needed for connection setup. Something like,\n+\n+[source, xml]\n+-----\n+<property>\n+  <name>hbase.masters</name>\n+  <value>master1:16000,master2:16001,master3:16000</value>\n+</property>\n+-----\n+\n+Clients should be robust enough to handle configuration changes to this parameter since master hosts can change (added/removed) over time and not every client can afford a restart.\n+\n+One thing to note here is that having masters in the init/read/write path for clients means that\n+\n+* At Least one active/standby master is now needed for connection creation. Earlier this was not a requirement because the clients looked up the cluster ID from the relevant znode and init successfully. So, technically a master need not be around to create a connection to the cluster.\n+* Masters are now active part of read write path in client life cycle under certain scenarios. If the client  cache of meta locations/active master is purged/stale, at least one master (active/stand-by) serving the latest information should exist. Earlier this information was served by ZK and clients look up the latest cluster ID/active master/meta locations from the relevant znodes and get going.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODYxMDYyNg=="}, "originalCommit": {"oid": "51d59f5c2147de397474525ca3d4918e36b5389c"}, "originalPosition": 60}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2775, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}