{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5MzE0MjE5", "number": 1730, "title": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction", "bodyText": "", "createdAt": "2020-05-18T07:35:53Z", "url": "https://github.com/apache/hbase/pull/1730", "merged": true, "mergeCommit": {"oid": "be57e40f365e34684be57f74eccbf22b0ef03db4"}, "closed": true, "closedAt": "2020-06-30T07:10:04Z", "author": {"login": "pengmq1"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABciqys7ABqjMzNDk3Njk5NzM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcwQPBQAFqTQzOTcyODAxMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a20c11a4da6696f4265301d4c3b15b827e446124", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/a20c11a4da6696f4265301d4c3b15b827e446124", "committedDate": "2020-05-18T07:31:31Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}, "afterCommit": {"oid": "764ea151e9bedfd7aa398456fc982bd4d17ebec8", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/764ea151e9bedfd7aa398456fc982bd4d17ebec8", "committedDate": "2020-05-19T02:09:57Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "764ea151e9bedfd7aa398456fc982bd4d17ebec8", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/764ea151e9bedfd7aa398456fc982bd4d17ebec8", "committedDate": "2020-05-19T02:09:57Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}, "afterCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/e3508fd533f9c9392494aff807c1270f306fc081", "committedDate": "2020-05-25T08:00:50Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4MDgwNTg3", "url": "https://github.com/apache/hbase/pull/1730#pullrequestreview-418080587", "createdAt": "2020-05-26T08:31:29Z", "commit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozMToyOVrOGaUAaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwODozNDoxNlrOGaUGxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0Mzk0NA==", "bodyText": "Great.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430243944", "createdAt": "2020-05-26T08:31:29Z", "author": {"login": "infraio"}, "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "diffHunk": "@@ -0,0 +1,122 @@\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+\n+# Heterogeneous Storage for Date Tiered Compaction\n+\n+## Objective\n+\n+Support DateTiredCompaction([HBASE-15181](https://issues.apache.org/jira/browse/HBASE-15181))\n+ for cold and hot data separation, support different storage policies for different time periods\n+ of data to get better performance, for example, we can configure the data of last 1 month in SSD,\n+ and 1 month ago data was in HDD.\n+\n++ Date Tiered Compaction (DTCP) is based on date tiering (date-aware), we hope to support\n+  the separation of cold and hot data, heterogeneous storage. Set different storage\n+  policies (in HDFS) for data in different time windows.\n++ DTCP designs different windows, and we can classify the windows according to\n+  the timestamps of the windows. For example: HOT window, WARM window, COLD window.\n++ DTCP divides storefiles into different windows, and performs minor Compaction within\n+  a time window. The storefile generated by Compaction will use the storage strategy of\n+  this window. For example, if a window is a HOT window, the storefile generated by compaction\n+  can be stored on the SSD. There are already WAL and the entire CF support storage policy\n+  (HBASE-12848, HBASE-14061), our goal is to achieve cold and hot separation in one CF or\n+  a region, using different storage policies.\n+\n+## Definition of hot and cold data\n+\n+Usually the data of the last 3 days can be defined as `HOT data`, hot age = 3 days.\n+ If the timestamp of the data is > (timestamp now - hot age), we think the data is hot data.\n+ Warm age, cold age can be defined in the same way. Only one type of data is allowed.\n+ ```\n+  if timestamp >  (now - hot age) , HOT data\n+  else if timestamp >  (now - warm age), WARM data\n+  else if timestamp >  (now - cold age), COLD data\n+  else  default, COLD data\n+```\n+\n+## Time window\n+When given a time now, it is the time when the compaction occurs. Each window and the size of\n+ the window are automatically calculated by DTCP, and the window boundary is rounded according\n+ to the base size.\n+Assuming that the base window size is 1 hour, and each tier has 3 windows, the current time is\n+ between 12:00 and 13:00. We have defined three types of winow (`HOT, WARM, COLD`). The type of\n+ winodw is determined by the timestamp at the beginning of the window and the timestamp now.\n+As shown in the figure 1 below, the type of each window can be determined by the age range\n+ (hot / warm / cold) where (now - window.startTimestamp) falls. Cold age can not need to be set,\n+ the default Long.MAX, meaning that the window with a very early time stamp belongs to the\n+ cold window.\n+![figure 1](https://raw.githubusercontent.com/pengmq1/images/master/F1-HDTCP.png \"figure 1\")\n+\n+## Example configuration\n+\n+| Configuration Key | value | Note |\n+|:---|:---:|:---|\n+|hbase.hstore.compaction.date.tiered.storage.policy.enable|true|if or not use storage policy for window. Default is false|\n+|hbase.hstore.compaction.date.tiered.hot.window.age.millis|3600000|hot data age\n+|hbase.hstore.compaction.date.tiered.hot.window.storage.policy|ALL_SSD|hot data storage policy, Corresponding HDFS storage policy\n+|hbase.hstore.compaction.date.tiered.warm.window.age.millis|20600000||\n+|hbase.hstore.compaction.date.tiered.warm.window.storage.policy|ONE_SSD||\n+|hbase.hstore.compaction.date.tiered.cold.window.age.millis|Long.MAX||\n+|hbase.hstore.compaction.date.tiered.cold.window.storage.policy|HOT||\n+\n+The original date tiered compaction related configuration has the same meaning and maintains\n+ compatibility.\n+If `hbase.hstore.compaction.date.tiered.storage.policy.enable = false`. DTCP still follows the\n+ original logic and has not changed.\n+\n+## Storage strategy\n+HDFS provides the following storage policies, you can refer to\n+ https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html\n+ \n+|Policy ID | Policy Name | Block Placement (3  replicas)|\n+|:---|:---|:---|\n+|15|Lasy_Persist|RAM_DISK: 1, DISK: 2|\n+|12|All_SSD|SSD: 3|\n+|10|One_SSD|SSD: 1, DISK: 2|\n+|7|Hot (default)|DISK: 3|\n+|5|Warm|DISK: 1, ARCHIVE: 2|\n+|2|Cold|ARCHIVE: 3|\n+\n+Date Tiered Compaction (DTCP) supports the output of multiple storefiles. We hope that these\n+ storefiles can be set with different storage policies (in HDFS). \n+ Therefore, through DateTieredMultiFileWriter to generate different StoreFileWriters with\n+  storage policy to achieve the purpose.\n+  \n+## Why use different child tmp dir \n+Before StoreFileWriter writes a storefile, we can create different dirs in the tmp directory\n+ of the region and set the corresponding storage policy for these dirs. This way\n+  StoreFileWriter can write files to different dirs.  \n+Since **HDFS** does not support the create file with the storage policy parameter\n+ (maybe I am wrong, I did not find the relevant interface on hadoop 2.6), and HDFS cannot\n+ set a storage policy for a file / dir path that does not yet exist. When the compaction ends,\n+ the storefile path must exist at this time, and I set the storage policy to Storefile.  \n+But, in HDFS, when the file is written first, and then the storage policy is set.\n+ The actual storage location of the data does not match the storage policy. For example,\n+ write three copies of a file (1 block) in the HDD, then set storage policy is ALL_SSD,\n+ but the data block will not be moved to the SSD immediately.\n+ \u201cHDFS wont move the file content across different block volumes on rename\u201d. Data movement\n+ requires the HDFS mover tool, or use HDFS SPS\n+ (for details, see https://issues.apache.org/jira/browse/HDFS-10285), so in order to\n+ avoid moving data blocks at the HDFS level, we can set the file parent directory to\n+ the storage policy we need before writing data. The new file automatically inherits the\n+ storage policy of the parent directory, and is written according to the correct disk\n+ type when writing. So as to avoid later data movement.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NDYwOQ==", "bodyText": "What will happen if the CF config storage policy and enable this feature too?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430244609", "createdAt": "2020-05-26T08:32:35Z", "author": {"login": "infraio"}, "path": "dev-support/design-docs/HBASE-24289-Heterogeneous Storage for Date Tiered Compaction.md", "diffHunk": "@@ -0,0 +1,122 @@\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+\n+# Heterogeneous Storage for Date Tiered Compaction\n+\n+## Objective\n+\n+Support DateTiredCompaction([HBASE-15181](https://issues.apache.org/jira/browse/HBASE-15181))\n+ for cold and hot data separation, support different storage policies for different time periods\n+ of data to get better performance, for example, we can configure the data of last 1 month in SSD,\n+ and 1 month ago data was in HDD.\n+\n++ Date Tiered Compaction (DTCP) is based on date tiering (date-aware), we hope to support\n+  the separation of cold and hot data, heterogeneous storage. Set different storage\n+  policies (in HDFS) for data in different time windows.\n++ DTCP designs different windows, and we can classify the windows according to\n+  the timestamps of the windows. For example: HOT window, WARM window, COLD window.\n++ DTCP divides storefiles into different windows, and performs minor Compaction within\n+  a time window. The storefile generated by Compaction will use the storage strategy of\n+  this window. For example, if a window is a HOT window, the storefile generated by compaction\n+  can be stored on the SSD. There are already WAL and the entire CF support storage policy\n+  (HBASE-12848, HBASE-14061), our goal is to achieve cold and hot separation in one CF or\n+  a region, using different storage policies.\n+\n+## Definition of hot and cold data\n+\n+Usually the data of the last 3 days can be defined as `HOT data`, hot age = 3 days.\n+ If the timestamp of the data is > (timestamp now - hot age), we think the data is hot data.\n+ Warm age, cold age can be defined in the same way. Only one type of data is allowed.\n+ ```\n+  if timestamp >  (now - hot age) , HOT data\n+  else if timestamp >  (now - warm age), WARM data\n+  else if timestamp >  (now - cold age), COLD data\n+  else  default, COLD data\n+```\n+\n+## Time window\n+When given a time now, it is the time when the compaction occurs. Each window and the size of\n+ the window are automatically calculated by DTCP, and the window boundary is rounded according\n+ to the base size.\n+Assuming that the base window size is 1 hour, and each tier has 3 windows, the current time is\n+ between 12:00 and 13:00. We have defined three types of winow (`HOT, WARM, COLD`). The type of\n+ winodw is determined by the timestamp at the beginning of the window and the timestamp now.\n+As shown in the figure 1 below, the type of each window can be determined by the age range\n+ (hot / warm / cold) where (now - window.startTimestamp) falls. Cold age can not need to be set,\n+ the default Long.MAX, meaning that the window with a very early time stamp belongs to the\n+ cold window.\n+![figure 1](https://raw.githubusercontent.com/pengmq1/images/master/F1-HDTCP.png \"figure 1\")\n+\n+## Example configuration\n+\n+| Configuration Key | value | Note |\n+|:---|:---:|:---|\n+|hbase.hstore.compaction.date.tiered.storage.policy.enable|true|if or not use storage policy for window. Default is false|\n+|hbase.hstore.compaction.date.tiered.hot.window.age.millis|3600000|hot data age\n+|hbase.hstore.compaction.date.tiered.hot.window.storage.policy|ALL_SSD|hot data storage policy, Corresponding HDFS storage policy\n+|hbase.hstore.compaction.date.tiered.warm.window.age.millis|20600000||\n+|hbase.hstore.compaction.date.tiered.warm.window.storage.policy|ONE_SSD||\n+|hbase.hstore.compaction.date.tiered.cold.window.age.millis|Long.MAX||\n+|hbase.hstore.compaction.date.tiered.cold.window.storage.policy|HOT||\n+\n+The original date tiered compaction related configuration has the same meaning and maintains", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NDgyMg==", "bodyText": "Can be removed.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430244822", "createdAt": "2020-05-26T08:33:00Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "diffHunk": "@@ -38,23 +38,34 @@\n \n   private final boolean needEmptyFile;\n \n+  private final Map<Long, String> lowerBoundariesPolicies;\n+\n   /**\n+   * @param lowerBoundariesPolicies each window to storage policy map.\n    * @param needEmptyFile whether need to create an empty store file if we haven't written out\n    *          anything.\n    */\n-  public DateTieredMultiFileWriter(List<Long> lowerBoundaries, boolean needEmptyFile) {\n+  public DateTieredMultiFileWriter(List<Long> lowerBoundaries,\n+      Map<Long, String> lowerBoundariesPolicies, boolean needEmptyFile) {\n     for (Long lowerBoundary : lowerBoundaries) {\n       lowerBoundary2Writer.put(lowerBoundary, null);\n     }\n     this.needEmptyFile = needEmptyFile;\n+    this.lowerBoundariesPolicies = lowerBoundariesPolicies;\n   }\n \n   @Override\n   public void append(Cell cell) throws IOException {\n     Map.Entry<Long, StoreFileWriter> entry = lowerBoundary2Writer.floorEntry(cell.getTimestamp());\n     StoreFileWriter writer = entry.getValue();\n     if (writer == null) {\n-      writer = writerFactory.createWriter();\n+      String lowerBoundaryStoragePolicy = lowerBoundariesPolicies.get(entry.getKey());\n+      if (lowerBoundaryStoragePolicy != null) {\n+        writer = writerFactory.createWriterWithStoragePolicy(lowerBoundaryStoragePolicy);\n+      } else {\n+        writer = writerFactory.createWriter();\n+      }\n+      //writer = writerFactory.createWriter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI0NTU3NQ==", "bodyText": "Only log once when region created? If so, can use info log.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r430245575", "createdAt": "2020-05-26T08:34:16Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "diffHunk": "@@ -547,6 +553,22 @@ public StoreFileWriter build() throws IOException {\n       CommonFSUtils.setStoragePolicy(this.fs, dir, policyName);\n \n       if (filePath == null) {\n+        // The stored file and related blocks will used the directory based StoragePolicy.\n+        // Because HDFS DistributedFileSystem does not support create files with storage policy\n+        // before version 3.3.0 (See HDFS-13209). Use child dir here is to make stored files\n+        // satisfy the specific storage policy when writing. So as to avoid later data movement.\n+        // We don't want to change whole temp dir to 'fileStoragePolicy'.\n+        if (fileStoragePolicy != null && !fileStoragePolicy.isEmpty()) {\n+          dir = new Path(dir, HConstants.STORAGE_POLICY_PREFIX + fileStoragePolicy);\n+          if (!fs.exists(dir)) {\n+            HRegionFileSystem.mkdirs(fs, conf, dir);\n+          }\n+          CommonFSUtils.setStoragePolicy(this.fs, dir, fileStoragePolicy);\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081"}, "originalPosition": 36}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e3508fd533f9c9392494aff807c1270f306fc081", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/e3508fd533f9c9392494aff807c1270f306fc081", "committedDate": "2020-05-25T08:00:50Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}, "afterCommit": {"oid": "f4d513daf5dbd75478320db6ab983fbd58920287", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/f4d513daf5dbd75478320db6ab983fbd58920287", "committedDate": "2020-06-02T03:20:55Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f4d513daf5dbd75478320db6ab983fbd58920287", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/f4d513daf5dbd75478320db6ab983fbd58920287", "committedDate": "2020-06-02T03:20:55Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}, "afterCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/c6322b4bf2a4f15a4168c82b283367584e85ebe0", "committedDate": "2020-06-08T09:55:23Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0NDg1OTA5", "url": "https://github.com/apache/hbase/pull/1730#pullrequestreview-434485909", "createdAt": "2020-06-21T09:40:04Z", "commit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQwOTo0MDowNVrOGmq3zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQwOTo0MzowMFrOGmq4uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTQ4NQ==", "bodyText": "ImmutableMap?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443201485", "createdAt": "2020-06-21T09:40:05Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.java", "diffHunk": "@@ -38,23 +38,33 @@\n \n   private final boolean needEmptyFile;\n \n+  private final Map<Long, String> lowerBoundariesPolicies;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTU4MQ==", "bodyText": "Cast it to DateTieredCompactionRequest with a locl variable and then make use of the casted instance?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443201581", "createdAt": "2020-06-21T09:41:18Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine.java", "diffHunk": "@@ -94,6 +94,7 @@ public void forceSelect(CompactionRequestImpl request) {\n         throws IOException {\n       if (request instanceof DateTieredCompactionRequest) {\n         return compactor.compact(request, ((DateTieredCompactionRequest) request).getBoundaries(),\n+          ((DateTieredCompactionRequest) request).getBoundariesPolicies(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIwMTcyMQ==", "bodyText": "Just use Strings.isNullOrEmpty in guava.", "url": "https://github.com/apache/hbase/pull/1730#discussion_r443201721", "createdAt": "2020-06-21T09:43:00Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileWriter.java", "diffHunk": "@@ -547,6 +553,20 @@ public StoreFileWriter build() throws IOException {\n       CommonFSUtils.setStoragePolicy(this.fs, dir, policyName);\n \n       if (filePath == null) {\n+        // The stored file and related blocks will used the directory based StoragePolicy.\n+        // Because HDFS DistributedFileSystem does not support create files with storage policy\n+        // before version 3.3.0 (See HDFS-13209). Use child dir here is to make stored files\n+        // satisfy the specific storage policy when writing. So as to avoid later data movement.\n+        // We don't want to change whole temp dir to 'fileStoragePolicy'.\n+        if (fileStoragePolicy != null && !fileStoragePolicy.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NjY2Mzgz", "url": "https://github.com/apache/hbase/pull/1730#pullrequestreview-438666383", "createdAt": "2020-06-27T02:50:47Z", "commit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1MDo0N1rOGpyr7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1MDo0N1rOGpyr7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTI0Nw==", "bodyText": "Method name start with \"test\"? incomingWindowHot => testIncomingWindowHot", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446475247", "createdAt": "2020-06-27T02:50:47Z", "author": {"login": "infraio"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration;\n+import org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category({ RegionServerTests.class, SmallTests.class })\n+public class TestDateTieredCompactionPolicyHeterogeneousStorage\n+    extends AbstractTestDateTieredCompactionPolicy {\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestDateTieredCompactionPolicyHeterogeneousStorage.class);\n+  public static final String HOT_WINDOW_SP = \"ALL_SSD\";\n+  public static final String WARM_WINDOW_SP = \"ONE_SSD\";\n+  public static final String COLD_WINDOW_SP = \"HOT\";\n+\n+  @Override\n+  protected void config() {\n+    super.config();\n+\n+    // Set up policy\n+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY,\n+      \"org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine\");\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_MAX_AGE_MILLIS_KEY, 100);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_INCOMING_WINDOW_MIN_KEY, 3);\n+    conf.setLong(ExponentialCompactionWindowFactory.BASE_WINDOW_MILLIS_KEY, 6);\n+    conf.setInt(ExponentialCompactionWindowFactory.WINDOWS_PER_TIER_KEY, 4);\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_SINGLE_OUTPUT_FOR_MINOR_COMPACTION_KEY,\n+      false);\n+\n+    // Special settings for compaction policy per window\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MIN_KEY, 2);\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MAX_KEY, 12);\n+    this.conf.setFloat(CompactionConfiguration.HBASE_HSTORE_COMPACTION_RATIO_KEY, 1.2F);\n+\n+    conf.setInt(HStore.BLOCKING_STOREFILES_KEY, 20);\n+    conf.setLong(HConstants.MAJOR_COMPACTION_PERIOD, 5);\n+\n+    // Set Storage Policy for different type window\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_STORAGE_POLICY_ENABLE_KEY, true);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_AGE_MILLIS_KEY, 6);\n+    conf.set(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_STORAGE_POLICY_KEY, HOT_WINDOW_SP);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_AGE_MILLIS_KEY, 12);\n+    conf.set(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_STORAGE_POLICY_KEY, WARM_WINDOW_SP);\n+    conf.set(CompactionConfiguration.DATE_TIERED_COLD_WINDOW_STORAGE_POLICY_KEY, COLD_WINDOW_SP);\n+  }\n+\n+  /**\n+   * Test for incoming window and is HOT window\n+   * window start >= now - hot age\n+   * @throws IOException with error\n+   */\n+  @Test\n+  public void incomingWindowHot() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NjY2NTA3", "url": "https://github.com/apache/hbase/pull/1730#pullrequestreview-438666507", "createdAt": "2020-06-27T02:52:53Z", "commit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Mjo1NFrOGpysww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Mjo1NFrOGpysww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTQ1OQ==", "bodyText": "Set storage policy for a file? This is not work before hdfs 3.3.0?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446475459", "createdAt": "2020-06-27T02:52:54Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java", "diffHunk": "@@ -1564,6 +1568,18 @@ public void deleteChangedReaderObserver(ChangedReadersObserver o) {\n     return sfs;\n   }\n \n+  // Set correct storage policy from the file name of DTCP.\n+  // Rename file will not change the storage policy.\n+  private void setStoragePolicyFromFileName(List<Path> newFiles) throws IOException {\n+    String prefix = HConstants.STORAGE_POLICY_PREFIX;\n+    for (Path newFile : newFiles) {\n+      if (newFile.getParent().getName().startsWith(prefix)) {\n+        CommonFSUtils.setStoragePolicy(fs.getFileSystem(), newFile,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NjY2NzI3", "url": "https://github.com/apache/hbase/pull/1730#pullrequestreview-438666727", "createdAt": "2020-06-27T02:56:47Z", "commit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Njo0N1rOGpyuFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMjo1Njo0N1rOGpyuFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ3NTc5OQ==", "bodyText": "Explain more about the unit test? What is the different between this and other tests? And there are some duplicate code in these test methods?", "url": "https://github.com/apache/hbase/pull/1730#discussion_r446475799", "createdAt": "2020-06-27T02:56:47Z", "author": {"login": "infraio"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDateTieredCompactionPolicyHeterogeneousStorage.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration;\n+import org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category({ RegionServerTests.class, SmallTests.class })\n+public class TestDateTieredCompactionPolicyHeterogeneousStorage\n+    extends AbstractTestDateTieredCompactionPolicy {\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestDateTieredCompactionPolicyHeterogeneousStorage.class);\n+  public static final String HOT_WINDOW_SP = \"ALL_SSD\";\n+  public static final String WARM_WINDOW_SP = \"ONE_SSD\";\n+  public static final String COLD_WINDOW_SP = \"HOT\";\n+\n+  @Override\n+  protected void config() {\n+    super.config();\n+\n+    // Set up policy\n+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY,\n+      \"org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine\");\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_MAX_AGE_MILLIS_KEY, 100);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_INCOMING_WINDOW_MIN_KEY, 3);\n+    conf.setLong(ExponentialCompactionWindowFactory.BASE_WINDOW_MILLIS_KEY, 6);\n+    conf.setInt(ExponentialCompactionWindowFactory.WINDOWS_PER_TIER_KEY, 4);\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_SINGLE_OUTPUT_FOR_MINOR_COMPACTION_KEY,\n+      false);\n+\n+    // Special settings for compaction policy per window\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MIN_KEY, 2);\n+    this.conf.setInt(CompactionConfiguration.HBASE_HSTORE_COMPACTION_MAX_KEY, 12);\n+    this.conf.setFloat(CompactionConfiguration.HBASE_HSTORE_COMPACTION_RATIO_KEY, 1.2F);\n+\n+    conf.setInt(HStore.BLOCKING_STOREFILES_KEY, 20);\n+    conf.setLong(HConstants.MAJOR_COMPACTION_PERIOD, 5);\n+\n+    // Set Storage Policy for different type window\n+    conf.setBoolean(CompactionConfiguration.DATE_TIERED_STORAGE_POLICY_ENABLE_KEY, true);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_AGE_MILLIS_KEY, 6);\n+    conf.set(CompactionConfiguration.DATE_TIERED_HOT_WINDOW_STORAGE_POLICY_KEY, HOT_WINDOW_SP);\n+    conf.setLong(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_AGE_MILLIS_KEY, 12);\n+    conf.set(CompactionConfiguration.DATE_TIERED_WARM_WINDOW_STORAGE_POLICY_KEY, WARM_WINDOW_SP);\n+    conf.set(CompactionConfiguration.DATE_TIERED_COLD_WINDOW_STORAGE_POLICY_KEY, COLD_WINDOW_SP);\n+  }\n+\n+  /**\n+   * Test for incoming window and is HOT window\n+   * window start >= now - hot age\n+   * @throws IOException with error\n+   */\n+  @Test\n+  public void incomingWindowHot() throws IOException {\n+    long[] minTimestamps = new long[] { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n+    long[] maxTimestamps = new long[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 };\n+    long[] sizes = new long[] { 30, 31, 32, 33, 34, 20, 21, 22, 23, 24, 25, 10, 11, 12, 13 };\n+    Map<Long, String> expected = new HashMap<>();\n+    // boundaries = { Long.MIN_VALUE, 12 }\n+    expected.put(12L, HOT_WINDOW_SP);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0"}, "originalPosition": 86}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c6322b4bf2a4f15a4168c82b283367584e85ebe0", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/c6322b4bf2a4f15a4168c82b283367584e85ebe0", "committedDate": "2020-06-08T09:55:23Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}, "afterCommit": {"oid": "4577978d43d5ed92d4ae12f86398d62216cac704", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/4577978d43d5ed92d4ae12f86398d62216cac704", "committedDate": "2020-06-28T08:02:22Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "42c1fc016c9c324f34dce416ef333dcb1e0ebc63", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/42c1fc016c9c324f34dce416ef333dcb1e0ebc63", "committedDate": "2020-06-29T01:33:19Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4577978d43d5ed92d4ae12f86398d62216cac704", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/4577978d43d5ed92d4ae12f86398d62216cac704", "committedDate": "2020-06-28T08:02:22Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}, "afterCommit": {"oid": "42c1fc016c9c324f34dce416ef333dcb1e0ebc63", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/42c1fc016c9c324f34dce416ef333dcb1e0ebc63", "committedDate": "2020-06-29T01:33:19Z", "message": "HBASE-24289 Heterogeneous Storage for Date Tiered Compaction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5NzI4MDEy", "url": "https://github.com/apache/hbase/pull/1730#pullrequestreview-439728012", "createdAt": "2020-06-30T07:09:20Z", "commit": {"oid": "42c1fc016c9c324f34dce416ef333dcb1e0ebc63"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4651, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}