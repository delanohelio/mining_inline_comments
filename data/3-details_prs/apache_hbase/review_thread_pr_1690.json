{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE2MDIxNjkw", "number": 1690, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxOTo1NTozN1rOD7YfVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMzozMzo0MlrOD73EdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNTkzODE1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxOTo1NTozN1rOGTrHJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxOTo1NTozN1rOGTrHJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzI4MjQ3MA==", "bodyText": "IIUC, the problem here is this (in your original patch). Like we discussed, shutdown() is running into flaky ZK issues because the ZK failure happens in the context of the shutdown() rpc. Even when that happens, the shutdown still goes through (though the RPC fails). So, once you enter that state, the subsequent RPCs won't ever return succeed (because the master is already down), so it will just wait until the waitFor() times out (and the test fails).", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423282470", "createdAt": "2020-05-11T19:55:37Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,42 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          LOG.debug(\"Attempting to establish connection.\");\n+          try (final Connection conn = htu.getConnection()) {\n+            conn.getAdmin().shutdown();\n+            LOG.info(\"Shutdown RPC sent.\");\n+            return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04260b5bf95670e9493400214c9b682267df8de2"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNTk1MzIzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQyMDowMDozMlrOGTrQ_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNjo1MTowMlrOGT4ZfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzI4NDk5MA==", "bodyText": "I think this is a bit hacky, instead, can we just issue one shutdown(), don't worry about the result of it (since we know it is going to succeed even if it runs into ZK issues) and then just join on the master thread? That means the code will look like this..\nmaster.start()\nhtu.waitFor(.... masterThread.getMaster().serverMgr() != null);\nThread.run() {\nconn.getAdmin().shutdown();\n// don't worry about the return value\n}\nmasterThread.join()\nshutdownThread.join()\nThoughts?", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423284990", "createdAt": "2020-05-11T20:00:32Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,42 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          LOG.debug(\"Attempting to establish connection.\");\n+          try (final Connection conn = htu.getConnection()) {\n+            conn.getAdmin().shutdown();\n+            LOG.info(\"Shutdown RPC sent.\");\n+            return true;\n+          } catch (IOException|CompletionException e) {\n+            LOG.error(\"Failed to establish connection.\");\n+            if (connectionFailedWithMaster(e)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04260b5bf95670e9493400214c9b682267df8de2"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUwMDE1Nw==", "bodyText": "This is what I thought of doing first thing, yesterday :)\nThen I felt better to check the exact error message and confirm. Although we are sure at this point about the failure, I don't have any strong opinion against not validating the exact error message. Let's do this as you mentioned.", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423500157", "createdAt": "2020-05-12T06:51:02Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +156,42 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        final long result = htu.waitFor(timeout, 1000, () -> {\n+          LOG.debug(\"Attempting to establish connection.\");\n+          try (final Connection conn = htu.getConnection()) {\n+            conn.getAdmin().shutdown();\n+            LOG.info(\"Shutdown RPC sent.\");\n+            return true;\n+          } catch (IOException|CompletionException e) {\n+            LOG.error(\"Failed to establish connection.\");\n+            if (connectionFailedWithMaster(e)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzI4NDk5MA=="}, "originalCommit": {"oid": "04260b5bf95670e9493400214c9b682267df8de2"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTc2Mjc2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoxMjo1NVrOGUQwPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoxMjo1NVrOGUQwPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzg5OTE5Ng==", "bodyText": "Add a quick comment about our discussion, with a jira reference? (incase someone else has to touch this code again)...", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423899196", "createdAt": "2020-05-12T17:12:55Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +153,33 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try (final Connection conn = htu.getConnection()) {\n+          conn.getAdmin().shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b3fb12a674324037180d9736eb5c8e5e7f60ce9"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTc2ODQ3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoxNDozN1rOGUQ0Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo1Njo1M1rOGUSa4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwMDE5NA==", "bodyText": "htu.getConnection() is managed by the mini cluster, remove it from try-with-resources block? Otherwise we might run into weird issues in teardown..", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423900194", "createdAt": "2020-05-12T17:14:37Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +153,33 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try (final Connection conn = htu.getConnection()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b3fb12a674324037180d9736eb5c8e5e7f60ce9"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNjQ5OQ==", "bodyText": "Yeah, sounds good.", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423926499", "createdAt": "2020-05-12T17:56:53Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +153,33 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try (final Connection conn = htu.getConnection()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwMDE5NA=="}, "originalCommit": {"oid": "3b3fb12a674324037180d9736eb5c8e5e7f60ce9"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzOTc3MTg3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoxNToyMlrOGUQ2Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo1NjozM1rOGUSaIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwMDcyMw==", "bodyText": "nit: remove..", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423900723", "createdAt": "2020-05-12T17:15:22Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -186,21 +202,12 @@ private static Configuration createMasterShutdownBeforeStartingAnyRegionServerCo\n     conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, 1);\n     // don't need a long write pipeline for this test.\n     conf.setInt(\"dfs.replication\", 1);\n-    return conf;\n-  }\n-\n-  /**\n-   * Create a new {@link Configuration} based on {@code baseConf} that has ZooKeeper connection\n-   * settings tuned very aggressively. The resulting client is used within a retry loop, so there's\n-   * no value in having the client itself do the retries. We want to iterate on the base\n-   * configuration because we're waiting for the mini-cluster to start and set it's ZK client port.\n-   *\n-   * @return a new, configured {@link Configuration} instance.\n-   */\n-  private static Configuration createResponsiveZkConfig(final Configuration baseConf) {\n-    final Configuration conf = HBaseConfiguration.create(baseConf);\n+    // reduce client retries\n+    conf.setInt(\"hbase.client.retries.number\", 3);\n+    // Recoverable ZK configs are tuned more aggressively\n     conf.setInt(ReadOnlyZKClient.RECOVERY_RETRY, 3);\n     conf.setInt(ReadOnlyZKClient.RECOVERY_RETRY_INTERVAL_MILLIS, 100);\n     return conf;\n   }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3b3fb12a674324037180d9736eb5c8e5e7f60ce9"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNjMwNg==", "bodyText": "Sure", "url": "https://github.com/apache/hbase/pull/1690#discussion_r423926306", "createdAt": "2020-05-12T17:56:33Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -186,21 +202,12 @@ private static Configuration createMasterShutdownBeforeStartingAnyRegionServerCo\n     conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, 1);\n     // don't need a long write pipeline for this test.\n     conf.setInt(\"dfs.replication\", 1);\n-    return conf;\n-  }\n-\n-  /**\n-   * Create a new {@link Configuration} based on {@code baseConf} that has ZooKeeper connection\n-   * settings tuned very aggressively. The resulting client is used within a retry loop, so there's\n-   * no value in having the client itself do the retries. We want to iterate on the base\n-   * configuration because we're waiting for the mini-cluster to start and set it's ZK client port.\n-   *\n-   * @return a new, configured {@link Configuration} instance.\n-   */\n-  private static Configuration createResponsiveZkConfig(final Configuration baseConf) {\n-    final Configuration conf = HBaseConfiguration.create(baseConf);\n+    // reduce client retries\n+    conf.setInt(\"hbase.client.retries.number\", 3);\n+    // Recoverable ZK configs are tuned more aggressively\n     conf.setInt(ReadOnlyZKClient.RECOVERY_RETRY, 3);\n     conf.setInt(ReadOnlyZKClient.RECOVERY_RETRY_INTERVAL_MILLIS, 100);\n     return conf;\n   }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwMDcyMw=="}, "originalCommit": {"oid": "3b3fb12a674324037180d9736eb5c8e5e7f60ce9"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MDkzMjAwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMzoyNTozM1rOGUcYGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMzozMzo1N1rOGVHFRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4OTYyNg==", "bodyText": "nit: what's the purpose of running this in a separate thread?", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424089626", "createdAt": "2020-05-12T23:25:33Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQyOTE0Mw==", "bodyText": "Oh yes, now I can make the call synchronous. The decision of making it go in separate thread goes to previous PR where Bharath and I thought of keeping this async until fixing HBASE-24070 : #1684 (comment)", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424429143", "createdAt": "2020-05-13T13:17:56Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4OTYyNg=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc4OTMxOA==", "bodyText": "Ah, and that one smells a bit like HBASE-23836.", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424789318", "createdAt": "2020-05-13T23:33:57Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4OTYyNg=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MDk0MjkyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMzozMDo0NFrOGUcekQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxMDowNjozN1rOGV-rGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ==", "bodyText": "Your above comment says \"make one shutdown and ignore failures\" but the warning here makes the reader think the shutdown call failed (or, that something failed. the message doesn't say what was attempting to establish a connection, or to what).\nThe IOException is highly generic -- we don't actually know if it was a connection failure, a failure on the server side processing the request, or a failed on the client to interpret the result.", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424091281", "createdAt": "2020-05-12T23:30:44Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQyMTA1NA==", "bodyText": "@ndimiduk Are you in favor of checking the root cause like the first commit: 04260b5#diff-144b3c3611227c12067b788b985183f9R182  &&  04260b5#diff-144b3c3611227c12067b788b985183f9R207 ?\ncause message Failed contacting masters after 1 attempts with MasterRegistry being wrapped inside RetriesExhaustedException.", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424421054", "createdAt": "2020-05-13T13:06:16Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc5NTkxNg==", "bodyText": "Thanks for the links to the previous commits. GH makes it difficult for me to follow previous conversation.\nOkay, so if you move this code out of the CompletableFuture, the CompletionException can just be re-thrown. It indicates a client-side problem, and there's nothing the test should try to recover.\nI would not catch IOException either, just let it bubble up. Instead, I would focus on meaningful subclasses of IOException. RetriesExhaustedException is a good place to work from: you know your client made some effort. There's potentially multiple cause instances under there, so i guess just pick one to work with, probably the last one. If it's a descendent of java.net.SocketException, you know the client couldn't get anywhere -- how should the test behave if the the master is not running?\nAfter that, I'm not really sure what's thrown in what cases. Our API's checked exception definitions aren't strong enough for me know by reading the interfaces. However, pretty much anything else means the client managed to get the RPC over to the server. I think that fact alone is enough to consider this part of the test has succeeded at its goal. Until the myriad other issues in master startup and shutdown are resolved, I think this is the best the client can hope for (for what it's worth, I think this test will continue to be flakey until those master-side problems are solved, and they cannot be resolved perfectly by client-side gymnastics).\nYou've looked at it, and seen the errors and test failures, more recently than I have, so what do you think? What brought you to this ticket in the first place? Are there more specific subclasses of IOException that are thrown, which you can use to reasonably address that specific condition? Paste the stack traces into Jira and the commit message so we can follow your effort.\nAnd thank you for your effort -- test fixing is usually thankless drudgery but it makes all of our lives better :)", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424795916", "createdAt": "2020-05-13T23:55:23Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MDIxMg==", "bodyText": "Thanks @ndimiduk\nLet me put the code reference of the link that I mentioned. This is what I had in my first commit:\n          } catch (IOException|CompletionException e) {\n            LOG.error(\"Failed to establish connection.\");\n            if (connectionFailedWithMaster(e)) {\n              return true;\n            }\n          }\n\n  private boolean connectionFailedWithMaster(Exception e) {\n    if (e instanceof RetriesExhaustedException) {\n      Throwable cause = e.getCause();\n      if (cause instanceof MasterRegistryFetchException) {\n        cause = cause.getCause();\n        if (cause instanceof RetriesExhaustedException) {\n          final String message = cause.getMessage();\n          return message != null && message\n            .startsWith(\"Failed contacting masters after 1 attempts\");\n        }\n      }\n    }\n    return false;\n  }\n\nreturn true in the first code block indicates returning true to waitFor().", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424880212", "createdAt": "2020-05-14T05:33:22Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MzM3Mw==", "bodyText": "@ndimiduk @bharathv\nI will remove CompletionException from catch list. And what about IOException? Good to follow above code to find out the root cause message and accordingly handle it?\nThis is what I had in my first commit and then Bharath and I came to conclusion that it's bit hacky and why not just handle IOException and not worry about cause. But now that I again think of it, you are right, although there are less chances of having different root cause for IOException but still it's good to be specific, that way not too hacky right?", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424883373", "createdAt": "2020-05-14T05:44:25Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0ODg2NQ==", "bodyText": "I think the MasterRegistryFetchException is only valid if we re-try shutdown() rpcs. First shutdown() throws an IOException (some subclass of it) and shuts down the master, the second (and later) shutdowns can't find a master to issue a shutdown and the stub fails and throws this MasterRegistryFetchException.\nNow that we decided to only do a single shutdown, we should either see the specific exception (derivate of IOException when shutdown failed) or no exception at all (if shutdown is a success, which is 99% of the test runs). I think we are good if we catch and log the first exception (don't remember the exact checked exception type).\nThe reason I suggested to keep the checks simple and catch the exception and log is because, if there was any issue in the RPC, that'd anyway reflect in the failure of shutdown join thread and the test fails and we have the exception logged (we don't need to go fancy on checking exception.getCause() recursively).\nI think what Nick is suggesting is to catch the specific subclass of IOException that is thrown if shutdown() fails.  If we do that, instead of logging the exception and test failing in masterThread.join(), (any other) exception is propagated to the test runner and test fails. Either way the test is fails and we will have the exception, so I'm fine with the approach. @ndimiduk Correct me if I got you wrong..", "url": "https://github.com/apache/hbase/pull/1690#discussion_r425448865", "createdAt": "2020-05-14T21:45:01Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTcwMDEyMg==", "bodyText": "Sounds good, but we can't catch the root cause Exception directly, which in our case is ConnectionClosedException. So we catch RetriesExhaustedException first and look for root cause:\n      } catch (RetriesExhaustedException e) {\n        if (e.getCause() instanceof ConnectionClosedException) {\n          LOG.info(\"Connection is Closed, ZK is not reachable.\", e);\n        } else {\n          throw e;\n        }\n      }", "url": "https://github.com/apache/hbase/pull/1690#discussion_r425700122", "createdAt": "2020-05-15T10:06:37Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,\n+          htu.waitFor(timeout, () -> masterThread.getMaster().getServerManager() != null));\n+\n+        // Master has come up far enough that we can terminate it without creating a zombie.\n+        LOG.debug(\"Attempting to establish connection.\");\n+        try {\n+          // HBASE-24327 : (Resolve Flaky connection issues)\n+          // shutdown() RPC can have flaky ZK connection issues.\n+          // e.g\n+          // ERROR [RpcServer.priority.RWQ.Fifo.read.handler=1,queue=1,port=53033]\n+          // master.HMaster(2878): ZooKeeper exception trying to set cluster as down in ZK\n+          // org.apache.zookeeper.KeeperException$SystemErrorException:\n+          // KeeperErrorCode = SystemError\n+          //\n+          // However, even when above flakes happen, shutdown call does get completed even if\n+          // RPC call has failure. Hence, subsequent retries will never succeed as HMaster is\n+          // already shutdown. Hence, it can fail. To resolve it, after making one shutdown()\n+          // call, we are ignoring IOException.\n+          htu.getConnection().getAdmin().shutdown();\n+          LOG.info(\"Shutdown RPC sent.\");\n+        } catch (IOException | CompletionException e) {\n+          LOG.warn(\"Failed to establish connection.\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MTI4MQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MDk0ODM2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMzozMzo0MlrOGUch6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzowOToxMFrOGUwuhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MjEzOQ==", "bodyText": "Since I wrote this code, I realized that this assertNotEquals wrapper is not necessary. Down inside the call hierarchy of this method, there's a default setting that has timeout throw an exception. If you provide an ExplainingPredicate with an implementation of String explainFailure() you can control the error message. A little more convenient than the outer assertion.", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424092139", "createdAt": "2020-05-12T23:33:42Z", "author": {"login": "ndimiduk"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQyMzA0Ng==", "bodyText": "Sure let me check that. I was also following Assertion errors so far on top of waitFor().", "url": "https://github.com/apache/hbase/pull/1690#discussion_r424423046", "createdAt": "2020-05-13T13:09:10Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -151,19 +152,46 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n       hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n         options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n       final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n       masterThread.start();\n-      // Switching to master registry exacerbated a race in the master bootstrap that can result\n-      // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n-      // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n-      // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n-      // is because the connection creation with ZK registry is so slow that by then the server\n-      // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n-      // wait() in the test, waiting for the server manager to become available.\n-      final long timeout = TimeUnit.MINUTES.toMillis(10);\n-      assertNotEquals(\"Timeout waiting for server manager to become available.\",\n-        -1, Waiter.waitFor(htu.getConfiguration(), timeout,\n-          () -> masterThread.getMaster().getServerManager() != null));\n-      htu.getConnection().getAdmin().shutdown();\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422, HBASE-23836). The race is essentially because\n+        // the server manager in HMaster is not initialized by the time shutdown() RPC (below) is\n+        // made to the master. The suspected reason as to why it was uncommon before HBASE-18095\n+        // is because the connection creation with ZK registry is so slow that by then the server\n+        // manager is usually init'ed in time for the RPC to be made. For now, adding an explicit\n+        // wait() in the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+        assertNotEquals(\"timeout waiting for server manager to become available.\", -1,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA5MjEzOQ=="}, "originalCommit": {"oid": "83c052241882534ecec30a36134334e919319256"}, "originalPosition": 62}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1767, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}