{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA3OTQ5NjQx", "number": 2574, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjoyMjoyM1rOEwkfbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNDowNDoyOFrOE0ZAZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY0OTc0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjoyMjoyM1rOHmOM_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNjo1Njo0MVrOHmrQYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MDYzNw==", "bodyText": "Better make it return an exception and make here 'throw throwOnInterrupt(e)' so the compiler will know that this is an exit point.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509840637", "createdAt": "2020-10-22T02:22:23Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -1174,7 +1178,7 @@ public HStore call() throws IOException {\n           LOG.info(\"Setting FlushNonSloppyStoresFirstPolicy for the region=\" + this);\n         }\n       } catch (InterruptedException e) {\n-        throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n+        throwOnInterrupt(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMxNjY0Mw==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510316643", "createdAt": "2020-10-22T16:56:41Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -1174,7 +1178,7 @@ public HStore call() throws IOException {\n           LOG.info(\"Setting FlushNonSloppyStoresFirstPolicy for the region=\" + this);\n         }\n       } catch (InterruptedException e) {\n-        throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n+        throwOnInterrupt(e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MDYzNw=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY1Njg0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjoyNjoyM1rOHmOQ_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNjo1Nzo0MlrOHmrS8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTY2Mg==", "bodyText": "Pity we need to repeat this call many times in the same method...", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509841662", "createdAt": "2020-10-22T02:26:23Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4550,6 +4619,10 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n     /** Keep track of the locks we hold so we can release them in finally clause */\n     List<RowLock> acquiredRowLocks = Lists.newArrayListWithCapacity(batchOp.size());\n     try {\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMxNzI5OQ==", "bodyText": "Yes, but on the other hand we are basically checking interrupt status manually, as one has to do in Java, so can check and exit early at only what we think are safe points, which is nice.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510317299", "createdAt": "2020-10-22T16:57:42Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4550,6 +4619,10 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n     /** Keep track of the locks we hold so we can release them in finally clause */\n     List<RowLock> acquiredRowLocks = Lists.newArrayListWithCapacity(batchOp.size());\n     try {\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTY2Mg=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY1ODcyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjoyNzoxMVrOHmOSAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMTo1ODozN1rOHrhk7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTkyMQ==", "bodyText": "Here the interrupt may come from the checkInterrupt call, not acquire lock?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509841921", "createdAt": "2020-10-22T02:27:11Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -6588,8 +6677,10 @@ protected RowLock getRowLockInternal(byte[] row, boolean readLock, final RowLock\n       success = true;\n       return result;\n     } catch (InterruptedException ie) {\n-      LOG.warn(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,\n-        getRegionInfo().getRegionNameAsString());\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMxOTQ2Mg==", "bodyText": "This was an existing case where the client would get back an IIOE. I didn't change it so the current semantics are preserved. It can be changed to use throwOnInterrupt", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510319462", "createdAt": "2020-10-22T17:01:08Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -6588,8 +6677,10 @@ protected RowLock getRowLockInternal(byte[] row, boolean readLock, final RowLock\n       success = true;\n       return result;\n     } catch (InterruptedException ie) {\n-      LOG.warn(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,\n-        getRegionInfo().getRegionNameAsString());\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTkyMQ=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIxNDk3NQ==", "bodyText": "I mean the log message. Waiting on row lock is not the only case now?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514214975", "createdAt": "2020-10-29T12:20:12Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -6588,8 +6677,10 @@ protected RowLock getRowLockInternal(byte[] row, boolean readLock, final RowLock\n       success = true;\n       return result;\n     } catch (InterruptedException ie) {\n-      LOG.warn(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,\n-        getRegionInfo().getRegionNameAsString());\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTkyMQ=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQwMDk0Mg==", "bodyText": "No the interrupt won't come from the checkInterrupt here", "url": "https://github.com/apache/hbase/pull/2574#discussion_r515400942", "createdAt": "2020-10-30T21:58:37Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -6588,8 +6677,10 @@ protected RowLock getRowLockInternal(byte[] row, boolean readLock, final RowLock\n       success = true;\n       return result;\n     } catch (InterruptedException ie) {\n-      LOG.warn(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,\n-        getRegionInfo().getRegionNameAsString());\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Thread interrupted waiting for lock on row: {}, in region {}\", rowKey,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTkyMQ=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY2NjEzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjozMToyMlrOHmOWZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNjo1ODowNlrOHmrT4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MzA0NQ==", "bodyText": "Maybe we could just use Collections.newSetFromMap(new ConcurrentHashMap<>()) here?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509843045", "createdAt": "2020-10-22T02:31:22Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8748,6 +8863,11 @@ public void startRegionOperation(Operation op) throws IOException {\n       throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() + \" is closing\");\n     }\n     lock(lock.readLock());\n+    // Update regionLockHolders ONLY for any startRegionOperation call that is invoked from an RPC handler\n+    Thread thisThread = Thread.currentThread();\n+    if (isInterruptableOp) {\n+      regionLockHolders.put(thisThread.hashCode(), thisThread);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2NDA2OQ==", "bodyText": "+1", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509864069", "createdAt": "2020-10-22T03:52:43Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8748,6 +8863,11 @@ public void startRegionOperation(Operation op) throws IOException {\n       throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() + \" is closing\");\n     }\n     lock(lock.readLock());\n+    // Update regionLockHolders ONLY for any startRegionOperation call that is invoked from an RPC handler\n+    Thread thisThread = Thread.currentThread();\n+    if (isInterruptableOp) {\n+      regionLockHolders.put(thisThread.hashCode(), thisThread);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MzA0NQ=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMxNzUzOA==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510317538", "createdAt": "2020-10-22T16:58:06Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8748,6 +8863,11 @@ public void startRegionOperation(Operation op) throws IOException {\n       throw new NotServingRegionException(getRegionInfo().getRegionNameAsString() + \" is closing\");\n     }\n     lock(lock.readLock());\n+    // Update regionLockHolders ONLY for any startRegionOperation call that is invoked from an RPC handler\n+    Thread thisThread = Thread.currentThread();\n+    if (isInterruptableOp) {\n+      regionLockHolders.put(thisThread.hashCode(), thisThread);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MzA0NQ=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY3MTUzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjozNDoyN1rOHmOZgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzowMjozOFrOHmre0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mzg0Mg==", "bodyText": "Usually we do not add this to a single test as we have a test timeout for the whole test file.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509843842", "createdAt": "2020-10-22T02:34:27Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7367,154 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test(timeout=20000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyMDMzOQ==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510320339", "createdAt": "2020-10-22T17:02:38Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7367,154 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test(timeout=20000)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mzg0Mg=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY3NDE0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjozNTo0NlrOHmOa3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzowMjo0N1rOHmrfGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0NDE5MQ==", "bodyText": "Use the config from the HBTU? Maybe new Configuration(TEST_UTIL.getConfiguration())", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509844191", "createdAt": "2020-10-22T02:35:46Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7367,154 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test(timeout=20000)\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = HBaseConfiguration.create();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyMDQxMQ==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510320411", "createdAt": "2020-10-22T17:02:47Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7367,154 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test(timeout=20000)\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = HBaseConfiguration.create();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0NDE5MQ=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MzY3NTM0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjozNjoyNlrOHmOblg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzowMzoxMFrOHmrgAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0NDM3NA==", "bodyText": "Just use TableNameTestRule.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r509844374", "createdAt": "2020-10-22T02:36:26Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TestName name = new TestName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyMDY0Mg==", "bodyText": "Did not know about this, ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r510320642", "createdAt": "2020-10-22T17:03:10Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,357 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TestName name = new TestName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0NDM3NA=="}, "originalCommit": {"oid": "57ef804122a4ad7242489337a4c1e643ea27ed0c"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzg1NTQ0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoyNDo1MlrOHoR-AQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxODoxOTo0MlrOHocodA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5OTQ4OQ==", "bodyText": "nit: Instead of using holderInterrupted, maybe we can throw AssertionError with message that holder should not have been interrupted?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r511999489", "createdAt": "2020-10-26T14:24:52Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7366,157 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Disable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, false);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA4MzYwNA==", "bodyText": "If you throw an assertion inside a Thread runnable it won't unwind to the JUnit test method that forked the thread, it will be caught by Thread.UncaughtExceptionHandler, which is not what I think we want. So I set the boolean and then test it in the unit test method where an assertion will propagate the way I want. Maybe JUnit installs its own UEH, didn't test, but the way I have done it makes more sense to me.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512083604", "createdAt": "2020-10-26T16:11:04Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7366,157 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Disable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, false);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5OTQ4OQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE3NDE5Ng==", "bodyText": "Oh yes, nvm, current approach is better.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512174196", "createdAt": "2020-10-26T18:19:42Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7366,157 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Disable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, false);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5OTQ4OQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzkwMTczOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDozMzo0MlrOHoSZ0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNjoxMjoyNFrOHoXKKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwNjYwOA==", "bodyText": "nit: if the only usage is to find diff b/ end and start, directly using System.currentTimeMillis() might be preferred option?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512006608", "createdAt": "2020-10-26T14:33:42Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7366,157 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Disable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, false);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);\n+          }\n+        } catch (Exception e) {\n+          throw new RuntimeException(e);\n+        } finally {\n+          try {\n+            region.closeRegionOperation();\n+          } catch (IOException e) {\n+          }\n+          LOG.info(\"Stopped region operation holder\");\n+        }\n+      }\n+    });\n+\n+    holder.start();\n+    latch.await();\n+    region.close();\n+    holder.join();\n+    region = null;\n+\n+    assertFalse(\"Region lock holder should not have been interrupted\", holderInterrupted.get());\n+  }\n+\n+  @Test\n+  public void testCloseInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Enable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);\n+          }\n+        } catch (Exception e) {\n+          throw new RuntimeException(e);\n+        } finally {\n+          try {\n+            region.closeRegionOperation();\n+          } catch (IOException e) {\n+          }\n+          LOG.info(\"Stopped region operation holder\");\n+        }\n+      }\n+    });\n+\n+    holder.start();\n+    latch.await();\n+    region.close();\n+    holder.join();\n+    region = null;\n+\n+    assertTrue(\"Region lock holder was not interrupted\", holderInterrupted.get());\n+  }\n+\n+  @Test\n+  public void testCloseAbort() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Enable close thread interrupt and server abort behavior\n+    // Set the close lock acquisition wait time to 5 seconds\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);\n+    conf.setInt(HRegion.CLOSE_WAIT_TIME, 5*1000);\n+    region = initHRegion(tableName, method, conf, families);\n+    RegionServerServices rsServices = mock(RegionServerServices.class);\n+    region.rsServices = rsServices;\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          // Hold the lock for 10 seconds no matter how many times we are interrupted\n+          int timeRemaining = 10 * 1000;\n+          while (timeRemaining > 0) {\n+            long start = EnvironmentEdgeManager.currentTime();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 315}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA4NDUyMw==", "bodyText": "We always use EnvironmentEdgeManager, especially in tests, per our coding conventions.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512084523", "createdAt": "2020-10-26T16:12:24Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java", "diffHunk": "@@ -7364,4 +7366,157 @@ protected HStoreForTesting(final HRegion region,\n       return super.doCompaction(cr, filesToCompact, user, compactionStartTime, newFiles);\n     }\n   }\n+\n+  @Test\n+  public void testCloseNoInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Disable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, false);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);\n+          }\n+        } catch (Exception e) {\n+          throw new RuntimeException(e);\n+        } finally {\n+          try {\n+            region.closeRegionOperation();\n+          } catch (IOException e) {\n+          }\n+          LOG.info(\"Stopped region operation holder\");\n+        }\n+      }\n+    });\n+\n+    holder.start();\n+    latch.await();\n+    region.close();\n+    holder.join();\n+    region = null;\n+\n+    assertFalse(\"Region lock holder should not have been interrupted\", holderInterrupted.get());\n+  }\n+\n+  @Test\n+  public void testCloseInterrupt() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Enable close thread interrupt and server abort behavior\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);\n+    region = initHRegion(tableName, method, conf, families);\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    final AtomicBoolean holderInterrupted = new AtomicBoolean();\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          try {\n+            Thread.sleep(10*1000);\n+          } catch (InterruptedException e) {\n+            LOG.info(\"Interrupted\");\n+            holderInterrupted.set(true);\n+          }\n+        } catch (Exception e) {\n+          throw new RuntimeException(e);\n+        } finally {\n+          try {\n+            region.closeRegionOperation();\n+          } catch (IOException e) {\n+          }\n+          LOG.info(\"Stopped region operation holder\");\n+        }\n+      }\n+    });\n+\n+    holder.start();\n+    latch.await();\n+    region.close();\n+    holder.join();\n+    region = null;\n+\n+    assertTrue(\"Region lock holder was not interrupted\", holderInterrupted.get());\n+  }\n+\n+  @Test\n+  public void testCloseAbort() throws Exception {\n+    byte[] cf1 = Bytes.toBytes(\"CF1\");\n+    byte[][] families = { cf1 };\n+\n+    Configuration conf = new Configuration(CONF);\n+    // Enable close thread interrupt and server abort behavior\n+    // Set the close lock acquisition wait time to 5 seconds\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);\n+    conf.setInt(HRegion.CLOSE_WAIT_TIME, 5*1000);\n+    region = initHRegion(tableName, method, conf, families);\n+    RegionServerServices rsServices = mock(RegionServerServices.class);\n+    region.rsServices = rsServices;\n+\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    Thread holder = new Thread(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          LOG.info(\"Starting region operation holder\");\n+          region.startRegionOperation(Operation.SCAN);\n+          latch.countDown();\n+          // Hold the lock for 10 seconds no matter how many times we are interrupted\n+          int timeRemaining = 10 * 1000;\n+          while (timeRemaining > 0) {\n+            long start = EnvironmentEdgeManager.currentTime();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwNjYwOA=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 315}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzk3OTQ4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDo0ODozMlrOHoTIbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNjoxMjo1MFrOHoXLPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxODU0MA==", "bodyText": "Although this is not an atomic operation on volatile, but we are just converting it's own value in sec to ms, hence we should be good here.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512018540", "createdAt": "2020-10-26T14:48:32Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -1679,22 +1688,82 @@ public void setTimeoutForWriteLock(long timeoutForWriteLock) {\n       }\n     }\n \n-    if (timeoutForWriteLock == null\n-        || timeoutForWriteLock == Long.MAX_VALUE) {\n-      // block waiting for the lock for closing\n-      lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n+    // Set the closing flag\n+    // From this point new arrivals at the region lock will get NSRE.\n+\n+    this.closing.set(true);\n+    LOG.info(\"Closing region {}\", this);\n+\n+    // Acquire the close lock\n+\n+    // The configuration parameter CLOSE_WAIT_ABORT is overloaded to enable both\n+    // the new regionserver abort condition and interrupts for running requests.\n+    // If CLOSE_WAIT_ABORT is not enabled there is no change from earlier behavior,\n+    // we will not attempt to interrupt threads servicing requests nor crash out\n+    // the regionserver if something remains stubborn.\n+\n+    boolean canAbort = conf.getBoolean(CLOSE_WAIT_ABORT, DEFAULT_CLOSE_WAIT_ABORT);\n+    boolean useTimedWait = false;\n+    if (timeoutForWriteLock == null || timeoutForWriteLock == Long.MAX_VALUE) {\n+      if (canAbort) {\n+        timeoutForWriteLock = conf.getLong(CLOSE_WAIT_TIME, DEFAULT_CLOSE_WAIT_TIME);\n+        useTimedWait = true;\n+      }\n     } else {\n+      // convert legacy use of timeoutForWriteLock in seconds to new use in millis\n+      timeoutForWriteLock = TimeUnit.SECONDS.toMillis(timeoutForWriteLock);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA4NDc5OQ==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512084799", "createdAt": "2020-10-26T16:12:50Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -1679,22 +1688,82 @@ public void setTimeoutForWriteLock(long timeoutForWriteLock) {\n       }\n     }\n \n-    if (timeoutForWriteLock == null\n-        || timeoutForWriteLock == Long.MAX_VALUE) {\n-      // block waiting for the lock for closing\n-      lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n+    // Set the closing flag\n+    // From this point new arrivals at the region lock will get NSRE.\n+\n+    this.closing.set(true);\n+    LOG.info(\"Closing region {}\", this);\n+\n+    // Acquire the close lock\n+\n+    // The configuration parameter CLOSE_WAIT_ABORT is overloaded to enable both\n+    // the new regionserver abort condition and interrupts for running requests.\n+    // If CLOSE_WAIT_ABORT is not enabled there is no change from earlier behavior,\n+    // we will not attempt to interrupt threads servicing requests nor crash out\n+    // the regionserver if something remains stubborn.\n+\n+    boolean canAbort = conf.getBoolean(CLOSE_WAIT_ABORT, DEFAULT_CLOSE_WAIT_ABORT);\n+    boolean useTimedWait = false;\n+    if (timeoutForWriteLock == null || timeoutForWriteLock == Long.MAX_VALUE) {\n+      if (canAbort) {\n+        timeoutForWriteLock = conf.getLong(CLOSE_WAIT_TIME, DEFAULT_CLOSE_WAIT_TIME);\n+        useTimedWait = true;\n+      }\n     } else {\n+      // convert legacy use of timeoutForWriteLock in seconds to new use in millis\n+      timeoutForWriteLock = TimeUnit.SECONDS.toMillis(timeoutForWriteLock);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxODU0MA=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODA3NTc2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTowNzoxMVrOHoUDTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNjoxNTo1N1rOHoXT5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzMzYxNQ==", "bodyText": "nit: we can remove NSRE here", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512033615", "createdAt": "2020-10-26T15:07:11Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8793,7 +8921,7 @@ public void closeRegionOperation(Operation operation) throws IOException {\n    * @throws InterruptedIOException if interrupted while waiting for a lock\n    */\n   private void startBulkRegionOperation(boolean writeLockNeeded)\n-      throws NotServingRegionException, RegionTooBusyException, InterruptedIOException {\n+      throws NotServingRegionException, IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA4NzAxMw==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512087013", "createdAt": "2020-10-26T16:15:57Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8793,7 +8921,7 @@ public void closeRegionOperation(Operation operation) throws IOException {\n    * @throws InterruptedIOException if interrupted while waiting for a lock\n    */\n   private void startBulkRegionOperation(boolean writeLockNeeded)\n-      throws NotServingRegionException, RegionTooBusyException, InterruptedIOException {\n+      throws NotServingRegionException, IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzMzYxNQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 336}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODA4Nzg1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTowOTozOFrOHoUKvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxODowNTowNFrOHocGWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzNTUxOA==", "bodyText": "maybe Waiting can be replaced with Waiting without time limit ?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512035518", "createdAt": "2020-10-26T15:09:38Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -1679,22 +1688,82 @@ public void setTimeoutForWriteLock(long timeoutForWriteLock) {\n       }\n     }\n \n-    if (timeoutForWriteLock == null\n-        || timeoutForWriteLock == Long.MAX_VALUE) {\n-      // block waiting for the lock for closing\n-      lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n+    // Set the closing flag\n+    // From this point new arrivals at the region lock will get NSRE.\n+\n+    this.closing.set(true);\n+    LOG.info(\"Closing region {}\", this);\n+\n+    // Acquire the close lock\n+\n+    // The configuration parameter CLOSE_WAIT_ABORT is overloaded to enable both\n+    // the new regionserver abort condition and interrupts for running requests.\n+    // If CLOSE_WAIT_ABORT is not enabled there is no change from earlier behavior,\n+    // we will not attempt to interrupt threads servicing requests nor crash out\n+    // the regionserver if something remains stubborn.\n+\n+    boolean canAbort = conf.getBoolean(CLOSE_WAIT_ABORT, DEFAULT_CLOSE_WAIT_ABORT);\n+    boolean useTimedWait = false;\n+    if (timeoutForWriteLock == null || timeoutForWriteLock == Long.MAX_VALUE) {\n+      if (canAbort) {\n+        timeoutForWriteLock = conf.getLong(CLOSE_WAIT_TIME, DEFAULT_CLOSE_WAIT_TIME);\n+        useTimedWait = true;\n+      }\n     } else {\n+      // convert legacy use of timeoutForWriteLock in seconds to new use in millis\n+      timeoutForWriteLock = TimeUnit.SECONDS.toMillis(timeoutForWriteLock);\n+      useTimedWait = true;\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug((useTimedWait ? \"Time limited wait\" : \"Waiting\") + \" for close lock on \" + this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE2NTQ2NA==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512165464", "createdAt": "2020-10-26T18:05:04Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -1679,22 +1688,82 @@ public void setTimeoutForWriteLock(long timeoutForWriteLock) {\n       }\n     }\n \n-    if (timeoutForWriteLock == null\n-        || timeoutForWriteLock == Long.MAX_VALUE) {\n-      // block waiting for the lock for closing\n-      lock.writeLock().lock(); // FindBugs: Complains UL_UNRELEASED_LOCK_EXCEPTION_PATH but seems fine\n+    // Set the closing flag\n+    // From this point new arrivals at the region lock will get NSRE.\n+\n+    this.closing.set(true);\n+    LOG.info(\"Closing region {}\", this);\n+\n+    // Acquire the close lock\n+\n+    // The configuration parameter CLOSE_WAIT_ABORT is overloaded to enable both\n+    // the new regionserver abort condition and interrupts for running requests.\n+    // If CLOSE_WAIT_ABORT is not enabled there is no change from earlier behavior,\n+    // we will not attempt to interrupt threads servicing requests nor crash out\n+    // the regionserver if something remains stubborn.\n+\n+    boolean canAbort = conf.getBoolean(CLOSE_WAIT_ABORT, DEFAULT_CLOSE_WAIT_ABORT);\n+    boolean useTimedWait = false;\n+    if (timeoutForWriteLock == null || timeoutForWriteLock == Long.MAX_VALUE) {\n+      if (canAbort) {\n+        timeoutForWriteLock = conf.getLong(CLOSE_WAIT_TIME, DEFAULT_CLOSE_WAIT_TIME);\n+        useTimedWait = true;\n+      }\n     } else {\n+      // convert legacy use of timeoutForWriteLock in seconds to new use in millis\n+      timeoutForWriteLock = TimeUnit.SECONDS.toMillis(timeoutForWriteLock);\n+      useTimedWait = true;\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug((useTimedWait ? \"Time limited wait\" : \"Waiting\") + \" for close lock on \" + this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzNTUxOA=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODEyNzYwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNToxNzozN1rOHoUjQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNjoxNjoyOFrOHoXVbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA0MTc5NQ==", "bodyText": "nit: put these lines by mistake? :)", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512041795", "createdAt": "2020-10-26T15:17:37Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -9000,6 +9129,49 @@ public long getReadPoint() {\n     return getReadPoint(IsolationLevel.READ_COMMITTED);\n   }\n \n+  /**\n+   * Interrupt any region options that have acquired the region lock via\n+   * {@link #startRegionOperation(org.apache.hadoop.hbase.regionserver.Region.Operation)},\n+   * or {@link #startBulkRegionOperation(boolean)}.\n+   */\n+  private void interruptRegionOperations() {\n+    for (Thread t: regionLockHolders) {\n+      t.interrupt();\n+    }\n+  }\n+\n+  /**\n+   * Check thread interrupt status and throw an exception if interrupted.\n+   * @throws NotServingRegionException if region is closing\n+   * @throws InterruptedIOException if interrupted but region is not closing\n+   */\n+  // Package scope for tests\n+  void checkInterrupt() throws NotServingRegionException, InterruptedIOException {\n+    if (Thread.interrupted()) {\n+      if (this.closing.get()) {\n+        throw new NotServingRegionException(\n+          getRegionInfo().getRegionNameAsString() + \" is closing\");\n+      }\n+      throw new InterruptedIOException();\n+    }\n+  }\n+\n+  /**\n+   * Throw the correct exception upon interrupt\n+   * @param t cause\n+   * @throws NotServingRegionException if region is closing\n+   * @throws InterruptedIOException in all cases except if region is closing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 425}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA4NzQwNw==", "bodyText": "Old stuff, will update", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512087407", "createdAt": "2020-10-26T16:16:28Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -9000,6 +9129,49 @@ public long getReadPoint() {\n     return getReadPoint(IsolationLevel.READ_COMMITTED);\n   }\n \n+  /**\n+   * Interrupt any region options that have acquired the region lock via\n+   * {@link #startRegionOperation(org.apache.hadoop.hbase.regionserver.Region.Operation)},\n+   * or {@link #startBulkRegionOperation(boolean)}.\n+   */\n+  private void interruptRegionOperations() {\n+    for (Thread t: regionLockHolders) {\n+      t.interrupt();\n+    }\n+  }\n+\n+  /**\n+   * Check thread interrupt status and throw an exception if interrupted.\n+   * @throws NotServingRegionException if region is closing\n+   * @throws InterruptedIOException if interrupted but region is not closing\n+   */\n+  // Package scope for tests\n+  void checkInterrupt() throws NotServingRegionException, InterruptedIOException {\n+    if (Thread.interrupted()) {\n+      if (this.closing.get()) {\n+        throw new NotServingRegionException(\n+          getRegionInfo().getRegionNameAsString() + \" is closing\");\n+      }\n+      throw new InterruptedIOException();\n+    }\n+  }\n+\n+  /**\n+   * Throw the correct exception upon interrupt\n+   * @param t cause\n+   * @throws NotServingRegionException if region is closing\n+   * @throws InterruptedIOException in all cases except if region is closing", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA0MTc5NQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 425}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODE1NTUxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNToyMzoxNVrOHoU0uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNjoxOTozM1rOHoXd6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA0NjI2Nw==", "bodyText": "Similar to how we passed conf all the way from HBaseTestingUtility methods to actual HRegion instance, we can also pass rsServices instance in same methods and we won't have to worry about changing this to non-final, but not a strong opinion if this means too many args in testing utility methods so it's upto you.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512046267", "createdAt": "2020-10-26T15:23:15Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -688,14 +688,17 @@ void sawNoSuchFamily() {\n   // Last flush time for each Store. Useful when we are flushing for each column\n   private final ConcurrentMap<HStore, Long> lastStoreFlushTimeMap = new ConcurrentHashMap<>();\n \n-  final RegionServerServices rsServices;\n+  protected RegionServerServices rsServices;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA4OTU3Nw==", "bodyText": "In the HRegion tests I mock RSS with mockito, to test if the HRegion code calls RSS#abort as it should. We don't need to make a real cluster test that aborts a server (and adds a lot of time to the unit test suite) just to confirm this. So, it needs to be nonfinal for mocking, but it does not need to be exposed as a settable thing, because nowhere else in the code is there a use for that.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512089577", "createdAt": "2020-10-26T16:19:33Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -688,14 +688,17 @@ void sawNoSuchFamily() {\n   // Last flush time for each Store. Useful when we are flushing for each column\n   private final ConcurrentMap<HStore, Long> lastStoreFlushTimeMap = new ConcurrentHashMap<>();\n \n-  final RegionServerServices rsServices;\n+  protected RegionServerServices rsServices;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA0NjI2Nw=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODE4MzM0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNToyODozOVrOHoVGFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxODowOTo0MFrOHocRxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1MDcwOQ==", "bodyText": "Test category LargeTests will take care of timeouts so we can remove all timeout from individual tests.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512050709", "createdAt": "2020-10-26T15:28:39Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TableNameTestRule name = new TableNameTestRule();\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setClass(HConstants.REGION_IMPL, InterruptInterceptingHRegion.class, Region.class);\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);\n+    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    TEST_UTIL.startMiniCluster();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n+  @Test(timeout=120000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE2ODM4OA==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512168388", "createdAt": "2020-10-26T18:09:40Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TableNameTestRule name = new TableNameTestRule();\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setClass(HConstants.REGION_IMPL, InterruptInterceptingHRegion.class, Region.class);\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);\n+    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    TEST_UTIL.startMiniCluster();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n+  @Test(timeout=120000)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1MDcwOQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODIwODg2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozMzo1MlrOHoVWMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxODoyMzozN1rOHocyEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1NDgzNQ==", "bodyText": "We don't want to restrict 5 min of wait time by updating CLOSE_WAIT_TIME ?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512054835", "createdAt": "2020-10-26T15:33:52Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TableNameTestRule name = new TableNameTestRule();\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setClass(HConstants.REGION_IMPL, InterruptInterceptingHRegion.class, Region.class);\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA5MDk1OA==", "bodyText": "Let me restate this as what I think you were getting at: \"Can we set a shorter abort interval so this test will not run for a long time if interrupts do not work as expected?\"\nSure, ok.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512090958", "createdAt": "2020-10-26T16:21:27Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TableNameTestRule name = new TableNameTestRule();\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setClass(HConstants.REGION_IMPL, InterruptInterceptingHRegion.class, Region.class);\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1NDgzNQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE3NjY1Ng==", "bodyText": "Done", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512176656", "createdAt": "2020-10-26T18:23:37Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionInterrupt.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.regionserver;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNameTestRule;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Append;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.Increment;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionObserver;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n+import org.apache.hadoop.hbase.filter.FilterBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.RegionServerTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALEdit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({RegionServerTests.class, LargeTests.class})\n+public class TestRegionInterrupt {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegionInterrupt.class);\n+\n+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRegionInterrupt.class);\n+\n+  static final int SLEEP_TIME = 10 * 1000;\n+  static final byte[] FAMILY = Bytes.toBytes(\"info\");\n+\n+  @Rule\n+  public TableNameTestRule name = new TableNameTestRule();\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setClass(HConstants.REGION_IMPL, InterruptInterceptingHRegion.class, Region.class);\n+    conf.setBoolean(HRegion.CLOSE_WAIT_ABORT, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1NDgzNQ=="}, "originalCommit": {"oid": "9f1c0546380f1b28d5c37fea1fbcff7bfd4f028a"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDQ3NTU3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNDowNzo1NlrOHoqtxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxMjozMzoyMFrOHqZpfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNDkzMg==", "bodyText": "Could we only call checkInterrupt() once before or after WAL synced, instead of a check after every step.\nPersonally I incline to check after WAL synced.\nIt also makes sense to call before WAL synced if considering the sync issue which occasionally happen.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512404932", "createdAt": "2020-10-27T04:07:56Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4569,13 +4665,29 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       // We should record the timestamp only after we have acquired the rowLock,\n       // otherwise, newer puts/deletes/increment/append are not guaranteed to have a newer\n       // timestamp\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       long now = EnvironmentEdgeManager.currentTime();\n       batchOp.prepareMiniBatchOperations(miniBatchOp, now, acquiredRowLocks);\n \n       // STEP 3. Build WAL edit\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       List<Pair<NonceKey, WALEdit>> walEdits = batchOp.buildWALEdits(miniBatchOp);\n \n       // STEP 4. Append the WALEdits to WAL and sync.\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation. This is the last place we can do it \"safely\" before\n+      // WAL appends.\n+      checkInterrupt();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c08de396bcb508eb2272af20a4bd103d9e9734a8"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkzNzY4OA==", "bodyText": "The steps here are each loops, so there's a chance that a fair amount of time elapses between each step, which is why I thought checking after each step offered value beyond just checking once.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512937688", "createdAt": "2020-10-27T18:36:33Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4569,13 +4665,29 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       // We should record the timestamp only after we have acquired the rowLock,\n       // otherwise, newer puts/deletes/increment/append are not guaranteed to have a newer\n       // timestamp\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       long now = EnvironmentEdgeManager.currentTime();\n       batchOp.prepareMiniBatchOperations(miniBatchOp, now, acquiredRowLocks);\n \n       // STEP 3. Build WAL edit\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       List<Pair<NonceKey, WALEdit>> walEdits = batchOp.buildWALEdits(miniBatchOp);\n \n       // STEP 4. Append the WALEdits to WAL and sync.\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation. This is the last place we can do it \"safely\" before\n+      // WAL appends.\n+      checkInterrupt();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNDkzMg=="}, "originalCommit": {"oid": "c08de396bcb508eb2272af20a4bd103d9e9734a8"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk1NzM2NQ==", "bodyText": "@Reidddddd I would agree checking beyond WAL sync would be useful if and only if we decide to protect WAL syncs from interrupts. Otherwise the sync will be interrupted and we will break out of the RPC processing with an IOException with cause InterruptedException (from the WAL sync future).", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512957365", "createdAt": "2020-10-27T19:09:55Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4569,13 +4665,29 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       // We should record the timestamp only after we have acquired the rowLock,\n       // otherwise, newer puts/deletes/increment/append are not guaranteed to have a newer\n       // timestamp\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       long now = EnvironmentEdgeManager.currentTime();\n       batchOp.prepareMiniBatchOperations(miniBatchOp, now, acquiredRowLocks);\n \n       // STEP 3. Build WAL edit\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       List<Pair<NonceKey, WALEdit>> walEdits = batchOp.buildWALEdits(miniBatchOp);\n \n       // STEP 4. Append the WALEdits to WAL and sync.\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation. This is the last place we can do it \"safely\" before\n+      // WAL appends.\n+      checkInterrupt();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNDkzMg=="}, "originalCommit": {"oid": "c08de396bcb508eb2272af20a4bd103d9e9734a8"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NTcwMw==", "bodyText": "Since we are on the topic of WAL syncs....\nWe could protect against interrupting a sync using a couple of different approaches. The simplest in the context of this proposed change would be to remove the current handler thread from the regionLockHolders set whenever we decide interrupting it is no longer the best choice, like a point of no return on the mutation code path.\nstartRegionOperation(Operation.PUT); // add thread to the set so it is interruptible\ntry {\n    ...\n    startWALOperation(); // remove thread from the set so it will not be interrupted\n    try {\n        ...\n    } finally {\n        endWALOperation(); // add thread to the set so it is eligible to be interrupted again\n    }\n    ...\n} finally {\n    endRegionOperation();  // remove thread from the set as it is no longer actively handling RPC\n}\n\nHowever we already have issues with sync timeouts or regionserver aborts during sync such that we are not introducing anything new here. In other words, if it is problematic that a WAL sync can be aborted before it completes, this already happens for other reasons, like timeouts (presumably because HDFS is slow), or HDFS level  pipeline recovery failure, or server shutdown.\nThinking about this though makes me think of the English expression \"opening a can of worms\". I don't know of a Chinese equivalent. I'm going to implement the proposal above out of an abundance of caution and we can decide upon further review if it makes sense.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r512965703", "createdAt": "2020-10-27T19:23:26Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4569,13 +4665,29 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       // We should record the timestamp only after we have acquired the rowLock,\n       // otherwise, newer puts/deletes/increment/append are not guaranteed to have a newer\n       // timestamp\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       long now = EnvironmentEdgeManager.currentTime();\n       batchOp.prepareMiniBatchOperations(miniBatchOp, now, acquiredRowLocks);\n \n       // STEP 3. Build WAL edit\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       List<Pair<NonceKey, WALEdit>> walEdits = batchOp.buildWALEdits(miniBatchOp);\n \n       // STEP 4. Append the WALEdits to WAL and sync.\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation. This is the last place we can do it \"safely\" before\n+      // WAL appends.\n+      checkInterrupt();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNDkzMg=="}, "originalCommit": {"oid": "c08de396bcb508eb2272af20a4bd103d9e9734a8"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE1NDEyMA==", "bodyText": "the new commit LGTM, let's wait QA.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r513154120", "createdAt": "2020-10-28T03:12:34Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4569,13 +4665,29 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       // We should record the timestamp only after we have acquired the rowLock,\n       // otherwise, newer puts/deletes/increment/append are not guaranteed to have a newer\n       // timestamp\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       long now = EnvironmentEdgeManager.currentTime();\n       batchOp.prepareMiniBatchOperations(miniBatchOp, now, acquiredRowLocks);\n \n       // STEP 3. Build WAL edit\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       List<Pair<NonceKey, WALEdit>> walEdits = batchOp.buildWALEdits(miniBatchOp);\n \n       // STEP 4. Append the WALEdits to WAL and sync.\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation. This is the last place we can do it \"safely\" before\n+      // WAL appends.\n+      checkInterrupt();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNDkzMg=="}, "originalCommit": {"oid": "c08de396bcb508eb2272af20a4bd103d9e9734a8"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyMjQ2Mg==", "bodyText": "After reading this discussion several times, I think the reason we do not want to niterrupte a WAL sync is that it may lead to a region server abort?\nI would say this is not the case here. I checked the code again, the actual sync is done in the disruptor thread, in the rpc thread we just block on a SyncFuture(as Andrew mentioned above), the interruption on the rpc thread will just lead to an IOException tp client, the actual sync operation will not be interrupted so we are safe.\nSo I do not think we need to disable interrupts here?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514222462", "createdAt": "2020-10-29T12:33:20Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4569,13 +4665,29 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       // We should record the timestamp only after we have acquired the rowLock,\n       // otherwise, newer puts/deletes/increment/append are not guaranteed to have a newer\n       // timestamp\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       long now = EnvironmentEdgeManager.currentTime();\n       batchOp.prepareMiniBatchOperations(miniBatchOp, now, acquiredRowLocks);\n \n       // STEP 3. Build WAL edit\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation.\n+      checkInterrupt();\n+\n       List<Pair<NonceKey, WALEdit>> walEdits = batchOp.buildWALEdits(miniBatchOp);\n \n       // STEP 4. Append the WALEdits to WAL and sync.\n+\n+      // Check for thread interrupt status in case we have been signaled from\n+      // #interruptRegionOperation. This is the last place we can do it \"safely\" before\n+      // WAL appends.\n+      checkInterrupt();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNDkzMg=="}, "originalCommit": {"oid": "c08de396bcb508eb2272af20a4bd103d9e9734a8"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxODI3OTIyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNzoxNzo1OVrOHp1Hjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNzoyNzoyNFrOHp1jHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYyMzk1MA==", "bodyText": "nit: Amend the comment to add what this boolean means?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r513623950", "createdAt": "2020-10-28T17:17:59Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -688,14 +689,17 @@ void sawNoSuchFamily() {\n   // Last flush time for each Store. Useful when we are flushing for each column\n   private final ConcurrentMap<HStore, Long> lastStoreFlushTimeMap = new ConcurrentHashMap<>();\n \n-  final RegionServerServices rsServices;\n+  protected RegionServerServices rsServices;\n   private RegionServerAccounting rsAccounting;\n   private long flushCheckInterval;\n   // flushPerChanges is to prevent too many changes in memstore\n   private long flushPerChanges;\n   private long blockingMemStoreSize;\n   // Used to guard closes\n   final ReentrantReadWriteLock lock;\n+  // Used to track interruptible holders of the region lock\n+  // Currently that is only RPC handler threads\n+  final ConcurrentHashMap<Thread, Boolean> regionLockHolders;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f0bc125cdb864af04895c86990cfcebe6eed6da"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYzMTAwNA==", "bodyText": "Ok", "url": "https://github.com/apache/hbase/pull/2574#discussion_r513631004", "createdAt": "2020-10-28T17:27:24Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -688,14 +689,17 @@ void sawNoSuchFamily() {\n   // Last flush time for each Store. Useful when we are flushing for each column\n   private final ConcurrentMap<HStore, Long> lastStoreFlushTimeMap = new ConcurrentHashMap<>();\n \n-  final RegionServerServices rsServices;\n+  protected RegionServerServices rsServices;\n   private RegionServerAccounting rsAccounting;\n   private long flushCheckInterval;\n   // flushPerChanges is to prevent too many changes in memstore\n   private long flushPerChanges;\n   private long blockingMemStoreSize;\n   // Used to guard closes\n   final ReentrantReadWriteLock lock;\n+  // Used to track interruptible holders of the region lock\n+  // Currently that is only RPC handler threads\n+  final ConcurrentHashMap<Thread, Boolean> regionLockHolders;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYyMzk1MA=="}, "originalCommit": {"oid": "3f0bc125cdb864af04895c86990cfcebe6eed6da"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMDAyODUzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQwMjo0OTowNFrOHqF10Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNToxMjoyNVrOHqgpqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg5NzkzNw==", "bodyText": "Only one q left, read operations are interruptible, but i couldn't find new interruption handling in read path. Do we just leave it as it is? (just for confirmation)", "url": "https://github.com/apache/hbase/pull/2574#discussion_r513897937", "createdAt": "2020-10-29T02:49:04Z", "author": {"login": "Reidddddd"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8730,12 +8868,22 @@ public void startRegionOperation() throws IOException {\n \n   @Override\n   public void startRegionOperation(Operation op) throws IOException {\n+    boolean isInterruptableOp = false;\n     switch (op) {\n-      case GET:  // read operations\n+      case GET:  // interruptible read operations\n       case SCAN:\n+        isInterruptableOp = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 435}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMzNzE5Mg==", "bodyText": "There are some checkInterrupts placed inside the scanner inner loops. Get, Exist, and Scan operations all share this path. I did not put checkInterrupt into the checkAndXXX operations because these are point ops that will complete quickly, but could if you feel this represents missing coverage. @Reidddddd\nOn the write path there are checkInterrupts placed at points in doMiniBatchMutation to catch the cases where we are likely to run a long time, and the row mutation processor also checks for interrupts. Again not every operation has a check, like Increment or Append, where the unit of work is small, but we could add them there too if you feel this represents missing coverage.\nMy opinion is the long running cases are where the check is definitely worth it -- scanners on the read side, batch mutations on the write side -- and others are marginal (and therefore not included).", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514337192", "createdAt": "2020-10-29T15:12:25Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8730,12 +8868,22 @@ public void startRegionOperation() throws IOException {\n \n   @Override\n   public void startRegionOperation(Operation op) throws IOException {\n+    boolean isInterruptableOp = false;\n     switch (op) {\n-      case GET:  // read operations\n+      case GET:  // interruptible read operations\n       case SCAN:\n+        isInterruptableOp = true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg5NzkzNw=="}, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 435}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMTYzMTcxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxMDozOToxOFrOHqV3yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNjozNDoxMlrOHqkitg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDE2MDU4NA==", "bodyText": "nit: entry.getValue() is enough", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514160584", "createdAt": "2020-10-29T10:39:18Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -9000,6 +9173,51 @@ public long getReadPoint() {\n     return getReadPoint(IsolationLevel.READ_COMMITTED);\n   }\n \n+  /**\n+   * Interrupt any region options that have acquired the region lock via\n+   * {@link #startRegionOperation(org.apache.hadoop.hbase.regionserver.Region.Operation)},\n+   * or {@link #startBulkRegionOperation(boolean)}.\n+   */\n+  private void interruptRegionOperations() {\n+    for (Map.Entry<Thread, Boolean> entry: regionLockHolders.entrySet()) {\n+      // An entry in this map will have a boolean value indicating if it is currently\n+      // eligible for interrupt; if so, we should interrupt it.\n+      if (entry.getValue().booleanValue()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 581}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwMDk1MA==", "bodyText": "The generated code will be the same. I opted for this to be pedantic in the code. It should be fine, right?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514400950", "createdAt": "2020-10-29T16:34:12Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -9000,6 +9173,51 @@ public long getReadPoint() {\n     return getReadPoint(IsolationLevel.READ_COMMITTED);\n   }\n \n+  /**\n+   * Interrupt any region options that have acquired the region lock via\n+   * {@link #startRegionOperation(org.apache.hadoop.hbase.regionserver.Region.Operation)},\n+   * or {@link #startBulkRegionOperation(boolean)}.\n+   */\n+  private void interruptRegionOperations() {\n+    for (Map.Entry<Thread, Boolean> entry: regionLockHolders.entrySet()) {\n+      // An entry in this map will have a boolean value indicating if it is currently\n+      // eligible for interrupt; if so, we should interrupt it.\n+      if (entry.getValue().booleanValue()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDE2MDU4NA=="}, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 581}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMTk5NTI5OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxMjoyNDo1MlrOHqZWOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMjowMDowM1rOHrhm2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIxNzUzMA==", "bodyText": "Why remove these comments? They are not the cases for now?", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514217530", "createdAt": "2020-10-29T12:24:52Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8777,23 +8936,36 @@ public void closeRegionOperation(Operation operation) throws IOException {\n     if (operation == Operation.SNAPSHOT) {\n       stores.values().forEach(HStore::postSnapshotOperation);\n     }\n+    regionLockHolders.remove(Thread.currentThread());\n     lock.readLock().unlock();\n     if (coprocessorHost != null) {\n       coprocessorHost.postCloseRegionOperation(operation);\n     }\n   }\n \n+  /**\n+   * If a handler thread is eligible for interrupt, make it ineligible. Should be paired\n+   * with {{@link #enableInterrupts()}.\n+   */\n+  protected void disableInterrupts() {\n+    regionLockHolders.computeIfPresent(Thread.currentThread(), (t,b) -> false);\n+  }\n+\n+  /**\n+   * If a handler thread was made ineligible for interrupt via {{@link #disableInterrupts()},\n+   * make it eligible again. No-op if interrupts are already enabled.\n+   */\n+  protected void enableInterrupts() {\n+    regionLockHolders.computeIfPresent(Thread.currentThread(), (t,b) -> true);\n+  }\n+\n   /**\n    * This method needs to be called before any public call that reads or\n    * modifies stores in bulk. It has to be called just before a try.\n    * #closeBulkRegionOperation needs to be called in the try's finally block\n    * Acquires a writelock and checks if the region is closing or closed.\n-   * @throws NotServingRegionException when the region is closing or closed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 508}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQwMDQ4OQ==", "bodyText": "I can put them back but I believe this was done in response to feedback from another reviewer", "url": "https://github.com/apache/hbase/pull/2574#discussion_r514400489", "createdAt": "2020-10-29T16:33:32Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8777,23 +8936,36 @@ public void closeRegionOperation(Operation operation) throws IOException {\n     if (operation == Operation.SNAPSHOT) {\n       stores.values().forEach(HStore::postSnapshotOperation);\n     }\n+    regionLockHolders.remove(Thread.currentThread());\n     lock.readLock().unlock();\n     if (coprocessorHost != null) {\n       coprocessorHost.postCloseRegionOperation(operation);\n     }\n   }\n \n+  /**\n+   * If a handler thread is eligible for interrupt, make it ineligible. Should be paired\n+   * with {{@link #enableInterrupts()}.\n+   */\n+  protected void disableInterrupts() {\n+    regionLockHolders.computeIfPresent(Thread.currentThread(), (t,b) -> false);\n+  }\n+\n+  /**\n+   * If a handler thread was made ineligible for interrupt via {{@link #disableInterrupts()},\n+   * make it eligible again. No-op if interrupts are already enabled.\n+   */\n+  protected void enableInterrupts() {\n+    regionLockHolders.computeIfPresent(Thread.currentThread(), (t,b) -> true);\n+  }\n+\n   /**\n    * This method needs to be called before any public call that reads or\n    * modifies stores in bulk. It has to be called just before a try.\n    * #closeBulkRegionOperation needs to be called in the try's finally block\n    * Acquires a writelock and checks if the region is closing or closed.\n-   * @throws NotServingRegionException when the region is closing or closed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIxNzUzMA=="}, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 508}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQwMTQzNA==", "bodyText": "Fixed this, put the javadoc back", "url": "https://github.com/apache/hbase/pull/2574#discussion_r515401434", "createdAt": "2020-10-30T22:00:03Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8777,23 +8936,36 @@ public void closeRegionOperation(Operation operation) throws IOException {\n     if (operation == Operation.SNAPSHOT) {\n       stores.values().forEach(HStore::postSnapshotOperation);\n     }\n+    regionLockHolders.remove(Thread.currentThread());\n     lock.readLock().unlock();\n     if (coprocessorHost != null) {\n       coprocessorHost.postCloseRegionOperation(operation);\n     }\n   }\n \n+  /**\n+   * If a handler thread is eligible for interrupt, make it ineligible. Should be paired\n+   * with {{@link #enableInterrupts()}.\n+   */\n+  protected void disableInterrupts() {\n+    regionLockHolders.computeIfPresent(Thread.currentThread(), (t,b) -> false);\n+  }\n+\n+  /**\n+   * If a handler thread was made ineligible for interrupt via {{@link #disableInterrupts()},\n+   * make it eligible again. No-op if interrupts are already enabled.\n+   */\n+  protected void enableInterrupts() {\n+    regionLockHolders.computeIfPresent(Thread.currentThread(), (t,b) -> true);\n+  }\n+\n   /**\n    * This method needs to be called before any public call that reads or\n    * modifies stores in bulk. It has to be called just before a try.\n    * #closeBulkRegionOperation needs to be called in the try's finally block\n    * Acquires a writelock and checks if the region is closing or closed.\n-   * @throws NotServingRegionException when the region is closing or closed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIxNzUzMA=="}, "originalCommit": {"oid": "7254af38791750d4b6c584959c5aa66c66cf75a0"}, "originalPosition": 508}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzcwMDA3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNDowMTo1M1rOHsFkJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwMDoyMjo0OVrOHsb8HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MDU2NQ==", "bodyText": "I think here we could have a checkInterrupt? And for me, I think before STEP 4, we could always interrupt the handler but I wonder whether it worth. The prepare mini batch and build WAL are all in memory operations so they should be fast. So I suggest that, we call checkInterrupt here, if we pass, then remove the handler from the Map.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r515990565", "createdAt": "2020-11-02T14:01:53Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4565,17 +4663,23 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       lock(this.updatesLock.readLock(), miniBatchOp.getReadyToWriteCount());\n       locked = true;\n \n+      // From this point until memstore update this operation should not be interrupted.\n+      disableInterrupts();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE2NjkwMg==", "bodyText": "Yes, here I can put a checkInterrupt. Let me do that.\nIf we remove the handler from the map at this point, no other mini batch will be eligible for interrupt.\nbatchMutate does this:\nwhile (!batchOp.isDone()) {\n    ...\n    doMiniBatchMutate(batchOp);\n    ...\n  }", "url": "https://github.com/apache/hbase/pull/2574#discussion_r516166902", "createdAt": "2020-11-02T18:15:09Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4565,17 +4663,23 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       lock(this.updatesLock.readLock(), miniBatchOp.getReadyToWriteCount());\n       locked = true;\n \n+      // From this point until memstore update this operation should not be interrupted.\n+      disableInterrupts();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MDU2NQ=="}, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzE0OQ==", "bodyText": "Oh, you are right, I recall that we have a loop and will only acquire row locks as many as possible in each loop. Then this should be fine.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r516357149", "createdAt": "2020-11-03T00:22:49Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -4565,17 +4663,23 @@ private void doMiniBatchMutate(BatchOperation<?> batchOp) throws IOException {\n       lock(this.updatesLock.readLock(), miniBatchOp.getReadyToWriteCount());\n       locked = true;\n \n+      // From this point until memstore update this operation should not be interrupted.\n+      disableInterrupts();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MDU2NQ=="}, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 194}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzcwODg1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNDowMzo1M1rOHsFpVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxODoxNjo1M1rOHsQYyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MTg5Mg==", "bodyText": "I wonder whether it is possible to return what we have to client instead of throwing an exception to client. Anyway, Can be a follow on issue, not a blocker here.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r515991892", "createdAt": "2020-11-02T14:03:53Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -7623,6 +7740,9 @@ protected boolean nextRow(ScannerContext scannerContext, Cell curRowCell) throws\n       Cell next;\n       while ((next = this.storeHeap.peek()) != null &&\n              CellUtil.matchingRows(next, curRowCell)) {\n+        // Check for thread interrupt status in case we have been signaled from\n+        // #interruptRegionOperation.\n+        checkInterrupt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE2Nzg4Mg==", "bodyText": "Good point. Agreed it would be a good follow up.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r516167882", "createdAt": "2020-11-02T18:16:53Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -7623,6 +7740,9 @@ protected boolean nextRow(ScannerContext scannerContext, Cell curRowCell) throws\n       Cell next;\n       while ((next = this.storeHeap.peek()) != null &&\n              CellUtil.matchingRows(next, curRowCell)) {\n+        // Check for thread interrupt status in case we have been signaled from\n+        // #interruptRegionOperation.\n+        checkInterrupt();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MTg5Mg=="}, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 280}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMzcxMTExOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNDowNDoyOFrOHsFqwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxODoyMjozMlrOHsQlGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MjI1OA==", "bodyText": "Same with the doMiniBatchMutation above.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r515992258", "createdAt": "2020-11-02T14:04:28Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8303,9 +8428,14 @@ public void processRowsWithLocks(RowProcessor<?,?> processor, long timeout,\n             prevRowLock = rowLock;\n           }\n         }\n+\n         // STEP 3. Region lock\n         lock(this.updatesLock.readLock(), acquiredRowLocks.isEmpty() ? 1 : acquiredRowLocks.size());\n         locked = true;\n+\n+        // From this point until memstore update this operation should not be interrupted.\n+        disableInterrupts();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE2NzY2Nw==", "bodyText": "Will add a checkInterrupt here too.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r516167667", "createdAt": "2020-11-02T18:16:28Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8303,9 +8428,14 @@ public void processRowsWithLocks(RowProcessor<?,?> processor, long timeout,\n             prevRowLock = rowLock;\n           }\n         }\n+\n         // STEP 3. Region lock\n         lock(this.updatesLock.readLock(), acquiredRowLocks.isEmpty() ? 1 : acquiredRowLocks.size());\n         locked = true;\n+\n+        // From this point until memstore update this operation should not be interrupted.\n+        disableInterrupts();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MjI1OA=="}, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE3MTAzMg==", "bodyText": "This code is not executed in a loop so removing the thread from the map would be fine. However the reason why I also do disable/enable interrupt is as follows: We already have to do this for doMiniBatchMutation. (The disable/enable pair in doMiniBatchMutation protects WAL update and memstore insert from interrupt and makes them \"atomic\" in this sense, and disable/enable makes sense there because doMiniBatchMutation is inside a loop.) The disable/enable interrupt pair is a new code discipline. This section of the code also has the same requirements. Apply the new code discipline here too for consistency.", "url": "https://github.com/apache/hbase/pull/2574#discussion_r516171032", "createdAt": "2020-11-02T18:22:32Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java", "diffHunk": "@@ -8303,9 +8428,14 @@ public void processRowsWithLocks(RowProcessor<?,?> processor, long timeout,\n             prevRowLock = rowLock;\n           }\n         }\n+\n         // STEP 3. Region lock\n         lock(this.updatesLock.readLock(), acquiredRowLocks.isEmpty() ? 1 : acquiredRowLocks.size());\n         locked = true;\n+\n+        // From this point until memstore update this operation should not be interrupted.\n+        disableInterrupts();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk5MjI1OA=="}, "originalCommit": {"oid": "e5a714d32e88ef6543d2d7f575c7ae6ad18b5484"}, "originalPosition": 306}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2455, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}