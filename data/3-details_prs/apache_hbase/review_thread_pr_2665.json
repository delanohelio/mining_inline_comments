{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIyMDMyNjgw", "number": 2665, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo0NjowOVrOE5xyVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo1Mzo1NVrOE5yC2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDE5OTkxOnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo0NjowOVrOH0dKaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxNDoyNlrOH0e4yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2NTgwMA==", "bodyText": "nit: mind adding some line breaks at <100 chars?", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524765800", "createdAt": "2020-11-16T23:46:09Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5NDA1OA==", "bodyText": "Added CRs for paragraphs I touched. Makes the patch bigger if that is ok.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524794058", "createdAt": "2020-11-17T00:14:26Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2NTgwMA=="}, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDIwNDg0OnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo0NzowMVrOH0dOMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo0NzowMVrOH0dOMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2Njc3MQ==", "bodyText": "s/createa/create a/", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524766771", "createdAt": "2020-11-16T23:47:01Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDIwOTk1OnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo0Nzo1NVrOH0dRpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxMzo1MVrOH0e2rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2NzY1NQ==", "bodyText": "These two actions must be performed in the order specified here? or is it okay to perform them in the opposite order?", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524767655", "createdAt": "2020-11-16T23:47:55Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MzUxOA==", "bodyText": "I specified the order documented here (before my time).", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524793518", "createdAt": "2020-11-17T00:13:51Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2NzY1NQ=="}, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDIyMDY3OnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo0OTo1N1rOH0dZcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxNjo1NFrOH0fECw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2OTY1MQ==", "bodyText": "Instead of going back and forth with instructions for < 2.4.0 and 2.4.0+, please write two sections, one for < 2.4.0 and the other for 2.4.0+. The words might be 80% duplicated, but it makes it crystal clear, what's applicable to which versions.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524769651", "createdAt": "2020-11-16T23:49:57Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5NjkzOQ==", "bodyText": "done", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524796939", "createdAt": "2020-11-17T00:16:54Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc2OTY1MQ=="}, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDIyMzU2OnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo1MDoyOVrOH0dbwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxNzoyNFrOH0fHJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MDI0MA==", "bodyText": "nit: s/Regards the META replicas count/Regarding the META replicas count/", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524770240", "createdAt": "2020-11-16T23:50:29Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5NzczMw==", "bodyText": "Thanks.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524797733", "createdAt": "2020-11-17T00:17:24Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MDI0MA=="}, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDIzMzY1OnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo1MjoxOVrOH0di8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDozMTo0NFrOH0fq9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MjA4Mg==", "bodyText": "You give a name to AND describe the old behavior, but you only give a name to the new behavior. Would be a kindness to describe and name both, OR just name them and link off to the documentation on this enum in code.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524772082", "createdAt": "2020-11-16T23:52:19Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.\n+Now you can alter the META table as you would a user-space table (if `hbase.meta.replica.count` is set, it\n+will take precedent over what is set for replica count in the META table updating META replica count to\n+match).\n+\n+==== Load Balancing META table load ====\n+\n+hbase-2.4.0 adds a new client-side `LoadBalance` mode. When enabled client-side, clients will try to read META replicas first before falling back on the primary. Before this,\n+the lookup mode -- now named `HedgedRead` -- had clients read the primary and if no response after a configurable amount of time had elapsed, it would start up reads against the replicas.\n+The new 'LoadBalance' mode helps alleviate hotspotting on the META table distributing the META read load.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjkwMg==", "bodyText": "Oh. Yes. Let me fix.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524806902", "createdAt": "2020-11-17T00:31:44Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.\n+Now you can alter the META table as you would a user-space table (if `hbase.meta.replica.count` is set, it\n+will take precedent over what is set for replica count in the META table updating META replica count to\n+match).\n+\n+==== Load Balancing META table load ====\n+\n+hbase-2.4.0 adds a new client-side `LoadBalance` mode. When enabled client-side, clients will try to read META replicas first before falling back on the primary. Before this,\n+the lookup mode -- now named `HedgedRead` -- had clients read the primary and if no response after a configurable amount of time had elapsed, it would start up reads against the replicas.\n+The new 'LoadBalance' mode helps alleviate hotspotting on the META table distributing the META read load.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MjA4Mg=="}, "originalCommit": {"oid": "55c9df02fc0c411c99229fbd794ab12d6e08132e"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDI0MjE4OnYy", "diffSide": "RIGHT", "path": "src/main/asciidoc/_chapters/architecture.adoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzo1Mzo1NVrOH0dpJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDozMToyNFrOH0fqjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MzY2OA==", "bodyText": "\"please do not\" is this language strong enough? What happens if i ship out this configuration accidentally? How badly will we muck up meta? Should the the system proactively defend against this configuration?", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524773668", "createdAt": "2020-11-16T23:53:55Z", "author": {"login": "ndimiduk"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.\n+Now you can alter the META table as you would a user-space table (if `hbase.meta.replica.count` is set, it\n+will take precedent over what is set for replica count in the META table updating META replica count to\n+match).\n+\n+==== Load Balancing META table load ====\n+\n+hbase-2.4.0 adds a new client-side `LoadBalance` mode. When enabled client-side, clients will try to read META replicas first before falling back on the primary. Before this,\n+the lookup mode -- now named `HedgedRead` -- had clients read the primary and if no response after a configurable amount of time had elapsed, it would start up reads against the replicas.\n+The new 'LoadBalance' mode helps alleviate hotspotting on the META table distributing the META read load.\n+\n+To enable the meta replica locator's load balance mode, please set the following configuration at on the client-side (only): set 'hbase.locator.meta.replicas.mode' to \"LoadBalance\".\n+Valid options for this configuration are `None`, `HedgedRead`, and `LoadBalance`. Option parse is case insensitive.\n+The default mode is `None` (which falls through to `HedgedRead`, the current default).  Please do not put this configuration in any hbase server's configuration, master or region server.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95b460d79052cbaeb3b5caed6f0e7b96bdfbca19"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5ODk2Ng==", "bodyText": "Change it to DO NOT. A follow-on will add code to defend against this being set server side.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524798966", "createdAt": "2020-11-17T00:18:10Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.\n+Now you can alter the META table as you would a user-space table (if `hbase.meta.replica.count` is set, it\n+will take precedent over what is set for replica count in the META table updating META replica count to\n+match).\n+\n+==== Load Balancing META table load ====\n+\n+hbase-2.4.0 adds a new client-side `LoadBalance` mode. When enabled client-side, clients will try to read META replicas first before falling back on the primary. Before this,\n+the lookup mode -- now named `HedgedRead` -- had clients read the primary and if no response after a configurable amount of time had elapsed, it would start up reads against the replicas.\n+The new 'LoadBalance' mode helps alleviate hotspotting on the META table distributing the META read load.\n+\n+To enable the meta replica locator's load balance mode, please set the following configuration at on the client-side (only): set 'hbase.locator.meta.replicas.mode' to \"LoadBalance\".\n+Valid options for this configuration are `None`, `HedgedRead`, and `LoadBalance`. Option parse is case insensitive.\n+The default mode is `None` (which falls through to `HedgedRead`, the current default).  Please do not put this configuration in any hbase server's configuration, master or region server.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MzY2OA=="}, "originalCommit": {"oid": "95b460d79052cbaeb3b5caed6f0e7b96bdfbca19"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjc5OA==", "bodyText": "Added HBASE-25294 to add code defense against the LoadBalance option being enabled for the Master-hosted client.", "url": "https://github.com/apache/hbase/pull/2665#discussion_r524806798", "createdAt": "2020-11-17T00:31:24Z", "author": {"login": "saintstack"}, "path": "src/main/asciidoc/_chapters/architecture.adoc", "diffHunk": "@@ -2865,26 +2865,51 @@ The first mechanism is store file refresher which is introduced in HBase-1.0+. S\n \n For turning this feature on, you should configure `hbase.regionserver.storefile.refresh.period` to a non-zero value. See Configuration section below.\n \n-==== Asnyc WAL replication\n-The second mechanism for propagation of writes to secondaries is done via \u201cAsync WAL Replication\u201d feature and is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n+[[async.wal.replication]]\n+==== Async WAL replication\n+The second mechanism for propagation of writes to secondaries is done via the \u201cAsync WAL Replication\u201d feature. It is only available in HBase-1.1+. This works similarly to HBase\u2019s multi-datacenter replication, but instead the data from a region is replicated to the secondary regions. Each secondary replica always receives and observes the writes in the same order that the primary region committed them. In some sense, this design can be thought of as \u201cin-cluster replication\u201d, where instead of replicating to a different datacenter, the data goes to secondary regions to keep secondary region\u2019s in-memory state up to date. The data files are shared between the primary region and the other replicas, so that there is no extra storage overhead. However, the secondary regions will have recent non-flushed data in their memstores, which increases the memory overhead. The primary region writes flush, compaction, and bulk load events to its WAL as well, which are also replicated through wal replication to secondaries. When they observe the flush/compaction or bulk load event, the secondary regions replay the event to pick up the new files and drop the old ones.\n \n Committing writes in the same order as in primary ensures that the secondaries won\u2019t diverge from the primary regions data, but since the log replication is asynchronous, the data might still be stale in secondary regions. Since this feature works as a replication endpoint, the performance and latency characteristics is expected to be similar to inter-cluster replication.\n \n Async WAL Replication is *disabled* by default. You can enable this feature by setting `hbase.region.replica.replication.enabled` to `true`.\n-Asyn WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you create a table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n+The Async WAL Replication feature will add a new replication peer named `region_replica_replication` as a replication peer when you createa table with region replication > 1 for the first time. Once enabled, if you want to disable this feature, you need to do two actions:\n * Set configuration property `hbase.region.replica.replication.enabled` to false in `hbase-site.xml` (see Configuration section below)\n * Disable the replication peer named `region_replica_replication` in the cluster using hbase shell or `Admin` class:\n [source,bourne]\n ----\n \thbase> disable_peer 'region_replica_replication'\n ----\n \n+Async WAL Replication and the `hbase:meta` table is a little more involved and gets its own section below; see <<async.wal.replication.meta>>\n+\n === Store File TTL\n In both of the write propagation approaches mentioned above, store files of the primary will be opened in secondaries independent of the primary region. So for files that the primary compacted away, the secondaries might still be referring to these files for reading. Both features are using HFileLinks to refer to files, but there is no protection (yet) for guaranteeing that the file will not be deleted prematurely. Thus, as a guard, you should set the configuration property `hbase.master.hfilecleaner.ttl` to a larger value, such as 1 hour to guarantee that you will not receive IOExceptions for requests going to replicas.\n \n+[[async.wal.replication.meta]]\n === Region replication for META table\u2019s region\n-Currently, Async WAL Replication is not done for the META table\u2019s WAL. The meta table\u2019s secondary replicas still refreshes themselves from the persistent store files. Hence the `hbase.regionserver.meta.storefile.refresh.period` needs to be set to a certain non-zero value for refreshing the meta store files. Note that this configuration is configured differently than\n-`hbase.regionserver.storefile.refresh.period`.\n+Up until hbase-2.4.0, Async WAL Replication did not work for the META table\u2019s WAL. The meta table\u2019s secondary replicas refreshed themselves from the persistent store files every `hbase.regionserver.meta.storefile.refresh.period`,\n+(a non-zero value). Note how the META replication period is distinct from the user-space `hbase.regionserver.storefile.refresh.period` value.\n+\n+Async WAL replication for META is a new feature in 2.4.0 still under active development. Use with caution.\n+Set `hbase.region.replica.replication.catalog.enabled` to enable async WAL Replication for META region replicas.\n+Its off by default.\n+\n+Regards the META replicas count, up to hbase-2.4.0, you would set the special property 'hbase.meta.replica.count'.\n+Now you can alter the META table as you would a user-space table (if `hbase.meta.replica.count` is set, it\n+will take precedent over what is set for replica count in the META table updating META replica count to\n+match).\n+\n+==== Load Balancing META table load ====\n+\n+hbase-2.4.0 adds a new client-side `LoadBalance` mode. When enabled client-side, clients will try to read META replicas first before falling back on the primary. Before this,\n+the lookup mode -- now named `HedgedRead` -- had clients read the primary and if no response after a configurable amount of time had elapsed, it would start up reads against the replicas.\n+The new 'LoadBalance' mode helps alleviate hotspotting on the META table distributing the META read load.\n+\n+To enable the meta replica locator's load balance mode, please set the following configuration at on the client-side (only): set 'hbase.locator.meta.replicas.mode' to \"LoadBalance\".\n+Valid options for this configuration are `None`, `HedgedRead`, and `LoadBalance`. Option parse is case insensitive.\n+The default mode is `None` (which falls through to `HedgedRead`, the current default).  Please do not put this configuration in any hbase server's configuration, master or region server.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc3MzY2OA=="}, "originalCommit": {"oid": "95b460d79052cbaeb3b5caed6f0e7b96bdfbca19"}, "originalPosition": 51}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2393, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}