{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwNzYzNjc1", "number": 1746, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo0NTowOVrOD-UzYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxNzo0OFrOD_6-Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Njc5MTM5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo0NTowOVrOGYW9wA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo0NTowOVrOGYW9wA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5NTI2NA==", "bodyText": "Would be cool if this was not inline in HMaster class. It too big as it is.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428195264", "createdAt": "2020-05-20T17:45:09Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -866,6 +877,50 @@ protected AssignmentManager createAssignmentManager(MasterServices master) {\n     return new AssignmentManager(master);\n   }\n \n+  private void createRootTable() throws IOException, KeeperException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Njc5OTAyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo0NzoxNlrOGYXCbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo0NzoxNlrOGYXCbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5NjQ2Mg==", "bodyText": "Good", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428196462", "createdAt": "2020-05-20T17:47:16Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -866,6 +877,50 @@ protected AssignmentManager createAssignmentManager(MasterServices master) {\n     return new AssignmentManager(master);\n   }\n \n+  private void createRootTable() throws IOException, KeeperException {\n+    RootTable rootTable = new RootTable(this, cleanerPool);\n+    rootTable.initialize();\n+    // try migrate data from zookeeper\n+    try (RegionScanner scanner =\n+      rootTable.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows = scanner.next(cells);\n+      if (!cells.isEmpty() || moreRows) {\n+        // notice that all replicas for a region are in the same row, so the migration can be\n+        // done with in a one row put, which means if we have data in root table then we can make\n+        // sure that the migration is done.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjgxMjc1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo1MDo1MlrOGYXLFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwMTo0OToyNVrOGYjjtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5ODY3OA==", "bodyText": "So, this row key is just serialized meta region name?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428198678", "createdAt": "2020-05-20T17:50:52Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -866,6 +877,50 @@ protected AssignmentManager createAssignmentManager(MasterServices master) {\n     return new AssignmentManager(master);\n   }\n \n+  private void createRootTable() throws IOException, KeeperException {\n+    RootTable rootTable = new RootTable(this, cleanerPool);\n+    rootTable.initialize();\n+    // try migrate data from zookeeper\n+    try (RegionScanner scanner =\n+      rootTable.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows = scanner.next(cells);\n+      if (!cells.isEmpty() || moreRows) {\n+        // notice that all replicas for a region are in the same row, so the migration can be\n+        // done with in a one row put, which means if we have data in root table then we can make\n+        // sure that the migration is done.\n+        LOG.info(\"Root table already has data in it, skip migrating...\");\n+        this.rootTable = rootTable;\n+        return;\n+      }\n+    }\n+    // start migrating\n+    byte[] row = MetaTableAccessor.getMetaKeyForRegion(RegionInfoBuilder.FIRST_META_REGIONINFO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMTU5MA==", "bodyText": "Yes, just like what we have done for normal regions which are stored in meta table.\nI think we should change the class and method name, as francis suggested. Can be done in another issue.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428401590", "createdAt": "2020-05-21T01:49:25Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -866,6 +877,50 @@ protected AssignmentManager createAssignmentManager(MasterServices master) {\n     return new AssignmentManager(master);\n   }\n \n+  private void createRootTable() throws IOException, KeeperException {\n+    RootTable rootTable = new RootTable(this, cleanerPool);\n+    rootTable.initialize();\n+    // try migrate data from zookeeper\n+    try (RegionScanner scanner =\n+      rootTable.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows = scanner.next(cells);\n+      if (!cells.isEmpty() || moreRows) {\n+        // notice that all replicas for a region are in the same row, so the migration can be\n+        // done with in a one row put, which means if we have data in root table then we can make\n+        // sure that the migration is done.\n+        LOG.info(\"Root table already has data in it, skip migrating...\");\n+        this.rootTable = rootTable;\n+        return;\n+      }\n+    }\n+    // start migrating\n+    byte[] row = MetaTableAccessor.getMetaKeyForRegion(RegionInfoBuilder.FIRST_META_REGIONINFO);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5ODY3OA=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjgxMzU3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo1MTowNVrOGYXLkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNzo1MTowNVrOGYXLkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5ODgwMQ==", "bodyText": "good", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428198801", "createdAt": "2020-05-20T17:51:05Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -866,6 +877,50 @@ protected AssignmentManager createAssignmentManager(MasterServices master) {\n     return new AssignmentManager(master);\n   }\n \n+  private void createRootTable() throws IOException, KeeperException {\n+    RootTable rootTable = new RootTable(this, cleanerPool);\n+    rootTable.initialize();\n+    // try migrate data from zookeeper\n+    try (RegionScanner scanner =\n+      rootTable.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows = scanner.next(cells);\n+      if (!cells.isEmpty() || moreRows) {\n+        // notice that all replicas for a region are in the same row, so the migration can be\n+        // done with in a one row put, which means if we have data in root table then we can make\n+        // sure that the migration is done.\n+        LOG.info(\"Root table already has data in it, skip migrating...\");\n+        this.rootTable = rootTable;\n+        return;\n+      }\n+    }\n+    // start migrating\n+    byte[] row = MetaTableAccessor.getMetaKeyForRegion(RegionInfoBuilder.FIRST_META_REGIONINFO);\n+    Put put = new Put(row);\n+    List<String> metaReplicaNodes = zooKeeper.getMetaReplicaNodes();\n+    StringBuilder info = new StringBuilder(\"Migrating meta location:\");\n+    for (String metaReplicaNode : metaReplicaNodes) {\n+      int replicaId = zooKeeper.getZNodePaths().getMetaReplicaIdFromZnode(metaReplicaNode);\n+      RegionState state = MetaTableLocator.getMetaRegionState(zooKeeper, replicaId);\n+      info.append(\" \").append(state);\n+      put.setTimestamp(state.getStamp());\n+      MetaTableAccessor.addRegionInfo(put, state.getRegion());\n+      if (state.getServerName() != null) {\n+        MetaTableAccessor.addLocation(put, state.getServerName(), HConstants.NO_SEQNUM, replicaId);\n+      }\n+      put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY).setRow(put.getRow())\n+        .setFamily(HConstants.CATALOG_FAMILY)\n+        .setQualifier(RegionStateStore.getStateColumn(replicaId)).setTimestamp(put.getTimestamp())\n+        .setType(Cell.Type.Put).setValue(Bytes.toBytes(state.getState().name())).build());\n+    }\n+    if (!put.isEmpty()) {\n+      LOG.info(info.toString());\n+    } else {\n+      LOG.info(\"No meta location avaiable on zookeeper, skip migrating...\");\n+    }\n+    this.rootTable = rootTable;\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjkyODY1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODoyMzo1MFrOGYYWBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNTo0OTo0N1rOGY4bqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIxNzg2Mw==", "bodyText": "It can't be kept totally internal?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428217863", "createdAt": "2020-05-20T18:23:50Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -552,4 +553,6 @@ default SplitWALManager getSplitWALManager(){\n    * @return The state of the load balancer, or false if the load balancer isn't defined.\n    */\n   boolean isBalancerOn();\n+\n+  RootTable getRootTable();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIyMzEyNA==", "bodyText": "I suppose AM gets it this way or some other internal services?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428223124", "createdAt": "2020-05-20T18:32:04Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -552,4 +553,6 @@ default SplitWALManager getSplitWALManager(){\n    * @return The state of the load balancer, or false if the load balancer isn't defined.\n    */\n   boolean isBalancerOn();\n+\n+  RootTable getRootTable();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIxNzg2Mw=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMTgyMw==", "bodyText": "Maybe we could add it as a parameter to the constructor of AssignmentManager?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428401823", "createdAt": "2020-05-21T01:50:21Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -552,4 +553,6 @@ default SplitWALManager getSplitWALManager(){\n    * @return The state of the load balancer, or false if the load balancer isn't defined.\n    */\n   boolean isBalancerOn();\n+\n+  RootTable getRootTable();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIxNzg2Mw=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODc0MzU5NA==", "bodyText": "If it the only consumer... yeah, maybe. No hurry.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428743594", "createdAt": "2020-05-21T15:49:47Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java", "diffHunk": "@@ -552,4 +553,6 @@ default SplitWALManager getSplitWALManager(){\n    * @return The state of the load balancer, or false if the load balancer isn't defined.\n    */\n   boolean isBalancerOn();\n+\n+  RootTable getRootTable();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIxNzg2Mw=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Njk5MDY0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODozNjo0NFrOGYY69A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODozNjo0NFrOGYY69A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIyNzMxNg==", "bodyText": "Nice", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428227316", "createdAt": "2020-05-20T18:36:44Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java", "diffHunk": "@@ -216,12 +196,32 @@ private void updateUserRegionLocation(RegionInfo regionInfo, State state,\n         .build());\n     LOG.info(info.toString());\n     updateRegionLocation(regionInfo, state, put);\n+    if (regionInfo.isMetaRegion() && regionInfo.isFirst()) {\n+      // mirror the meta location to\n+      mirrorMetaLocation(regionInfo, regionLocation, state);\n+    }\n   }\n \n-  private void updateRegionLocation(RegionInfo regionInfo, State state, Put put)\n+  public void mirrorMetaLocation(RegionInfo regionInfo, ServerName serverName, State state)\n       throws IOException {\n-    try (Table table = master.getConnection().getTable(TableName.META_TABLE_NAME)) {\n-      table.put(put);\n+    try {\n+      MetaTableLocator.setMetaLocation(master.getZooKeeper(), serverName, regionInfo.getReplicaId(),\n+        state);\n+    } catch (KeeperException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  private void updateRegionLocation(RegionInfo regionInfo, State state, Put put)\n+    throws IOException {\n+    try {\n+      if (regionInfo.isMetaRegion()) {\n+        master.getRootTable().update(put);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NzAwNzQzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODozOTo0M1rOGYZE-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNTo0OTowN1rOGY4ZqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIyOTg4MA==", "bodyText": "Is this at ${root.dir}/RootTable?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428229880", "createdAt": "2020-05-20T18:39:43Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.root;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;\n+import org.apache.hadoop.hbase.master.cleaner.DirScanPool;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;\n+import org.apache.hadoop.hbase.region.LocalRegion;\n+import org.apache.hadoop.hbase.region.LocalRegionParams;\n+import org.apache.hadoop.hbase.regionserver.BloomType;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Used to store the location of meta region.\n+ */\n+@InterfaceAudience.Private\n+public class RootTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RootTable.class);\n+\n+  static final String MAX_WALS_KEY = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_MAX_WALS = 10;\n+\n+  static final String USE_HSYNC_KEY = \"hbase.root.table.region.wal.hsync\";\n+\n+  static final String RING_BUFFER_SLOT_COUNT = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_RING_BUFFER_SLOT_COUNT = 64;\n+\n+  static final String ROOT_TABLE_DIR = \"RootTable\";\n+\n+  static final String HFILECLEANER_PLUGINS = \"hbase.root.table.region.hfilecleaner.plugins\";\n+\n+  static final String FLUSH_SIZE_KEY = \"hbase.root.table.region.flush.size\";\n+\n+  static final long DEFAULT_FLUSH_SIZE = TableDescriptorBuilder.DEFAULT_MEMSTORE_FLUSH_SIZE;\n+\n+  static final String FLUSH_PER_CHANGES_KEY = \"hbase.root.table.region.flush.per.changes\";\n+\n+  private static final long DEFAULT_FLUSH_PER_CHANGES = 1_000_000;\n+\n+  static final String FLUSH_INTERVAL_MS_KEY = \"hbase.root.table.region.flush.interval.ms\";\n+\n+  // default to flush every 15 minutes, for safety\n+  private static final long DEFAULT_FLUSH_INTERVAL_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final String COMPACT_MIN_KEY = \"hbase.root.table.region.compact.min\";\n+\n+  private static final int DEFAULT_COMPACT_MIN = 4;\n+\n+  static final String ROLL_PERIOD_MS_KEY = \"hbase.root.table.region.walroll.period.ms\";\n+\n+  private static final long DEFAULT_ROLL_PERIOD_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final TableName TABLE_NAME = TableName.valueOf(\"master:root\");\n+\n+  private static final TableDescriptor TABLE_DESC = TableDescriptorBuilder.newBuilder(TABLE_NAME)\n+    .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY)\n+      .setMaxVersions(HConstants.DEFAULT_HBASE_META_VERSIONS).setInMemory(true)\n+      .setBlocksize(HConstants.DEFAULT_HBASE_META_BLOCK_SIZE).setBloomFilterType(BloomType.ROWCOL)\n+      .setDataBlockEncoding(DataBlockEncoding.ROW_INDEX_V1).build())\n+    .build();\n+\n+  private final Server server;\n+\n+  private final DirScanPool cleanerPool;\n+\n+  private LocalRegion region;\n+\n+  public RootTable(Server server, DirScanPool cleanerPool) {\n+    this.server = server;\n+    this.cleanerPool = cleanerPool;\n+  }\n+\n+  public void initialize() throws IOException {\n+    LOG.info(\"Initializing root table...\");\n+    LocalRegionParams params = new LocalRegionParams().server(server).regionDirName(ROOT_TABLE_DIR)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMTg4Nw==", "bodyText": "Yes.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428401887", "createdAt": "2020-05-21T01:50:35Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.root;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;\n+import org.apache.hadoop.hbase.master.cleaner.DirScanPool;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;\n+import org.apache.hadoop.hbase.region.LocalRegion;\n+import org.apache.hadoop.hbase.region.LocalRegionParams;\n+import org.apache.hadoop.hbase.regionserver.BloomType;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Used to store the location of meta region.\n+ */\n+@InterfaceAudience.Private\n+public class RootTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RootTable.class);\n+\n+  static final String MAX_WALS_KEY = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_MAX_WALS = 10;\n+\n+  static final String USE_HSYNC_KEY = \"hbase.root.table.region.wal.hsync\";\n+\n+  static final String RING_BUFFER_SLOT_COUNT = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_RING_BUFFER_SLOT_COUNT = 64;\n+\n+  static final String ROOT_TABLE_DIR = \"RootTable\";\n+\n+  static final String HFILECLEANER_PLUGINS = \"hbase.root.table.region.hfilecleaner.plugins\";\n+\n+  static final String FLUSH_SIZE_KEY = \"hbase.root.table.region.flush.size\";\n+\n+  static final long DEFAULT_FLUSH_SIZE = TableDescriptorBuilder.DEFAULT_MEMSTORE_FLUSH_SIZE;\n+\n+  static final String FLUSH_PER_CHANGES_KEY = \"hbase.root.table.region.flush.per.changes\";\n+\n+  private static final long DEFAULT_FLUSH_PER_CHANGES = 1_000_000;\n+\n+  static final String FLUSH_INTERVAL_MS_KEY = \"hbase.root.table.region.flush.interval.ms\";\n+\n+  // default to flush every 15 minutes, for safety\n+  private static final long DEFAULT_FLUSH_INTERVAL_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final String COMPACT_MIN_KEY = \"hbase.root.table.region.compact.min\";\n+\n+  private static final int DEFAULT_COMPACT_MIN = 4;\n+\n+  static final String ROLL_PERIOD_MS_KEY = \"hbase.root.table.region.walroll.period.ms\";\n+\n+  private static final long DEFAULT_ROLL_PERIOD_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final TableName TABLE_NAME = TableName.valueOf(\"master:root\");\n+\n+  private static final TableDescriptor TABLE_DESC = TableDescriptorBuilder.newBuilder(TABLE_NAME)\n+    .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY)\n+      .setMaxVersions(HConstants.DEFAULT_HBASE_META_VERSIONS).setInMemory(true)\n+      .setBlocksize(HConstants.DEFAULT_HBASE_META_BLOCK_SIZE).setBloomFilterType(BloomType.ROWCOL)\n+      .setDataBlockEncoding(DataBlockEncoding.ROW_INDEX_V1).build())\n+    .build();\n+\n+  private final Server server;\n+\n+  private final DirScanPool cleanerPool;\n+\n+  private LocalRegion region;\n+\n+  public RootTable(Server server, DirScanPool cleanerPool) {\n+    this.server = server;\n+    this.cleanerPool = cleanerPool;\n+  }\n+\n+  public void initialize() throws IOException {\n+    LOG.info(\"Initializing root table...\");\n+    LocalRegionParams params = new LocalRegionParams().server(server).regionDirName(ROOT_TABLE_DIR)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIyOTg4MA=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODc0MzA4MQ==", "bodyText": "Yeah, would be good if a general location for master local store.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428743081", "createdAt": "2020-05-21T15:49:07Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.root;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;\n+import org.apache.hadoop.hbase.master.cleaner.DirScanPool;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;\n+import org.apache.hadoop.hbase.region.LocalRegion;\n+import org.apache.hadoop.hbase.region.LocalRegionParams;\n+import org.apache.hadoop.hbase.regionserver.BloomType;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Used to store the location of meta region.\n+ */\n+@InterfaceAudience.Private\n+public class RootTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RootTable.class);\n+\n+  static final String MAX_WALS_KEY = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_MAX_WALS = 10;\n+\n+  static final String USE_HSYNC_KEY = \"hbase.root.table.region.wal.hsync\";\n+\n+  static final String RING_BUFFER_SLOT_COUNT = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_RING_BUFFER_SLOT_COUNT = 64;\n+\n+  static final String ROOT_TABLE_DIR = \"RootTable\";\n+\n+  static final String HFILECLEANER_PLUGINS = \"hbase.root.table.region.hfilecleaner.plugins\";\n+\n+  static final String FLUSH_SIZE_KEY = \"hbase.root.table.region.flush.size\";\n+\n+  static final long DEFAULT_FLUSH_SIZE = TableDescriptorBuilder.DEFAULT_MEMSTORE_FLUSH_SIZE;\n+\n+  static final String FLUSH_PER_CHANGES_KEY = \"hbase.root.table.region.flush.per.changes\";\n+\n+  private static final long DEFAULT_FLUSH_PER_CHANGES = 1_000_000;\n+\n+  static final String FLUSH_INTERVAL_MS_KEY = \"hbase.root.table.region.flush.interval.ms\";\n+\n+  // default to flush every 15 minutes, for safety\n+  private static final long DEFAULT_FLUSH_INTERVAL_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final String COMPACT_MIN_KEY = \"hbase.root.table.region.compact.min\";\n+\n+  private static final int DEFAULT_COMPACT_MIN = 4;\n+\n+  static final String ROLL_PERIOD_MS_KEY = \"hbase.root.table.region.walroll.period.ms\";\n+\n+  private static final long DEFAULT_ROLL_PERIOD_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final TableName TABLE_NAME = TableName.valueOf(\"master:root\");\n+\n+  private static final TableDescriptor TABLE_DESC = TableDescriptorBuilder.newBuilder(TABLE_NAME)\n+    .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY)\n+      .setMaxVersions(HConstants.DEFAULT_HBASE_META_VERSIONS).setInMemory(true)\n+      .setBlocksize(HConstants.DEFAULT_HBASE_META_BLOCK_SIZE).setBloomFilterType(BloomType.ROWCOL)\n+      .setDataBlockEncoding(DataBlockEncoding.ROW_INDEX_V1).build())\n+    .build();\n+\n+  private final Server server;\n+\n+  private final DirScanPool cleanerPool;\n+\n+  private LocalRegion region;\n+\n+  public RootTable(Server server, DirScanPool cleanerPool) {\n+    this.server = server;\n+    this.cleanerPool = cleanerPool;\n+  }\n+\n+  public void initialize() throws IOException {\n+    LOG.info(\"Initializing root table...\");\n+    LocalRegionParams params = new LocalRegionParams().server(server).regionDirName(ROOT_TABLE_DIR)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIyOTg4MA=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NzAyMTcwOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODo0Mjo0OFrOGYZNbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwMTo1MzoxMlrOGYjnZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIzMjA0NA==", "bodyText": "It'd have its own Region?\nSeems overkill having a full Region just for meta locations?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428232044", "createdAt": "2020-05-20T18:42:48Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.root;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;\n+import org.apache.hadoop.hbase.master.cleaner.DirScanPool;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;\n+import org.apache.hadoop.hbase.region.LocalRegion;\n+import org.apache.hadoop.hbase.region.LocalRegionParams;\n+import org.apache.hadoop.hbase.regionserver.BloomType;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Used to store the location of meta region.\n+ */\n+@InterfaceAudience.Private\n+public class RootTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RootTable.class);\n+\n+  static final String MAX_WALS_KEY = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_MAX_WALS = 10;\n+\n+  static final String USE_HSYNC_KEY = \"hbase.root.table.region.wal.hsync\";\n+\n+  static final String RING_BUFFER_SLOT_COUNT = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_RING_BUFFER_SLOT_COUNT = 64;\n+\n+  static final String ROOT_TABLE_DIR = \"RootTable\";\n+\n+  static final String HFILECLEANER_PLUGINS = \"hbase.root.table.region.hfilecleaner.plugins\";\n+\n+  static final String FLUSH_SIZE_KEY = \"hbase.root.table.region.flush.size\";\n+\n+  static final long DEFAULT_FLUSH_SIZE = TableDescriptorBuilder.DEFAULT_MEMSTORE_FLUSH_SIZE;\n+\n+  static final String FLUSH_PER_CHANGES_KEY = \"hbase.root.table.region.flush.per.changes\";\n+\n+  private static final long DEFAULT_FLUSH_PER_CHANGES = 1_000_000;\n+\n+  static final String FLUSH_INTERVAL_MS_KEY = \"hbase.root.table.region.flush.interval.ms\";\n+\n+  // default to flush every 15 minutes, for safety\n+  private static final long DEFAULT_FLUSH_INTERVAL_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final String COMPACT_MIN_KEY = \"hbase.root.table.region.compact.min\";\n+\n+  private static final int DEFAULT_COMPACT_MIN = 4;\n+\n+  static final String ROLL_PERIOD_MS_KEY = \"hbase.root.table.region.walroll.period.ms\";\n+\n+  private static final long DEFAULT_ROLL_PERIOD_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final TableName TABLE_NAME = TableName.valueOf(\"master:root\");\n+\n+  private static final TableDescriptor TABLE_DESC = TableDescriptorBuilder.newBuilder(TABLE_NAME)\n+    .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY)\n+      .setMaxVersions(HConstants.DEFAULT_HBASE_META_VERSIONS).setInMemory(true)\n+      .setBlocksize(HConstants.DEFAULT_HBASE_META_BLOCK_SIZE).setBloomFilterType(BloomType.ROWCOL)\n+      .setDataBlockEncoding(DataBlockEncoding.ROW_INDEX_V1).build())\n+    .build();\n+\n+  private final Server server;\n+\n+  private final DirScanPool cleanerPool;\n+\n+  private LocalRegion region;\n+\n+  public RootTable(Server server, DirScanPool cleanerPool) {\n+    this.server = server;\n+    this.cleanerPool = cleanerPool;\n+  }\n+\n+  public void initialize() throws IOException {\n+    LOG.info(\"Initializing root table...\");\n+    LocalRegionParams params = new LocalRegionParams().server(server).regionDirName(ROOT_TABLE_DIR)\n+      .tableDescriptor(TABLE_DESC);\n+    Configuration conf = server.getConfiguration();\n+    long flushSize = conf.getLong(FLUSH_SIZE_KEY, DEFAULT_FLUSH_SIZE);\n+    long flushPerChanges = conf.getLong(FLUSH_PER_CHANGES_KEY, DEFAULT_FLUSH_PER_CHANGES);\n+    long flushIntervalMs = conf.getLong(FLUSH_INTERVAL_MS_KEY, DEFAULT_FLUSH_INTERVAL_MS);\n+    int compactMin = conf.getInt(COMPACT_MIN_KEY, DEFAULT_COMPACT_MIN);\n+    params.flushSize(flushSize).flushPerChanges(flushPerChanges).flushIntervalMs(flushIntervalMs)\n+      .compactMin(compactMin);\n+    int maxWals = conf.getInt(MAX_WALS_KEY, DEFAULT_MAX_WALS);\n+    params.maxWals(maxWals);\n+    if (conf.get(USE_HSYNC_KEY) != null) {\n+      params.useHsync(conf.getBoolean(USE_HSYNC_KEY, false));\n+    }\n+    params.ringBufferSlotCount(conf.getInt(RING_BUFFER_SLOT_COUNT, DEFAULT_RING_BUFFER_SLOT_COUNT));\n+    long rollPeriodMs = conf.getLong(ROLL_PERIOD_MS_KEY, DEFAULT_ROLL_PERIOD_MS);\n+    params.rollPeriodMs(rollPeriodMs)\n+      .archivedWalSuffix(MasterProcedureUtil.ARCHIVED_PROC_WAL_SUFFIX)\n+      .hfileCleanerPlugins(HFILECLEANER_PLUGINS).cleanerPool(cleanerPool);\n+    region = LocalRegion.create(params);\n+  }\n+\n+  public void update(Put put) throws IOException {\n+    region.update(r -> r.put(put));\n+  }\n+\n+  public void delete(Delete delete) throws IOException {\n+    region.update(r -> r.delete(delete));\n+  }\n+\n+  public RegionScanner getScanner(Scan scan) throws IOException {\n+    return region.getScanner(scan);\n+  }\n+\n+  public void close(boolean abort) {\n+    LOG.info(\"Closing root table, isAbort={}\", abort);\n+    if (region != null) {\n+      region.close(abort);\n+    }\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMjUzNA==", "bodyText": "Yes. I've posted on jira, we can introduce a single region, maybe called 'master-local-store', to store all the master local data, such as the root table and procedures. If we think this is a better way, we should be hurry to finish the work before 2.3.0 is out, otherwise we need to deal with the migration from the procedure store region again...", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428402534", "createdAt": "2020-05-21T01:53:12Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/root/RootTable.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.root;\n+\n+import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;\n+import org.apache.hadoop.hbase.master.cleaner.DirScanPool;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;\n+import org.apache.hadoop.hbase.region.LocalRegion;\n+import org.apache.hadoop.hbase.region.LocalRegionParams;\n+import org.apache.hadoop.hbase.regionserver.BloomType;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Used to store the location of meta region.\n+ */\n+@InterfaceAudience.Private\n+public class RootTable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RootTable.class);\n+\n+  static final String MAX_WALS_KEY = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_MAX_WALS = 10;\n+\n+  static final String USE_HSYNC_KEY = \"hbase.root.table.region.wal.hsync\";\n+\n+  static final String RING_BUFFER_SLOT_COUNT = \"hbase.root.table.region.maxwals\";\n+\n+  private static final int DEFAULT_RING_BUFFER_SLOT_COUNT = 64;\n+\n+  static final String ROOT_TABLE_DIR = \"RootTable\";\n+\n+  static final String HFILECLEANER_PLUGINS = \"hbase.root.table.region.hfilecleaner.plugins\";\n+\n+  static final String FLUSH_SIZE_KEY = \"hbase.root.table.region.flush.size\";\n+\n+  static final long DEFAULT_FLUSH_SIZE = TableDescriptorBuilder.DEFAULT_MEMSTORE_FLUSH_SIZE;\n+\n+  static final String FLUSH_PER_CHANGES_KEY = \"hbase.root.table.region.flush.per.changes\";\n+\n+  private static final long DEFAULT_FLUSH_PER_CHANGES = 1_000_000;\n+\n+  static final String FLUSH_INTERVAL_MS_KEY = \"hbase.root.table.region.flush.interval.ms\";\n+\n+  // default to flush every 15 minutes, for safety\n+  private static final long DEFAULT_FLUSH_INTERVAL_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final String COMPACT_MIN_KEY = \"hbase.root.table.region.compact.min\";\n+\n+  private static final int DEFAULT_COMPACT_MIN = 4;\n+\n+  static final String ROLL_PERIOD_MS_KEY = \"hbase.root.table.region.walroll.period.ms\";\n+\n+  private static final long DEFAULT_ROLL_PERIOD_MS = TimeUnit.MINUTES.toMillis(15);\n+\n+  static final TableName TABLE_NAME = TableName.valueOf(\"master:root\");\n+\n+  private static final TableDescriptor TABLE_DESC = TableDescriptorBuilder.newBuilder(TABLE_NAME)\n+    .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY)\n+      .setMaxVersions(HConstants.DEFAULT_HBASE_META_VERSIONS).setInMemory(true)\n+      .setBlocksize(HConstants.DEFAULT_HBASE_META_BLOCK_SIZE).setBloomFilterType(BloomType.ROWCOL)\n+      .setDataBlockEncoding(DataBlockEncoding.ROW_INDEX_V1).build())\n+    .build();\n+\n+  private final Server server;\n+\n+  private final DirScanPool cleanerPool;\n+\n+  private LocalRegion region;\n+\n+  public RootTable(Server server, DirScanPool cleanerPool) {\n+    this.server = server;\n+    this.cleanerPool = cleanerPool;\n+  }\n+\n+  public void initialize() throws IOException {\n+    LOG.info(\"Initializing root table...\");\n+    LocalRegionParams params = new LocalRegionParams().server(server).regionDirName(ROOT_TABLE_DIR)\n+      .tableDescriptor(TABLE_DESC);\n+    Configuration conf = server.getConfiguration();\n+    long flushSize = conf.getLong(FLUSH_SIZE_KEY, DEFAULT_FLUSH_SIZE);\n+    long flushPerChanges = conf.getLong(FLUSH_PER_CHANGES_KEY, DEFAULT_FLUSH_PER_CHANGES);\n+    long flushIntervalMs = conf.getLong(FLUSH_INTERVAL_MS_KEY, DEFAULT_FLUSH_INTERVAL_MS);\n+    int compactMin = conf.getInt(COMPACT_MIN_KEY, DEFAULT_COMPACT_MIN);\n+    params.flushSize(flushSize).flushPerChanges(flushPerChanges).flushIntervalMs(flushIntervalMs)\n+      .compactMin(compactMin);\n+    int maxWals = conf.getInt(MAX_WALS_KEY, DEFAULT_MAX_WALS);\n+    params.maxWals(maxWals);\n+    if (conf.get(USE_HSYNC_KEY) != null) {\n+      params.useHsync(conf.getBoolean(USE_HSYNC_KEY, false));\n+    }\n+    params.ringBufferSlotCount(conf.getInt(RING_BUFFER_SLOT_COUNT, DEFAULT_RING_BUFFER_SLOT_COUNT));\n+    long rollPeriodMs = conf.getLong(ROLL_PERIOD_MS_KEY, DEFAULT_ROLL_PERIOD_MS);\n+    params.rollPeriodMs(rollPeriodMs)\n+      .archivedWalSuffix(MasterProcedureUtil.ARCHIVED_PROC_WAL_SUFFIX)\n+      .hfileCleanerPlugins(HFILECLEANER_PLUGINS).cleanerPool(cleanerPool);\n+    region = LocalRegion.create(params);\n+  }\n+\n+  public void update(Put put) throws IOException {\n+    region.update(r -> r.put(put));\n+  }\n+\n+  public void delete(Delete delete) throws IOException {\n+    region.update(r -> r.delete(delete));\n+  }\n+\n+  public RegionScanner getScanner(Scan scan) throws IOException {\n+    return region.getScanner(scan);\n+  }\n+\n+  public void close(boolean abort) {\n+    LOG.info(\"Closing root table, isAbort={}\", abort);\n+    if (region != null) {\n+      region.close(abort);\n+    }\n+  }\n+}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIzMjA0NA=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NzA3ODYxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/region/LocalRegion.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODo1OToxMFrOGYZxAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNTo0NjoyOFrOGY4Qgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI0MTE1NA==", "bodyText": "We still need to do this snowflaking?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428241154", "createdAt": "2020-05-20T18:59:10Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/region/LocalRegion.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.region;\n+\n+import static org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseIOException;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionInfoBuilder;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.cleaner.HFileCleaner;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.regionserver.HRegion.FlushResult;\n+import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.CommonFSUtils;\n+import org.apache.hadoop.hbase.util.HFileArchiveUtil;\n+import org.apache.hadoop.hbase.util.RecoverLeaseFSUtils;\n+import org.apache.hadoop.hbase.wal.AbstractFSWALProvider;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALFactory;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n+import org.apache.hbase.thirdparty.com.google.common.math.IntMath;\n+\n+/**\n+ * A region that stores data in a separated directory on WAL file system.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMjY4NQ==", "bodyText": "Sorry, what does 'snowflaking' mean? ...", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428402685", "createdAt": "2020-05-21T01:53:51Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/region/LocalRegion.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.region;\n+\n+import static org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseIOException;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionInfoBuilder;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.cleaner.HFileCleaner;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.regionserver.HRegion.FlushResult;\n+import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.CommonFSUtils;\n+import org.apache.hadoop.hbase.util.HFileArchiveUtil;\n+import org.apache.hadoop.hbase.util.RecoverLeaseFSUtils;\n+import org.apache.hadoop.hbase.wal.AbstractFSWALProvider;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALFactory;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n+import org.apache.hbase.thirdparty.com.google.common.math.IntMath;\n+\n+/**\n+ * A region that stores data in a separated directory on WAL file system.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI0MTE1NA=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODc0MDczOQ==", "bodyText": "Pardon me. 'snowflakes' are purportedly unique; no two are alike. 'snowflaking' is making something 'special', a one-off. I was asking if we need to do the special trick where this local region runs on the WAL FS exclusively making it different to how other Regions do their storage spread across FS's... one for data and another for WAL.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r428740739", "createdAt": "2020-05-21T15:46:28Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/region/LocalRegion.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.region;\n+\n+import static org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseIOException;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionInfoBuilder;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.cleaner.HFileCleaner;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.regionserver.HRegion.FlushResult;\n+import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;\n+import org.apache.hadoop.hbase.regionserver.RegionScanner;\n+import org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.CommonFSUtils;\n+import org.apache.hadoop.hbase.util.HFileArchiveUtil;\n+import org.apache.hadoop.hbase.util.RecoverLeaseFSUtils;\n+import org.apache.hadoop.hbase.wal.AbstractFSWALProvider;\n+import org.apache.hadoop.hbase.wal.WAL;\n+import org.apache.hadoop.hbase.wal.WALFactory;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n+import org.apache.hbase.thirdparty.com.google.common.math.IntMath;\n+\n+/**\n+ * A region that stores data in a separated directory on WAL file system.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI0MTE1NA=="}, "originalCommit": {"oid": "6d996c1f4b67961894afb83c09310bba6956a061"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MDUyNzE1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMTozNzoyOFrOGaaS-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMjo0MzoxN1rOGacdNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0NzAwMg==", "bodyText": "Why don't delete in previous for-loop? Same with the delete for znode.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430347002", "createdAt": "2020-05-26T11:37:28Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "diffHunk": "@@ -43,73 +49,103 @@\n \n   private final HMaster master;\n \n-  public MasterMetaBootstrap(HMaster master) {\n+  private final LocalStore localStore;\n+\n+  public MasterMetaBootstrap(HMaster master, LocalStore localStore) {\n     this.master = master;\n+    this.localStore = localStore;\n   }\n \n   /**\n    * For assigning hbase:meta replicas only.\n-   * TODO: The way this assign runs, nothing but chance to stop all replicas showing up on same\n-   * server as the hbase:meta region.\n    */\n-  void assignMetaReplicas()\n-      throws IOException, InterruptedException, KeeperException {\n+  void assignMetaReplicas() throws IOException, InterruptedException, KeeperException {\n     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,\n-           HConstants.DEFAULT_META_REPLICA_NUM);\n-    if (numReplicas <= 1) {\n-      // No replicaas to assign. Return.\n-      return;\n-    }\n-    final AssignmentManager assignmentManager = master.getAssignmentManager();\n-    if (!assignmentManager.isMetaLoaded()) {\n-      throw new IllegalStateException(\"hbase:meta must be initialized first before we can \" +\n-          \"assign out its replicas\");\n-    }\n-    ServerName metaServername = MetaTableLocator.getMetaRegionLocation(this.master.getZooKeeper());\n-    for (int i = 1; i < numReplicas; i++) {\n-      // Get current meta state for replica from zk.\n-      RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper(), i);\n-      RegionInfo hri = RegionReplicaUtil.getRegionInfoForReplica(\n-          RegionInfoBuilder.FIRST_META_REGIONINFO, i);\n-      LOG.debug(hri.getRegionNameAsString() + \" replica region state from zookeeper=\" + metaState);\n-      if (metaServername.equals(metaState.getServerName())) {\n-        metaState = null;\n-        LOG.info(hri.getRegionNameAsString() +\n-          \" old location is same as current hbase:meta location; setting location as null...\");\n+      HConstants.DEFAULT_META_REPLICA_NUM);\n+    // only try to assign meta replicas when there are more than 1 replicas\n+    if (numReplicas > 1) {\n+      final AssignmentManager am = master.getAssignmentManager();\n+      if (!am.isMetaLoaded()) {\n+        throw new IllegalStateException(\n+          \"hbase:meta must be initialized first before we can \" + \"assign out its replicas\");\n       }\n-      // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n-      // down hosting server which calls AM#stop.\n-      if (metaState != null && metaState.getServerName() != null) {\n-        // Try to retain old assignment.\n-        assignmentManager.assign(hri, metaState.getServerName());\n-      } else {\n-        assignmentManager.assign(hri);\n+      RegionStates regionStates = am.getRegionStates();\n+      for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+        if (!RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+          continue;\n+        }\n+        RegionState regionState = regionStates.getRegionState(regionInfo);\n+        Set<ServerName> metaServerNames = new HashSet<ServerName>();\n+        if (regionState.getServerName() != null) {\n+          metaServerNames.add(regionState.getServerName());\n+        }\n+        for (int i = 1; i < numReplicas; i++) {\n+          RegionInfo secondaryRegionInfo = RegionReplicaUtil.getRegionInfoForReplica(regionInfo, i);\n+          RegionState secondaryRegionState = regionStates.getRegionState(secondaryRegionInfo);\n+          ServerName sn = null;\n+          if (secondaryRegionState != null) {\n+            sn = secondaryRegionState.getServerName();\n+            if (sn != null && !metaServerNames.add(sn)) {\n+              LOG.info(\"{} old location {} is same with other hbase:meta replica location;\" +\n+                \" setting location as null...\", secondaryRegionInfo.getRegionNameAsString(), sn);\n+              sn = null;\n+            }\n+          }\n+          // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n+          // down hosting server which calls AM#stop.\n+          if (sn != null) {\n+            am.assign(secondaryRegionInfo, sn);\n+          } else {\n+            am.assign(secondaryRegionInfo);\n+          }\n+        }\n       }\n     }\n+    // always try to remomve excess meta replicas\n     unassignExcessMetaReplica(numReplicas);\n   }\n \n   private void unassignExcessMetaReplica(int numMetaReplicasConfigured) {\n-    final ZKWatcher zooKeeper = master.getZooKeeper();\n-    // unassign the unneeded replicas (for e.g., if the previous master was configured\n-    // with a replication of 3 and now it is 2, we need to unassign the 1 unneeded replica)\n-    try {\n-      List<String> metaReplicaZnodes = zooKeeper.getMetaReplicaNodes();\n-      for (String metaReplicaZnode : metaReplicaZnodes) {\n-        int replicaId = zooKeeper.getZNodePaths().getMetaReplicaIdFromZnode(metaReplicaZnode);\n-        if (replicaId >= numMetaReplicasConfigured) {\n-          RegionState r = MetaTableLocator.getMetaRegionState(zooKeeper, replicaId);\n-          LOG.info(\"Closing excess replica of meta region \" + r.getRegion());\n-          // send a close and wait for a max of 30 seconds\n-          ServerManager.closeRegionSilentlyAndWait(master.getAsyncClusterConnection(),\n-              r.getServerName(), r.getRegion(), 30000);\n-          ZKUtil.deleteNode(zooKeeper, zooKeeper.getZNodePaths().getZNodeForReplica(replicaId));\n+    ZKWatcher zooKeeper = master.getZooKeeper();\n+    AssignmentManager am = master.getAssignmentManager();\n+    RegionStates regionStates = am.getRegionStates();\n+    Map<RegionInfo, Integer> region2MaxReplicaId = new HashMap<>();\n+    for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+      RegionInfo primaryRegionInfo = RegionReplicaUtil.getRegionInfoForDefaultReplica(regionInfo);\n+      region2MaxReplicaId.compute(primaryRegionInfo,\n+        (k, v) -> v == null ? regionInfo.getReplicaId() : Math.max(v, regionInfo.getReplicaId()));\n+      if (regionInfo.getReplicaId() < numMetaReplicasConfigured) {\n+        continue;\n+      }\n+      RegionState regionState = regionStates.getRegionState(regionInfo);\n+      try {\n+        ServerManager.closeRegionSilentlyAndWait(master.getAsyncClusterConnection(),\n+          regionState.getServerName(), regionInfo, 30000);\n+        if (regionInfo.isFirst()) {\n+          // for compatibility, also try to remove the replicas on zk.\n+          ZKUtil.deleteNode(zooKeeper,\n+            zooKeeper.getZNodePaths().getZNodeForReplica(regionInfo.getReplicaId()));\n         }\n+      } catch (Exception e) {\n+        // ignore the exception since we don't want the master to be wedged due to potential\n+        // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n+        LOG.warn(\"Ignoring exception \" + e);\n       }\n-    } catch (Exception ex) {\n-      // ignore the exception since we don't want the master to be wedged due to potential\n-      // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n-      LOG.warn(\"Ignoring exception \" + ex);\n+      regionStates.deleteRegion(regionInfo);\n     }\n+    region2MaxReplicaId.forEach((regionInfo, maxReplicaId) -> {\n+      if (maxReplicaId >= numMetaReplicasConfigured) {\n+        byte[] metaRow = MetaTableAccessor.getMetaKeyForRegion(regionInfo);\n+        Delete delete = MetaTableAccessor.removeRegionReplica(metaRow, numMetaReplicasConfigured,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM4MDk2Mw==", "bodyText": "If we could have multiple meta regions, maybe we will have different meta replicas for different regions, think of we fail in the middle of this process. So use another loop for safety.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430380963", "createdAt": "2020-05-26T12:40:42Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "diffHunk": "@@ -43,73 +49,103 @@\n \n   private final HMaster master;\n \n-  public MasterMetaBootstrap(HMaster master) {\n+  private final LocalStore localStore;\n+\n+  public MasterMetaBootstrap(HMaster master, LocalStore localStore) {\n     this.master = master;\n+    this.localStore = localStore;\n   }\n \n   /**\n    * For assigning hbase:meta replicas only.\n-   * TODO: The way this assign runs, nothing but chance to stop all replicas showing up on same\n-   * server as the hbase:meta region.\n    */\n-  void assignMetaReplicas()\n-      throws IOException, InterruptedException, KeeperException {\n+  void assignMetaReplicas() throws IOException, InterruptedException, KeeperException {\n     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,\n-           HConstants.DEFAULT_META_REPLICA_NUM);\n-    if (numReplicas <= 1) {\n-      // No replicaas to assign. Return.\n-      return;\n-    }\n-    final AssignmentManager assignmentManager = master.getAssignmentManager();\n-    if (!assignmentManager.isMetaLoaded()) {\n-      throw new IllegalStateException(\"hbase:meta must be initialized first before we can \" +\n-          \"assign out its replicas\");\n-    }\n-    ServerName metaServername = MetaTableLocator.getMetaRegionLocation(this.master.getZooKeeper());\n-    for (int i = 1; i < numReplicas; i++) {\n-      // Get current meta state for replica from zk.\n-      RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper(), i);\n-      RegionInfo hri = RegionReplicaUtil.getRegionInfoForReplica(\n-          RegionInfoBuilder.FIRST_META_REGIONINFO, i);\n-      LOG.debug(hri.getRegionNameAsString() + \" replica region state from zookeeper=\" + metaState);\n-      if (metaServername.equals(metaState.getServerName())) {\n-        metaState = null;\n-        LOG.info(hri.getRegionNameAsString() +\n-          \" old location is same as current hbase:meta location; setting location as null...\");\n+      HConstants.DEFAULT_META_REPLICA_NUM);\n+    // only try to assign meta replicas when there are more than 1 replicas\n+    if (numReplicas > 1) {\n+      final AssignmentManager am = master.getAssignmentManager();\n+      if (!am.isMetaLoaded()) {\n+        throw new IllegalStateException(\n+          \"hbase:meta must be initialized first before we can \" + \"assign out its replicas\");\n       }\n-      // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n-      // down hosting server which calls AM#stop.\n-      if (metaState != null && metaState.getServerName() != null) {\n-        // Try to retain old assignment.\n-        assignmentManager.assign(hri, metaState.getServerName());\n-      } else {\n-        assignmentManager.assign(hri);\n+      RegionStates regionStates = am.getRegionStates();\n+      for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+        if (!RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+          continue;\n+        }\n+        RegionState regionState = regionStates.getRegionState(regionInfo);\n+        Set<ServerName> metaServerNames = new HashSet<ServerName>();\n+        if (regionState.getServerName() != null) {\n+          metaServerNames.add(regionState.getServerName());\n+        }\n+        for (int i = 1; i < numReplicas; i++) {\n+          RegionInfo secondaryRegionInfo = RegionReplicaUtil.getRegionInfoForReplica(regionInfo, i);\n+          RegionState secondaryRegionState = regionStates.getRegionState(secondaryRegionInfo);\n+          ServerName sn = null;\n+          if (secondaryRegionState != null) {\n+            sn = secondaryRegionState.getServerName();\n+            if (sn != null && !metaServerNames.add(sn)) {\n+              LOG.info(\"{} old location {} is same with other hbase:meta replica location;\" +\n+                \" setting location as null...\", secondaryRegionInfo.getRegionNameAsString(), sn);\n+              sn = null;\n+            }\n+          }\n+          // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n+          // down hosting server which calls AM#stop.\n+          if (sn != null) {\n+            am.assign(secondaryRegionInfo, sn);\n+          } else {\n+            am.assign(secondaryRegionInfo);\n+          }\n+        }\n       }\n     }\n+    // always try to remomve excess meta replicas\n     unassignExcessMetaReplica(numReplicas);\n   }\n \n   private void unassignExcessMetaReplica(int numMetaReplicasConfigured) {\n-    final ZKWatcher zooKeeper = master.getZooKeeper();\n-    // unassign the unneeded replicas (for e.g., if the previous master was configured\n-    // with a replication of 3 and now it is 2, we need to unassign the 1 unneeded replica)\n-    try {\n-      List<String> metaReplicaZnodes = zooKeeper.getMetaReplicaNodes();\n-      for (String metaReplicaZnode : metaReplicaZnodes) {\n-        int replicaId = zooKeeper.getZNodePaths().getMetaReplicaIdFromZnode(metaReplicaZnode);\n-        if (replicaId >= numMetaReplicasConfigured) {\n-          RegionState r = MetaTableLocator.getMetaRegionState(zooKeeper, replicaId);\n-          LOG.info(\"Closing excess replica of meta region \" + r.getRegion());\n-          // send a close and wait for a max of 30 seconds\n-          ServerManager.closeRegionSilentlyAndWait(master.getAsyncClusterConnection(),\n-              r.getServerName(), r.getRegion(), 30000);\n-          ZKUtil.deleteNode(zooKeeper, zooKeeper.getZNodePaths().getZNodeForReplica(replicaId));\n+    ZKWatcher zooKeeper = master.getZooKeeper();\n+    AssignmentManager am = master.getAssignmentManager();\n+    RegionStates regionStates = am.getRegionStates();\n+    Map<RegionInfo, Integer> region2MaxReplicaId = new HashMap<>();\n+    for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+      RegionInfo primaryRegionInfo = RegionReplicaUtil.getRegionInfoForDefaultReplica(regionInfo);\n+      region2MaxReplicaId.compute(primaryRegionInfo,\n+        (k, v) -> v == null ? regionInfo.getReplicaId() : Math.max(v, regionInfo.getReplicaId()));\n+      if (regionInfo.getReplicaId() < numMetaReplicasConfigured) {\n+        continue;\n+      }\n+      RegionState regionState = regionStates.getRegionState(regionInfo);\n+      try {\n+        ServerManager.closeRegionSilentlyAndWait(master.getAsyncClusterConnection(),\n+          regionState.getServerName(), regionInfo, 30000);\n+        if (regionInfo.isFirst()) {\n+          // for compatibility, also try to remove the replicas on zk.\n+          ZKUtil.deleteNode(zooKeeper,\n+            zooKeeper.getZNodePaths().getZNodeForReplica(regionInfo.getReplicaId()));\n         }\n+      } catch (Exception e) {\n+        // ignore the exception since we don't want the master to be wedged due to potential\n+        // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n+        LOG.warn(\"Ignoring exception \" + e);\n       }\n-    } catch (Exception ex) {\n-      // ignore the exception since we don't want the master to be wedged due to potential\n-      // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n-      LOG.warn(\"Ignoring exception \" + ex);\n+      regionStates.deleteRegion(regionInfo);\n     }\n+    region2MaxReplicaId.forEach((regionInfo, maxReplicaId) -> {\n+      if (maxReplicaId >= numMetaReplicasConfigured) {\n+        byte[] metaRow = MetaTableAccessor.getMetaKeyForRegion(regionInfo);\n+        Delete delete = MetaTableAccessor.removeRegionReplica(metaRow, numMetaReplicasConfigured,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0NzAwMg=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM4MjM5MA==", "bodyText": "Oh, I think the problem is that, only when we finish scanning the replicas for a region, we can know how many replicas we can remove, then we create a single Delete which deletes all the replicas. And for zk, different replicas will have different znode, so just delete it everytime...", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430382390", "createdAt": "2020-05-26T12:43:17Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "diffHunk": "@@ -43,73 +49,103 @@\n \n   private final HMaster master;\n \n-  public MasterMetaBootstrap(HMaster master) {\n+  private final LocalStore localStore;\n+\n+  public MasterMetaBootstrap(HMaster master, LocalStore localStore) {\n     this.master = master;\n+    this.localStore = localStore;\n   }\n \n   /**\n    * For assigning hbase:meta replicas only.\n-   * TODO: The way this assign runs, nothing but chance to stop all replicas showing up on same\n-   * server as the hbase:meta region.\n    */\n-  void assignMetaReplicas()\n-      throws IOException, InterruptedException, KeeperException {\n+  void assignMetaReplicas() throws IOException, InterruptedException, KeeperException {\n     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,\n-           HConstants.DEFAULT_META_REPLICA_NUM);\n-    if (numReplicas <= 1) {\n-      // No replicaas to assign. Return.\n-      return;\n-    }\n-    final AssignmentManager assignmentManager = master.getAssignmentManager();\n-    if (!assignmentManager.isMetaLoaded()) {\n-      throw new IllegalStateException(\"hbase:meta must be initialized first before we can \" +\n-          \"assign out its replicas\");\n-    }\n-    ServerName metaServername = MetaTableLocator.getMetaRegionLocation(this.master.getZooKeeper());\n-    for (int i = 1; i < numReplicas; i++) {\n-      // Get current meta state for replica from zk.\n-      RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper(), i);\n-      RegionInfo hri = RegionReplicaUtil.getRegionInfoForReplica(\n-          RegionInfoBuilder.FIRST_META_REGIONINFO, i);\n-      LOG.debug(hri.getRegionNameAsString() + \" replica region state from zookeeper=\" + metaState);\n-      if (metaServername.equals(metaState.getServerName())) {\n-        metaState = null;\n-        LOG.info(hri.getRegionNameAsString() +\n-          \" old location is same as current hbase:meta location; setting location as null...\");\n+      HConstants.DEFAULT_META_REPLICA_NUM);\n+    // only try to assign meta replicas when there are more than 1 replicas\n+    if (numReplicas > 1) {\n+      final AssignmentManager am = master.getAssignmentManager();\n+      if (!am.isMetaLoaded()) {\n+        throw new IllegalStateException(\n+          \"hbase:meta must be initialized first before we can \" + \"assign out its replicas\");\n       }\n-      // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n-      // down hosting server which calls AM#stop.\n-      if (metaState != null && metaState.getServerName() != null) {\n-        // Try to retain old assignment.\n-        assignmentManager.assign(hri, metaState.getServerName());\n-      } else {\n-        assignmentManager.assign(hri);\n+      RegionStates regionStates = am.getRegionStates();\n+      for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+        if (!RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+          continue;\n+        }\n+        RegionState regionState = regionStates.getRegionState(regionInfo);\n+        Set<ServerName> metaServerNames = new HashSet<ServerName>();\n+        if (regionState.getServerName() != null) {\n+          metaServerNames.add(regionState.getServerName());\n+        }\n+        for (int i = 1; i < numReplicas; i++) {\n+          RegionInfo secondaryRegionInfo = RegionReplicaUtil.getRegionInfoForReplica(regionInfo, i);\n+          RegionState secondaryRegionState = regionStates.getRegionState(secondaryRegionInfo);\n+          ServerName sn = null;\n+          if (secondaryRegionState != null) {\n+            sn = secondaryRegionState.getServerName();\n+            if (sn != null && !metaServerNames.add(sn)) {\n+              LOG.info(\"{} old location {} is same with other hbase:meta replica location;\" +\n+                \" setting location as null...\", secondaryRegionInfo.getRegionNameAsString(), sn);\n+              sn = null;\n+            }\n+          }\n+          // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n+          // down hosting server which calls AM#stop.\n+          if (sn != null) {\n+            am.assign(secondaryRegionInfo, sn);\n+          } else {\n+            am.assign(secondaryRegionInfo);\n+          }\n+        }\n       }\n     }\n+    // always try to remomve excess meta replicas\n     unassignExcessMetaReplica(numReplicas);\n   }\n \n   private void unassignExcessMetaReplica(int numMetaReplicasConfigured) {\n-    final ZKWatcher zooKeeper = master.getZooKeeper();\n-    // unassign the unneeded replicas (for e.g., if the previous master was configured\n-    // with a replication of 3 and now it is 2, we need to unassign the 1 unneeded replica)\n-    try {\n-      List<String> metaReplicaZnodes = zooKeeper.getMetaReplicaNodes();\n-      for (String metaReplicaZnode : metaReplicaZnodes) {\n-        int replicaId = zooKeeper.getZNodePaths().getMetaReplicaIdFromZnode(metaReplicaZnode);\n-        if (replicaId >= numMetaReplicasConfigured) {\n-          RegionState r = MetaTableLocator.getMetaRegionState(zooKeeper, replicaId);\n-          LOG.info(\"Closing excess replica of meta region \" + r.getRegion());\n-          // send a close and wait for a max of 30 seconds\n-          ServerManager.closeRegionSilentlyAndWait(master.getAsyncClusterConnection(),\n-              r.getServerName(), r.getRegion(), 30000);\n-          ZKUtil.deleteNode(zooKeeper, zooKeeper.getZNodePaths().getZNodeForReplica(replicaId));\n+    ZKWatcher zooKeeper = master.getZooKeeper();\n+    AssignmentManager am = master.getAssignmentManager();\n+    RegionStates regionStates = am.getRegionStates();\n+    Map<RegionInfo, Integer> region2MaxReplicaId = new HashMap<>();\n+    for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+      RegionInfo primaryRegionInfo = RegionReplicaUtil.getRegionInfoForDefaultReplica(regionInfo);\n+      region2MaxReplicaId.compute(primaryRegionInfo,\n+        (k, v) -> v == null ? regionInfo.getReplicaId() : Math.max(v, regionInfo.getReplicaId()));\n+      if (regionInfo.getReplicaId() < numMetaReplicasConfigured) {\n+        continue;\n+      }\n+      RegionState regionState = regionStates.getRegionState(regionInfo);\n+      try {\n+        ServerManager.closeRegionSilentlyAndWait(master.getAsyncClusterConnection(),\n+          regionState.getServerName(), regionInfo, 30000);\n+        if (regionInfo.isFirst()) {\n+          // for compatibility, also try to remove the replicas on zk.\n+          ZKUtil.deleteNode(zooKeeper,\n+            zooKeeper.getZNodePaths().getZNodeForReplica(regionInfo.getReplicaId()));\n         }\n+      } catch (Exception e) {\n+        // ignore the exception since we don't want the master to be wedged due to potential\n+        // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n+        LOG.warn(\"Ignoring exception \" + e);\n       }\n-    } catch (Exception ex) {\n-      // ignore the exception since we don't want the master to be wedged due to potential\n-      // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n-      LOG.warn(\"Ignoring exception \" + ex);\n+      regionStates.deleteRegion(regionInfo);\n     }\n+    region2MaxReplicaId.forEach((regionInfo, maxReplicaId) -> {\n+      if (maxReplicaId >= numMetaReplicasConfigured) {\n+        byte[] metaRow = MetaTableAccessor.getMetaKeyForRegion(regionInfo);\n+        Delete delete = MetaTableAccessor.removeRegionReplica(metaRow, numMetaReplicasConfigured,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0NzAwMg=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MDUzMDUzOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMTozODo0MVrOGaaVJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMTo0NDoyMVrOGa2rjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0NzU1OA==", "bodyText": "Donot need unassign first?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430347558", "createdAt": "2020-05-26T11:38:41Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "diffHunk": "@@ -43,73 +49,103 @@\n \n   private final HMaster master;\n \n-  public MasterMetaBootstrap(HMaster master) {\n+  private final LocalStore localStore;\n+\n+  public MasterMetaBootstrap(HMaster master, LocalStore localStore) {\n     this.master = master;\n+    this.localStore = localStore;\n   }\n \n   /**\n    * For assigning hbase:meta replicas only.\n-   * TODO: The way this assign runs, nothing but chance to stop all replicas showing up on same\n-   * server as the hbase:meta region.\n    */\n-  void assignMetaReplicas()\n-      throws IOException, InterruptedException, KeeperException {\n+  void assignMetaReplicas() throws IOException, InterruptedException, KeeperException {\n     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,\n-           HConstants.DEFAULT_META_REPLICA_NUM);\n-    if (numReplicas <= 1) {\n-      // No replicaas to assign. Return.\n-      return;\n-    }\n-    final AssignmentManager assignmentManager = master.getAssignmentManager();\n-    if (!assignmentManager.isMetaLoaded()) {\n-      throw new IllegalStateException(\"hbase:meta must be initialized first before we can \" +\n-          \"assign out its replicas\");\n-    }\n-    ServerName metaServername = MetaTableLocator.getMetaRegionLocation(this.master.getZooKeeper());\n-    for (int i = 1; i < numReplicas; i++) {\n-      // Get current meta state for replica from zk.\n-      RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper(), i);\n-      RegionInfo hri = RegionReplicaUtil.getRegionInfoForReplica(\n-          RegionInfoBuilder.FIRST_META_REGIONINFO, i);\n-      LOG.debug(hri.getRegionNameAsString() + \" replica region state from zookeeper=\" + metaState);\n-      if (metaServername.equals(metaState.getServerName())) {\n-        metaState = null;\n-        LOG.info(hri.getRegionNameAsString() +\n-          \" old location is same as current hbase:meta location; setting location as null...\");\n+      HConstants.DEFAULT_META_REPLICA_NUM);\n+    // only try to assign meta replicas when there are more than 1 replicas\n+    if (numReplicas > 1) {\n+      final AssignmentManager am = master.getAssignmentManager();\n+      if (!am.isMetaLoaded()) {\n+        throw new IllegalStateException(\n+          \"hbase:meta must be initialized first before we can \" + \"assign out its replicas\");\n       }\n-      // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n-      // down hosting server which calls AM#stop.\n-      if (metaState != null && metaState.getServerName() != null) {\n-        // Try to retain old assignment.\n-        assignmentManager.assign(hri, metaState.getServerName());\n-      } else {\n-        assignmentManager.assign(hri);\n+      RegionStates regionStates = am.getRegionStates();\n+      for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+        if (!RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+          continue;\n+        }\n+        RegionState regionState = regionStates.getRegionState(regionInfo);\n+        Set<ServerName> metaServerNames = new HashSet<ServerName>();\n+        if (regionState.getServerName() != null) {\n+          metaServerNames.add(regionState.getServerName());\n+        }\n+        for (int i = 1; i < numReplicas; i++) {\n+          RegionInfo secondaryRegionInfo = RegionReplicaUtil.getRegionInfoForReplica(regionInfo, i);\n+          RegionState secondaryRegionState = regionStates.getRegionState(secondaryRegionInfo);\n+          ServerName sn = null;\n+          if (secondaryRegionState != null) {\n+            sn = secondaryRegionState.getServerName();\n+            if (sn != null && !metaServerNames.add(sn)) {\n+              LOG.info(\"{} old location {} is same with other hbase:meta replica location;\" +\n+                \" setting location as null...\", secondaryRegionInfo.getRegionNameAsString(), sn);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM4MDQ3Ng==", "bodyText": "I think the logic here is that, if we have a location, then the assign will not have effect as we assign it to the same region server. If not, it means the region is not online, then we can just assign it to a new regionserver.\nAnyway, I agree with you that the above logic is a bit flaky, but this is old behavior. So maybe we should fix it in another issue, not only on the feature branch?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430380476", "createdAt": "2020-05-26T12:39:54Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "diffHunk": "@@ -43,73 +49,103 @@\n \n   private final HMaster master;\n \n-  public MasterMetaBootstrap(HMaster master) {\n+  private final LocalStore localStore;\n+\n+  public MasterMetaBootstrap(HMaster master, LocalStore localStore) {\n     this.master = master;\n+    this.localStore = localStore;\n   }\n \n   /**\n    * For assigning hbase:meta replicas only.\n-   * TODO: The way this assign runs, nothing but chance to stop all replicas showing up on same\n-   * server as the hbase:meta region.\n    */\n-  void assignMetaReplicas()\n-      throws IOException, InterruptedException, KeeperException {\n+  void assignMetaReplicas() throws IOException, InterruptedException, KeeperException {\n     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,\n-           HConstants.DEFAULT_META_REPLICA_NUM);\n-    if (numReplicas <= 1) {\n-      // No replicaas to assign. Return.\n-      return;\n-    }\n-    final AssignmentManager assignmentManager = master.getAssignmentManager();\n-    if (!assignmentManager.isMetaLoaded()) {\n-      throw new IllegalStateException(\"hbase:meta must be initialized first before we can \" +\n-          \"assign out its replicas\");\n-    }\n-    ServerName metaServername = MetaTableLocator.getMetaRegionLocation(this.master.getZooKeeper());\n-    for (int i = 1; i < numReplicas; i++) {\n-      // Get current meta state for replica from zk.\n-      RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper(), i);\n-      RegionInfo hri = RegionReplicaUtil.getRegionInfoForReplica(\n-          RegionInfoBuilder.FIRST_META_REGIONINFO, i);\n-      LOG.debug(hri.getRegionNameAsString() + \" replica region state from zookeeper=\" + metaState);\n-      if (metaServername.equals(metaState.getServerName())) {\n-        metaState = null;\n-        LOG.info(hri.getRegionNameAsString() +\n-          \" old location is same as current hbase:meta location; setting location as null...\");\n+      HConstants.DEFAULT_META_REPLICA_NUM);\n+    // only try to assign meta replicas when there are more than 1 replicas\n+    if (numReplicas > 1) {\n+      final AssignmentManager am = master.getAssignmentManager();\n+      if (!am.isMetaLoaded()) {\n+        throw new IllegalStateException(\n+          \"hbase:meta must be initialized first before we can \" + \"assign out its replicas\");\n       }\n-      // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n-      // down hosting server which calls AM#stop.\n-      if (metaState != null && metaState.getServerName() != null) {\n-        // Try to retain old assignment.\n-        assignmentManager.assign(hri, metaState.getServerName());\n-      } else {\n-        assignmentManager.assign(hri);\n+      RegionStates regionStates = am.getRegionStates();\n+      for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+        if (!RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+          continue;\n+        }\n+        RegionState regionState = regionStates.getRegionState(regionInfo);\n+        Set<ServerName> metaServerNames = new HashSet<ServerName>();\n+        if (regionState.getServerName() != null) {\n+          metaServerNames.add(regionState.getServerName());\n+        }\n+        for (int i = 1; i < numReplicas; i++) {\n+          RegionInfo secondaryRegionInfo = RegionReplicaUtil.getRegionInfoForReplica(regionInfo, i);\n+          RegionState secondaryRegionState = regionStates.getRegionState(secondaryRegionInfo);\n+          ServerName sn = null;\n+          if (secondaryRegionState != null) {\n+            sn = secondaryRegionState.getServerName();\n+            if (sn != null && !metaServerNames.add(sn)) {\n+              LOG.info(\"{} old location {} is same with other hbase:meta replica location;\" +\n+                \" setting location as null...\", secondaryRegionInfo.getRegionNameAsString(), sn);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0NzU1OA=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgxMjA0NA==", "bodyText": "Yes. Maybe another issuer.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430812044", "createdAt": "2020-05-27T01:44:21Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMetaBootstrap.java", "diffHunk": "@@ -43,73 +49,103 @@\n \n   private final HMaster master;\n \n-  public MasterMetaBootstrap(HMaster master) {\n+  private final LocalStore localStore;\n+\n+  public MasterMetaBootstrap(HMaster master, LocalStore localStore) {\n     this.master = master;\n+    this.localStore = localStore;\n   }\n \n   /**\n    * For assigning hbase:meta replicas only.\n-   * TODO: The way this assign runs, nothing but chance to stop all replicas showing up on same\n-   * server as the hbase:meta region.\n    */\n-  void assignMetaReplicas()\n-      throws IOException, InterruptedException, KeeperException {\n+  void assignMetaReplicas() throws IOException, InterruptedException, KeeperException {\n     int numReplicas = master.getConfiguration().getInt(HConstants.META_REPLICAS_NUM,\n-           HConstants.DEFAULT_META_REPLICA_NUM);\n-    if (numReplicas <= 1) {\n-      // No replicaas to assign. Return.\n-      return;\n-    }\n-    final AssignmentManager assignmentManager = master.getAssignmentManager();\n-    if (!assignmentManager.isMetaLoaded()) {\n-      throw new IllegalStateException(\"hbase:meta must be initialized first before we can \" +\n-          \"assign out its replicas\");\n-    }\n-    ServerName metaServername = MetaTableLocator.getMetaRegionLocation(this.master.getZooKeeper());\n-    for (int i = 1; i < numReplicas; i++) {\n-      // Get current meta state for replica from zk.\n-      RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper(), i);\n-      RegionInfo hri = RegionReplicaUtil.getRegionInfoForReplica(\n-          RegionInfoBuilder.FIRST_META_REGIONINFO, i);\n-      LOG.debug(hri.getRegionNameAsString() + \" replica region state from zookeeper=\" + metaState);\n-      if (metaServername.equals(metaState.getServerName())) {\n-        metaState = null;\n-        LOG.info(hri.getRegionNameAsString() +\n-          \" old location is same as current hbase:meta location; setting location as null...\");\n+      HConstants.DEFAULT_META_REPLICA_NUM);\n+    // only try to assign meta replicas when there are more than 1 replicas\n+    if (numReplicas > 1) {\n+      final AssignmentManager am = master.getAssignmentManager();\n+      if (!am.isMetaLoaded()) {\n+        throw new IllegalStateException(\n+          \"hbase:meta must be initialized first before we can \" + \"assign out its replicas\");\n       }\n-      // These assigns run inline. All is blocked till they complete. Only interrupt is shutting\n-      // down hosting server which calls AM#stop.\n-      if (metaState != null && metaState.getServerName() != null) {\n-        // Try to retain old assignment.\n-        assignmentManager.assign(hri, metaState.getServerName());\n-      } else {\n-        assignmentManager.assign(hri);\n+      RegionStates regionStates = am.getRegionStates();\n+      for (RegionInfo regionInfo : regionStates.getRegionsOfTable(TableName.META_TABLE_NAME)) {\n+        if (!RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+          continue;\n+        }\n+        RegionState regionState = regionStates.getRegionState(regionInfo);\n+        Set<ServerName> metaServerNames = new HashSet<ServerName>();\n+        if (regionState.getServerName() != null) {\n+          metaServerNames.add(regionState.getServerName());\n+        }\n+        for (int i = 1; i < numReplicas; i++) {\n+          RegionInfo secondaryRegionInfo = RegionReplicaUtil.getRegionInfoForReplica(regionInfo, i);\n+          RegionState secondaryRegionState = regionStates.getRegionState(secondaryRegionInfo);\n+          ServerName sn = null;\n+          if (secondaryRegionState != null) {\n+            sn = secondaryRegionState.getServerName();\n+            if (sn != null && !metaServerNames.add(sn)) {\n+              LOG.info(\"{} old location {} is same with other hbase:meta replica location;\" +\n+                \" setting location as null...\", secondaryRegionInfo.getRegionNameAsString(), sn);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0NzU1OA=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MDU0MTA3OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMTo0MTo1MlrOGaabjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMjoyMDo1NFrOGabrfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0OTE5Ng==", "bodyText": "Seems the old code didn't setLastHost and setOpenSeqNum? Why add this?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430349196", "createdAt": "2020-05-26T11:41:52Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "diffHunk": "@@ -225,23 +230,52 @@ public void start() throws IOException, KeeperException {\n     // Start the Assignment Thread\n     startAssignmentThread();\n \n-    // load meta region state\n-    ZKWatcher zkw = master.getZooKeeper();\n-    // it could be null in some tests\n-    if (zkw != null) {\n-      RegionState regionState = MetaTableLocator.getMetaRegionState(zkw);\n-      RegionStateNode regionNode =\n-        regionStates.getOrCreateRegionStateNode(RegionInfoBuilder.FIRST_META_REGIONINFO);\n-      regionNode.lock();\n-      try {\n-        regionNode.setRegionLocation(regionState.getServerName());\n-        regionNode.setState(regionState.getState());\n-        if (regionNode.getProcedure() != null) {\n-          regionNode.getProcedure().stateLoaded(this, regionNode);\n+    // load meta region states.\n+    // notice that, here we will load all replicas, and in MasterMetaBootstrap we may assign new\n+    // replicas, or remove excess replicas.\n+    try (RegionScanner scanner =\n+      localStore.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows;\n+      do {\n+        moreRows = scanner.next(cells);\n+        if (cells.isEmpty()) {\n+          continue;\n         }\n-        setMetaAssigned(regionState.getRegion(), regionState.getState() == State.OPEN);\n-      } finally {\n-        regionNode.unlock();\n+        Result result = Result.create(cells);\n+        cells.clear();\n+        RegionStateStore\n+          .visitMetaEntry((r, regionInfo, state, regionLocation, lastHost, openSeqNum) -> {\n+            RegionStateNode regionNode = regionStates.getOrCreateRegionStateNode(regionInfo);\n+            regionNode.lock();\n+            try {\n+              regionNode.setState(state);\n+              regionNode.setLastHost(lastHost);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM2OTY2Mg==", "bodyText": "It's just because we do not have these fields in the protobuf message which is stored on zookeeper...", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430369662", "createdAt": "2020-05-26T12:20:54Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "diffHunk": "@@ -225,23 +230,52 @@ public void start() throws IOException, KeeperException {\n     // Start the Assignment Thread\n     startAssignmentThread();\n \n-    // load meta region state\n-    ZKWatcher zkw = master.getZooKeeper();\n-    // it could be null in some tests\n-    if (zkw != null) {\n-      RegionState regionState = MetaTableLocator.getMetaRegionState(zkw);\n-      RegionStateNode regionNode =\n-        regionStates.getOrCreateRegionStateNode(RegionInfoBuilder.FIRST_META_REGIONINFO);\n-      regionNode.lock();\n-      try {\n-        regionNode.setRegionLocation(regionState.getServerName());\n-        regionNode.setState(regionState.getState());\n-        if (regionNode.getProcedure() != null) {\n-          regionNode.getProcedure().stateLoaded(this, regionNode);\n+    // load meta region states.\n+    // notice that, here we will load all replicas, and in MasterMetaBootstrap we may assign new\n+    // replicas, or remove excess replicas.\n+    try (RegionScanner scanner =\n+      localStore.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows;\n+      do {\n+        moreRows = scanner.next(cells);\n+        if (cells.isEmpty()) {\n+          continue;\n         }\n-        setMetaAssigned(regionState.getRegion(), regionState.getState() == State.OPEN);\n-      } finally {\n-        regionNode.unlock();\n+        Result result = Result.create(cells);\n+        cells.clear();\n+        RegionStateStore\n+          .visitMetaEntry((r, regionInfo, state, regionLocation, lastHost, openSeqNum) -> {\n+            RegionStateNode regionNode = regionStates.getOrCreateRegionStateNode(regionInfo);\n+            regionNode.lock();\n+            try {\n+              regionNode.setState(state);\n+              regionNode.setLastHost(lastHost);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM0OTE5Ng=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MDU0NzU1OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMTo0Mzo1OFrOGaafjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMTo0NDozM1rOGa2rvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM1MDIyMg==", "bodyText": "This compatibility is for which case?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430350222", "createdAt": "2020-05-26T11:43:58Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "diffHunk": "@@ -225,23 +230,52 @@ public void start() throws IOException, KeeperException {\n     // Start the Assignment Thread\n     startAssignmentThread();\n \n-    // load meta region state\n-    ZKWatcher zkw = master.getZooKeeper();\n-    // it could be null in some tests\n-    if (zkw != null) {\n-      RegionState regionState = MetaTableLocator.getMetaRegionState(zkw);\n-      RegionStateNode regionNode =\n-        regionStates.getOrCreateRegionStateNode(RegionInfoBuilder.FIRST_META_REGIONINFO);\n-      regionNode.lock();\n-      try {\n-        regionNode.setRegionLocation(regionState.getServerName());\n-        regionNode.setState(regionState.getState());\n-        if (regionNode.getProcedure() != null) {\n-          regionNode.getProcedure().stateLoaded(this, regionNode);\n+    // load meta region states.\n+    // notice that, here we will load all replicas, and in MasterMetaBootstrap we may assign new\n+    // replicas, or remove excess replicas.\n+    try (RegionScanner scanner =\n+      localStore.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows;\n+      do {\n+        moreRows = scanner.next(cells);\n+        if (cells.isEmpty()) {\n+          continue;\n         }\n-        setMetaAssigned(regionState.getRegion(), regionState.getState() == State.OPEN);\n-      } finally {\n-        regionNode.unlock();\n+        Result result = Result.create(cells);\n+        cells.clear();\n+        RegionStateStore\n+          .visitMetaEntry((r, regionInfo, state, regionLocation, lastHost, openSeqNum) -> {\n+            RegionStateNode regionNode = regionStates.getOrCreateRegionStateNode(regionInfo);\n+            regionNode.lock();\n+            try {\n+              regionNode.setState(state);\n+              regionNode.setLastHost(lastHost);\n+              regionNode.setRegionLocation(regionLocation);\n+              regionNode.setOpenSeqNum(openSeqNum);\n+              if (regionNode.getProcedure() != null) {\n+                regionNode.getProcedure().stateLoaded(this, regionNode);\n+              }\n+              if (RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+                setMetaAssigned(regionInfo, state == State.OPEN);\n+              }\n+            } finally {\n+              regionNode.unlock();\n+            }\n+            if (regionInfo.isFirst()) {\n+              // for compatibility, mirror the meta region state to zookeeper", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM2OTA4OA==", "bodyText": "For communication with old clients, they will load the meta location from zookeeper.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430369088", "createdAt": "2020-05-26T12:19:52Z", "author": {"login": "Apache9"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "diffHunk": "@@ -225,23 +230,52 @@ public void start() throws IOException, KeeperException {\n     // Start the Assignment Thread\n     startAssignmentThread();\n \n-    // load meta region state\n-    ZKWatcher zkw = master.getZooKeeper();\n-    // it could be null in some tests\n-    if (zkw != null) {\n-      RegionState regionState = MetaTableLocator.getMetaRegionState(zkw);\n-      RegionStateNode regionNode =\n-        regionStates.getOrCreateRegionStateNode(RegionInfoBuilder.FIRST_META_REGIONINFO);\n-      regionNode.lock();\n-      try {\n-        regionNode.setRegionLocation(regionState.getServerName());\n-        regionNode.setState(regionState.getState());\n-        if (regionNode.getProcedure() != null) {\n-          regionNode.getProcedure().stateLoaded(this, regionNode);\n+    // load meta region states.\n+    // notice that, here we will load all replicas, and in MasterMetaBootstrap we may assign new\n+    // replicas, or remove excess replicas.\n+    try (RegionScanner scanner =\n+      localStore.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows;\n+      do {\n+        moreRows = scanner.next(cells);\n+        if (cells.isEmpty()) {\n+          continue;\n         }\n-        setMetaAssigned(regionState.getRegion(), regionState.getState() == State.OPEN);\n-      } finally {\n-        regionNode.unlock();\n+        Result result = Result.create(cells);\n+        cells.clear();\n+        RegionStateStore\n+          .visitMetaEntry((r, regionInfo, state, regionLocation, lastHost, openSeqNum) -> {\n+            RegionStateNode regionNode = regionStates.getOrCreateRegionStateNode(regionInfo);\n+            regionNode.lock();\n+            try {\n+              regionNode.setState(state);\n+              regionNode.setLastHost(lastHost);\n+              regionNode.setRegionLocation(regionLocation);\n+              regionNode.setOpenSeqNum(openSeqNum);\n+              if (regionNode.getProcedure() != null) {\n+                regionNode.getProcedure().stateLoaded(this, regionNode);\n+              }\n+              if (RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+                setMetaAssigned(regionInfo, state == State.OPEN);\n+              }\n+            } finally {\n+              regionNode.unlock();\n+            }\n+            if (regionInfo.isFirst()) {\n+              // for compatibility, mirror the meta region state to zookeeper", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM1MDIyMg=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgxMjA5Mg==", "bodyText": "Got it.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430812092", "createdAt": "2020-05-27T01:44:33Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "diffHunk": "@@ -225,23 +230,52 @@ public void start() throws IOException, KeeperException {\n     // Start the Assignment Thread\n     startAssignmentThread();\n \n-    // load meta region state\n-    ZKWatcher zkw = master.getZooKeeper();\n-    // it could be null in some tests\n-    if (zkw != null) {\n-      RegionState regionState = MetaTableLocator.getMetaRegionState(zkw);\n-      RegionStateNode regionNode =\n-        regionStates.getOrCreateRegionStateNode(RegionInfoBuilder.FIRST_META_REGIONINFO);\n-      regionNode.lock();\n-      try {\n-        regionNode.setRegionLocation(regionState.getServerName());\n-        regionNode.setState(regionState.getState());\n-        if (regionNode.getProcedure() != null) {\n-          regionNode.getProcedure().stateLoaded(this, regionNode);\n+    // load meta region states.\n+    // notice that, here we will load all replicas, and in MasterMetaBootstrap we may assign new\n+    // replicas, or remove excess replicas.\n+    try (RegionScanner scanner =\n+      localStore.getScanner(new Scan().addFamily(HConstants.CATALOG_FAMILY))) {\n+      List<Cell> cells = new ArrayList<>();\n+      boolean moreRows;\n+      do {\n+        moreRows = scanner.next(cells);\n+        if (cells.isEmpty()) {\n+          continue;\n         }\n-        setMetaAssigned(regionState.getRegion(), regionState.getState() == State.OPEN);\n-      } finally {\n-        regionNode.unlock();\n+        Result result = Result.create(cells);\n+        cells.clear();\n+        RegionStateStore\n+          .visitMetaEntry((r, regionInfo, state, regionLocation, lastHost, openSeqNum) -> {\n+            RegionStateNode regionNode = regionStates.getOrCreateRegionStateNode(regionInfo);\n+            regionNode.lock();\n+            try {\n+              regionNode.setState(state);\n+              regionNode.setLastHost(lastHost);\n+              regionNode.setRegionLocation(regionLocation);\n+              regionNode.setOpenSeqNum(openSeqNum);\n+              if (regionNode.getProcedure() != null) {\n+                regionNode.getProcedure().stateLoaded(this, regionNode);\n+              }\n+              if (RegionReplicaUtil.isDefaultReplica(regionInfo)) {\n+                setMetaAssigned(regionInfo, state == State.OPEN);\n+              }\n+            } finally {\n+              regionNode.unlock();\n+            }\n+            if (regionInfo.isFirst()) {\n+              // for compatibility, mirror the meta region state to zookeeper", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM1MDIyMg=="}, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MDU1NTQ5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMTo0Njo0MFrOGaakwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxMTo0Njo0MFrOGaakwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDM1MTU1Mw==", "bodyText": "mirror the meta location to zookeeper?", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430351553", "createdAt": "2020-05-26T11:46:40Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateStore.java", "diffHunk": "@@ -216,12 +200,32 @@ private void updateUserRegionLocation(RegionInfo regionInfo, State state,\n         .build());\n     LOG.info(info.toString());\n     updateRegionLocation(regionInfo, state, put);\n+    if (regionInfo.isMetaRegion() && regionInfo.isFirst()) {\n+      // mirror the meta location to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "848a8a2e480e06de52f5698cc1d54a4c64f28313"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzUwODM3OnYy", "diffSide": "RIGHT", "path": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowMTozN1rOGa32AQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowMTozN1rOGa32AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMTEwNQ==", "bodyText": "Man. Region Replicas are messy in hbase:meta. Not your fault. For later.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430831105", "createdAt": "2020-05-27T03:01:37Z", "author": {"login": "saintstack"}, "path": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java", "diffHunk": "@@ -1403,6 +1403,21 @@ private static void deleteFromMetaTable(final Connection connection, final List<\n     }\n   }\n \n+  public static Delete removeRegionReplica(byte[] metaRow, int replicaIndexToDeleteFrom,\n+    int numReplicasToRemove) {\n+    int absoluteIndex = replicaIndexToDeleteFrom + numReplicasToRemove;\n+    long now = EnvironmentEdgeManager.currentTime();\n+    Delete deleteReplicaLocations = new Delete(metaRow);\n+    for (int i = replicaIndexToDeleteFrom; i < absoluteIndex; i++) {\n+      deleteReplicaLocations.addColumns(getCatalogFamily(), getServerColumn(i), now);\n+      deleteReplicaLocations.addColumns(getCatalogFamily(), getSeqNumColumn(i), now);\n+      deleteReplicaLocations.addColumns(getCatalogFamily(), getStartCodeColumn(i), now);\n+      deleteReplicaLocations.addColumns(getCatalogFamily(), getServerNameColumn(i), now);\n+      deleteReplicaLocations.addColumns(getCatalogFamily(), getRegionStateColumn(i), now);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "976d0c4e5b732a23773bd306f79e8017344b58f3"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzUxMTM4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowMzo0N1rOGa34GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowMzo0N1rOGa34GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMTY0MA==", "bodyText": "Good.\nWas going to suggest AM ask the Master for its localStore but that probably TMI for the AM to know of. This is better separation of concerns.\nGood.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430831640", "createdAt": "2020-05-27T03:03:47Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -866,8 +874,50 @@ protected void initializeZKBasedSystemTrackers()\n \n   // Will be overriden in test to inject customized AssignmentManager\n   @VisibleForTesting\n-  protected AssignmentManager createAssignmentManager(MasterServices master) {\n-    return new AssignmentManager(master);\n+  protected AssignmentManager createAssignmentManager(MasterServices master,\n+    LocalStore localStore) {\n+    return new AssignmentManager(master, localStore);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "976d0c4e5b732a23773bd306f79e8017344b58f3"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzUxOTM4OnYy", "diffSide": "LEFT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowOTozNFrOGa39Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzowOTozNFrOGa39Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzMjk3OQ==", "bodyText": "Radical. No ZKW in AM. Good.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430832979", "createdAt": "2020-05-27T03:09:34Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java", "diffHunk": "@@ -225,23 +230,52 @@ public void start() throws IOException, KeeperException {\n     // Start the Assignment Thread\n     startAssignmentThread();\n \n-    // load meta region state\n-    ZKWatcher zkw = master.getZooKeeper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "976d0c4e5b732a23773bd306f79e8017344b58f3"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4MzUzMDk0OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/store/LocalStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxNzo0OFrOGa4EVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMzoxNzo0OFrOGa4EVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgzNDc3Mw==", "bodyText": "Yeah, the ROW_INDEX_V1 is good change.", "url": "https://github.com/apache/hbase/pull/1746#discussion_r430834773", "createdAt": "2020-05-27T03:17:48Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/store/LocalStore.java", "diffHunk": "@@ -87,6 +90,10 @@\n   public static final byte[] PROC_FAMILY = Bytes.toBytes(\"proc\");\n \n   private static final TableDescriptor TABLE_DESC = TableDescriptorBuilder.newBuilder(TABLE_NAME)\n+    .setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(HConstants.CATALOG_FAMILY)\n+      .setMaxVersions(HConstants.DEFAULT_HBASE_META_VERSIONS).setInMemory(true)\n+      .setBlocksize(HConstants.DEFAULT_HBASE_META_BLOCK_SIZE).setBloomFilterType(BloomType.ROWCOL)\n+      .setDataBlockEncoding(DataBlockEncoding.ROW_INDEX_V1).build())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "976d0c4e5b732a23773bd306f79e8017344b58f3"}, "originalPosition": 24}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3014, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}