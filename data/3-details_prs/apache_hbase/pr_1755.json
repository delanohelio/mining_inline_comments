{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxNTA5NjU0", "number": 1755, "title": "HBASE-24069 Provide an ExponentialBackOffPolicy sleep between failed \u2026", "bodyText": "\u2026region close requests", "createdAt": "2020-05-21T18:32:18Z", "url": "https://github.com/apache/hbase/pull/1755", "merged": true, "mergeCommit": {"oid": "3c138845d935b7183d02f1312cf8a1e5ce690572"}, "closed": true, "closedAt": "2020-05-31T21:41:18Z", "author": {"login": "sguggilam"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcjh-5eAH2gAyNDIxNTA5NjU0OmJjMWJlM2NjZDY4ZGMxMTk3ODM1M2NhM2FkY2JhNDVmMjVmYzc2M2Q=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcmytayAFqTQyMTUyOTgwNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/bc1be3ccd68dc11978353ca3adcba45f25fc763d", "committedDate": "2020-05-21T18:29:00Z", "message": "HBASE-24069 Provide an ExponentialBackOffPolicy sleep between failed region close requests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NjgwNjEz", "url": "https://github.com/apache/hbase/pull/1755#pullrequestreview-418680613", "createdAt": "2020-05-26T21:08:42Z", "commit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTowODo0MlrOGawYuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTowODo0MlrOGawYuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODkyMA==", "bodyText": "We can race between test and put, potentially undercounting (by ref overwrite). Test if the return of putIfAbsent is not null. If not null, use that instead of the atomic instance you just created. Then increment. If the code that updates failedOpenTracker also has this racy pattern it should be fixed too.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430708920", "createdAt": "2020-05-26T21:08:42Z", "author": {"login": "apurtell"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1972,6 +1975,13 @@ private void unassign(final HRegionInfo region,\n       final RegionState state, final int versionOfClosingNode,\n       final ServerName dest, final boolean transitionInZK,\n       final ServerName src) {\n+    String encodedName = region.getEncodedName();\n+    AtomicInteger failedCloseCount = failedCloseTracker.get(encodedName);\n+    if (failedCloseCount == null) {\n+      failedCloseCount = new AtomicInteger();\n+      failedCloseTracker.put(encodedName, failedCloseCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzUyMDMz", "url": "https://github.com/apache/hbase/pull/1755#pullrequestreview-418752033", "createdAt": "2020-05-26T23:46:41Z", "commit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMzo0Njo0MVrOGaz89Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwMDoyMjo0OVrOGa0liA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2NzM0OQ==", "bodyText": "Aren't all the codepaths reaching this point, expected to take an exclusive lock on the region.encodedName()? If so, wondering if we should worry about the non-thread-safe access for this map. I checked all the callers, all except one path in forceRegionStateToOffline() follow this pattern, we should probably fix that.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430767349", "createdAt": "2020-05-26T23:46:41Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1972,6 +1975,13 @@ private void unassign(final HRegionInfo region,\n       final RegionState state, final int versionOfClosingNode,\n       final ServerName dest, final boolean transitionInZK,\n       final ServerName src) {\n+    String encodedName = region.getEncodedName();\n+    AtomicInteger failedCloseCount = failedCloseTracker.get(encodedName);\n+    if (failedCloseCount == null) {\n+      failedCloseCount = new AtomicInteger();\n+      failedCloseTracker.put(encodedName, failedCloseCount);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODkyMA=="}, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2Nzg1Mw==", "bodyText": "Is there any change in functionality of this section of diff? I think the answer is no and its mostly indents, but I wanted to double check..can you please confirm?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430767853", "createdAt": "2020-05-26T23:48:25Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1997,80 +2007,76 @@ private void unassign(final HRegionInfo region,\n       }\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n       } catch (Throwable t) {\n         long sleepTime = 0;\n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjExMQ==", "bodyText": "Pardon my ignorance but I don't fully understand the fix. I think the ask in the jira to spread out the unassigns a bit by using a backoff based approach. To do that why not just fix the sleepTime above in L2075 to use an exponential backoff based approach?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430776111", "createdAt": "2020-05-27T00:17:06Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NzczNg==", "bodyText": "javadoc for the method says \"Send CLOSE RPC if the server is online, otherwise, offline the region.\". With this delayed callables, aren't we violating that? If we hit this block, we will still be in the CLOSING state with the thread pool retrying in the background. Curious if that causes any issues in the statemachine.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r430777736", "createdAt": "2020-05-27T00:22:49Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -2079,16 +2085,29 @@ private void unassign(final HRegionInfo region,\n         }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n     }\n-    // Run out of attempts\n-    if (state != null) {\n-      regionStates.updateRegionState(region, State.FAILED_CLOSE);\n+\n+    long sleepTime = backoffPolicy.getBackoffTime(retryConfig,\n+      getFailedAttempts(encodedName, failedCloseTracker));\n+    if (failedCloseCount.incrementAndGet() <= maximumAttempts && sleepTime > 0) {\n+      if (failedCloseTracker.containsKey(encodedName)) {\n+        // Sleep before trying unassign if this region has failed to close before\n+        scheduledThreadPoolExecutor.schedule(new DelayedUnAssignCallable(this, region, state,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc1be3ccd68dc11978353ca3adcba45f25fc763d"}, "originalPosition": 182}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4707df93606c6f511c22cb05206caf463f092689", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/4707df93606c6f511c22cb05206caf463f092689", "committedDate": "2020-05-29T18:20:57Z", "message": "Incorporate review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMzM5ODk0", "url": "https://github.com/apache/hbase/pull/1755#pullrequestreview-421339894", "createdAt": "2020-05-29T22:59:01Z", "commit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMjo1OTowMVrOGcuemQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMzoyMTo0NlrOGcux2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3NDgwOQ==", "bodyText": "nit: (t instanceof RegionServerAbortedException) is redundant (per static analysis check in the IDE)...", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432774809", "createdAt": "2020-05-29T22:59:01Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3NjIzMA==", "bodyText": "merge this logging and the logging in L2074 and log towards the end?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432776230", "createdAt": "2020-05-29T23:05:34Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3Njc2Mg==", "bodyText": "How about a more readable condition here?\nif (regionCloseFailed || anyOtherExceptionThrown) { // set these flags above\n}\nI think that makes the flow easier to understand.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432776762", "createdAt": "2020-05-29T23:07:50Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n+      // If sleepTime is not set by any of the cases, set it to sleep for\n+      // configured exponential backoff time\n+      if (sleepTime == 0 && i != maximumAttempts) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3Njk1OQ==", "bodyText": "do this before propagating the interrupt flag?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432776959", "createdAt": "2020-05-29T23:08:38Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n+      // If sleepTime is not set by any of the cases, set it to sleep for\n+      // configured exponential backoff time\n+      if (sleepTime == 0 && i != maximumAttempts) {\n+        sleepTime = backoffPolicy.getBackoffTime(retryConfig, i);\n+        LOG.info(\"Waiting for \" + sleepTime + \"milliseconds exponential backoff time for \"\n+            + region.getRegionNameAsString() + \" before next retry \" + (i + 1) + \" of \"\n+            + this.maximumAttempts);\n+      }\n+      try {\n+        if (sleepTime > 0 && i != maximumAttempts) {\n+          Thread.sleep(sleepTime);\n+        }\n+      } catch (InterruptedException ie) {\n+        LOG.warn(\"Failed to unassign \" + region.getRegionNameAsString() + \" since interrupted\", ie);\n+        Thread.currentThread().interrupt();\n+        if (state != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3NzE0Nw==", "bodyText": "nit: Can you please add a few comments on the code flow so that its easier for other to follow?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432777147", "createdAt": "2020-05-29T23:09:23Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java", "diffHunk": "@@ -1995,97 +1995,100 @@ private void unassign(final HRegionInfo region,\n         }\n         return;\n       }\n+      long sleepTime = 0;\n       try {\n         // Send CLOSE RPC\n-        if (serverManager.sendRegionClose(server, region,\n-          versionOfClosingNode, dest, transitionInZK)) {\n-          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" +\n-            region.getRegionNameAsString());\n+        if (serverManager.sendRegionClose(server, region, versionOfClosingNode, dest,\n+          transitionInZK)) {\n+          LOG.debug(\"Sent CLOSE to \" + server + \" for region \" + region.getRegionNameAsString());\n           if (useZKForAssignment && !transitionInZK && state != null) {\n             // Retry to make sure the region is\n             // closed so as to avoid double assignment.\n-            unassign(region, state, versionOfClosingNode,\n-              dest, transitionInZK, src);\n+            unassign(region, state, versionOfClosingNode, dest, transitionInZK, src);\n           }\n           return;\n         }\n         // This never happens. Currently regionserver close always return true.\n         // Todo; this can now happen (0.96) if there is an exception in a coprocessor\n-        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \" +\n-          region.getRegionNameAsString());\n-      } catch (Throwable t) {\n-        long sleepTime = 0;\n+        LOG.warn(\"Server \" + server + \" region CLOSE RPC returned false for \"\n+            + region.getRegionNameAsString());\n+      } catch (Throwable t) {       \n         Configuration conf = this.server.getConfiguration();\n         if (t instanceof RemoteException) {\n-          t = ((RemoteException)t).unwrapRemoteException();\n+          t = ((RemoteException) t).unwrapRemoteException();\n         }\n         boolean logRetries = true;\n-        if (t instanceof RegionServerAbortedException\n-            || t instanceof RegionServerStoppedException\n+        if (t instanceof RegionServerAbortedException || t instanceof RegionServerStoppedException\n             || t instanceof ServerNotRunningYetException) {\n           // RS is aborting or stopping, we cannot offline the region since the region may need\n-          // to do WAL recovery. Until we see  the RS expiration, we should retry.\n+          // to do WAL recovery. Until we see the RS expiration, we should retry.\n           sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n             RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n \n         } else if (t instanceof NotServingRegionException) {\n-          LOG.debug(\"Offline \" + region.getRegionNameAsString()\n-            + \", it's not any more on \" + server, t);\n+          LOG.debug(\n+            \"Offline \" + region.getRegionNameAsString() + \", it's not any more on \" + server, t);\n           if (transitionInZK) {\n             deleteClosingOrClosedNode(region, server);\n           }\n           if (state != null) {\n             regionOffline(region);\n           }\n           return;\n-        } else if ((t instanceof FailedServerException) || (state != null &&\n-            t instanceof RegionAlreadyInTransitionException)) {\n-          if (t instanceof FailedServerException) {\n-            sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n+        } else if ((t instanceof FailedServerException)\n+            || (state != null && t instanceof RegionAlreadyInTransitionException)) {\n+              if (t instanceof FailedServerException) {\n+                sleepTime = 1L + conf.getInt(RpcClient.FAILED_SERVER_EXPIRY_KEY,\n                   RpcClient.FAILED_SERVER_EXPIRY_DEFAULT);\n-          } else {\n-            // RS is already processing this region, only need to update the timestamp\n-            LOG.debug(\"update \" + state + \" the timestamp.\");\n-            state.updateTimestampToNow();\n-            if (maxWaitTime < 0) {\n-              maxWaitTime =\n-                  EnvironmentEdgeManager.currentTime()\n-                      + conf.getLong(ALREADY_IN_TRANSITION_WAITTIME,\n-                        DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n-            }\n-            long now = EnvironmentEdgeManager.currentTime();\n-            if (now < maxWaitTime) {\n-              LOG.debug(\"Region is already in transition; \"\n-                + \"waiting up to \" + (maxWaitTime - now) + \"ms\", t);\n-              sleepTime = 100;\n-              i--; // reset the try count\n-              logRetries = false;\n+              } else {\n+                // RS is already processing this region, only need to update the timestamp\n+                LOG.debug(\"update \" + state + \" the timestamp.\");\n+                state.updateTimestampToNow();\n+                if (maxWaitTime < 0) {\n+                  maxWaitTime = EnvironmentEdgeManager.currentTime() + conf.getLong(\n+                    ALREADY_IN_TRANSITION_WAITTIME, DEFAULT_ALREADY_IN_TRANSITION_WAITTIME);\n+                }\n+                long now = EnvironmentEdgeManager.currentTime();\n+                if (now < maxWaitTime) {\n+                  LOG.debug(\"Region is already in transition; \" + \"waiting up to \"\n+                      + (maxWaitTime - now) + \"ms\",\n+                    t);\n+                  sleepTime = 100;\n+                  i--; // reset the try count\n+                  logRetries = false;\n+                }\n+              }\n             }\n-          }\n-        }\n-\n-        try {\n-          if (sleepTime > 0) {\n-            Thread.sleep(sleepTime);\n-          }\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"Failed to unassign \"\n-            + region.getRegionNameAsString() + \" since interrupted\", ie);\n-          Thread.currentThread().interrupt();\n-          if (state != null) {\n-            regionStates.updateRegionState(region, State.FAILED_CLOSE);\n-          }\n-          return;\n-        }\n \n         if (logRetries) {\n-          LOG.info(\"Server \" + server + \" returned \" + t + \" for \"\n-            + region.getRegionNameAsString() + \", try=\" + i\n-            + \" of \" + this.maximumAttempts, t);\n+          LOG.info(\"Server \" + server + \" returned \" + t + \" for \" + region.getRegionNameAsString()\n+              + \", try=\" + i + \" of \" + this.maximumAttempts,\n+            t);\n           // Presume retry or server will expire.\n         }\n       }\n+      // If sleepTime is not set by any of the cases, set it to sleep for", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3ODM4OA==", "bodyText": "nit: We use TestTableName.getTableName() that automatically gets the TableName with the running test name.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432778388", "createdAt": "2020-05-29T23:15:16Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "diffHunk": "@@ -598,6 +601,68 @@ public void testCloseFailed() throws Exception {\n     }\n   }\n \n+  /**\n+   * This tests region close with exponential backoff\n+   */\n+  @Test(timeout = 60000)\n+  public void testCloseRegionWithExponentialBackOff() throws Exception {\n+    String table = \"testCloseRegionWithExponentialBackOff\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc3OTczOA==", "bodyText": "Does the test reliably fail without the patch, meaning it ends up in FAILED_CLOSE?", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432779738", "createdAt": "2020-05-29T23:21:46Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "diffHunk": "@@ -598,6 +601,68 @@ public void testCloseFailed() throws Exception {\n     }\n   }\n \n+  /**\n+   * This tests region close with exponential backoff\n+   */\n+  @Test(timeout = 60000)\n+  public void testCloseRegionWithExponentialBackOff() throws Exception {\n+    String table = \"testCloseRegionWithExponentialBackOff\";\n+    // Set the backoff time between each retry for failed close\n+    TEST_UTIL.getMiniHBaseCluster().getConf().setLong(\"hbase.assignment.retry.sleep.initial\", 1000);\n+    HMaster activeMaster = TEST_UTIL.getHBaseCluster().getMaster();\n+    TEST_UTIL.getMiniHBaseCluster().stopMaster(activeMaster.getServerName());\n+    TEST_UTIL.getMiniHBaseCluster().startMaster(); // restart the master for conf take into affect\n+\n+    try {\n+      ScheduledThreadPoolExecutor scheduledThreadPoolExecutor =\n+          new ScheduledThreadPoolExecutor(1, Threads.newDaemonThreadFactory(\"ExponentialBackOff\"));\n+\n+      HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(table));\n+      desc.addFamily(new HColumnDescriptor(FAMILY));\n+      admin.createTable(desc);\n+\n+      Table meta = new HTable(conf, TableName.META_TABLE_NAME);\n+      HRegionInfo hri =\n+          new HRegionInfo(desc.getTableName(), Bytes.toBytes(\"A\"), Bytes.toBytes(\"Z\"));\n+      MetaTableAccessor.addRegionToMeta(meta, hri);\n+\n+      HMaster master = TEST_UTIL.getHBaseCluster().getMaster();\n+      AssignmentManager am = master.getAssignmentManager();\n+      assertTrue(TEST_UTIL.assignRegion(hri));\n+      ServerName sn = am.getRegionStates().getRegionServerOfRegion(hri);\n+      TEST_UTIL.assertRegionOnServer(hri, sn, 6000);\n+\n+      MyRegionObserver.preCloseEnabled.set(true);\n+      // Unset the precloseEnabled flag after 1 second for next retry to succeed\n+      scheduledThreadPoolExecutor.schedule(new Runnable() {\n+        @Override\n+        public void run() {\n+          MyRegionObserver.preCloseEnabled.set(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4707df93606c6f511c22cb05206caf463f092689"}, "originalPosition": 80}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "419a0cb34484f02aa0b2d90102c8ff35d84e97c3", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/419a0cb34484f02aa0b2d90102c8ff35d84e97c3", "committedDate": "2020-05-30T01:20:47Z", "message": "Incorporate review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxNDQ1MTY2", "url": "https://github.com/apache/hbase/pull/1755#pullrequestreview-421445166", "createdAt": "2020-05-30T21:30:09Z", "commit": {"oid": "419a0cb34484f02aa0b2d90102c8ff35d84e97c3"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQyMTozMDowOVrOGc1cdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQyMTozMDowOVrOGc1cdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg4ODk0OQ==", "bodyText": "you already get a \"TableName\", you don't need to convert it to string and back to TableName. Also, TEST_UTIL.deleteTable() takes TableName as argument.", "url": "https://github.com/apache/hbase/pull/1755#discussion_r432888949", "createdAt": "2020-05-30T21:30:09Z", "author": {"login": "bharathv"}, "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerOnCluster.java", "diffHunk": "@@ -598,6 +605,68 @@ public void testCloseFailed() throws Exception {\n     }\n   }\n \n+  /**\n+   * This tests region close with exponential backoff\n+   */\n+  @Test(timeout = 60000)\n+  public void testCloseRegionWithExponentialBackOff() throws Exception {\n+    String table = testTableName.getTableName().toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "419a0cb34484f02aa0b2d90102c8ff35d84e97c3"}, "originalPosition": 68}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "802a545118550eb75a2cba5c1ec8fa0ec576ddeb", "author": {"user": null}, "url": "https://github.com/apache/hbase/commit/802a545118550eb75a2cba5c1ec8fa0ec576ddeb", "committedDate": "2020-05-30T22:53:41Z", "message": "fix checkstyle errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxNTI5ODA1", "url": "https://github.com/apache/hbase/pull/1755#pullrequestreview-421529805", "createdAt": "2020-05-31T21:40:04Z", "commit": {"oid": "802a545118550eb75a2cba5c1ec8fa0ec576ddeb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4713, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}