{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1NTEzNTgz", "number": 1913, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMDowODoyMFrOEGJi1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwNDo0OTozNFrOEGmOeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0ODgzMjg2OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMDowODoyMFrOGkxOtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0NzozN1rOGlQhZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwODUwMw==", "bodyText": "Log => WAL?", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441208503", "createdAt": "2020-06-17T00:08:20Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -233,7 +247,7 @@ public static boolean splitLogFile(Path walDir, FileStatus logfile, FileSystem w\n   }\n \n   /**\n-   * log splitting implementation, splits one log file.\n+   * Log splitting implementation, splits one log file.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMTE4OA==", "bodyText": "Let me change.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441721188", "createdAt": "2020-06-17T17:47:37Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -233,7 +247,7 @@ public static boolean splitLogFile(Path walDir, FileStatus logfile, FileSystem w\n   }\n \n   /**\n-   * log splitting implementation, splits one log file.\n+   * Log splitting implementation, splits one log file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwODUwMw=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0OTQ5NjAxOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNjozNToxOVrOGk3h0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0NzoyOVrOGlQhJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMTY5Ng==", "bodyText": "Per one WAL file split, every region will have one recovered edits file. If another WAL file also get split, we will have one more right?", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441311696", "createdAt": "2020-06-17T06:35:19Z", "author": {"login": "anoopsjohn"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "diffHunk": "@@ -26,20 +26,20 @@\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n-\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.MultipleIOException;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-\n import org.apache.hbase.thirdparty.com.google.common.collect.Lists;\n \n /**\n  * Class that manages the output streams from the log splitting process.\n- * Every region only has one recovered edits.\n+ * Every region only has one recovered edits file.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxNDQ3NA==", "bodyText": "Yes. Right.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441314474", "createdAt": "2020-06-17T06:41:58Z", "author": {"login": "infraio"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "diffHunk": "@@ -26,20 +26,20 @@\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n-\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.MultipleIOException;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-\n import org.apache.hbase.thirdparty.com.google.common.collect.Lists;\n \n /**\n  * Class that manages the output streams from the log splitting process.\n- * Every region only has one recovered edits.\n+ * Every region only has one recovered edits file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMTY5Ng=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMTEyNw==", "bodyText": "Let me add qualification here.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441721127", "createdAt": "2020-06-17T17:47:29Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "diffHunk": "@@ -26,20 +26,20 @@\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n-\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.MultipleIOException;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-\n import org.apache.hbase.thirdparty.com.google.common.collect.Lists;\n \n /**\n  * Class that manages the output streams from the log splitting process.\n- * Every region only has one recovered edits.\n+ * Every region only has one recovered edits file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMTY5Ng=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0OTUwMTYyOnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNjozNzozMVrOGk3lQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo1NDozNFrOGlQxuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMjU3Nw==", "bodyText": "This defaults to false right?", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441312577", "createdAt": "2020-06-17T06:37:31Z", "author": {"login": "anoopsjohn"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -115,7 +119,18 @@\n \n   private final String tmpDirName;\n \n+  /**\n+   * Split WAL directly to hfiles instead of into intermediary 'recovered.edits' files.\n+   */\n+  public static final String WAL_SPLIT_TO_HFILE = \"hbase.wal.split.to.hfile\";\n+  public static final boolean DEFAULT_WAL_SPLIT_TO_HFILE = false;\n+\n+  /**\n+   * True if we are to run with bounded amount of writers rather than let the count blossom.\n+   * Bounded writing tends to have higher throughput.\n+   */\n   public final static String SPLIT_WRITER_CREATION_BOUNDED = \"hbase.split.writer.creation.bounded\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMDk1Mg==", "bodyText": "Let me add this.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441720952", "createdAt": "2020-06-17T17:47:09Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -115,7 +119,18 @@\n \n   private final String tmpDirName;\n \n+  /**\n+   * Split WAL directly to hfiles instead of into intermediary 'recovered.edits' files.\n+   */\n+  public static final String WAL_SPLIT_TO_HFILE = \"hbase.wal.split.to.hfile\";\n+  public static final boolean DEFAULT_WAL_SPLIT_TO_HFILE = false;\n+\n+  /**\n+   * True if we are to run with bounded amount of writers rather than let the count blossom.\n+   * Bounded writing tends to have higher throughput.\n+   */\n   public final static String SPLIT_WRITER_CREATION_BOUNDED = \"hbase.split.writer.creation.bounded\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMjU3Nw=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyNTM2OQ==", "bodyText": "I note that it defaults to 'false' but man, this should be 'true'.  Let me test.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441725369", "createdAt": "2020-06-17T17:54:34Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -115,7 +119,18 @@\n \n   private final String tmpDirName;\n \n+  /**\n+   * Split WAL directly to hfiles instead of into intermediary 'recovered.edits' files.\n+   */\n+  public static final String WAL_SPLIT_TO_HFILE = \"hbase.wal.split.to.hfile\";\n+  public static final boolean DEFAULT_WAL_SPLIT_TO_HFILE = false;\n+\n+  /**\n+   * True if we are to run with bounded amount of writers rather than let the count blossom.\n+   * Bounded writing tends to have higher throughput.\n+   */\n   public final static String SPLIT_WRITER_CREATION_BOUNDED = \"hbase.split.writer.creation.bounded\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMjU3Nw=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MDMzNjk4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMDozMDo1N1rOGk_zJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwMjoxMjoyOVrOGldN7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ==", "bodyText": "Good to guard with LOG.isTraceEnabled()?", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441447205", "createdAt": "2020-06-17T10:30:57Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMDY1NQ==", "bodyText": "Why you say? The slf4j doesn't do any work creating log string if we are not at log TRACE level.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441720655", "createdAt": "2020-06-17T17:46:38Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0NTg1NQ==", "bodyText": "This is to avoid the cost of parameter construction: http://logging.apache.org/log4j/1.2/manual.html#performance", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441745855", "createdAt": "2020-06-17T18:30:25Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0Njg3Ng==", "bodyText": "My bad, this is old doc for apache log4j, not slf4j", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441746876", "createdAt": "2020-06-17T18:32:22Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc1MDU5OQ==", "bodyText": "You are right, I just looked into one of the implementors of slf4j (Log4jLoggerAdapter) and it already covers isTraceEnabled check:\n    public void trace(String format, Object arg) {\n        if (isTraceEnabled()) {\n            FormattingTuple ft = MessageFormatter.format(format, arg);\n            logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable());\n        }\n    }\n\nWe are following explicit guarding for the cases of Trace and Debug level log at multiple places, all are redundant checks since library already does it for us.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441750599", "createdAt": "2020-06-17T18:39:00Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MDU2MQ==", "bodyText": "Thanks @virajjasani", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441840561", "createdAt": "2020-06-17T21:19:32Z", "author": {"login": "saintstack"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkyOTE5Nw==", "bodyText": "IIRC, this extra check could be used only if the parameter self will cause a heavy call.", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441929197", "createdAt": "2020-06-18T02:12:29Z", "author": {"login": "bsglz"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MDMzNzQ4OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMDozMTowNlrOGk_zfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMDozMTowNlrOGk_zfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzI5Mg==", "bodyText": "same here", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441447292", "createdAt": "2020-06-17T10:31:06Z", "author": {"login": "virajjasani"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -118,6 +122,7 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n         openingWritersNum.decrementAndGet();\n       } finally {\n         writer.close();\n+        LOG.trace(\"Closed {}, edits={}\", writer.getPath(), familyCells.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b22ca97fcc775401b0a330b7a014df26107c13b"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MzUzMjA5OnYy", "diffSide": "RIGHT", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwNDo0OTozNFrOGlffEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwNDo0OTozNFrOGlffEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk2NjM1NQ==", "bodyText": "Just asking. this can be in DEBUG mode right rather than trace?", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441966355", "createdAt": "2020-06-18T04:49:34Z", "author": {"login": "ramkrish86"}, "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "diffHunk": "@@ -81,6 +83,7 @@ private RecoveredEditsWriter getRecoveredEditsWriter(TableName tableName, byte[]\n     if (ret == null) {\n       return null;\n     }\n+    LOG.trace(\"Created {}\", ret.path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "36d85465ab24abea1fd98ffdf9adb95712364fd6"}, "originalPosition": 36}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2825, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}