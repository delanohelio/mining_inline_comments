{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1MDU1MDE1", "number": 259, "title": "Sharded Integrity Checks Job", "bodyText": "Description:\nThis builds on the work in #186 to create a sharded implementation of the Atlas Checks framework. By using input data that has been cut into \"shards\" the job can make better use of parallelization and keep a lower memory footprint. This vastly improves performance when run on a Spark cluster with large scale data sets.\nOutside of hardware requirements, there are a few things of note when comparing this job to the current one.\n\nThe input data must be Atlas files in the structure input_folder/iso_country_code/iso_z-x-y.atlas. PBF input is not yet supported.\nThis job requires a reference to how the Atlases are sharded. This can be provided via a sharding.txt file in that input path, or by using the sharding argument. Dynamic and SlippyTile sharding schemas are accepted.\nDue to the nature of the parallelization, some flags will be duplicated internally by this job. These flags are de-duplicated before being output, but this makes this job more sensitive to nondeterministic flagging behavior.\nBy default this job loads Atlas files using DynamicAtlases, and stores spark RDDs using memory first. Both of these settings work best when using the job at scale on a Spark cluster. For running on a single machine with more limited hardware, it is recommended to set the multiAtlas and sparkStorageDiskOnly to true.\nThis job is not currently able to upload flags directly to MapRoulette.\n\nKnown Issue:\n\nDue to the way the atlas data is loaded per shard, with a limited expansion distance, some data is just too large to process correctly. This is generally only an issue for very large relations, where only a subset of their members are loaded. This can cause incorrect flagging of these features.\n\nThere are some changes that effect the larger framework, by updating existing classes to better serve the new job:\n\nCommon program arguments and functions are now stored in the IntegrityChecksCommandArguments class.\nA new unique identifier system has been added to CheckFlags, to allow for better de-duplication. This system generates a set of AtlasObject to identify the flags. This change has been propagated through the flag outputs and difference commands. The old identifier still remains, and this should be completely reverse compatible.\nSome deprecated classes in org.openstreetmap.atlas.checks.event have been removed, and all references have been replaced with the equivalent classes from org.openstreetmap.atlas.event in the Atlas library. This is a breaking change.\nThe FlaggedObject class has a new method for converting to a CompleteEntity. This is used to disconnect the entity from its atlas, making it less memory intensive to perform the flag de-duplication process. This is a breaking change.\n\nPotential Impact:\nThis new job will likely vary in flag counts, in comparison to the old job. This is due to the nondeterministic sensitivity and large feature loading issue.\nThere are a couple breaking changes with the org.openstreetmap.atlas.checks.event package and FlaggedObject class.\nUnit Test Approach:\nUnit tests have been added to cover all major changes. Most of them are pretty strait forward. The unit tests for the new Spark Job are some what limited and nontraditional. This is due to known limitations with unit testing Spark. Testing is generally limited to running the entire job. These test do this using test inputs an checking for the correct number of outputs.\nTest Results:\nThe new job was run at scale on a Spark cluster, and compared with the outputs of the current job.\nRun time for the new job was about 4 times faster than the current job.\nEvaluation of the differences in flag output found there to be no variation outside of items  effected by the nondeterministic sensitivity and know issue with large features. That variation only effects about 0.1% of flags.", "createdAt": "2020-02-13T19:45:57Z", "url": "https://github.com/osmlab/atlas-checks/pull/259", "merged": true, "mergeCommit": {"oid": "4596d6915ac08c7f9a1f079709862411696a8480"}, "closed": true, "closedAt": "2020-02-28T22:42:54Z", "author": {"login": "Bentleysb"}, "timelineItems": {"totalCount": 61, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcD-8XJAH2gAyMzc1MDU1MDE1OjJhYzgzYzJhOGEwYjg5NjBkOWQ3NGRjNDZhNzJiYjdkNGYxYzI3MzM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcI3xazAFqTM2NjcyNTgzMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2ac83c2a8a0b8960d9d74dc46a72bb7d4f1c2733", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/2ac83c2a8a0b8960d9d74dc46a72bb7d4f1c2733", "committedDate": "2020-02-13T18:07:54Z", "message": "save work"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a881e2e458184c8f4635e1efd80752c7bfd6c4b3", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/a881e2e458184c8f4635e1efd80752c7bfd6c4b3", "committedDate": "2020-02-13T18:07:54Z", "message": "framework up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca3bc628b0a3b297f7e678396769e40437bc3733", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/ca3bc628b0a3b297f7e678396769e40437bc3733", "committedDate": "2020-02-13T18:07:54Z", "message": "formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e199faaa60d9af03774c45d35f1539adefff92aa", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/e199faaa60d9af03774c45d35f1539adefff92aa", "committedDate": "2020-02-13T18:07:54Z", "message": "processor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b08b6530fbdd9e6a1d7324fb7ca0c99ae340c0bd", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/b08b6530fbdd9e6a1d7324fb7ca0c99ae340c0bd", "committedDate": "2020-02-13T18:07:54Z", "message": "Produce flags set up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67ddca41372fb17b19f57b3948bad8879b7ff7c4", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/67ddca41372fb17b19f57b3948bad8879b7ff7c4", "committedDate": "2020-02-13T18:07:55Z", "message": "testing begining"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b61a5a7fe832b0f07466713b0abb8a74c432798d", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/b61a5a7fe832b0f07466713b0abb8a74c432798d", "committedDate": "2020-02-13T18:07:55Z", "message": "Testing improvements"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "823433018c08105ff96a7c5ec258ac71225b4355", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/823433018c08105ff96a7c5ec258ac71225b4355", "committedDate": "2020-02-13T18:07:55Z", "message": "Clean up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "163734574b61b84609fb75632f8b9062de4ed09f", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/163734574b61b84609fb75632f8b9062de4ed09f", "committedDate": "2020-02-13T18:07:55Z", "message": "testing additions and adjustments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf0a81efe06ff7e8ff6483f80452b0fcb0ee0241", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/cf0a81efe06ff7e8ff6483f80452b0fcb0ee0241", "committedDate": "2020-02-13T18:07:55Z", "message": "Rearrangment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b805d318b843ef9b2fad4afa89eed6a419569de4", "author": {"user": {"login": "jklamer", "name": "Jack Klamer"}}, "url": "https://github.com/osmlab/atlas-checks/commit/b805d318b843ef9b2fad4afa89eed6a419569de4", "committedDate": "2020-02-13T18:07:55Z", "message": "javadoc fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "acb9ceaba40c8c53b76b47c55c2c0c974a08cb49", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/acb9ceaba40c8c53b76b47c55c2c0c974a08cb49", "committedDate": "2020-02-13T18:07:55Z", "message": "update IncompleteSharding"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44708fc7c9897d768639cd7120f4e4c641410066", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/44708fc7c9897d768639cd7120f4e4c641410066", "committedDate": "2020-02-13T18:07:55Z", "message": "loadChecksForCountry checks white/blacklists"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e9db5a4aac1441de46c1a76de77ac5cd2e2bc8d", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/6e9db5a4aac1441de46c1a76de77ac5cd2e2bc8d", "committedDate": "2020-02-13T18:07:55Z", "message": "multipolygon fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f976eb2b938c29012ff294df297412c794fafdb1", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/f976eb2b938c29012ff294df297412c794fafdb1", "committedDate": "2020-02-13T18:07:55Z", "message": "dynamic atlas"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b53575e3125a6286b1f3ad1b73c42834acfba67b", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/b53575e3125a6286b1f3ad1b73c42834acfba67b", "committedDate": "2020-02-13T18:07:55Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c41c680b9330a5302c62408343deb15433bd7e9", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/7c41c680b9330a5302c62408343deb15433bd7e9", "committedDate": "2020-02-13T18:07:55Z", "message": "contrify parralelize"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9a4c91f56d93a5537f920dab8a514c69493f7808", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/9a4c91f56d93a5537f920dab8a514c69493f7808", "committedDate": "2020-02-13T18:07:55Z", "message": "count"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee18fc82f575fb0e402e4b1f7703786c89cd25c3", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/ee18fc82f575fb0e402e4b1f7703786c89cd25c3", "committedDate": "2020-02-13T18:07:55Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d79651f5bd1f2f7c3c61204bdea7702967a3f7b3", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/d79651f5bd1f2f7c3c61204bdea7702967a3f7b3", "committedDate": "2020-02-13T18:07:55Z", "message": "para stream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d4f415c5b32a0cef9824d0acb3bab3741db7bc6", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/4d4f415c5b32a0cef9824d0acb3bab3741db7bc6", "committedDate": "2020-02-13T18:07:55Z", "message": "pool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d72983eec90b35d23d88c0a42fa985ff3621078e", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/d72983eec90b35d23d88c0a42fa985ff3621078e", "committedDate": "2020-02-13T18:07:55Z", "message": "unique ids"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7014e207bd4ac2930307ff34c42978f8a1dd1a9c", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/7014e207bd4ac2930307ff34c42978f8a1dd1a9c", "committedDate": "2020-02-13T18:07:55Z", "message": "rm 2nd metrics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa917b779d42efb44cb4dafa4055a4157423ddfb", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/aa917b779d42efb44cb4dafa4055a4157423ddfb", "committedDate": "2020-02-13T18:07:55Z", "message": "unique container tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6ef37538923e85bb7533a1ce76b98555656d25a", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/c6ef37538923e85bb7533a1ce76b98555656d25a", "committedDate": "2020-02-13T18:07:55Z", "message": "test rule"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2439e1b2c0da33b079e0d20d0e613e0875fd875a", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/2439e1b2c0da33b079e0d20d0e613e0875fd875a", "committedDate": "2020-02-13T18:07:55Z", "message": "revert MR"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "925e6a04e1df75c8bc420225735af8420b1f87d3", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/925e6a04e1df75c8bc420225735af8420b1f87d3", "committedDate": "2020-02-13T18:07:55Z", "message": "sharded task tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2052888e2d785752d4c462c38a42ce982cab7ae", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/f2052888e2d785752d4c462c38a42ce982cab7ae", "committedDate": "2020-02-13T18:07:55Z", "message": "flagged relation tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6050f5d1a6a577ac588a3ed17e4c5afbac454e25", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/6050f5d1a6a577ac588a3ed17e4c5afbac454e25", "committedDate": "2020-02-13T18:07:55Z", "message": "spark job tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7044acd017d9a183b8543903a51e75b9b1eb59dd", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/7044acd017d9a183b8543903a51e75b9b1eb59dd", "committedDate": "2020-02-13T18:07:55Z", "message": "sharded job clean up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2013666244cefc02d4125e7ec892067b93fc7916", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/2013666244cefc02d4125e7ec892067b93fc7916", "committedDate": "2020-02-13T18:07:55Z", "message": "docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4cfbe5f32ca0be371e258900f2440b59e8b6854a", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/4cfbe5f32ca0be371e258900f2440b59e8b6854a", "committedDate": "2020-02-13T18:07:55Z", "message": "no grouping"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "750ef70c0eac43dd559cf3966330e901e9d74b2d", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/750ef70c0eac43dd559cf3966330e901e9d74b2d", "committedDate": "2020-02-13T18:07:55Z", "message": "rm deffered loading"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "181982487541bbfdfd2eb6e241e7608d9f0eed68", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/181982487541bbfdfd2eb6e241e7608d9f0eed68", "committedDate": "2020-02-13T18:07:55Z", "message": "pre load"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f67c510d849dd28a7914927e7f75ba053d3f6c3", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/9f67c510d849dd28a7914927e7f75ba053d3f6c3", "committedDate": "2020-02-13T18:07:55Z", "message": "deffer and laod"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e85877e00abd2944614e25a6083ddbc352bf306b", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/e85877e00abd2944614e25a6083ddbc352bf306b", "committedDate": "2020-02-13T18:07:55Z", "message": "bring back multi"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf4a91abf92e68ebf6191d9adfa2abc1e67be2ed", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/cf4a91abf92e68ebf6191d9adfa2abc1e67be2ed", "committedDate": "2020-02-13T18:07:55Z", "message": "return multi option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a8f0d24f2d8b8f64b5f8280f868cc97c1beae049", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/a8f0d24f2d8b8f64b5f8280f868cc97c1beae049", "committedDate": "2020-02-13T18:07:55Z", "message": "clean up switches"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b4e46c0a5e9b3cd10d87876d3965c782a071f7a", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/6b4e46c0a5e9b3cd10d87876d3965c782a071f7a", "committedDate": "2020-02-13T18:07:55Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/ce93633e9228afb81fc2f6871fb5c2bfa8764130", "committedDate": "2020-02-13T18:07:55Z", "message": "input note"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NjE1Nzc5", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-358615779", "createdAt": "2020-02-13T23:06:31Z", "commit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMzowNjozMVrOFpmpkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMzoxMToyMFrOFpmvqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE2OTE3MA==", "bodyText": "Is getting Checks per country not going to be supported moving forward? // Is it depended on the being able to access the persisted RDD?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379169170", "createdAt": "2020-02-13T23:06:31Z", "author": {"login": "jklamer"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3MDcyOQ==", "bodyText": "I believe making a new one of these WILL work for every atlas fetcher, but if you can make it a transient static lazy initialized I believe that will mean each JVM working in the spark cluster will use its own, which may allow for optimizations.", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379170729", "createdAt": "2020-02-13T23:11:20Z", "author": {"login": "jklamer"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)\n+                .reduceByKey(UniqueCheckFlagContainer::combine)\n+                .foreach(processFlags(output, fileHelper, outputFormats));\n+\n+        logger.info(\"Sharded checks completed in {}\", start.elapsedSince());\n+    }\n+\n+    @Override\n+    protected SwitchList switches()\n+    {\n+        return super.switches().with(EXPANSION_DISTANCE, MULTI_ATLAS, SPARK_STORAGE_DISK_ONLY,\n+                SHARDING);\n+    }\n+\n+    /**\n+     * Get the fetcher to use for Atlas files. The fetcher uses a hadoop cache to reduce remote\n+     * reads.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param country\n+     *            {@link String} country code\n+     * @param configuration\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @return {@link Function} that fetches atlases/\n+     */\n+    private Function<Shard, Optional<Atlas>> atlasFetcher(final String input, final String country,\n+            final Map<String, String> configuration)\n+    {\n+        final HadoopAtlasFileCache cache = new HadoopAtlasFileCache(input, configuration);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 280}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5MTA3ODYz", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-359107863", "createdAt": "2020-02-14T17:38:49Z", "commit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxNzozODo0OVrOFp-aJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxODoxNToyNlrOFp_XiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU1ODQzOA==", "bodyText": "These two variables can be local variables.", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379558438", "createdAt": "2020-02-14T17:38:49Z", "author": {"login": "smaheshwaram"}, "path": "src/main/java/org/openstreetmap/atlas/checks/base/CheckResourceLoader.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private final MultiMap<String, String> countryGroups = new MultiMap<>();\n     private final Boolean enabledByDefault;\n     private final String enabledKeyTemplate;\n+    private final String countryWhitelistTemplate = \"%s.\" + BaseCheck.PARAMETER_WHITELIST_COUNTRIES;\n+    private final String countryBlacklistTemplate = \"%s.\" + BaseCheck.PARAMETER_BLACKLIST_COUNTRIES;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzI3NA==", "bodyText": "We can define this String literal \"ItemId\" in GeoJsonUtils class!!", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379563274", "createdAt": "2020-02-14T17:50:10Z", "author": {"login": "smaheshwaram"}, "path": "src/main/java/org/openstreetmap/atlas/checks/commands/AtlasChecksLogDiffSubCommand.java", "diffHunk": "@@ -78,19 +82,28 @@ public AtlasChecksLogDiffSubCommand()\n     }\n \n     /**\n-     * Get the atlas ids from an array of features.\n+     * Get the unique ids for a flag. Fall back to getting the atlas ids from the features for\n+     * reverse compatibility.\n      *\n-     * @param features\n-     *            a {@link JsonArray} of features\n+     * @param flagJson\n+     *            a {@link JsonObject} of a flag\n      * @return a {@link Set} of {@link String} ids\n      */\n-    private Set<String> getAtlasIdentifiers(final JsonArray features)\n+    private Set<String> getIdentifiers(final JsonObject flagJson)\n     {\n-        return Iterables.stream(features)\n-                .filter(object -> object.getAsJsonObject().get(PROPERTIES).getAsJsonObject()\n-                        .has(IDENTIFIER))\n-                .map(object -> object.getAsJsonObject().get(PROPERTIES).getAsJsonObject()\n-                        .get(IDENTIFIER).getAsString())\n-                .collectToSet();\n+        final JsonObject flagProperties = flagJson.get(PROPERTIES).getAsJsonObject();\n+        return flagProperties.has(IDENTIFIERS)\n+                ? Iterables.stream(flagProperties.get(IDENTIFIERS).getAsJsonArray())\n+                        .map(JsonElement::getAsString).collectToSet()\n+                : Iterables.stream(flagJson.get(FEATURES).getAsJsonArray()).filter(object -> object\n+                        .getAsJsonObject().get(PROPERTIES).getAsJsonObject().has(IDENTIFIER)\n+                        || object.getAsJsonObject().get(PROPERTIES).getAsJsonObject().has(\"ItemId\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2ODk1Nw==", "bodyText": "Is this method called from anywhere?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379568957", "createdAt": "2020-02-14T18:02:48Z", "author": {"login": "smaheshwaram"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -134,6 +81,27 @@ public static void main(final String[] args)\n         new IntegrityCheckSparkJob().run(args);\n     }\n \n+    /**\n+     * Gets complex entities\n+     *\n+     * @param check\n+     *            A {@link BaseCheck} object\n+     * @param atlas\n+     *            An {@link Atlas} object\n+     * @return An {@link Iterable} of {@link ComplexEntity}s\n+     */\n+    protected static Iterable<ComplexEntity> findComplexEntities(final BaseCheck check,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDE1Mg==", "bodyText": "Will POOL_DURATION_BEFORE_KILL value remains the same? Also, may I know if it is a random assigned value or any estimated value.", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379574152", "createdAt": "2020-02-14T18:15:26Z", "author": {"login": "smaheshwaram"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -153,35 +121,11 @@ private static void executeChecks(final String country, final Atlas atlas,\n     {\n         final Pool checkExecutionPool = new Pool(checksToRun.size(), \"Check execution pool\",\n                 POOL_DURATION_BEFORE_KILL);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 137}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5173acf2db44ac12eb05735bf6da342ce0c9cc5", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/e5173acf2db44ac12eb05735bf6da342ce0c9cc5", "committedDate": "2020-02-18T19:48:31Z", "message": "add md docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "committedDate": "2020-02-24T17:45:19Z", "message": "clean up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzNTk3OTYz", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-363597963", "createdAt": "2020-02-24T18:17:48Z", "commit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxODoxNzo0OFrOFtqylQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNzo0MjowMFrOFuPLiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMTMxNw==", "bodyText": "Are you able to import the event package from atlas, or is there a conflicting package issue ?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r383431317", "createdAt": "2020-02-24T18:17:48Z", "author": {"login": "danielduhh"}, "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagEvent.java", "diffHunk": "@@ -23,25 +23,29 @@\n import org.openstreetmap.atlas.geography.geojson.GeoJsonObject;\n import org.openstreetmap.atlas.tags.HighwayTag;\n \n+import com.google.gson.Gson;\n import com.google.gson.JsonArray;\n import com.google.gson.JsonElement;\n import com.google.gson.JsonObject;\n import com.google.gson.JsonPrimitive;\n \n /**\n- * Wraps a {@link CheckFlag} for submission to the {@link EventService} for handling {@link Check}\n- * results\n+ * Wraps a {@link CheckFlag} for submission to the\n+ * {@link org.openstreetmap.atlas.event.EventService} for handling {@link Check} results\n  *\n  * @author mkalender, bbreithaupt\n  */\n-public final class CheckFlagEvent extends Event\n+public final class CheckFlagEvent extends org.openstreetmap.atlas.event.Event", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMjUyMg==", "bodyText": "Is toJsonTree converting the ids to an array? If so, what's the advantage of using an array vs string?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r383432522", "createdAt": "2020-02-24T18:20:31Z", "author": {"login": "danielduhh"}, "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagEvent.java", "diffHunk": "@@ -143,6 +147,7 @@ else if (flaggedRelations.size() != 1 && !feature.has(GEOMETRY))\n         flagProperties.add(\"feature_properties\", featureProperties);\n         flagProperties.add(\"feature_osmids\", uniqueFeatureOsmIds);\n         flagProperties.addProperty(\"feature_count\", featureProperties.size());\n+        flagProperties.add(IDENTIFIERS, GSON.toJsonTree(flag.getUniqueIdentifiers()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxNjk4MA==", "bodyText": "Unrelated to you changes but.. whats the purpose of the configJson switch?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384016980", "createdAt": "2020-02-25T17:24:00Z", "author": {"login": "danielduhh"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob\n+{\n+    /**\n+     * @author brian_l_davis\n+     */\n+    protected enum OutputFormats\n+    {\n+        FLAGS,\n+        GEOJSON,\n+        METRICS,\n+        TIPPECANOE\n+    }\n+\n+    @Deprecated\n+    protected static final Switch<String> ATLAS_FOLDER = new Switch<>(\"inputFolder\",\n+            \"Path of folder which contains Atlas file(s)\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    protected static final String OUTPUT_ATLAS_FOLDER = \"atlas\";\n+    // Outputs\n+    protected static final String OUTPUT_FLAG_FOLDER = \"flag\";\n+    protected static final String OUTPUT_GEOJSON_FOLDER = \"geojson\";\n+    protected static final String OUTPUT_METRIC_FOLDER = \"metric\";\n+    protected static final String OUTPUT_TIPPECANOE_FOLDER = \"tippecanoe\";\n+    static final Switch<List<String>> CHECK_FILTER = new Switch<>(\"checkFilter\",\n+            \"Comma-separated list of checks to run\",\n+            checks -> Arrays.asList(checks.split(CommonConstants.COMMA)), Optionality.OPTIONAL);\n+    // Configuration\n+    static final Switch<StringList> CONFIGURATION_FILES = new Switch<>(\"configFiles\",\n+            \"Comma-separated list of configuration datasources.\",\n+            value -> StringList.split(value, CommonConstants.COMMA), Optionality.OPTIONAL);\n+    static final Switch<String> CONFIGURATION_JSON = new Switch<>(\"configJson\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxODY3OA==", "bodyText": "How is this used?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384018678", "createdAt": "2020-02-25T17:26:42Z", "author": {"login": "danielduhh"}, "path": "src/main/java/org/openstreetmap/atlas/checks/flag/CheckFlag.java", "diffHunk": "@@ -48,18 +49,18 @@\n  * @author cuthbertm\n  * @author mgostintsev\n  * @author brian_l_davis\n+ * @author bbreithaupt\n  */\n public class CheckFlag implements Iterable<Location>, Located, Serializable\n {\n-    private static final long serialVersionUID = -1287808902452203852L;\n-    private static final Logger logger = LoggerFactory.getLogger(CheckFlag.class);\n-\n     private static final Distance TEN_METERS = Distance.meters(10);\n-\n-    private final String identifier;\n+    private static final String NULL_IDENTIFIERS = \"nullnull\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAyNTgwNA==", "bodyText": "multi -> multiAtlas", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384025804", "createdAt": "2020-02-25T17:38:59Z", "author": {"login": "danielduhh"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)\n+                .reduceByKey(UniqueCheckFlagContainer::combine)\n+                .foreach(processFlags(output, fileHelper, outputFormats));\n+\n+        logger.info(\"Sharded checks completed in {}\", start.elapsedSince());\n+    }\n+\n+    @Override\n+    protected SwitchList switches()\n+    {\n+        return super.switches().with(EXPANSION_DISTANCE, MULTI_ATLAS, SPARK_STORAGE_DISK_ONLY,\n+                SHARDING);\n+    }\n+\n+    /**\n+     * Get the fetcher to use for Atlas files. The fetcher uses a hadoop cache to reduce remote\n+     * reads.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param country\n+     *            {@link String} country code\n+     * @param configuration\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @return {@link Function} that fetches atlases/\n+     */\n+    private Function<Shard, Optional<Atlas>> atlasFetcher(final String input, final String country,\n+            final Map<String, String> configuration)\n+    {\n+        final HadoopAtlasFileCache cache = new HadoopAtlasFileCache(input, configuration);\n+        final AtlasResourceLoader loader = new AtlasResourceLoader();\n+        return (Function<Shard, Optional<Atlas>> & Serializable) shard -> cache.get(country, shard)\n+                .map(loader::load);\n+    }\n+\n+    /**\n+     * Process {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s through an event service to\n+     * produce output files.\n+     *\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param outputFormats\n+     *            {@link Set} of\n+     *            {@link org.openstreetmap.atlas.checks.distributed.IntegrityChecksCommandArguments.OutputFormats}\n+     * @return {@link VoidFunction} that takes a {@link Tuple2} of a {@link String} country code and\n+     *         a {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private VoidFunction<Tuple2<String, UniqueCheckFlagContainer>> processFlags(final String output,\n+            final SparkFileHelper fileHelper, final Set<OutputFormats> outputFormats)\n+    {\n+        return tuple ->\n+        {\n+            final String country = tuple._1();\n+            final UniqueCheckFlagContainer flagContainer = tuple._2();\n+            final EventService eventService = EventService.get(country);\n+\n+            if (outputFormats.contains(OutputFormats.FLAGS))\n+            {\n+                eventService.register(new CheckFlagFileProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_FLAG_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.GEOJSON))\n+            {\n+\n+                eventService.register(new CheckFlagGeoJsonProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_GEOJSON_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.TIPPECANOE))\n+            {\n+                eventService.register(new CheckFlagTippecanoeProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_TIPPECANOE_FOLDER, country)));\n+            }\n+\n+            flagContainer.reconstructEvents().parallel().forEach(eventService::post);\n+            eventService.complete();\n+        };\n+    }\n+\n+    /**\n+     * {@link PairFunction} to run each {@link ShardedCheckFlagsTask} through to produce\n+     * {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param configurationMap\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param sharding\n+     *            spark {@link Broadcast} of the current {@link Sharding}\n+     * @param shardDistanceExpansion\n+     *            {@link Distance} to expand the shard group\n+     * @param multi\n+     *            boolean whether to use a multi or dynamic Atlas\n+     * @return {@link PairFunction} that takes {@link ShardedCheckFlagsTask} and returns a\n+     *         {@link Tuple2} of a {@link String} country code and {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private PairFunction<ShardedCheckFlagsTask, String, UniqueCheckFlagContainer> produceFlags(\n+            final String input, final String output, final Map<String, String> configurationMap,\n+            final SparkFileHelper fileHelper, final Broadcast<Sharding> sharding,\n+            final Distance shardDistanceExpansion, final boolean multi)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 359}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAyNzUyOA==", "bodyText": "\ud83d\ude4f Thanks for adding this!", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384027528", "createdAt": "2020-02-25T17:42:00Z", "author": {"login": "danielduhh"}, "path": "docs/shardedchecks.md", "diffHunk": "@@ -0,0 +1,58 @@\n+# Sharded Atlas Checks", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY0MzYyNjU0", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-364362654", "createdAt": "2020-02-25T18:51:39Z", "commit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxODo1MTozOVrOFuQ_1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxOTozMDozNFrOFuSRww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1NzMwMw==", "bodyText": "Remove \"this\" keyword as COUNTRY_WHITELIST_TEMPLATE is a static variable.", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384057303", "createdAt": "2020-02-25T18:51:39Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/base/CheckResourceLoader.java", "diffHunk": "@@ -310,4 +313,20 @@ private boolean isEnabledByConfiguration(final Configuration configuration,\n         final String key = String.format(this.enabledKeyTemplate, checkClass.getSimpleName());\n         return configuration.get(key, this.enabledByDefault).value();\n     }\n+\n+    private boolean isEnabledByConfiguration(final Configuration configuration,\n+            final Class checkClass, final String country)\n+    {\n+        final List<String> countryWhitelist = configuration\n+                .get(String.format(this.COUNTRY_WHITELIST_TEMPLATE, checkClass.getSimpleName()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1NzYwNQ==", "bodyText": "Same as my previous comment.", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384057605", "createdAt": "2020-02-25T18:52:10Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/base/CheckResourceLoader.java", "diffHunk": "@@ -310,4 +313,20 @@ private boolean isEnabledByConfiguration(final Configuration configuration,\n         final String key = String.format(this.enabledKeyTemplate, checkClass.getSimpleName());\n         return configuration.get(key, this.enabledByDefault).value();\n     }\n+\n+    private boolean isEnabledByConfiguration(final Configuration configuration,\n+            final Class checkClass, final String country)\n+    {\n+        final List<String> countryWhitelist = configuration\n+                .get(String.format(this.COUNTRY_WHITELIST_TEMPLATE, checkClass.getSimpleName()),\n+                        Collections.emptyList())\n+                .value();\n+        final List<String> countryBlacklist = configuration\n+                .get(String.format(this.COUNTRY_BLACKLIST_TEMPLATE, checkClass.getSimpleName()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2MTgzNQ==", "bodyText": "@Bentleysb do you know which class adds the \"ItemId\" to the flag?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384061835", "createdAt": "2020-02-25T18:59:40Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/commands/AtlasChecksLogDiffSubCommand.java", "diffHunk": "@@ -78,19 +82,28 @@ public AtlasChecksLogDiffSubCommand()\n     }\n \n     /**\n-     * Get the atlas ids from an array of features.\n+     * Get the unique ids for a flag. Fall back to getting the atlas ids from the features for\n+     * reverse compatibility.\n      *\n-     * @param features\n-     *            a {@link JsonArray} of features\n+     * @param flagJson\n+     *            a {@link JsonObject} of a flag\n      * @return a {@link Set} of {@link String} ids\n      */\n-    private Set<String> getAtlasIdentifiers(final JsonArray features)\n+    private Set<String> getIdentifiers(final JsonObject flagJson)\n     {\n-        return Iterables.stream(features)\n-                .filter(object -> object.getAsJsonObject().get(PROPERTIES).getAsJsonObject()\n-                        .has(IDENTIFIER))\n-                .map(object -> object.getAsJsonObject().get(PROPERTIES).getAsJsonObject()\n-                        .get(IDENTIFIER).getAsString())\n-                .collectToSet();\n+        final JsonObject flagProperties = flagJson.get(PROPERTIES).getAsJsonObject();\n+        return flagProperties.has(IDENTIFIERS)\n+                ? Iterables.stream(flagProperties.get(IDENTIFIERS).getAsJsonArray())\n+                        .map(JsonElement::getAsString).collectToSet()\n+                : Iterables.stream(flagJson.get(FEATURES).getAsJsonArray()).filter(object -> object\n+                        .getAsJsonObject().get(PROPERTIES).getAsJsonObject().has(IDENTIFIER)\n+                        || object.getAsJsonObject().get(PROPERTIES).getAsJsonObject().has(\"ItemId\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzI3NA=="}, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NDU2NA==", "bodyText": "What are these warnings?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384064564", "createdAt": "2020-02-25T19:04:37Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -211,7 +133,7 @@ public String getName()\n         return \"Integrity Check Spark Job\";\n     }\n \n-    @SuppressWarnings({ \"rawtypes\" })\n+    @SuppressWarnings({ \"rawtypes\", \"unchecked\" })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NTM4NA==", "bodyText": "Why is this deprecated?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384065384", "createdAt": "2020-02-25T19:06:13Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob\n+{\n+    /**\n+     * @author brian_l_davis\n+     */\n+    protected enum OutputFormats\n+    {\n+        FLAGS,\n+        GEOJSON,\n+        METRICS,\n+        TIPPECANOE\n+    }\n+\n+    @Deprecated\n+    protected static final Switch<String> ATLAS_FOLDER = new Switch<>(\"inputFolder\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NjU5MA==", "bodyText": "nit: missing serialVersionUID", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384066590", "createdAt": "2020-02-25T19:08:30Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NDE5MQ==", "bodyText": "Couldn't we import this?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384074191", "createdAt": "2020-02-25T19:23:04Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagGeoJsonProcessor.java", "diffHunk": "@@ -18,11 +18,13 @@\n import com.google.gson.JsonObject;\n \n /**\n- * A {@link Processor} for {@link CheckFlagEvent}s to write them into GeoJson files\n+ * A {@link org.openstreetmap.atlas.event.Processor} for {@link CheckFlagEvent}s to write them into\n+ * GeoJson files\n  *\n  * @author brian_l_davis\n  */\n-public final class CheckFlagGeoJsonProcessor implements Processor<CheckFlagEvent>\n+public final class CheckFlagGeoJsonProcessor\n+        implements org.openstreetmap.atlas.event.Processor<CheckFlagEvent>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NDU1Mg==", "bodyText": "Why are these changes?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384074552", "createdAt": "2020-02-25T19:23:41Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagGeoJsonProcessor.java", "diffHunk": "@@ -111,7 +113,7 @@ public void process(final CheckFlagEvent event)\n \n     @Override\n     @Subscribe\n-    public void process(final ShutdownEvent event)\n+    public void process(final org.openstreetmap.atlas.event.ShutdownEvent event)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NTY1NQ==", "bodyText": "nit: Could we do a static import here?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384075655", "createdAt": "2020-02-25T19:25:45Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/flag/CheckFlag.java", "diffHunk": "@@ -451,6 +469,22 @@ public int hashCode()\n         return new MultiIterable<>(getShapes()).iterator();\n     }\n \n+    /**\n+     * Decouple the {@link CheckFlag} from any\n+     * {@link org.openstreetmap.atlas.geography.atlas.Atlas}s by making all the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3ODI3NQ==", "bodyText": "What are these changes?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384078275", "createdAt": "2020-02-25T19:30:34Z", "author": {"login": "sayas01"}, "path": "src/main/java/org/openstreetmap/atlas/checks/flag/FlaggedObject.java", "diffHunk": "@@ -17,14 +18,14 @@\n  */\n public abstract class FlaggedObject implements Serializable, Located\n {\n-    protected static final String COUNTRY_MISSING = \"NA\";\n     protected static final String AREA_TAG = \"Area\";\n+    protected static final String COUNTRY_MISSING = \"NA\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 14}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5df81546617e673ea2e312c314d6b01e6d0b9eb", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/a5df81546617e673ea2e312c314d6b01e6d0b9eb", "committedDate": "2020-02-25T19:37:23Z", "message": "clean up 2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef91ee49f8e3173833c52b0709658024619c0603", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/ef91ee49f8e3173833c52b0709658024619c0603", "committedDate": "2020-02-25T20:00:17Z", "message": "clean up 3"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "committedDate": "2020-02-25T20:03:12Z", "message": "clean up 4"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY1MzMyNTM0", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-365332534", "createdAt": "2020-02-27T00:04:29Z", "commit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QwMDowNDoyOVrOFvA14w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QwMToxMjo0MFrOFvCFwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0MTE4Nw==", "bodyText": "300 minutes, right?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384841187", "createdAt": "2020-02-27T00:04:29Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -153,35 +121,11 @@ private static void executeChecks(final String country, final Atlas atlas,\n     {\n         final Pool checkExecutionPool = new Pool(checksToRun.size(), \"Check execution pool\",\n                 POOL_DURATION_BEFORE_KILL);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDE1Mg=="}, "originalCommit": {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0NTUxOQ==", "bodyText": "I think you still need the serial number if IntegrityChecksCommandArguments has any subclasses, and we plan on reading/writing objects of those classes somewhere. But since I don't think we're doing that it should be fine for now\nSee https://stackoverflow.com/questions/893259/should-an-abstract-class-have-a-serialversionuid", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384845519", "createdAt": "2020-02-27T00:18:46Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NjU5MA=="}, "originalCommit": {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0ODI3Ng==", "bodyText": "If this condition fails we could log that we're skipping the file. Could be useful for debugging in case a shard file has a bad name and doesn't get loaded", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384848276", "createdAt": "2020-02-27T00:27:26Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob\n+{\n+    /**\n+     * @author brian_l_davis\n+     */\n+    protected enum OutputFormats\n+    {\n+        FLAGS,\n+        GEOJSON,\n+        METRICS,\n+        TIPPECANOE\n+    }\n+\n+    @Deprecated\n+    protected static final Switch<String> ATLAS_FOLDER = new Switch<>(\"inputFolder\",\n+            \"Path of folder which contains Atlas file(s)\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    protected static final String OUTPUT_ATLAS_FOLDER = \"atlas\";\n+    // Outputs\n+    protected static final String OUTPUT_FLAG_FOLDER = \"flag\";\n+    protected static final String OUTPUT_GEOJSON_FOLDER = \"geojson\";\n+    protected static final String OUTPUT_METRIC_FOLDER = \"metric\";\n+    protected static final String OUTPUT_TIPPECANOE_FOLDER = \"tippecanoe\";\n+    static final Switch<List<String>> CHECK_FILTER = new Switch<>(\"checkFilter\",\n+            \"Comma-separated list of checks to run\",\n+            checks -> Arrays.asList(checks.split(CommonConstants.COMMA)), Optionality.OPTIONAL);\n+    // Configuration\n+    static final Switch<StringList> CONFIGURATION_FILES = new Switch<>(\"configFiles\",\n+            \"Comma-separated list of configuration datasources.\",\n+            value -> StringList.split(value, CommonConstants.COMMA), Optionality.OPTIONAL);\n+    static final Switch<String> CONFIGURATION_JSON = new Switch<>(\"configJson\",\n+            \"Json formatted configuration.\", StringConverter.IDENTITY, Optionality.OPTIONAL);\n+    static final Switch<String> COUNTRIES = new Switch<>(\"countries\",\n+            \"Comma-separated list of country ISO3 codes to be processed\", StringConverter.IDENTITY,\n+            Optionality.REQUIRED);\n+    static final Switch<MapRouletteConfiguration> MAP_ROULETTE = new Switch<>(\"maproulette\",\n+            \"Map roulette server information, format <Host>:<Port>:<ProjectName>:<ApiKey>, projectName is optional.\",\n+            MapRouletteConfiguration::parse, Optionality.OPTIONAL);\n+    static final Switch<Set<OutputFormats>> OUTPUT_FORMATS = new Switch<>(\"outputFormats\",\n+            \"Comma-separated list of output formats (flags, metrics, geojson, tippecanoe).\",\n+            csvFormats -> Stream.of(csvFormats.split(\",\"))\n+                    .map(format -> Enum.valueOf(OutputFormats.class, format.toUpperCase()))\n+                    .collect(Collectors.toSet()),\n+            Optionality.OPTIONAL, \"flags,metrics\");\n+    static final Switch<Rectangle> PBF_BOUNDING_BOX = new Switch<>(\"pbfBoundingBox\",\n+            \"OSM protobuf data will be loaded only in this bounding box\", Rectangle::forString,\n+            Optionality.OPTIONAL);\n+    static final Switch<Boolean> PBF_SAVE_INTERMEDIATE_ATLAS = new Switch<>(\"savePbfAtlas\",\n+            \"Saves intermediate atlas files created when processing OSM protobuf data.\",\n+            Boolean::valueOf, Optionality.OPTIONAL, \"false\");\n+    private static final String ATLAS_FILENAME_PATTERN_FORMAT = \"^%s_([0-9]+)-([0-9]+)-([0-9]+)\";\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(IntegrityChecksCommandArguments.class);\n+\n+    /**\n+     * Creates a map from country name to {@link List} of {@link Shard} definitions from\n+     * {@link Atlas} files.\n+     *\n+     * @param countries\n+     *            Set of countries to find out shards for\n+     * @param pathResolver\n+     *            {@link AtlasFilePathResolver} to search for {@link Atlas} files\n+     * @param atlasFolder\n+     *            Path to {@link Atlas} folder\n+     * @param sparkContext\n+     *            Spark context (or configuration) as a key-value map\n+     * @return A map from country name to {@link List} of {@link Shard} definitions\n+     */\n+    public static MultiMap<String, Shard> countryShardMapFromShardFiles(final Set<String> countries,\n+            final AtlasFilePathResolver pathResolver, final String atlasFolder,\n+            final Map<String, String> sparkContext)\n+    {\n+        final MultiMap<String, Shard> countryShardMap = new MultiMap<>();\n+        logger.info(\"Building country shard map from country shard files.\");\n+\n+        countries.forEach(country ->\n+        {\n+            final String countryDirectory = pathResolver.resolvePath(atlasFolder, country);\n+            final CountrySpecificAtlasFilePathFilter atlasFilter = new CountrySpecificAtlasFilePathFilter(\n+                    country);\n+            final Pattern atlasFilePattern = Pattern\n+                    .compile(String.format(ATLAS_FILENAME_PATTERN_FORMAT, country));\n+\n+            // Go over shard files for the country and use file name pattern to find out shards\n+            FileSystemHelper.listResourcesRecursively(countryDirectory, sparkContext, atlasFilter)\n+                    .forEach(shardFile ->\n+                    {\n+                        final String shardFileName = shardFile.getName();\n+                        final Matcher matcher = atlasFilePattern.matcher(shardFileName);\n+                        if (matcher.find())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1MzQwNA==", "bodyText": "Since we're reading in the raw country string are we handling invalid country strings?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384853404", "createdAt": "2020-02-27T00:44:46Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NDY0NQ==", "bodyText": "What if we load shards for countries that are given in the configuration but have no checks in countryChecks?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384854645", "createdAt": "2020-02-27T00:48:44Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NTcyMw==", "bodyText": "I suppose this could happen if we specify a country in the config and a checkFilter but the checks are disabled in that country, so we'd want to exit", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384855723", "createdAt": "2020-02-27T00:52:10Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NDY0NQ=="}, "originalCommit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg2MTYzMw==", "bodyText": "Wondering how useful it'd be to have a configurable DynamicAtlasPolicy, or one defined in a file", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384861633", "createdAt": "2020-02-27T01:12:40Z", "author": {"login": "seancoulter"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)\n+                .reduceByKey(UniqueCheckFlagContainer::combine)\n+                .foreach(processFlags(output, fileHelper, outputFormats));\n+\n+        logger.info(\"Sharded checks completed in {}\", start.elapsedSince());\n+    }\n+\n+    @Override\n+    protected SwitchList switches()\n+    {\n+        return super.switches().with(EXPANSION_DISTANCE, MULTI_ATLAS, SPARK_STORAGE_DISK_ONLY,\n+                SHARDING);\n+    }\n+\n+    /**\n+     * Get the fetcher to use for Atlas files. The fetcher uses a hadoop cache to reduce remote\n+     * reads.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param country\n+     *            {@link String} country code\n+     * @param configuration\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @return {@link Function} that fetches atlases/\n+     */\n+    private Function<Shard, Optional<Atlas>> atlasFetcher(final String input, final String country,\n+            final Map<String, String> configuration)\n+    {\n+        final HadoopAtlasFileCache cache = new HadoopAtlasFileCache(input, configuration);\n+        final AtlasResourceLoader loader = new AtlasResourceLoader();\n+        return (Function<Shard, Optional<Atlas>> & Serializable) shard -> cache.get(country, shard)\n+                .map(loader::load);\n+    }\n+\n+    /**\n+     * Process {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s through an event service to\n+     * produce output files.\n+     *\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param outputFormats\n+     *            {@link Set} of\n+     *            {@link org.openstreetmap.atlas.checks.distributed.IntegrityChecksCommandArguments.OutputFormats}\n+     * @return {@link VoidFunction} that takes a {@link Tuple2} of a {@link String} country code and\n+     *         a {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private VoidFunction<Tuple2<String, UniqueCheckFlagContainer>> processFlags(final String output,\n+            final SparkFileHelper fileHelper, final Set<OutputFormats> outputFormats)\n+    {\n+        return tuple ->\n+        {\n+            final String country = tuple._1();\n+            final UniqueCheckFlagContainer flagContainer = tuple._2();\n+            final EventService eventService = EventService.get(country);\n+\n+            if (outputFormats.contains(OutputFormats.FLAGS))\n+            {\n+                eventService.register(new CheckFlagFileProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_FLAG_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.GEOJSON))\n+            {\n+\n+                eventService.register(new CheckFlagGeoJsonProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_GEOJSON_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.TIPPECANOE))\n+            {\n+                eventService.register(new CheckFlagTippecanoeProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_TIPPECANOE_FOLDER, country)));\n+            }\n+\n+            flagContainer.reconstructEvents().parallel().forEach(eventService::post);\n+            eventService.complete();\n+        };\n+    }\n+\n+    /**\n+     * {@link PairFunction} to run each {@link ShardedCheckFlagsTask} through to produce\n+     * {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param configurationMap\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param sharding\n+     *            spark {@link Broadcast} of the current {@link Sharding}\n+     * @param shardDistanceExpansion\n+     *            {@link Distance} to expand the shard group\n+     * @param multiAtlas\n+     *            boolean whether to use a multi or dynamic Atlas\n+     * @return {@link PairFunction} that takes {@link ShardedCheckFlagsTask} and returns a\n+     *         {@link Tuple2} of a {@link String} country code and {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private PairFunction<ShardedCheckFlagsTask, String, UniqueCheckFlagContainer> produceFlags(\n+            final String input, final String output, final Map<String, String> configurationMap,\n+            final SparkFileHelper fileHelper, final Broadcast<Sharding> sharding,\n+            final Distance shardDistanceExpansion, final boolean multiAtlas)\n+    {\n+        return task ->\n+        {\n+            // Get the atlas\n+            final Function<Shard, Optional<Atlas>> fetcher = this.atlasFetcher(input,\n+                    task.getCountry(), configurationMap);\n+            final Atlas atlas;\n+\n+            // Use dynamic or multi atlas (multi runs faster locally)\n+            if (multiAtlas)\n+            {\n+                atlas = new MultiAtlas(\n+                        StreamSupport\n+                                .stream(sharding.getValue()\n+                                        .shards(task.getShard().bounds()\n+                                                .expand(shardDistanceExpansion))\n+                                        .spliterator(), true)\n+                                .map(fetcher).filter(Optional::isPresent).map(Optional::get)\n+                                .collect(Collectors.toList()));\n+            }\n+            else\n+            {\n+                final DynamicAtlasPolicy policy = new DynamicAtlasPolicy(fetcher,\n+                        sharding.getValue(), Collections.singleton(task.getShard()),\n+                        task.getShard().bounds().expand(shardDistanceExpansion))\n+                                .withDeferredLoading(true).withAggressivelyExploreRelations(true)\n+                                .withExtendIndefinitely(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5"}, "originalPosition": 386}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0d2a7c70308b20a7cc15ef1c95654a474f6bb6c", "author": {"user": {"login": "danielduhh", "name": "Daniel B"}}, "url": "https://github.com/osmlab/atlas-checks/commit/e0d2a7c70308b20a7cc15ef1c95654a474f6bb6c", "committedDate": "2020-02-27T01:54:50Z", "message": "Merge branch 'dev' into shardedChecks-dynamic-noGroup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "baf9f93d9ccb5a2895dcea01631b53d8179c2fc0", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/baf9f93d9ccb5a2895dcea01631b53d8179c2fc0", "committedDate": "2020-02-27T17:48:44Z", "message": "clean up 5"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY1ODc0MjE5", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-365874219", "createdAt": "2020-02-27T17:55:00Z", "commit": {"oid": "baf9f93d9ccb5a2895dcea01631b53d8179c2fc0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NTowMVrOFvbREw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NTowMVrOFvbREw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NDEzMQ==", "bodyText": "One of @seancoulter's comments made me realize that there are a lot of soft failures used in the start method to short circuit execution of the new spark job. These were added in the original PR, so I had just left them alone. I am now wondering if it might be better to throw exceptions, to make it more evident why the job did not do anything. Does anyone have an opinion on this?", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385274131", "createdAt": "2020-02-27T17:55:01Z", "author": {"login": "Bentleysb"}, "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,435 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+    private static final long serialVersionUID = -8038802870994470017L;\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baf9f93d9ccb5a2895dcea01631b53d8179c2fc0"}, "originalPosition": 167}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8da8b39f52ea9d122b28c0b1e4084935efca9c73", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/8da8b39f52ea9d122b28c0b1e4084935efca9c73", "committedDate": "2020-02-28T04:41:22Z", "message": "code smells"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NTgxMzM0", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-366581334", "createdAt": "2020-02-28T17:59:39Z", "commit": {"oid": "8da8b39f52ea9d122b28c0b1e4084935efca9c73"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NjQ1OTg5", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-366645989", "createdAt": "2020-02-28T19:50:55Z", "commit": {"oid": "8da8b39f52ea9d122b28c0b1e4084935efca9c73"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324", "author": {"user": {"login": "Bentleysb", "name": "Bentley Breithaupt"}}, "url": "https://github.com/osmlab/atlas-checks/commit/bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324", "committedDate": "2020-02-28T19:55:27Z", "message": "hard fails"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NjUzOTkx", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-366653991", "createdAt": "2020-02-28T20:05:16Z", "commit": {"oid": "bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NzI0MTM0", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-366724134", "createdAt": "2020-02-28T22:31:46Z", "commit": {"oid": "bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NzI1NDEz", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-366725413", "createdAt": "2020-02-28T22:35:05Z", "commit": {"oid": "bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NzI1ODMy", "url": "https://github.com/osmlab/atlas-checks/pull/259#pullrequestreview-366725832", "createdAt": "2020-02-28T22:36:14Z", "commit": {"oid": "bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3281, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}