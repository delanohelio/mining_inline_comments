{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI1MDI0MDMz", "number": 484, "title": "Overload Failure Retry Implementation", "bodyText": "Overload detectors are developed to recognize CPU overload, near out of memory situations, and request pileups with increased latency. Once overload is detected, some amount of requests will be dropped by the framework before reaching application code and an HTTP/1.1 503 Service Unavailable error will be thrown.\nThis implementation provides a client retry mechanism that automatically retries certain requests if it is reasonable to do so.\nIn the case of overload failure, we must distinguish between two different situations:\nA. Only a single backend host or a small subset of backend hosts in the cluster are overloaded.\nB. A large subset of backend hosts in the cluster are overloaded.\nFor case A, it is very likely that the backend cluster has remaining capacity in other hosts to handle the request. Therefore, it is safe to retry the request immediately by sending the request to a different host in the same cluster.\nFor case B, the entire backend cluster is degraded. Retry will not only result in another overload failure, but also cause extra burden in the backend cluster. Therefore, the request should not be retried and the overload failure should bubble all the way up to the caller.\nOur approach is to make the retry decision on the server side by attaching a ServerRetryTracker into the ServerRetryFilter layer. It stores the number of requests categorized by number of retry attempts. It uses the information to estimate a ratio of how many requests are being retried in the cluster. The ratio is then compared with  {@link ServerRetryFilter#_maxRequestRetryRatio} to make a decision on whether or not to retry in the next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum} intervals by aggregating the recorded requests.\nThis simulator for the ServerRetryTracker and ServerRetryFilter can be found here: #483", "createdAt": "2020-11-21T00:32:50Z", "url": "https://github.com/linkedin/rest.li/pull/484", "merged": true, "mergeCommit": {"oid": "bda98704c4cbfd940a41c5519deee0837124736e"}, "closed": true, "closedAt": "2020-12-23T10:05:59Z", "author": {"login": "rickzx"}, "timelineItems": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdef3MBAH2gAyNTI1MDI0MDMzOjk1M2U3NWNlMzQwNTA1NDBiMzRjYmU5Y2EyNzFlMmMwMGRhOGEzODM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdo7aeDgH2gAyNTI1MDI0MDMzOmZmYjY2ZTVhNjE4ZGI0ZTM1NzE4M2Y5NDZjZjA5YzUzYmRlOGM0NTk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/953e75ce34050540b34cbe9ca271e2c00da8a383", "committedDate": "2020-11-20T23:22:18Z", "message": "Overload Failure Retry Implementation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2OTg1Mzgy", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-536985382", "createdAt": "2020-11-24T01:30:16Z", "commit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwMTozMDoxN1rOH4mSbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwMTo1OTozNlrOH4nXbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwOTYxNA==", "bodyText": "Should we break the loop once it is true?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529109614", "createdAt": "2020-11-24T01:30:17Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +330,33 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+      boolean hasRetriableRequestException = false;\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          hasRetriableRequestException = true;\n+\n+          if (((RetriableRequestException) throwable).getDoNotRetryOverride())\n+          {\n+            return false;\n+          }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMjY1NA==", "bodyText": "I kind of forgot what's the granularity of the D2Client, if a host is making calls to 50 different downstream d2Services, does it use one instance of D2Client or 50 instances of D2Clients?\nThe further questions is that what's  the granularity of the client side retry ratio? Is it one ratio for each d2Service, or a downstream service can be penalized by another downstream?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529112654", "createdAt": "2020-11-24T01:39:50Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/D2ClientBuilder.java", "diffHunk": "@@ -146,6 +146,8 @@ public D2Client build()\n                   _config._executorService,\n                   _config.retry,\n                   _config.retryLimit,\n+                  _config.maxClientRequestRetryRatio,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMzg4Nw==", "bodyText": "A follow up question is that should this be a client-defined ratio or server-defined ratio in lps d2, pros and cons?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529113887", "createdAt": "2020-11-24T01:43:42Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/D2ClientBuilder.java", "diffHunk": "@@ -146,6 +146,8 @@ public D2Client build()\n                   _config._executorService,\n                   _config.retry,\n                   _config.retryLimit,\n+                  _config.maxClientRequestRetryRatio,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMjY1NA=="}, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExNzcwNw==", "bodyText": "Let's add java doc for this, so that we know what this override means", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529117707", "createdAt": "2020-11-24T01:49:33Z", "author": {"login": "rachelhanhan"}, "path": "r2-core/src/main/java/com/linkedin/r2/RetriableRequestException.java", "diffHunk": "@@ -66,4 +68,14 @@ public RetriableRequestException(Throwable cause)\n   {\n     super(cause);\n   }\n+\n+  public void setDoNotRetryOverride(boolean doNotRetryOverride)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEyNzI3Nw==", "bodyText": "Should we only obtain the lock if we hit the update time? Otherwise we obtain the lock on each individual request", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529127277", "createdAt": "2020-11-24T01:59:36Z", "author": {"login": "rachelhanhan"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private int getRetryAttempts(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    return retryAttemptsHeader == null ? 0 : Integer.parseInt(retryAttemptsHeader);\n+  }\n+\n+  private void updateRetryDecision()\n+  {\n+    long currentTime = _clock.currentTimeMillis();\n+\n+    synchronized (_lock)\n+    {\n+      // Check if the current interval is stale\n+      if (currentTime >= _lastRollOverTime + _updateIntervalMs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3NzE3NjQ3", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-537717647", "createdAt": "2020-11-24T16:55:01Z", "commit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxNjo1NTowMlrOH5MJaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMzoxNDowMVrOH5b9MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTcyOTg5Nw==", "bodyText": "More readable IMO:\npublic static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS(5).toMillis();", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529729897", "createdAt": "2020-11-24T16:55:02Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +62,39 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = 5000L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2Mzk4Ng==", "bodyText": "If the number of retry attempts is zero can we omit this header? Same for below.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529763986", "createdAt": "2020-11-24T17:45:57Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -154,11 +216,14 @@ public void onSuccess(ByteString result)\n     }\n \n     @Override\n-    public boolean doRetryRequest(StreamRequest request, RequestContext context)\n+    public boolean doRetryRequest(StreamRequest request, RequestContext context, int numberOfRetryAttempts)\n     {\n       if (_recorded == true && _content != null)\n       {\n-        final StreamRequest newRequest = request.builder().build(EntityStreams.newEntityStream(new ByteStringWriter(_content)));\n+        final StreamRequest newRequest = request.builder()\n+            .addHeaderValue(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS, Integer.toString(numberOfRetryAttempts))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2NTgxNw==", "bodyText": "Would suggest changing \"happens\" to happened or occurred", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529765817", "createdAt": "2020-11-24T17:48:51Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -234,9 +303,16 @@ public void onError(Throwable e)\n             int attempts = exclusionSet.size();\n             if (attempts <= _limit)\n             {\n-              LOG.warn(\"A retriable exception happens. Going to retry. This is attempt {}. Current exclusion set: \",\n-                  attempts, \". Current exclusion set: \" + exclusionSet);\n-              retry = doRetryRequest(_request, _context);\n+              if (isBelowClientRetryRatio())\n+              {\n+                LOG.warn(\"A retriable exception happens. Going to retry. This is attempt {}. Current exclusion set: {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3MjM5Nw==", "bodyText": "Or just return true if the override isn't present", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529772397", "createdAt": "2020-11-24T17:59:16Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +330,33 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+      boolean hasRetriableRequestException = false;\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          hasRetriableRequestException = true;\n+\n+          if (((RetriableRequestException) throwable).getDoNotRetryOverride())\n+          {\n+            return false;\n+          }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwOTYxNA=="}, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5MTAzNA==", "bodyText": "You're modifying the exception here, but not doing anything with it. Is this supposed to call nextFilter.onError?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529791034", "createdAt": "2020-11-24T18:28:24Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ClientRetryFilter.java", "diffHunk": "@@ -73,9 +74,13 @@ public void onStreamError(Throwable ex,\n     {\n       nextFilter.onError(new RetriableRequestException(retryAttr), requestContext, wireAttrs);\n     }\n-    else\n-    {\n-      nextFilter.onError(ex, requestContext, wireAttrs);\n+    else {\n+      Throwable[] throwables = ExceptionUtils.getThrowables(ex);\n+      for (Throwable throwable : throwables) {\n+        if (throwable instanceof RetriableRequestException) {\n+          ((RetriableRequestException) throwable).setDoNotRetryOverride(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5MTk2Ng==", "bodyText": "Consider TimeUnit.SECONDS.toMillis(5)", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529791966", "createdAt": "2020-11-24T18:29:56Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -45,6 +49,57 @@\n public class ServerRetryFilter implements RestFilter, StreamFilter\n {\n   private static final Logger LOG = LoggerFactory.getLogger(ServerRetryFilter.class);\n+  private static final int DEFAULT_RETRY_LIMIT = 3;\n+  private static final long DEFAULT_UPDATE_INTERVAL_MS = 5000L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNjA3OQ==", "bodyText": "We're going to need to think about how to roll this out safely when not all clients have been updated. For example if we have an overload filter and change it to use Retriable exceptions the clients that are on this new version will behave correctly, but any that are on an older version will blindly retry without having a concept of the client retry ratio (and not providing data to the server about often it has had to retry).\nOne option is to use the presence of the number of retry attempts header as an indicator that the client is participating in the retry tracking (and so my earlier suggestion of omitting the header if the value is zero won't work). In that case an overload filter would not want to use the retriable exception unless it knew the client was up to date.\nIt would be nice to centralize that logic here. We could add a flag to RetriableRequestException, or use a different exception to indicate that the client should only retry if it's up to date.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529826079", "createdAt": "2020-11-24T19:28:58Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTg2MzI3NA==", "bodyText": "If the header is absent should it be counted as zero? This depends on whether we're omitting the header if it's zero or not. If we're not omitting it (and it's looking like we might need to include it to know if the client is participating) then I think we should ignore requests that don't have the header. Otherwise we can get a very skewed version of the situation where clients that aren't tracking things are counted as reporting zero, when they really don't have the data at all and therefore shouldn't be considered in calculating the ratio.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529863274", "createdAt": "2020-11-24T20:38:30Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private int getRetryAttempts(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    return retryAttemptsHeader == null ? 0 : Integer.parseInt(retryAttemptsHeader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk4ODQ0Ng==", "bodyText": "The concurrency safety of this class is a little hard to follow currently. It looks like things should be safe, because you are using this and rollOverStats while holding a lock, and add and rollOverStats are synchronized and should be safe to use together. It would be nice to have the safety be much clearer though.\nOne option would be to make this synchronized for clarity. This will introduce contention with calls to add, but the method is small and should execute quickly, and not very frequently. Another option would be to consolidate this into a rolloverStatsAsNeededAndGetRetryRatio where this class can manage the thread safety and use separate locks to guard the list and the array. Finally you could just document the current behavior with comments.\nWhichever way you choose I'd suggest annotating the class with @ThreadSafe or @NotThreadSafe, and annotate the fields accessed concurrently with @GuardedBy.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529988446", "createdAt": "2020-11-24T23:13:30Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private int getRetryAttempts(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    return retryAttemptsHeader == null ? 0 : Integer.parseInt(retryAttemptsHeader);\n+  }\n+\n+  private void updateRetryDecision()\n+  {\n+    long currentTime = _clock.currentTimeMillis();\n+\n+    synchronized (_lock)\n+    {\n+      // Check if the current interval is stale\n+      if (currentTime >= _lastRollOverTime + _updateIntervalMs)\n+      {\n+          // Rollover stale intervals until the current interval is reached\n+          for (long time = currentTime; time >= _lastRollOverTime + _updateIntervalMs; time -= _updateIntervalMs)\n+          {\n+            _serverRetryTracker.rollOverStats();\n+          }\n+\n+          _doNotRetry = _serverRetryTracker.getRetryRatio() > _maxRequestRetryRatio;\n+          _lastRollOverTime = currentTime;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryFilter#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker\n+  {\n+    private final int _retryLimit;\n+    private final int _aggregatedIntervalNum;\n+\n+    private final LinkedList<int[]> _retryAttemptsCounter;\n+    private final int[] _aggregatedRetryAttemptsCounter;\n+\n+    private ServerRetryTracker(int retryLimit, int aggregatedIntervalNum)\n+    {\n+      _retryLimit = retryLimit;\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+\n+      _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+      _retryAttemptsCounter = new LinkedList<>();\n+      _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+    }\n+\n+    public synchronized void add(int numberOfRetryAttempts)\n+    {\n+      if (numberOfRetryAttempts <= _retryLimit)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      } else\n+      {\n+        LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      }\n+    }\n+\n+    public synchronized void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      int[] intervalToAggregate = _retryAttemptsCounter.getLast();\n+      for (int i = 0; i < _retryLimit; i++)\n+      {\n+        _aggregatedRetryAttemptsCounter[i] += intervalToAggregate[i];\n+      }\n+\n+      if (_retryAttemptsCounter.size() > _aggregatedIntervalNum)\n+      {\n+        // discard the oldest interval\n+        int[] intervalToDiscard = _retryAttemptsCounter.removeFirst();\n+        for (int i = 0; i < _retryLimit; i++)\n+        {\n+          _aggregatedRetryAttemptsCounter[i] -= intervalToDiscard[i];\n+        }\n+      }\n+\n+      // append a new interval\n+      _retryAttemptsCounter.addLast(new int[_retryLimit + 1]);\n+    }\n+\n+    public double getRetryRatio()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk4ODkxMw==", "bodyText": "Suggest changing \"Retries\" to \"Retry\"", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529988913", "createdAt": "2020-11-24T23:14:01Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/transport/http/common/HttpConstants.java", "diffHunk": "@@ -11,6 +11,11 @@\n    */\n   public static final String HEADER_RESPONSE_COMPRESSION_THRESHOLD = \"X-Response-Compression-Threshold\";\n \n+  /**\n+   * Custom header for the number of retries.\n+   */\n+  public static final String HEADER_NUMBER_OF_RETRY_ATTEMPTS = \"X-Number-Of-Retries-Attempts\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383"}, "originalPosition": 7}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95d213301357489f6945bb522dc19a185ce3e8c0", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/95d213301357489f6945bb522dc19a185ce3e8c0", "committedDate": "2020-11-30T18:38:53Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}, "afterCommit": {"oid": "0f4f1177c51323e20c88dfe4226a87f5a55b2ec8", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/0f4f1177c51323e20c88dfe4226a87f5a55b2ec8", "committedDate": "2020-11-30T18:48:08Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0f4f1177c51323e20c88dfe4226a87f5a55b2ec8", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/0f4f1177c51323e20c88dfe4226a87f5a55b2ec8", "committedDate": "2020-11-30T18:48:08Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}, "afterCommit": {"oid": "fae5f6003083cb92f2fc6fc480ee3a86f038946b", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/fae5f6003083cb92f2fc6fc480ee3a86f038946b", "committedDate": "2020-11-30T19:04:03Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "committedDate": "2020-11-30T19:39:46Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fae5f6003083cb92f2fc6fc480ee3a86f038946b", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/fae5f6003083cb92f2fc6fc480ee3a86f038946b", "committedDate": "2020-11-30T19:04:03Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}, "afterCommit": {"oid": "05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "committedDate": "2020-11-30T19:39:46Z", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "committedDate": "2020-12-05T04:38:21Z", "message": "Add retryCount and retryCountTotal to CallTracker. Make RetryClient use the new CallTracker"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ2NTUyMTU1", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-546552155", "createdAt": "2020-12-07T21:30:04Z", "commit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMTozMDowNVrOIA7jqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMTo0OTo0NlrOIA8RAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NjY5Ng==", "bodyText": "I saw that when we invoke CallTracker.startCall, it's doing a lot more than just counting retry count and total call count, it's also counts the latency, outstanding latency and other stuff. Is this a little bit heavy for retry ratio? Especially that the CallTracker is used again in TrackerClient to track each server host's performance, this may defeat the purpose of CallTracker.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537846696", "createdAt": "2020-12-07T21:30:05Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +75,31 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS.toMillis(5);\n   private static final Logger LOG = LoggerFactory.getLogger(RetryClient.class);\n \n+  private final Clock _clock;\n+  private final LoadBalancer _balancer;\n   private final int _limit;\n+  private final long _updateIntervalMs;\n \n-  public RetryClient(D2Client d2Client, int limit)\n+  Map<String, CallTracker> _retryTrackerMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0ODIxOA==", "bodyText": "Break this loop if one throwable is retriable?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537848218", "createdAt": "2020-12-07T21:32:42Z", "author": {"login": "rachelhanhan"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ClientRetryFilter.java", "diffHunk": "@@ -73,8 +74,13 @@ public void onStreamError(Throwable ex,\n     {\n       nextFilter.onError(new RetriableRequestException(retryAttr), requestContext, wireAttrs);\n     }\n-    else\n-    {\n+    else {\n+      Throwable[] throwables = ExceptionUtils.getThrowables(ex);\n+      for (Throwable throwable : throwables) {\n+        if (throwable instanceof RetriableRequestException) {\n+          ((RetriableRequestException) throwable).setDoNotRetryOverride(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1ODMwNQ==", "bodyText": "I'm a little bit confused, what's the purpose of tracking # of retries in TrackerClient here? Each trackerClient corresponds to one server host, do we need to track retry count on server host basis?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537858305", "createdAt": "2020-12-07T21:49:46Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/TrackerClientImpl.java", "diffHunk": "@@ -307,4 +309,17 @@ else if (throwable instanceof StreamException)\n     }\n     return false;\n   }\n+\n+  private CallCompletion startCall(Request request)\n+  {\n+    String retryHeader = request.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryHeader != null && Integer.parseInt(retryHeader) > 0)\n+    {\n+      return _callTracker.startCall(true);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ2Njc1ODM2", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-546675836", "createdAt": "2020-12-08T01:30:30Z", "commit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMTozMDozMFrOIBCoZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNzowMToyN1rOICfXvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk2MjU5Nw==", "bodyText": "I'm not that familiar with the different abstraction levels of D2 and if this client could be used by more than one thread. If so this should be a ConcurrentHashMap and for clarity the declared type should be ConcurrentMap", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537962597", "createdAt": "2020-12-08T01:30:30Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +75,31 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS.toMillis(5);\n   private static final Logger LOG = LoggerFactory.getLogger(RetryClient.class);\n \n+  private final Clock _clock;\n+  private final LoadBalancer _balancer;\n   private final int _limit;\n+  private final long _updateIntervalMs;\n \n-  public RetryClient(D2Client d2Client, int limit)\n+  Map<String, CallTracker> _retryTrackerMap;\n+\n+  public RetryClient(D2Client d2Client, LoadBalancer balancer, int limit)\n+  {\n+    this(d2Client, balancer, limit, DEFAULT_UPDATE_INTERVAL_MS, SystemClock.instance());\n+  }\n+\n+  public RetryClient(D2Client d2Client, LoadBalancer balancer, int limit, long updateIntervalMs, Clock clock)\n   {\n     super(d2Client);\n+    _balancer = balancer;\n     _limit = limit;\n-    LOG.debug(\"Retry client created with limit set to: \", _limit);\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+    _retryTrackerMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk3NzQ5Mw==", "bodyText": "I agree with Ruxin's other comment that CallTracker seems pretty heavyweight here. I also don't think it will handle the data in the way that you want. It's meant for reporting stats at intervals, but AFAIK can't report across intervals which I think is called for here. For example if the interval is 10s and we're near the end of the current interval at 8s that means the stats are from a window that started 18s ago (I believe it only reports on data from an interval that has completed, not the current interval). But we want to know how many retry attempts there were in the last 10s right? I believe the CallTracker will only give you data in the context of a rotating window, not a sliding one. I would think we would want this data to be as up to date as possible.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537977493", "createdAt": "2020-12-08T02:07:48Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +75,31 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS.toMillis(5);\n   private static final Logger LOG = LoggerFactory.getLogger(RetryClient.class);\n \n+  private final Clock _clock;\n+  private final LoadBalancer _balancer;\n   private final int _limit;\n+  private final long _updateIntervalMs;\n \n-  public RetryClient(D2Client d2Client, int limit)\n+  Map<String, CallTracker> _retryTrackerMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk4ODY5Nw==", "bodyText": "It looks like getLoadBalancedServiceProperties without providing a callback is deprecated which may be because it makes a call out to get the data? If so maybe this should be cached and not called on each error.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537988697", "createdAt": "2020-12-08T02:39:09Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -231,12 +302,34 @@ public void onError(Throwable e)\n           }\n           else\n           {\n+            double maxClientRequestRetryRatio;\n+            try\n+            {\n+              Map<String, Object> transportClientProperties =\n+                  _balancer.getLoadBalancedServiceProperties(_serviceName).getTransportClientProperties();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk5MTIxOA==", "bodyText": "If retry is false we will be calling endCall here and also endCallWithError below which doesn't seem right.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537991218", "createdAt": "2020-12-08T02:45:53Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -231,12 +302,34 @@ public void onError(Throwable e)\n           }\n           else\n           {\n+            double maxClientRequestRetryRatio;\n+            try\n+            {\n+              Map<String, Object> transportClientProperties =\n+                  _balancer.getLoadBalancedServiceProperties(_serviceName).getTransportClientProperties();\n+              maxClientRequestRetryRatio = MapUtil.getWithDefault(transportClientProperties, PropertyKeys.HTTP_MAX_CLIENT_REQUEST_RETRY_RATIO,\n+                  HttpClientFactory.DEFAULT_MAX_CLIENT_REQUEST_RETRY_RATIO, Double.class);\n+            } catch (ServiceUnavailableException ex)\n+            {\n+              LOG.warn(\"Failed to fetch transportClientProperties \", ex);\n+              maxClientRequestRetryRatio = HttpClientFactory.DEFAULT_MAX_CLIENT_REQUEST_RETRY_RATIO;\n+            }\n+\n             int attempts = exclusionSet.size();\n             if (attempts <= _limit)\n             {\n-              LOG.warn(\"A retriable exception happens. Going to retry. This is attempt {}. Current exclusion set: \",\n-                  attempts, \". Current exclusion set: \" + exclusionSet);\n-              retry = doRetryRequest(_request, _context);\n+              CallTracker callTracker = _retryTrackerMap.get(_serviceName);\n+              if (callTracker.getCallStats().getRetryRate() <= maxClientRequestRetryRatio)\n+              {\n+                LOG.warn(\"A retriable exception occurred. Going to retry. This is attempt {}. Current exclusion set: {}\",\n+                    attempts, exclusionSet);\n+                _callCompletion.endCall();\n+                retry = doRetryRequest(_request, _context, attempts);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODA5Njk2Ng==", "bodyText": "As noted above this doesn't seem like the best fit for a metrics class to use. It also seems like a poor fit based on the changes needed to the interface here and associated class. This is a fairly generic type that also happens to mirror an internal LI class and interface. Adding something specific to the retry use case seems like a design smell, and diverging from the internal implementation also should be avoided if possible IMO.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r538096966", "createdAt": "2020-12-08T07:30:03Z", "author": {"login": "bbarkley"}, "path": "degrader/src/main/java/com/linkedin/util/degrader/CallTracker.java", "diffHunk": "@@ -115,11 +121,18 @@\n   void trackCallWithError(long duration);\n \n   /**\n-   * Indicates the start of a method invocation\n+   * Indicates the start of a non-retry method invocation\n    * @return an object that can be used to indicate completion of the call\n    */\n   CallCompletion startCall();\n \n+  /**\n+   * Indicates the start of a method invocation\n+   * @param isRetry whether the call is a retry\n+   * @return an object that can be used to indicate completion of the call\n+   */\n+  CallCompletion startCall(boolean isRetry);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ1ODk1MQ==", "bodyText": "This class has enough logic and edge cases that it should have a decent set of unit tests IMO, and potentially be pulled out to a top level class.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539458951", "createdAt": "2020-12-09T16:33:47Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -75,13 +116,160 @@ public void onStreamError(Throwable ex,\n       if (cause instanceof RetriableRequestException)\n       {\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_serverRetryTracker.isBelowRetryRatio())\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n+        else\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private void updateRetryTracker(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryAttemptsHeader != null)\n+    {\n+      _serverRetryTracker.add(Integer.parseInt(retryAttemptsHeader));\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTI3OQ==", "bodyText": "Is this necessary to do here? The add call happening on each request with the header will be doing this already, so I would think it would only be needed if the majority of clients haven't updated (so that their interval data will be current if there aren't recent requests coming in from updated clients), or if there is a concern that there aren't many requests coming in, and they are very long running, in which case the initial state calculated by add will be out of date.\nIf the first issue is the concern I'd suggest pulling this up to do conditionally in processError if the header wasn't present.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539479279", "createdAt": "2020-12-09T16:58:14Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -75,13 +116,160 @@ public void onStreamError(Throwable ex,\n       if (cause instanceof RetriableRequestException)\n       {\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_serverRetryTracker.isBelowRetryRatio())\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n+        else\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private void updateRetryTracker(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryAttemptsHeader != null)\n+    {\n+      _serverRetryTracker.add(Integer.parseInt(retryAttemptsHeader));\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker\n+  {\n+    private final int _retryLimit;\n+    private final int _aggregatedIntervalNum;\n+    private final double _maxRequestRetryRatio;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private boolean _isBelowRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<int[]> _retryAttemptsCounter;\n+    private final int[] _aggregatedRetryAttemptsCounter;\n+\n+    private ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+    {\n+      _retryLimit = retryLimit;\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _maxRequestRetryRatio = maxRequestRetryRatio;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+\n+      _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+      _retryAttemptsCounter = new LinkedList<>();\n+      _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+    }\n+\n+    public void add(int numberOfRetryAttempts)\n+    {\n+      if (numberOfRetryAttempts <= _retryLimit)\n+      {\n+        synchronized (_counterLock)\n+        {\n+          _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+        }\n+      } else\n+      {\n+        LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      }\n+\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        int[] intervalToAggregate = _retryAttemptsCounter.getLast();\n+        for (int i = 0; i < _retryLimit; i++)\n+        {\n+          _aggregatedRetryAttemptsCounter[i] += intervalToAggregate[i];\n+        }\n+\n+        if (_retryAttemptsCounter.size() > _aggregatedIntervalNum)\n+        {\n+          // discard the oldest interval\n+          int[] intervalToDiscard = _retryAttemptsCounter.removeFirst();\n+          for (int i = 0; i < _retryLimit; i++)\n+          {\n+            _aggregatedRetryAttemptsCounter[i] -= intervalToDiscard[i];\n+          }\n+        }\n+\n+        // append a new interval\n+        _retryAttemptsCounter.addLast(new int[_retryLimit + 1]);\n+      }\n+    }\n+\n+    public boolean isBelowRetryRatio()\n+    {\n+      updateRetryDecision();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4MjA0Ng==", "bodyText": "Is this protecting against malicious clients? Why would we be getting a value that is above the limit? If something is misconfigured and clients are trying more than they should I would think we would want to track this as another entry for the max number of retries that are allowed instead of ignoring it.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539482046", "createdAt": "2020-12-09T17:01:27Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -75,13 +116,160 @@ public void onStreamError(Throwable ex,\n       if (cause instanceof RetriableRequestException)\n       {\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_serverRetryTracker.isBelowRetryRatio())\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n+        else\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private void updateRetryTracker(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryAttemptsHeader != null)\n+    {\n+      _serverRetryTracker.add(Integer.parseInt(retryAttemptsHeader));\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker\n+  {\n+    private final int _retryLimit;\n+    private final int _aggregatedIntervalNum;\n+    private final double _maxRequestRetryRatio;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private boolean _isBelowRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<int[]> _retryAttemptsCounter;\n+    private final int[] _aggregatedRetryAttemptsCounter;\n+\n+    private ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+    {\n+      _retryLimit = retryLimit;\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _maxRequestRetryRatio = maxRequestRetryRatio;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+\n+      _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+      _retryAttemptsCounter = new LinkedList<>();\n+      _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+    }\n+\n+    public void add(int numberOfRetryAttempts)\n+    {\n+      if (numberOfRetryAttempts <= _retryLimit)\n+      {\n+        synchronized (_counterLock)\n+        {\n+          _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+        }\n+      } else\n+      {\n+        LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8"}, "originalPosition": 143}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "08a310abb23548bed7c1b69d649638592331e26b", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/08a310abb23548bed7c1b69d649638592331e26b", "committedDate": "2020-12-10T06:23:33Z", "message": "Update tracking logic in RetryClient and ServerRetryFilter"}, "afterCommit": {"oid": "dc79c0124df82abb853b452fe4807139a6092e12", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/dc79c0124df82abb853b452fe4807139a6092e12", "committedDate": "2020-12-10T09:27:30Z", "message": "Update tracking logic in RetryClient and ServerRetryFilter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/180dc7f6631ec3375e02cefc68e39f7f723877be", "committedDate": "2020-12-10T09:30:09Z", "message": "Update tracking logic in RetryClient and ServerRetryFilter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dc79c0124df82abb853b452fe4807139a6092e12", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/dc79c0124df82abb853b452fe4807139a6092e12", "committedDate": "2020-12-10T09:27:30Z", "message": "Update tracking logic in RetryClient and ServerRetryFilter"}, "afterCommit": {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/180dc7f6631ec3375e02cefc68e39f7f723877be", "committedDate": "2020-12-10T09:30:09Z", "message": "Update tracking logic in RetryClient and ServerRetryFilter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NDY3MTA3", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-549467107", "createdAt": "2020-12-10T18:04:29Z", "commit": {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxODowNDoyOVrOIDWdOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxODo1NDozOVrOIDYb1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM4NDU3MQ==", "bodyText": "When you get the _transportClientProperties in async manner, you will not immediately obtain the value. After you invoke _balancer.getLoadBalancedServiceProperties asynchronously, I think when you execute to line 312, for sure it will be null, we should notify a callback instead.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540384571", "createdAt": "2020-12-10T18:04:29Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -231,12 +296,43 @@ public void onError(Throwable e)\n           }\n           else\n           {\n+            double maxClientRequestRetryRatio;\n+            _balancer.getLoadBalancedServiceProperties(_serviceName, new Callback<ServiceProperties>() {\n+              @Override\n+              public void onError(Throwable e) {\n+                LOG.warn(\"Failed to fetch transportClientProperties \", e);\n+              }\n+\n+              @Override\n+              public void onSuccess(ServiceProperties result) {\n+                _transportClientProperties = result.getTransportClientProperties();\n+              }\n+            });\n+\n+            if (_transportClientProperties == null)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM5MzAxOA==", "bodyText": "I feel using int[2] to track total count and retry count is a bit implicit without java docs, can we use a new data structure to represent the count?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540393018", "createdAt": "2020-12-10T18:17:12Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +350,134 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private static class ClientRetryTracker\n+  {\n+    private static final int COUNTER_TOTAL_COUNT_INDEX = 0;\n+    private static final int COUNTER_RETRY_COUNT_INDEX = 1;\n+\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<int[]> _retryCounter;\n+    private final int[] _aggregatedRetryCounter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be"}, "originalPosition": 290}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQxNjk4Mg==", "bodyText": "Wondering why isn't this line under the counterlock?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540416982", "createdAt": "2020-12-10T18:54:39Z", "author": {"login": "rachelhanhan"}, "path": "r2-core/src/main/java/com/linkedin/r2/util/ServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.util.clock.Clock;\n+import java.util.LinkedList;\n+import org.checkerframework.checker.lock.qual.GuardedBy;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+ * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+ * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+ * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+ * intervals by aggregating the recorded requests.\n+ */\n+public class ServerRetryTracker\n+{\n+  private static final Logger LOG = LoggerFactory.getLogger(ServerRetryTracker.class);\n+  private final int _retryLimit;\n+  private final int _aggregatedIntervalNum;\n+  private final double _maxRequestRetryRatio;\n+  private final long _updateIntervalMs;\n+  private final Clock _clock;\n+\n+  private final Object _counterLock = new Object();\n+  private final Object _updateLock = new Object();\n+\n+  @GuardedBy(\"_updateLock\")\n+  private volatile long _lastRollOverTime;\n+  private boolean _isBelowRetryRatio;\n+\n+  @GuardedBy(\"_counterLock\")\n+  private final LinkedList<int[]> _retryAttemptsCounter;\n+  private final int[] _aggregatedRetryAttemptsCounter;\n+\n+  public ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+  {\n+    _retryLimit = retryLimit;\n+    _aggregatedIntervalNum = aggregatedIntervalNum;\n+    _maxRequestRetryRatio = maxRequestRetryRatio;\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+\n+    _lastRollOverTime = clock.currentTimeMillis();\n+    _isBelowRetryRatio = true;\n+\n+    _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+    _retryAttemptsCounter = new LinkedList<>();\n+    _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+  }\n+\n+  public void add(int numberOfRetryAttempts)\n+  {\n+    if (numberOfRetryAttempts <= _retryLimit)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      }\n+    } else\n+    {\n+      LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      _retryAttemptsCounter.getLast()[_retryLimit] += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be"}, "originalPosition": 80}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/daeaece4e560b0c27feaa4a62404cdf7700fa0d5", "committedDate": "2020-12-12T04:26:05Z", "message": "Change isBelowRetryRatio to async call. Other minor fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUxNzczNjg4", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-551773688", "createdAt": "2020-12-14T18:25:01Z", "commit": {"oid": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQxODoyNTowMVrOIFeTmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQxODoyODoxMFrOIFeiRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjYxMDMzMA==", "bodyText": "minor: this line seems redundant", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542610330", "createdAt": "2020-12-14T18:25:01Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +338,201 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;\n+\n+    private ClientRetryTracker(int aggregatedIntervalNum, long updateIntervalMs, Clock clock, String serviceName)\n+    {\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+      _serviceName = serviceName;\n+\n+      _lastRollOverTime = clock.currentTimeMillis();\n+      _currentAggregatedRetryRatio = 0;\n+\n+      _aggregatedRetryCounter = new RetryCounter();\n+      _retryCounter = new LinkedList<>();\n+      _retryCounter.add(new RetryCounter());\n+    }\n+\n+    public void add(boolean isRetry)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        if (isRetry)\n+        {\n+          _retryCounter.getLast().addToRetryRequestCount(1);\n+        }\n+\n+        _retryCounter.getLast().addToTotalRequestCount(1);\n+      }\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        RetryCounter intervalToAggregate = _retryCounter.getLast();\n+        _aggregatedRetryCounter.addToTotalRequestCount(intervalToAggregate.getTotalRequestCount());\n+        _aggregatedRetryCounter.addToRetryRequestCount(intervalToAggregate.getRetryRequestCount());\n+\n+        if (_retryCounter.size() > _aggregatedIntervalNum)\n+        {\n+          // discard the oldest interval\n+          RetryCounter intervalToDiscard = _retryCounter.removeFirst();\n+          _aggregatedRetryCounter.subtractFromTotalRequestCount(intervalToDiscard.getTotalRequestCount());\n+          _aggregatedRetryCounter.subtractFromRetryRequestCount(intervalToDiscard.getRetryRequestCount());;\n+        }\n+\n+        // append a new interval\n+        _retryCounter.addLast(new RetryCounter());\n+      }\n+    }\n+\n+    public void isBelowRetryRatio(SuccessCallback<Boolean> callback)\n+    {\n+      _balancer.getLoadBalancedServiceProperties(_serviceName, new Callback<ServiceProperties>()\n+      {\n+        @Override\n+        public void onError(Throwable e)\n+        {\n+          LOG.warn(\"Failed to fetch transportClientProperties \", e);\n+          double maxClientRequestRetryRatio = HttpClientFactory.DEFAULT_MAX_CLIENT_REQUEST_RETRY_RATIO;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5"}, "originalPosition": 334}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjYxNDA4Nw==", "bodyText": "Minor: if you always operate on total and retry count together, maybe you can merge these 2 lines into 2 method? Will leave the decision to you", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542614087", "createdAt": "2020-12-14T18:28:10Z", "author": {"login": "rachelhanhan"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +338,201 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;\n+\n+    private ClientRetryTracker(int aggregatedIntervalNum, long updateIntervalMs, Clock clock, String serviceName)\n+    {\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+      _serviceName = serviceName;\n+\n+      _lastRollOverTime = clock.currentTimeMillis();\n+      _currentAggregatedRetryRatio = 0;\n+\n+      _aggregatedRetryCounter = new RetryCounter();\n+      _retryCounter = new LinkedList<>();\n+      _retryCounter.add(new RetryCounter());\n+    }\n+\n+    public void add(boolean isRetry)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        if (isRetry)\n+        {\n+          _retryCounter.getLast().addToRetryRequestCount(1);\n+        }\n+\n+        _retryCounter.getLast().addToTotalRequestCount(1);\n+      }\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        RetryCounter intervalToAggregate = _retryCounter.getLast();\n+        _aggregatedRetryCounter.addToTotalRequestCount(intervalToAggregate.getTotalRequestCount());\n+        _aggregatedRetryCounter.addToRetryRequestCount(intervalToAggregate.getRetryRequestCount());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5"}, "originalPosition": 311}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8f60ca25421270cf28115289e27f10349fca412a", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/8f60ca25421270cf28115289e27f10349fca412a", "committedDate": "2020-12-14T22:00:48Z", "message": "Add a listener interface for retry request"}, "afterCommit": {"oid": "177cc61b814d5f6b4e1f47b53a352df57a7b78d5", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/177cc61b814d5f6b4e1f47b53a352df57a7b78d5", "committedDate": "2020-12-14T22:02:18Z", "message": "Add a listener interface for retry request"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "committedDate": "2020-12-14T23:47:07Z", "message": "Add a listener interface for retry request"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "177cc61b814d5f6b4e1f47b53a352df57a7b78d5", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/177cc61b814d5f6b4e1f47b53a352df57a7b78d5", "committedDate": "2020-12-14T22:02:18Z", "message": "Add a listener interface for retry request"}, "afterCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "committedDate": "2020-12-14T23:47:07Z", "message": "Add a listener interface for retry request"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyMjQ2OTc1", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-552246975", "createdAt": "2020-12-15T09:17:36Z", "commit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "state": "DISMISSED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToxNzozN1rOIGAs8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToxODowM1rOIGAuGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE3Mzg3Mw==", "bodyText": "The code is setting a default of 0.2 for this, any reason why it cannot be added in the schema?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543173873", "createdAt": "2020-12-15T09:17:37Z", "author": {"login": "karthikbalasub"}, "path": "d2-schemas/src/main/pegasus/com/linkedin/d2/D2TransportClientProperties.pdl", "diffHunk": "@@ -141,4 +141,9 @@ record D2TransportClientProperties {\n      */\n     HTTP_2\n   }\n+\n+  /**\n+   * Maximum ratio of retry requests to total requests per client\n+   */\n+  maxClientRequestRetryRatio: optional double", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE3NDE3MQ==", "bodyText": "Is this config applicable only when retry is enabled? If so, please document that here.\nAlso why are other retry configs (retry, retryLimit, Interval) etc not part of the client properties?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543174171", "createdAt": "2020-12-15T09:18:03Z", "author": {"login": "karthikbalasub"}, "path": "d2-schemas/src/main/pegasus/com/linkedin/d2/D2TransportClientProperties.pdl", "diffHunk": "@@ -141,4 +141,9 @@ record D2TransportClientProperties {\n      */\n     HTTP_2\n   }\n+\n+  /**\n+   * Maximum ratio of retry requests to total requests per client", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc9e7dfec28b7ca363a745c769c4e1e5f9044f13", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/bc9e7dfec28b7ca363a745c769c4e1e5f9044f13", "committedDate": "2020-12-15T18:24:38Z", "message": "Update D2TransportClientProperties.pdl documentation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93aafa2122db2f6bc7c56b27c4b239ff6e7bf593", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/93aafa2122db2f6bc7c56b27c4b239ff6e7bf593", "committedDate": "2020-12-15T21:10:37Z", "message": "Merge branch 'master' into overload_failure_retry"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyOTU0NTcw", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-552954570", "createdAt": "2020-12-15T21:59:52Z", "commit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMTo1OTo1MlrOIGh5BA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMjozNjoxM1rOIGjGLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzcxNzYzNg==", "bodyText": "Really minor, but I'd suggest reworking this to avoid duplicating the increment logic and making the intention clearer:\nif (numAttempts > _retryLimit) {\n  LOG.warn(...)\n  numAttempts = _retryLimit;\n}\n... // synchronize/increment logic", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543717636", "createdAt": "2020-12-15T21:59:52Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/util/ServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.util.clock.Clock;\n+import java.util.LinkedList;\n+import org.checkerframework.checker.lock.qual.GuardedBy;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+ * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+ * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+ * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+ * intervals by aggregating the recorded requests.\n+ */\n+public class ServerRetryTracker\n+{\n+  private static final Logger LOG = LoggerFactory.getLogger(ServerRetryTracker.class);\n+  private final int _retryLimit;\n+  private final int _aggregatedIntervalNum;\n+  private final double _maxRequestRetryRatio;\n+  private final long _updateIntervalMs;\n+  private final Clock _clock;\n+\n+  private final Object _counterLock = new Object();\n+  private final Object _updateLock = new Object();\n+\n+  @GuardedBy(\"_updateLock\")\n+  private volatile long _lastRollOverTime;\n+  private boolean _isBelowRetryRatio;\n+\n+  @GuardedBy(\"_counterLock\")\n+  private final LinkedList<int[]> _retryAttemptsCounter;\n+  private final int[] _aggregatedRetryAttemptsCounter;\n+\n+  public ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+  {\n+    _retryLimit = retryLimit;\n+    _aggregatedIntervalNum = aggregatedIntervalNum;\n+    _maxRequestRetryRatio = maxRequestRetryRatio;\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+\n+    _lastRollOverTime = clock.currentTimeMillis();\n+    _isBelowRetryRatio = true;\n+\n+    _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+    _retryAttemptsCounter = new LinkedList<>();\n+    _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+  }\n+\n+  public void add(int numberOfRetryAttempts)\n+  {\n+    if (numberOfRetryAttempts <= _retryLimit)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      }\n+    } else\n+    {\n+      LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[_retryLimit] += 1;\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczMjYwMQ==", "bodyText": "Can you also add some simple tests covering retry limits of 0 and 1?", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543732601", "createdAt": "2020-12-15T22:26:56Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/test/java/com/linkedin/r2/util/TestServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.r2.filter.transport.ServerRetryFilter;\n+import com.linkedin.util.clock.SettableClock;\n+import org.testng.Assert;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+\n+public class TestServerRetryTracker\n+{\n+  private ServerRetryTracker _serverRetryTracker;\n+  private SettableClock _clock;\n+\n+  @BeforeMethod\n+  public void setUp()\n+  {\n+    _clock = new SettableClock();\n+    _serverRetryTracker = new ServerRetryTracker(ServerRetryFilter.DEFAULT_RETRY_LIMIT,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczNjUzNw==", "bodyText": "This test is doing a LOT. With unit tests it's best to write small focused tests that exercise one condition/scenario/code path, which makes it easier to understand the purpose of the test as well as often making it easier to track down what is breaking (if one large test is failing but there are actually three asserts that are failing it's less obvious than three separate tests failing).\nI'd suggest pulling the multiple window scenarios into separate tests at least.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543736537", "createdAt": "2020-12-15T22:34:32Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/test/java/com/linkedin/r2/util/TestServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.r2.filter.transport.ServerRetryFilter;\n+import com.linkedin.util.clock.SettableClock;\n+import org.testng.Assert;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+\n+public class TestServerRetryTracker\n+{\n+  private ServerRetryTracker _serverRetryTracker;\n+  private SettableClock _clock;\n+\n+  @BeforeMethod\n+  public void setUp()\n+  {\n+    _clock = new SettableClock();\n+    _serverRetryTracker = new ServerRetryTracker(ServerRetryFilter.DEFAULT_RETRY_LIMIT,\n+        ServerRetryFilter.DEFAULT_AGGREGATED_INTERVAL_NUM, ServerRetryFilter.DEFAULT_MAX_REQUEST_RETRY_RATIO,\n+        ServerRetryFilter.DEFAULT_UPDATE_INTERVAL_MS, _clock);\n+  }\n+\n+  @Test\n+  public void testEmptyServerRetryTracker()\n+  {\n+    for (int i = 0; i < 10; i++)\n+    {\n+      Assert.assertTrue(_serverRetryTracker.isBelowRetryRatio());\n+      Assert.assertEquals(_serverRetryTracker.getRetryRatio(),0.0, 0.0001);\n+      _clock.addDuration(ServerRetryFilter.DEFAULT_UPDATE_INTERVAL_MS);\n+    }\n+  }\n+\n+  @Test\n+  public void testServerRetryTracker()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczNzM4OA==", "bodyText": "I didn't see this covered in the test cases - probably worth adding.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543737388", "createdAt": "2020-12-15T22:36:13Z", "author": {"login": "bbarkley"}, "path": "r2-core/src/main/java/com/linkedin/r2/util/ServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.util.clock.Clock;\n+import java.util.LinkedList;\n+import org.checkerframework.checker.lock.qual.GuardedBy;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+ * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+ * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+ * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+ * intervals by aggregating the recorded requests.\n+ */\n+public class ServerRetryTracker\n+{\n+  private static final Logger LOG = LoggerFactory.getLogger(ServerRetryTracker.class);\n+  private final int _retryLimit;\n+  private final int _aggregatedIntervalNum;\n+  private final double _maxRequestRetryRatio;\n+  private final long _updateIntervalMs;\n+  private final Clock _clock;\n+\n+  private final Object _counterLock = new Object();\n+  private final Object _updateLock = new Object();\n+\n+  @GuardedBy(\"_updateLock\")\n+  private volatile long _lastRollOverTime;\n+  private boolean _isBelowRetryRatio;\n+\n+  @GuardedBy(\"_counterLock\")\n+  private final LinkedList<int[]> _retryAttemptsCounter;\n+  private final int[] _aggregatedRetryAttemptsCounter;\n+\n+  public ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+  {\n+    _retryLimit = retryLimit;\n+    _aggregatedIntervalNum = aggregatedIntervalNum;\n+    _maxRequestRetryRatio = maxRequestRetryRatio;\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+\n+    _lastRollOverTime = clock.currentTimeMillis();\n+    _isBelowRetryRatio = true;\n+\n+    _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+    _retryAttemptsCounter = new LinkedList<>();\n+    _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+  }\n+\n+  public void add(int numberOfRetryAttempts)\n+  {\n+    if (numberOfRetryAttempts <= _retryLimit)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      }\n+    } else\n+    {\n+      LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[_retryLimit] += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef"}, "originalPosition": 82}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eedf854ded4cff74ed4985694ce54baebde14fb4", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/eedf854ded4cff74ed4985694ce54baebde14fb4", "committedDate": "2020-12-16T02:54:52Z", "message": "Update ServerRetryTracker test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/b1f9f26799106bb12de486b173645518b16a9c1b", "committedDate": "2020-12-17T19:18:54Z", "message": "Remove listener interface from RetryClient"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0OTgxNTM3", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-554981537", "createdAt": "2020-12-17T20:57:10Z", "commit": {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDo1NzoxMFrOIIIfbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwNTozOTowN1rOIIT1Lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5ODYzNg==", "bodyText": "Probably want to use computeIfAbsent here - otherwise the tracker will be created each time even if it's not needed.", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545398636", "createdAt": "2020-12-17T20:57:10Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -108,8 +148,21 @@ public void streamRequest(StreamRequest request, Callback<StreamResponse> callba\n   @Override\n   public void streamRequest(StreamRequest request, RequestContext requestContext, Callback<StreamResponse> callback)\n   {\n-    final Callback<StreamResponse> transportCallback = new StreamRetryRequestCallback(request, requestContext, callback);\n-    _d2Client.streamRequest(request, requestContext, transportCallback);\n+    StreamRequest newRequest = request.builder()\n+        .addHeaderValue(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS, \"0\")\n+        .build(request.getEntityStream());\n+    ClientRetryTracker retryTracker = updateRetryTracker(newRequest.getURI(), false);\n+    final Callback<StreamResponse> transportCallback = new StreamRetryRequestCallback(newRequest, requestContext, callback, retryTracker);\n+    _d2Client.streamRequest(newRequest, requestContext, transportCallback);\n+  }\n+\n+  private ClientRetryTracker updateRetryTracker(URI uri, boolean isRetry)\n+  {\n+    String serviceName = LoadBalancerUtil.getServiceNameFromUri(uri);\n+    _retryTrackerMap.putIfAbsent(serviceName, new ClientRetryTracker(_aggregatedIntervalNum, _updateIntervalMs, _clock, serviceName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MTA0Ng==", "bodyText": "Minor: if the _updateLock is guarding it there should be a separate annotation for it", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545581046", "createdAt": "2020-12-18T05:27:21Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +334,200 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b"}, "originalPosition": 274}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MTE4Ng==", "bodyText": "Same as above - additional annotation if the lock protects this", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545581186", "createdAt": "2020-12-18T05:27:45Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +334,200 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4NDQzMA==", "bodyText": "Is this making a call out to something like ZK to get this data? If so doing this on each invocation that could add a lot of latency. Ideally the result would be cached with some TTL (or maybe just fetched once the first time - do we expect this to change in ZK?).", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545584430", "createdAt": "2020-12-18T05:39:07Z", "author": {"login": "bbarkley"}, "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +334,200 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;\n+\n+    private ClientRetryTracker(int aggregatedIntervalNum, long updateIntervalMs, Clock clock, String serviceName)\n+    {\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+      _serviceName = serviceName;\n+\n+      _lastRollOverTime = clock.currentTimeMillis();\n+      _currentAggregatedRetryRatio = 0;\n+\n+      _aggregatedRetryCounter = new RetryCounter();\n+      _retryCounter = new LinkedList<>();\n+      _retryCounter.add(new RetryCounter());\n+    }\n+\n+    public void add(boolean isRetry)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        if (isRetry)\n+        {\n+          _retryCounter.getLast().addToRetryRequestCount(1);\n+        }\n+\n+        _retryCounter.getLast().addToTotalRequestCount(1);\n+      }\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        RetryCounter intervalToAggregate = _retryCounter.getLast();\n+        _aggregatedRetryCounter.addToTotalRequestCount(intervalToAggregate.getTotalRequestCount());\n+        _aggregatedRetryCounter.addToRetryRequestCount(intervalToAggregate.getRetryRequestCount());\n+\n+        if (_retryCounter.size() > _aggregatedIntervalNum)\n+        {\n+          // discard the oldest interval\n+          RetryCounter intervalToDiscard = _retryCounter.removeFirst();\n+          _aggregatedRetryCounter.subtractFromTotalRequestCount(intervalToDiscard.getTotalRequestCount());\n+          _aggregatedRetryCounter.subtractFromRetryRequestCount(intervalToDiscard.getRetryRequestCount());\n+        }\n+\n+        // append a new interval\n+        _retryCounter.addLast(new RetryCounter());\n+      }\n+    }\n+\n+    public void isBelowRetryRatio(SuccessCallback<Boolean> callback)\n+    {\n+      _balancer.getLoadBalancedServiceProperties(_serviceName, new Callback<ServiceProperties>()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b"}, "originalPosition": 333}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "82fd99516f3b7ab31141360911aa36d168854391", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/82fd99516f3b7ab31141360911aa36d168854391", "committedDate": "2020-12-18T08:17:46Z", "message": "Minor fixes"}, "afterCommit": {"oid": "c68fcf122ca5fb9fae481bac37245c6a0fc5a190", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/c68fcf122ca5fb9fae481bac37245c6a0fc5a190", "committedDate": "2020-12-18T08:30:10Z", "message": "Minor fixes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c68fcf122ca5fb9fae481bac37245c6a0fc5a190", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/c68fcf122ca5fb9fae481bac37245c6a0fc5a190", "committedDate": "2020-12-18T08:30:10Z", "message": "Minor fixes"}, "afterCommit": {"oid": "1f5120926850b937193abf18111be77b49f26e7b", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/1f5120926850b937193abf18111be77b49f26e7b", "committedDate": "2020-12-18T09:11:38Z", "message": "Minor fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1Njg3NDIz", "url": "https://github.com/linkedin/rest.li/pull/484#pullrequestreview-555687423", "createdAt": "2020-12-18T17:42:38Z", "commit": {"oid": "1f5120926850b937193abf18111be77b49f26e7b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "committedDate": "2020-12-23T02:26:18Z", "message": "Minor fixes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5980c2a197e1aba925ae479d0f793e24d83edd3f", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/5980c2a197e1aba925ae479d0f793e24d83edd3f", "committedDate": "2020-12-23T02:11:58Z", "message": "Merge branch 'master' into overload_failure_retry"}, "afterCommit": {"oid": "89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "committedDate": "2020-12-23T02:26:18Z", "message": "Minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be5257674b409d25c7221fdfbcab3b1f804e3535", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/be5257674b409d25c7221fdfbcab3b1f804e3535", "committedDate": "2020-12-23T08:30:27Z", "message": "Merge branch 'master' into overload_failure_retry"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "75b5875693072ae038a6b5d9c0b324e57e3494bc", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/75b5875693072ae038a6b5d9c0b324e57e3494bc", "committedDate": "2020-12-23T04:35:49Z", "message": "Merge branch 'master' into overload_failure_retry"}, "afterCommit": {"oid": "be5257674b409d25c7221fdfbcab3b1f804e3535", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/be5257674b409d25c7221fdfbcab3b1f804e3535", "committedDate": "2020-12-23T08:30:27Z", "message": "Merge branch 'master' into overload_failure_retry"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffb66e5a618db4e357183f946cf09c53bde8c459", "author": {"user": {"login": "rickzx", "name": "Rick Zhou"}}, "url": "https://github.com/linkedin/rest.li/commit/ffb66e5a618db4e357183f946cf09c53bde8c459", "committedDate": "2020-12-23T09:07:31Z", "message": "Update CHANGELOG and gradle.properties"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4584, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}