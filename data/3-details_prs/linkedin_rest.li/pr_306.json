{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxNjMzNzcz", "number": 306, "title": "Add protobuf stream data decoder", "bodyText": "", "createdAt": "2020-05-21T23:15:56Z", "url": "https://github.com/linkedin/rest.li/pull/306", "merged": true, "mergeCommit": {"oid": "e396f0a72ff9fbaaaf2cc36a332d4fa459e65008"}, "closed": true, "closedAt": "2020-05-27T18:52:31Z", "author": {"login": "aman1309"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcjnR3zgFqTQxNjU3ODA5Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABclcdL3AFqTQxOTQzODk1Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2NTc4MDk3", "url": "https://github.com/linkedin/rest.li/pull/306#pullrequestreview-416578097", "createdAt": "2020-05-22T00:33:43Z", "commit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwMDozMzo0NFrOGZHJeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwMDozOTowNVrOGZHOjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NDY5OA==", "bodyText": "final?", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428984698", "createdAt": "2020-05-22T00:33:44Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NDg3Ng==", "bodyText": "final", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428984876", "createdAt": "2020-05-22T00:34:29Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private SymbolTable _symbolTable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NDkzOQ==", "bodyText": "final", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428984939", "createdAt": "2020-05-22T00:34:44Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private SymbolTable _symbolTable;\n+\n+    private Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private TextBuffer _textBuffer;  //buffer to hold parsed string characters.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTI0Ng==", "bodyText": "switch instead of if else here?", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428985246", "createdAt": "2020-05-22T00:35:55Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private SymbolTable _symbolTable;\n+\n+    private Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()\n+    {\n+      if(_currComplexObjTokenSize == 0)\n+      {\n+        if (!_complexObjTokenSizeStack.isEmpty())\n+        {\n+          _currComplexObjTokenSize = _complexObjTokenSizeStack.pop();\n+        }\n+        return isCurrList() ? END_ARRAY : END_OBJECT;\n+      }\n+      return NOT_AVAILABLE;\n+    }\n+\n+    private Token readStringReference() throws IOException\n+    {\n+      Token refToken = readInt32();\n+      if (refToken == NOT_AVAILABLE)\n+      {\n+        return NOT_AVAILABLE;\n+      }\n+      if ((_stringValue = _symbolTable.getSymbolName(_intValue)) == null)\n+      {\n+        throw new DataDecodingException(\"Error decoding string reference\");\n+      }\n+      return STRING;\n+    }\n+\n+    private Token finishToken(Token token)\n+    {\n+      _currentToken = token;\n+      if (_currentToken == START_OBJECT)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTg2OA==", "bodyText": "Does the test pass if you set this to 1? That will be the ultimate litmus test since you would have accounted for all edge cases.", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428985868", "createdAt": "2020-05-22T00:38:29Z", "author": {"login": "karthikrg"}, "path": "data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.ChunkedByteStringWriter;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.TestUtil;\n+import com.linkedin.data.codec.CodecDataProviders;\n+import com.linkedin.data.codec.ProtobufCodecOptions;\n+import com.linkedin.data.codec.ProtobufDataCodec;\n+import com.linkedin.entitystream.EntityStream;\n+import com.linkedin.entitystream.EntityStreams;\n+import com.linkedin.entitystream.Writer;\n+import java.util.concurrent.ExecutionException;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.*;\n+\n+\n+public class TestProtobufDataDecoder\n+{\n+  @Test(dataProvider = \"protobufCodecData\", dataProviderClass = CodecDataProviders.class)\n+  public void testDecoder(String testName, DataComplex dataComplex, boolean enableFixedLengthFloatDoubles)\n+      throws Exception {\n+    ProtobufDataCodec codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(false).build());\n+    byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    DataComplex decodedDataComplex = decode(bytes);\n+    assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n+    codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(true).build());\n+    bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    decodedDataComplex = decode(bytes);\n+    assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n+  }\n+\n+  @Test(dataProvider = \"numbersData\", dataProviderClass = CodecDataProviders.class)\n+  public void testNumbers(Object number) throws Exception\n+  {\n+    DataMap dataMap = new DataMap();\n+    dataMap.put(\"number\", number);\n+    byte[] bytes =\n+        TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+    assertEquals(decode(bytes), dataMap);\n+  }\n+\n+  @Test\n+  public void testIntValues() throws Exception\n+  {\n+    int inc = (Integer.MAX_VALUE - Integer.MAX_VALUE / 100) / 10000;\n+    for (int i = Integer.MAX_VALUE / 100; i <= Integer.MAX_VALUE && i > 0; i += inc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"int\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getInteger(\"int\"), Integer.valueOf(i));\n+    }\n+    for (int i = Integer.MIN_VALUE; i <= Integer.MIN_VALUE / 100 && i < 0; i += inc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"int\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getInteger(\"int\"), Integer.valueOf(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testLongValues() throws Exception\n+  {\n+    long longInc = (Long.MAX_VALUE - Long.MAX_VALUE / 100L) / 10000L;\n+    for (long i = Long.MAX_VALUE / 100L; i <= Long.MAX_VALUE && i > 0; i += longInc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"long\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getLong(\"long\"), Long.valueOf(i));\n+    }\n+    for (long i = Long.MIN_VALUE; i <= Long.MIN_VALUE / 100L && i < 0; i += longInc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"long\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getLong(\"long\"), Long.valueOf(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testInvalidMap() throws Exception\n+  {\n+    DataList dataList = new DataList();\n+    dataList.add(1);\n+    dataList.add(2);\n+    dataList.add(4);\n+    byte[] bytes =\n+        TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataList);\n+    decode(bytes);\n+\n+    try\n+    {\n+      decodeMap(bytes);\n+      fail(\"Parsing list as map.\");\n+    }\n+    catch (ExecutionException e)\n+    {\n+      // Expected.\n+    }\n+  }\n+\n+  @Test\n+  public void testInvalidList() throws Exception\n+  {\n+    DataMap dataMap = new DataMap();\n+    dataMap.put(\"key\", true);\n+    byte[] bytes =\n+        TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+    decode(bytes);\n+\n+    try\n+    {\n+      decodeList(bytes);\n+      fail(\"Parsing map as list\");\n+    }\n+    catch (ExecutionException e)\n+    {\n+      // Expected.\n+    }\n+  }\n+\n+  private static DataComplex decode(byte[] bytes) throws Exception\n+  {\n+    ProtobufDataDecoder<DataComplex> decoder = new ProtobufDataDecoder<>(null, AbstractDataDecoder.START_TOKENS);\n+    return decode(bytes, decoder);\n+  }\n+\n+  private static DataMap decodeMap(byte[] bytes) throws Exception\n+  {\n+    ProtobufDataDecoder<DataMap> decoder =\n+        new ProtobufDataDecoder<>(null, AbstractDataDecoder.START_OBJECT_TOKEN);\n+    return decode(bytes, decoder);\n+  }\n+\n+  private static DataList decodeList(byte[] bytes) throws Exception\n+  {\n+    ProtobufDataDecoder<DataList> decoder =\n+        new ProtobufDataDecoder<>(null, AbstractDataDecoder.START_ARRAY_TOKEN);\n+    return decode(bytes, decoder);\n+  }\n+\n+  private static <T extends DataComplex> T decode(byte[] bytes, ProtobufDataDecoder<T> decoder) throws Exception\n+  {\n+    Writer<ByteString> writer = new ChunkedByteStringWriter(bytes, 3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTk5Nw==", "bodyText": "We should take setEnableASCIIOnlyStrings as a dataprovider param", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428985997", "createdAt": "2020-05-22T00:39:05Z", "author": {"login": "karthikrg"}, "path": "data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.ChunkedByteStringWriter;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.TestUtil;\n+import com.linkedin.data.codec.CodecDataProviders;\n+import com.linkedin.data.codec.ProtobufCodecOptions;\n+import com.linkedin.data.codec.ProtobufDataCodec;\n+import com.linkedin.entitystream.EntityStream;\n+import com.linkedin.entitystream.EntityStreams;\n+import com.linkedin.entitystream.Writer;\n+import java.util.concurrent.ExecutionException;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.*;\n+\n+\n+public class TestProtobufDataDecoder\n+{\n+  @Test(dataProvider = \"protobufCodecData\", dataProviderClass = CodecDataProviders.class)\n+  public void testDecoder(String testName, DataComplex dataComplex, boolean enableFixedLengthFloatDoubles)\n+      throws Exception {\n+    ProtobufDataCodec codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(false).build());\n+    byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    DataComplex decodedDataComplex = decode(bytes);\n+    assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n+    codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(true).build());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454"}, "originalPosition": 50}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0ebe473e4165034ace9961425ffd37d760232454", "author": {"user": {"login": "aman1309", "name": "Aman Gupta"}}, "url": "https://github.com/linkedin/rest.li/commit/0ebe473e4165034ace9961425ffd37d760232454", "committedDate": "2020-05-21T23:14:15Z", "message": "Add protobuf stream data decoder"}, "afterCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "author": {"user": {"login": "aman1309", "name": "Aman Gupta"}}, "url": "https://github.com/linkedin/rest.li/commit/a02dc6839e3934b6a1b197c499f6e18722b068dd", "committedDate": "2020-05-22T08:30:40Z", "message": "Add protobuf stream data decoder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af27bfb3a8a06952b4dd5c4bca9cc44d8242a9ff", "author": {"user": {"login": "aman1309", "name": "Aman Gupta"}}, "url": "https://github.com/linkedin/rest.li/commit/af27bfb3a8a06952b4dd5c4bca9cc44d8242a9ff", "committedDate": "2020-05-22T10:07:07Z", "message": "Add protobuf stream data decoder"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4OTU3NDk1", "url": "https://github.com/linkedin/rest.li/pull/306#pullrequestreview-418957495", "createdAt": "2020-05-27T08:14:39Z", "commit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwODoxNDozOVrOGa-SgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwOToyMzowNVrOGbA4xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkzNjcwNQ==", "bodyText": "_endOfInput", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430936705", "createdAt": "2020-05-27T08:14:39Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NDQzNg==", "bodyText": "It would be good to add some inline comments in this method.. See my comments below:", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430944436", "createdAt": "2020-05-27T08:27:24Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NDQ4OA==", "bodyText": "Explain why you read the size first for some ordinals.", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430944488", "createdAt": "2020-05-27T08:27:30Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NDkzMw==", "bodyText": "This comment doesn't explain clearly what is happening. I see you are trying to see if the current map/list is completed. Doc that.", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430944933", "createdAt": "2020-05-27T08:28:14Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NTM3MQ==", "bodyText": "Why is this not the first check?", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430945371", "createdAt": "2020-05-27T08:28:58Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NjY1MA==", "bodyText": "Not clear why you had to do this? For raw bytes, you should clear the array when you finished reading the expected number of bytes?", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430946650", "createdAt": "2020-05-27T08:30:58Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NzM0Nw==", "bodyText": "I suggest renaming this to \"checkEndComplexObj\" as you are not really reading from the data.", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430947347", "createdAt": "2020-05-27T08:32:09Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0OTkxMw==", "bodyText": "Comment this is done for reading key,value pairs for each map item", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430949913", "createdAt": "2020-05-27T08:36:21Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()\n+    {\n+      if(_currComplexObjTokenSize == 0)\n+      {\n+        if (!_complexObjTokenSizeStack.isEmpty())\n+        {\n+          _currComplexObjTokenSize = _complexObjTokenSizeStack.pop();\n+        }\n+        return isCurrList() ? END_ARRAY : END_OBJECT;\n+      }\n+      return NOT_AVAILABLE;\n+    }\n+\n+    private Token readStringReference() throws IOException\n+    {\n+      Token refToken = readInt32();\n+      if (refToken == NOT_AVAILABLE)\n+      {\n+        return NOT_AVAILABLE;\n+      }\n+      if ((_stringValue = _symbolTable.getSymbolName(_intValue)) == null)\n+      {\n+        throw new DataDecodingException(\"Error decoding string reference\");\n+      }\n+      return STRING;\n+    }\n+\n+    private Token finishToken(Token token)\n+    {\n+      _currentToken = token;\n+      switch (_currentToken)\n+      {\n+        case START_OBJECT:\n+          if (_currComplexObjTokenSize > 0)\n+          {\n+            _complexObjTokenSizeStack.push(_currComplexObjTokenSize);\n+          }\n+          _currComplexObjTokenSize = _intValue << 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk3NDQwMg==", "bodyText": "rename this to _textBufferPos", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430974402", "createdAt": "2020-05-27T09:15:24Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk3OTI2OQ==", "bodyText": "Add doc here, explain the different scenarios:\n\nnew string, fully readable\nnew string, partially readable\nreading remaining part of a string.\nand include details of the different variables used for each scenario. For example _intValue represents the remaining chars to read for scenario 3.", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430979269", "createdAt": "2020-05-27T09:23:05Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()\n+    {\n+      if(_currComplexObjTokenSize == 0)\n+      {\n+        if (!_complexObjTokenSizeStack.isEmpty())\n+        {\n+          _currComplexObjTokenSize = _complexObjTokenSizeStack.pop();\n+        }\n+        return isCurrList() ? END_ARRAY : END_OBJECT;\n+      }\n+      return NOT_AVAILABLE;\n+    }\n+\n+    private Token readStringReference() throws IOException\n+    {\n+      Token refToken = readInt32();\n+      if (refToken == NOT_AVAILABLE)\n+      {\n+        return NOT_AVAILABLE;\n+      }\n+      if ((_stringValue = _symbolTable.getSymbolName(_intValue)) == null)\n+      {\n+        throw new DataDecodingException(\"Error decoding string reference\");\n+      }\n+      return STRING;\n+    }\n+\n+    private Token finishToken(Token token)\n+    {\n+      _currentToken = token;\n+      switch (_currentToken)\n+      {\n+        case START_OBJECT:\n+          if (_currComplexObjTokenSize > 0)\n+          {\n+            _complexObjTokenSizeStack.push(_currComplexObjTokenSize);\n+          }\n+          _currComplexObjTokenSize = _intValue << 1;\n+          break;\n+        case START_ARRAY:\n+          if (_currComplexObjTokenSize > 0)\n+          {\n+            _complexObjTokenSizeStack.push(_currComplexObjTokenSize);\n+          }\n+          _currComplexObjTokenSize = _intValue;\n+          break;\n+        case NOT_AVAILABLE:\n+          break;\n+        default:\n+          _currComplexObjTokenSize--;\n+      }\n+      return _currentToken;\n+    }\n+\n+    @Override\n+    public int getComplexObjSize()\n+    {\n+      return _currentToken == START_OBJECT || _currentToken == START_ARRAY ? _intValue : -1;\n+    }\n+\n+    @Override\n+    public String getString() throws IOException\n+    {\n+      if (_currentToken != STRING)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: String value is not available\");\n+      }\n+      return _stringValue;\n+    }\n+\n+    @Override\n+    public ByteString getRawBytes() throws IOException {\n+      if (_currentToken != RAW_BYTES)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return ByteString.unsafeWrap(_bytesValue);\n+    }\n+\n+    @Override\n+    public int getIntValue() throws IOException\n+    {\n+      if (_currentToken != INTEGER)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: int value is not available\");\n+      }\n+      return _intValue;\n+    }\n+\n+    @Override\n+    public long getLongValue() throws IOException\n+    {\n+      if (_currentToken != LONG)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return _longValue;\n+    }\n+\n+    @Override\n+    public float getFloatValue() throws IOException\n+    {\n+      if (_currentToken != FLOAT)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return Float.intBitsToFloat(_intValue);\n+    }\n+\n+    @Override\n+    public double getDoubleValue() throws IOException\n+    {\n+      if (_currentToken != DOUBLE)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return Double.longBitsToDouble(_longValue);\n+    }\n+\n+    /*\n+    * Non blocking ProtoReader Implementation\n+    */\n+\n+    private Token readASCIIString() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd"}, "originalPosition": 368}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a5aac743af2d7104422fb19b75686b0427f5135", "author": {"user": {"login": "aman1309", "name": "Aman Gupta"}}, "url": "https://github.com/linkedin/rest.li/commit/1a5aac743af2d7104422fb19b75686b0427f5135", "committedDate": "2020-05-27T16:07:35Z", "message": "address review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "author": {"user": {"login": "aman1309", "name": "Aman Gupta"}}, "url": "https://github.com/linkedin/rest.li/commit/a02dc6839e3934b6a1b197c499f6e18722b068dd", "committedDate": "2020-05-22T08:30:40Z", "message": "Add protobuf stream data decoder"}, "afterCommit": {"oid": "1a5aac743af2d7104422fb19b75686b0427f5135", "author": {"user": {"login": "aman1309", "name": "Aman Gupta"}}, "url": "https://github.com/linkedin/rest.li/commit/1a5aac743af2d7104422fb19b75686b0427f5135", "committedDate": "2020-05-27T16:07:35Z", "message": "address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NDM4OTU2", "url": "https://github.com/linkedin/rest.li/pull/306#pullrequestreview-419438956", "createdAt": "2020-05-27T17:10:30Z", "commit": {"oid": "1a5aac743af2d7104422fb19b75686b0427f5135"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4807, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}