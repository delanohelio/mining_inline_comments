{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3MzM2NDY3", "number": 292, "reviewThreads": {"totalCount": 40, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzowMTowMlrOD8TjoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjo0MDozNVrOD-2DFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTYxNTY5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/ByteString.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzowMTowMlrOGVKamg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzowMTowMlrOGVKamg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0MzkzMA==", "bodyText": "nit: broken link", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r424843930", "createdAt": "2020-05-14T03:01:02Z", "author": {"login": "evanw555"}, "path": "data/src/main/java/com/linkedin/data/ByteString.java", "diffHunk": "@@ -481,6 +481,26 @@ public int feed(ByteArrayFeeder feeder, int index) throws IOException\n     return returnIndex < _byteArrays.getArraySize() ? returnIndex : -1;\n   }\n \n+  /**\n+   * Feeds a chunk of this {@link ByteString} to a @{@link com.linkedin.data.Data.DataParser}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTYxODI0OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzowMjo0N1rOGVKcNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzowMjo0N1rOGVKcNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0NDM0Mg==", "bodyText": "nit: fix date", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r424844342", "createdAt": "2020-05-14T03:02:47Z", "author": {"login": "evanw555"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+   Copyright (c) 2018 LinkedIn Corp.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTYzMTU5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/ByteString.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzoxMTozN1rOGVKkMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMDoyMTozOVrOGVszMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0NjM4NQ==", "bodyText": "Would it be possible to reuse the implementation of #feed(ByteArrayFeeder, int) by having Data.DataParser implement ByteArrayFeeder? It looks like it already satisfies that interface anyway.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r424846385", "createdAt": "2020-05-14T03:11:37Z", "author": {"login": "evanw555"}, "path": "data/src/main/java/com/linkedin/data/ByteString.java", "diffHunk": "@@ -481,6 +481,26 @@ public int feed(ByteArrayFeeder feeder, int index) throws IOException\n     return returnIndex < _byteArrays.getArraySize() ? returnIndex : -1;\n   }\n \n+  /**\n+   * Feeds a chunk of this {@link ByteString} to a @{@link com.linkedin.data.Data.DataParser}\n+   * without copying the underlying byte[].\n+   *\n+   * @param parser the feeder to feed the bytes to\n+   * @param index the index of the chunk to feed\n+   *\n+   * @throws IOException if an error occurs while writing to the feeder\n+   *\n+   * @return The next index to feed or -1 if no more indices are left to feed.\n+   */\n+  public int feed(Data.DataParser parser, int index) throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM1NzQ1OA==", "bodyText": "ByteArrayFeeder is part of jackson lib, using it for generic parser here would be weird, that's why copied required interface methods for us here.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r425357458", "createdAt": "2020-05-14T18:47:54Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/ByteString.java", "diffHunk": "@@ -481,6 +481,26 @@ public int feed(ByteArrayFeeder feeder, int index) throws IOException\n     return returnIndex < _byteArrays.getArraySize() ? returnIndex : -1;\n   }\n \n+  /**\n+   * Feeds a chunk of this {@link ByteString} to a @{@link com.linkedin.data.Data.DataParser}\n+   * without copying the underlying byte[].\n+   *\n+   * @param parser the feeder to feed the bytes to\n+   * @param index the index of the chunk to feed\n+   *\n+   * @throws IOException if an error occurs while writing to the feeder\n+   *\n+   * @return The next index to feed or -1 if no more indices are left to feed.\n+   */\n+  public int feed(Data.DataParser parser, int index) throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0NjM4NQ=="}, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQwNzI4Mg==", "bodyText": "Ah, I see. Yeah we would have less control over the interface then, could cause difficulties.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r425407282", "createdAt": "2020-05-14T20:21:39Z", "author": {"login": "evanw555"}, "path": "data/src/main/java/com/linkedin/data/ByteString.java", "diffHunk": "@@ -481,6 +481,26 @@ public int feed(ByteArrayFeeder feeder, int index) throws IOException\n     return returnIndex < _byteArrays.getArraySize() ? returnIndex : -1;\n   }\n \n+  /**\n+   * Feeds a chunk of this {@link ByteString} to a @{@link com.linkedin.data.Data.DataParser}\n+   * without copying the underlying byte[].\n+   *\n+   * @param parser the feeder to feed the bytes to\n+   * @param index the index of the chunk to feed\n+   *\n+   * @throws IOException if an error occurs while writing to the feeder\n+   *\n+   * @return The next index to feed or -1 if no more indices are left to feed.\n+   */\n+  public int feed(Data.DataParser parser, int index) throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0NjM4NQ=="}, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTY0MzYwOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/Data.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwMzoxOTozNlrOGVKrWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODo0ODoyOVrOGVpx5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0ODIxNw==", "bodyText": "Something about this doesn't sit right with me. I think it would be more appropriate for this interface to be its own top-level class DataParser outside of this file. The Data.Token enum can probably be it's own thing as well e.g. DataToken or even tied with the parser DataParser.Token, up to you. Packing everything into this Data class doesn't seem intuitive to me.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r424848217", "createdAt": "2020-05-14T03:19:36Z", "author": {"login": "evanw555"}, "path": "data/src/main/java/com/linkedin/data/Data.java", "diffHunk": "@@ -131,6 +130,107 @@\n     TYPE_MAP.put(ByteString.class, (byte) 9);\n   }\n \n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  public enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Data parser interface invoked by stream decoder.\n+   *\n+   * This interface contains methods that are invoked when parsing a Data object.\n+   * Each method represents a different kind of event/read action\n+   *\n+   * Methods can throw IOException as a checked exception to\n+   * indicate parsing error.\n+   *\n+   * @author amgupta1\n+   */\n+  public interface DataParser", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM1Nzc5OQ==", "bodyText": "moved it to separate file", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r425357799", "createdAt": "2020-05-14T18:48:29Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/Data.java", "diffHunk": "@@ -131,6 +130,107 @@\n     TYPE_MAP.put(ByteString.class, (byte) 9);\n   }\n \n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  public enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Data parser interface invoked by stream decoder.\n+   *\n+   * This interface contains methods that are invoked when parsing a Data object.\n+   * Each method represents a different kind of event/read action\n+   *\n+   * Methods can throw IOException as a checked exception to\n+   * indicate parsing error.\n+   *\n+   * @author amgupta1\n+   */\n+  public interface DataParser", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg0ODIxNw=="}, "originalCommit": {"oid": "9f2f58ec8eff28783c2b811feb9f638dcd96c0dd"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1ODA3MTg1OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/ByteString.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNzo0NDoyN1rOGXBaOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNzo0NDoyN1rOGXBaOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5MzUzMA==", "bodyText": "Typically APIs take offset and length. The jackson API is kinda unique in the sense that it takes end. How about we make the API contract take offset, length and modify the jackson impl to pass in end internally?", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r426793530", "createdAt": "2020-05-18T17:44:27Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/ByteString.java", "diffHunk": "@@ -481,6 +481,26 @@ public int feed(ByteArrayFeeder feeder, int index) throws IOException\n     return returnIndex < _byteArrays.getArraySize() ? returnIndex : -1;\n   }\n \n+  /**\n+   * Feeds a chunk of this {@link ByteString} to a {@link com.linkedin.data.DataParser}\n+   * without copying the underlying byte[].\n+   *\n+   * @param parser the feeder to feed the bytes to\n+   * @param index the index of the chunk to feed\n+   *\n+   * @throws IOException if an error occurs while writing to the feeder\n+   *\n+   * @return The next index to feed or -1 if no more indices are left to feed.\n+   */\n+  public int feed(DataParser parser, int index) throws IOException\n+  {\n+    ByteArray byteArray = _byteArrays.get(index);\n+    int end = byteArray.getOffset() + byteArray.getLength();\n+    parser.feedInput(byteArray.getArray(), byteArray.getOffset(), end);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1ODA3NjMyOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNzo0NTozNlrOGXBcwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwODozOTozOFrOGYBMEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDE3OA==", "bodyText": "I am not sure this is the best way to model it. Protobuf for example has the notion of several more internal types, like say string reference. An Ordinal model may work better since it supports extensibility.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r426794178", "createdAt": "2020-05-18T17:45:36Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgyMDU1NQ==", "bodyText": "I think this is okay, concrete parser implementations can use internal tokens and then map them to these tokens in nextToken().\nthe Json parser is already doing this for number types.\nThe tokens defined here are the types needed/allowed for DataComplex objects.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427820555", "createdAt": "2020-05-20T08:11:03Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDE3OA=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgzODQ4Mw==", "bodyText": "Yes I had ordinal in mind but these token should be based on datatypes we allow ie. Data.DATA_TYPES. We can wrap specific ordinals to these token like StringRef to String.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427838483", "createdAt": "2020-05-20T08:39:38Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDE3OA=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1ODA3NzQ3OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNzo0NTo1N1rOGXBdiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNzo0NTo1N1rOGXBdiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDM3Nw==", "bodyText": "Nit: Missing trailing linefeed", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r426794377", "createdAt": "2020-05-18T17:45:57Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param end Offset after last byte contained in the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int end) throws IOException;\n+\n+  /**\n+   * Method that should be called after last chunk of data to parse has been fed\n+   * (with {@link #feedInput(byte[], int, int)}). After calling this method,\n+   * no more data can be fed; and parser assumes no more data will be available.\n+   */\n+  void endOfInput();\n+\n+  /**\n+   * Main iteration method, which will advance stream enough\n+   * to determine type of the next token, if any. If none\n+   * remaining (stream has no content other than possible\n+   * white space before ending), null will be returned.\n+   *\n+   * @return Next token from the stream, if any found, or null\n+   *   to indicate end-of-input\n+   */\n+  Token nextToken() throws IOException;\n+\n+  /**\n+   * Method for accessing textual representation of the current token;\n+   * if no current token (before first call to {@link #nextToken}, or\n+   * after encountering end-of-input), returns null.\n+   * Method can be called for any token type.\n+   */\n+  String getString() throws IOException;\n+\n+  /**\n+   * Numeric accessor that can be called when the current\n+   * token is of type {@link Token#INTEGER} and\n+   * it can be expressed as a value of Java int primitive type.\n+   */\n+  int getIntValue() throws IOException;\n+\n+  /**\n+   * Numeric accessor that can be called when the current\n+   * token is of type {@link Token#LONG} and\n+   * it can be expressed as a Java long primitive type.\n+   */\n+  long getLongValue() throws IOException;\n+\n+  /**\n+   * Numeric accessor that can be called when the current\n+   * token is of type {@link Token#FLOAT} and\n+   * it can be expressed as a Java float primitive type.\n+   */\n+  float getFloatValue() throws IOException;\n+\n+  /**\n+   * Numeric accessor that can be called when the current\n+   * token is of type {@link Token#DOUBLE} and\n+   * it can be expressed as a Java double primitive type.\n+   */\n+  double getDoubleValue() throws IOException;\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1ODA4MDc3OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNzo0Njo1NFrOGXBfpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMDoxOTo0M1rOGYE9CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDkxNw==", "bodyText": "This should be protected. Protobuf for example can do this much more efficiently by not using DataMapBuilder since it is a length prefixed encoding format.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r426794917", "createdAt": "2020-05-18T17:46:54Z", "author": {"login": "karthikrg"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _currDataMapBuilder = new DataMapBuilder();\n+    _expectedTokens = expectedTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   * Method can throw IOException\n+   */\n+  protected abstract DataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      DataParser.Token token;\n+      while ((token = _parser.nextToken()) != null)\n+      {\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            validate(START_OBJECT);\n+            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+            // need to create a new one.\n+            if (_currDataMapBuilder.inUse())\n+            {\n+              _currDataMapBuilder = new DataMapBuilder();\n+            }\n+            _currDataMapBuilder.setInUse(true);\n+            push(_currDataMapBuilder, false);\n+            break;\n+          case END_OBJECT:\n+            validate(END_OBJECT);\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            validate(START_ARRAY);\n+            push(new DataList(), true);\n+            break;\n+          case END_ARRAY:\n+            validate(END_ARRAY);\n+            pop();\n+            break;\n+          case STRING:\n+            validate(STRING);\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            validate(BOOL_TRUE);\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            validate(BOOL_FALSE);\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            validate(NULL);\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(DataParser.Token token)\n+  {\n+    if (!_expectedTokens.contains(token))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg0NDQ2NQ==", "bodyText": "I had thought about this but protodecoder overriding this would mean private stacks and other variables internal to AbstractDecoder would have to be exposed and introduce the potential risk of error.\nAnother approach was to maintain size stack in ProtobufParser and use it to return END objects but size would still need to be exposed to Abstract path since it required at initiating complex objects with know size/capacity.\ntherefore adding size check branch in abstract code path would be better. Abstract methods will call parser getSize if object tokens are returned and if not invalid use it.\nSince it adds only a if check on int in main code path for all types of decoder it should be fine as could also be used by potential other future codecs.\nLet me know if it looks right, we can discuss further.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427844465", "createdAt": "2020-05-20T08:48:35Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _currDataMapBuilder = new DataMapBuilder();\n+    _expectedTokens = expectedTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   * Method can throw IOException\n+   */\n+  protected abstract DataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      DataParser.Token token;\n+      while ((token = _parser.nextToken()) != null)\n+      {\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            validate(START_OBJECT);\n+            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+            // need to create a new one.\n+            if (_currDataMapBuilder.inUse())\n+            {\n+              _currDataMapBuilder = new DataMapBuilder();\n+            }\n+            _currDataMapBuilder.setInUse(true);\n+            push(_currDataMapBuilder, false);\n+            break;\n+          case END_OBJECT:\n+            validate(END_OBJECT);\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            validate(START_ARRAY);\n+            push(new DataList(), true);\n+            break;\n+          case END_ARRAY:\n+            validate(END_ARRAY);\n+            pop();\n+            break;\n+          case STRING:\n+            validate(STRING);\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            validate(BOOL_TRUE);\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            validate(BOOL_FALSE);\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            validate(NULL);\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(DataParser.Token token)\n+  {\n+    if (!_expectedTokens.contains(token))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDkxNw=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkwMDE2OQ==", "bodyText": "Added size optimization implementation in this PR only for context", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427900169", "createdAt": "2020-05-20T10:19:43Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _currDataMapBuilder = new DataMapBuilder();\n+    _expectedTokens = expectedTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   * Method can throw IOException\n+   */\n+  protected abstract DataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      DataParser.Token token;\n+      while ((token = _parser.nextToken()) != null)\n+      {\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            validate(START_OBJECT);\n+            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+            // need to create a new one.\n+            if (_currDataMapBuilder.inUse())\n+            {\n+              _currDataMapBuilder = new DataMapBuilder();\n+            }\n+            _currDataMapBuilder.setInUse(true);\n+            push(_currDataMapBuilder, false);\n+            break;\n+          case END_OBJECT:\n+            validate(END_OBJECT);\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            validate(START_ARRAY);\n+            push(new DataList(), true);\n+            break;\n+          case END_ARRAY:\n+            validate(END_ARRAY);\n+            pop();\n+            break;\n+          case STRING:\n+            validate(STRING);\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            validate(BOOL_TRUE);\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            validate(BOOL_FALSE);\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            validate(NULL);\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(DataParser.Token token)\n+  {\n+    if (!_expectedTokens.contains(token))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc5NDkxNw=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 255}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDM3NTM3OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzozOTozMlrOGX-_dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzozOTozMlrOGX-_dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwMjQ4Nw==", "bodyText": "Add javadoc\nexpectedTokens -> expectedStartTokens", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427802487", "createdAt": "2020-05-20T07:39:32Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDM3NjYwOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/JacksonSmileDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzozOTo1NFrOGX_AQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzozOTo1NFrOGX_AQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwMjY5MA==", "bodyText": "nit: expectedFirstTokens", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427802690", "createdAt": "2020-05-20T07:39:54Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/JacksonSmileDataDecoder.java", "diffHunk": "@@ -28,11 +31,20 @@\n  */\n public class JacksonSmileDataDecoder<T extends DataComplex> extends AbstractJacksonDataDecoder<T>\n {\n+  /**\n+   * Deprecated, use {@link #JacksonSmileDataDecoder(SmileFactory, EnumSet)} instead\n+   */\n+  @Deprecated\n   protected JacksonSmileDataDecoder(SmileFactory smileFactory, byte expectedFirstToken)\n   {\n     super(smileFactory, expectedFirstToken);\n   }\n \n+  protected JacksonSmileDataDecoder(SmileFactory smileFactory, EnumSet<DataParser.Token> expectedFirstToken)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDM3OTk0OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/JacksonJsonDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0MDo0OVrOGX_CXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0MDo0OVrOGX_CXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwMzIzMA==", "bodyText": "nit: expectedFirstTokens", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427803230", "createdAt": "2020-05-20T07:40:49Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/JacksonJsonDataDecoder.java", "diffHunk": "@@ -28,11 +30,20 @@\n  */\n public class JacksonJsonDataDecoder<T extends DataComplex> extends AbstractJacksonDataDecoder<T> implements JsonDataDecoder<T>\n {\n+  /**\n+   * Deprecated, use {@link #JacksonJsonDataDecoder(EnumSet)} instead\n+   */\n+  @Deprecated\n   protected JacksonJsonDataDecoder(byte expectedFirstToken)\n   {\n     super(JacksonStreamDataCodec.JSON_FACTORY, expectedFirstToken);\n   }\n \n+  protected JacksonJsonDataDecoder(EnumSet<DataParser.Token> expectedFirstToken)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDM5MTM2OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0NDowNVrOGX_JaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwODo1OTo0NlrOGYCAcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwNTAzMw==", "bodyText": "If this is only for stream decoders, move this to com.linkedin.data.codec.entitystream", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427805033", "createdAt": "2020-05-20T07:44:05Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg1MTg4OQ==", "bodyText": "This DataParser (renamed to NonBlockingDataParser) interface could be later adapted/used anywhere fit not necessarily for stream similar to traverseCallback.\nAlso, bytestring use this, so didn't want to create a cyclic dependency that why added it here", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427851889", "createdAt": "2020-05-20T08:59:46Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwNTAzMw=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDM5MjUyOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0NDoyOVrOGX_KNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0NDoyOVrOGX_KNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwNTIzOQ==", "bodyText": "Rename this to StreamDataParser ?", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427805239", "createdAt": "2020-05-20T07:44:29Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDM5NDM2OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0NTowMlrOGX_LaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0NTowMlrOGX_LaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwNTU0NA==", "bodyText": "nit: don'y break the line.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427805544", "createdAt": "2020-05-20T07:45:02Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQwMDMwOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0Njo0MlrOGX_PGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0Njo0MlrOGX_PGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwNjQ5MQ==", "bodyText": "Agree with Karthik R, passing in offset  and length feels more natural.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427806491", "createdAt": "2020-05-20T07:46:42Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param end Offset after last byte contained in the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int end) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQwODcxOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0OTowNFrOGX_UTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo0OTowNFrOGX_UTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwNzgyMw==", "bodyText": "Create a separate token for END_OF_INPUT so there is no confusion between null and Token.NULL", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427807823", "createdAt": "2020-05-20T07:49:04Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param end Offset after last byte contained in the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int end) throws IOException;\n+\n+  /**\n+   * Method that should be called after last chunk of data to parse has been fed\n+   * (with {@link #feedInput(byte[], int, int)}). After calling this method,\n+   * no more data can be fed; and parser assumes no more data will be available.\n+   */\n+  void endOfInput();\n+\n+  /**\n+   * Main iteration method, which will advance stream enough\n+   * to determine type of the next token, if any. If none\n+   * remaining (stream has no content other than possible\n+   * white space before ending), null will be returned.\n+   *\n+   * @return Next token from the stream, if any found, or null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQyMTA4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1Mjo0MVrOGX_cJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1Mjo0MVrOGX_cJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgwOTgyOQ==", "bodyText": "Add comments on these enums if relevant.\nI see comments needed atleast for NULL, NOT_AVAILABLE and the START/END tokens.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427809829", "createdAt": "2020-05-20T07:52:41Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQyNDUxOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1MzozOFrOGX_eOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1MzozOFrOGX_eOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxMDM2Mg==", "bodyText": "Document that this should be called only after nextToken() returns NOT_AVAILABLE after the initial feed.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427810362", "createdAt": "2020-05-20T07:53:38Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQyNjc5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/DataParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1NDoyMFrOGX_fwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwOTozMjowMlrOGYDPTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxMDc1Mw==", "bodyText": "Document when NOT_AVAILABLE will be returned.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427810753", "createdAt": "2020-05-20T07:54:20Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param end Offset after last byte contained in the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int end) throws IOException;\n+\n+  /**\n+   * Method that should be called after last chunk of data to parse has been fed\n+   * (with {@link #feedInput(byte[], int, int)}). After calling this method,\n+   * no more data can be fed; and parser assumes no more data will be available.\n+   */\n+  void endOfInput();\n+\n+  /**\n+   * Main iteration method, which will advance stream enough", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg3MjA3OQ==", "bodyText": "added this in enum when not_available should be returned", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427872079", "createdAt": "2020-05-20T09:32:02Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/DataParser.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by stream decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to\n+ * indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface DataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    START_OBJECT,\n+    END_OBJECT,\n+    START_ARRAY,\n+    END_ARRAY,\n+    STRING,\n+    INTEGER,\n+    LONG,\n+    FLOAT,\n+    DOUBLE,\n+    BOOL_TRUE,\n+    BOOL_FALSE,\n+    NULL,\n+    NOT_AVAILABLE\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param end Offset after last byte contained in the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int end) throws IOException;\n+\n+  /**\n+   * Method that should be called after last chunk of data to parse has been fed\n+   * (with {@link #feedInput(byte[], int, int)}). After calling this method,\n+   * no more data can be fed; and parser assumes no more data will be available.\n+   */\n+  void endOfInput();\n+\n+  /**\n+   * Main iteration method, which will advance stream enough", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxMDc1Mw=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQzMzY4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1NjoxMVrOGX_j6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1NjoxMVrOGX_j6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxMTgxOQ==", "bodyText": "redundant.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427811819", "createdAt": "2020-05-20T07:56:11Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _currDataMapBuilder = new DataMapBuilder();\n+    _expectedTokens = expectedTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   * Method can throw IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQ0NTk0OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNzo1OToyM1rOGX_rhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwOToxMzoyNVrOGYChjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxMzc2Ng==", "bodyText": "Why is validate missing for these?\nI think this can be simplified by just calling validate(token) before the switch statement as the mapping from the parsers underlying token to the DataParser.Token is already done.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427813766", "createdAt": "2020-05-20T07:59:23Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _currDataMapBuilder = new DataMapBuilder();\n+    _expectedTokens = expectedTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   * Method can throw IOException\n+   */\n+  protected abstract DataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      DataParser.Token token;\n+      while ((token = _parser.nextToken()) != null)\n+      {\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            validate(START_OBJECT);\n+            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+            // need to create a new one.\n+            if (_currDataMapBuilder.inUse())\n+            {\n+              _currDataMapBuilder = new DataMapBuilder();\n+            }\n+            _currDataMapBuilder.setInUse(true);\n+            push(_currDataMapBuilder, false);\n+            break;\n+          case END_OBJECT:\n+            validate(END_OBJECT);\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            validate(START_ARRAY);\n+            push(new DataList(), true);\n+            break;\n+          case END_ARRAY:\n+            validate(END_ARRAY);\n+            pop();\n+            break;\n+          case STRING:\n+            validate(STRING);\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg2MDM2NA==", "bodyText": "thanks, missed it while moving these types from number type token case", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427860364", "createdAt": "2020-05-20T09:13:25Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,361 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.DataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.DataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link com.linkedin.data.DataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<DataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<DataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<DataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<DataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<DataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<DataParser.Token> NONE = EnumSet.noneOf(DataParser.Token.class);\n+  protected static final EnumSet<DataParser.Token> START_TOKENS =\n+      EnumSet.of(DataParser.Token.START_OBJECT, DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_ARRAY_TOKEN = EnumSet.of(DataParser.Token.START_ARRAY);\n+  protected static final EnumSet<DataParser.Token> START_OBJECT_TOKEN = EnumSet.of(DataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private DataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<DataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<DataParser.Token> expectedTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _currDataMapBuilder = new DataMapBuilder();\n+    _expectedTokens = expectedTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   * Method can throw IOException\n+   */\n+  protected abstract DataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      DataParser.Token token;\n+      while ((token = _parser.nextToken()) != null)\n+      {\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            validate(START_OBJECT);\n+            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+            // need to create a new one.\n+            if (_currDataMapBuilder.inUse())\n+            {\n+              _currDataMapBuilder = new DataMapBuilder();\n+            }\n+            _currDataMapBuilder.setInUse(true);\n+            push(_currDataMapBuilder, false);\n+            break;\n+          case END_OBJECT:\n+            validate(END_OBJECT);\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            validate(START_ARRAY);\n+            push(new DataList(), true);\n+            break;\n+          case END_ARRAY:\n+            validate(END_ARRAY);\n+            pop();\n+            break;\n+          case STRING:\n+            validate(STRING);\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxMzc2Ng=="}, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 207}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDQ3NjM2OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractJacksonDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwODowNzo1MFrOGX_-xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwODowNzo1MFrOGX_-xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgxODY5NA==", "bodyText": "I think this can be worded better. Something like\n\"Invalid state: Parser cannot accept more data\"", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r427818694", "createdAt": "2020-05-20T08:07:50Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractJacksonDataDecoder.java", "diffHunk": "@@ -76,328 +61,147 @@\n     }\n   }\n \n-  private static final byte VALUE = (byte) (SIMPLE_VALUE.bitPattern | START_OBJECT.bitPattern | START_ARRAY.bitPattern);\n-  private static final byte NEXT_OBJECT_FIELD = (byte) (FIELD_NAME.bitPattern | END_OBJECT.bitPattern);\n-  private static final byte NEXT_ARRAY_ITEM = (byte) (VALUE | END_ARRAY.bitPattern);\n-\n   private final JsonFactory _jsonFactory;\n \n-  private CompletableFuture<T> _completable;\n-  private T _result;\n-  private ReadHandle _readHandle;\n-\n-  private JsonParser _jsonParser;\n-  private ByteArrayFeeder _byteArrayFeeder;\n-\n-  private Deque<DataComplex> _stack;\n-  private Deque<String> _currFieldStack;\n-  private String _currField;\n-  // Expected tokens represented by a bit pattern. Every bit represents a token.\n-  private byte _expectedTokens;\n-  private boolean _isCurrList;\n-\n-  private ByteString _currentChunk;\n-  private int _currentChunkIndex = -1;\n-\n-  private DataMapBuilder _currDataMapBuilder;\n-\n+  /**\n+   * Deprecated, use {@link #AbstractJacksonDataDecoder(JsonFactory, EnumSet)} instead\n+   */\n+  @Deprecated\n   protected AbstractJacksonDataDecoder(JsonFactory jsonFactory, byte expectedFirstToken)\n   {\n+    super();\n     _jsonFactory = jsonFactory;\n-    _completable = new CompletableFuture<>();\n-    _result = null;\n-    _stack = new ArrayDeque<>();\n-    _currFieldStack = new ArrayDeque<>();\n-    _expectedTokens = expectedFirstToken;\n-    _currDataMapBuilder = new DataMapBuilder();\n+    EnumSet<DataParser.Token> expectedDataToken = NONE;\n+    if ((expectedFirstToken & Token.START_OBJECT.bitPattern) != 0) {\n+      expectedDataToken.add(DataParser.Token.START_OBJECT);\n+    }\n+    if ((expectedFirstToken & Token.START_ARRAY.bitPattern) != 0) {\n+      expectedDataToken.add(DataParser.Token.START_ARRAY);\n+    }\n+    _expectedTokens = expectedDataToken;\n   }\n \n   protected AbstractJacksonDataDecoder(JsonFactory jsonFactory)\n   {\n-    this(jsonFactory, (byte) (START_OBJECT.bitPattern | START_ARRAY.bitPattern));\n+    this(jsonFactory, START_TOKENS);\n   }\n \n-  @Override\n-  public void onInit(ReadHandle rh)\n+  protected AbstractJacksonDataDecoder(JsonFactory jsonFactory, EnumSet<DataParser.Token> expectedFirstToken)\n   {\n-    _readHandle = rh;\n-\n-    try\n-    {\n-      _jsonParser = _jsonFactory.createNonBlockingByteArrayParser();\n-      _byteArrayFeeder = (ByteArrayFeeder)_jsonParser;\n-    }\n-    catch (IOException e)\n-    {\n-      handleException(e);\n-    }\n-\n-    _readHandle.request(1);\n+    super(expectedFirstToken);\n+    _jsonFactory = jsonFactory;\n   }\n \n   @Override\n-  public void onDataAvailable(ByteString data)\n-  {\n-    // Process chunk incrementally without copying the data in the interest of performance.\n-    _currentChunk = data;\n-    _currentChunkIndex = 0;\n-\n-    processCurrentChunk();\n+  protected DataParser createDataParser() throws IOException {\n+    return new JacksonStreamDataParser(_jsonFactory);\n   }\n \n-  private void readNextChunk()\n+  class JacksonStreamDataParser implements DataParser\n   {\n-    if (_currentChunkIndex == -1)\n-    {\n-      _readHandle.request(1);\n-      return;\n-    }\n+    private JsonParser _jsonParser;\n+    private ByteArrayFeeder _byteArrayFeeder;\n+    private JsonToken _previousTokenReturned;\n \n-    processCurrentChunk();\n-  }\n-\n-  private void processCurrentChunk()\n-  {\n-    try\n+    public JacksonStreamDataParser(JsonFactory jsonFactory) throws IOException\n     {\n-      _currentChunkIndex = _currentChunk.feed(_byteArrayFeeder, _currentChunkIndex);\n-      processTokens();\n+      _jsonParser = jsonFactory.createNonBlockingByteArrayParser();\n+      _byteArrayFeeder = (ByteArrayFeeder) _jsonParser;\n     }\n-    catch (IOException e)\n-    {\n-      handleException(e);\n-    }\n-  }\n \n-  private void processTokens()\n-  {\n-    try\n+    @Override\n+    public void feedInput(byte[] data, int offset, int end) throws IOException\n     {\n-      JsonToken token;\n-      while ((token = _jsonParser.nextToken()) != null)\n+      if(_byteArrayFeeder.needMoreInput())\n       {\n-        switch (token)\n-        {\n-          case START_OBJECT:\n-            validate(START_OBJECT);\n-            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n-            // need to create a new one.\n-            if (_currDataMapBuilder.inUse())\n-            {\n-              _currDataMapBuilder = new DataMapBuilder();\n-            }\n-            _currDataMapBuilder.setInUse(true);\n-            push(_currDataMapBuilder, false);\n-            break;\n-          case END_OBJECT:\n-            validate(END_OBJECT);\n-            pop();\n-            break;\n-          case START_ARRAY:\n-            validate(START_ARRAY);\n-            push(new DataList(), true);\n-            break;\n-          case END_ARRAY:\n-            validate(END_ARRAY);\n-            pop();\n-            break;\n-          case FIELD_NAME:\n-            validate(FIELD_NAME);\n-            _currField = _jsonParser.getCurrentName();\n-            _expectedTokens = VALUE;\n-            break;\n-          case VALUE_STRING:\n-            validate(SIMPLE_VALUE);\n-            addValue(_jsonParser.getText());\n-            break;\n-          case VALUE_NUMBER_INT:\n-          case VALUE_NUMBER_FLOAT:\n-            validate(SIMPLE_VALUE);\n-            JsonParser.NumberType numberType = _jsonParser.getNumberType();\n-            switch (numberType)\n-            {\n-              case INT:\n-                addValue(_jsonParser.getIntValue());\n-                break;\n-              case LONG:\n-                addValue(_jsonParser.getLongValue());\n-                break;\n-              case FLOAT:\n-                addValue(_jsonParser.getFloatValue());\n-                break;\n-              case DOUBLE:\n-                addValue(_jsonParser.getDoubleValue());\n-                break;\n-              case BIG_INTEGER:\n-              case BIG_DECIMAL:\n-              default:\n-                handleException(new Exception(\"Unexpected number value type \" + numberType + \" at \" + _jsonParser.getTokenLocation()));\n-                break;\n-            }\n-            break;\n-          case VALUE_TRUE:\n-            validate(SIMPLE_VALUE);\n-            addValue(Boolean.TRUE);\n-            break;\n-          case VALUE_FALSE:\n-            validate(SIMPLE_VALUE);\n-            addValue(Boolean.FALSE);\n-            break;\n-          case VALUE_NULL:\n-            validate(SIMPLE_VALUE);\n-            addValue(Data.NULL);\n-            break;\n-          case NOT_AVAILABLE:\n-            readNextChunk();\n-            return;\n-          default:\n-            handleException(new Exception(\"Unexpected token \" + token + \" at \" + _jsonParser.getTokenLocation()));\n-        }\n+        _byteArrayFeeder.feedInput(data, offset, end);\n+      }\n+      else\n+      {\n+        throw new IOException(\"Byte Array Feeder is not ok to feed more data.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b96784684dfbd49d5954715cf66568fb7b605a67"}, "originalPosition": 270}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODUxODEwOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjozMjo0OVrOGYnyrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTowMDo0OFrOGYri4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MDk1Ng==", "bodyText": "The other package is also in the same module, so there wouldn't be a cyclic dependency.\nI'm suggesting that as this is closely associated with the non blocking decoder and all our non-blocking implementation is in the data.codec.entitystream package.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428470956", "createdAt": "2020-05-21T06:32:49Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ5ODAyMw==", "bodyText": "com.linkedin.data.codec.entitystream is sub package of com.linkedin.data\nso ideally, com.linkedin.data shouldn't be using references from com.linkedin.data.codec.entitystream\nwhich would happen is I move this to entitystream as ByteString use DataParser", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428498023", "createdAt": "2020-05-21T07:45:46Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MDk1Ng=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUzMjQ1MA==", "bodyText": "There is no convention against reference sub-packages. ByteString already does that. Even classes in java (Eg java.util.Map) has similar references.\nI think we can also create a separate package com.linkedin.data.parser\nThis parse doesn't fit with the other classes in data package.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428532450", "createdAt": "2020-05-21T09:00:48Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MDk1Ng=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODUyMTA1OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjozNDoxNVrOGYn0pA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTowMjoyM1rOGYrlhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MTQ2MA==", "bodyText": "the \"or\" part is not clear to me. can you explain the usecase?", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428471460", "createdAt": "2020-05-21T06:34:15Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    /**\n+     * START_OBJECT is returned when encountering signals starting of an Object/map value.\n+     */\n+    START_OBJECT,\n+    /**\n+     * END_OBJECT is returned when encountering signals ending of an Object/map value\n+     */\n+    END_OBJECT,\n+    /**\n+     * START_ARRAY is returned when encountering signals starting of an Array value\n+     */\n+    START_ARRAY,\n+    /**\n+     * END_ARRAY is returned when encountering signals ending of an Array value\n+     */\n+    END_ARRAY,\n+    /**\n+     * STRING is returned when encountering a string value, field name or reference\n+     */\n+    STRING,\n+    /**\n+     * RAW_BYTES is returned when encountering chunk of raw bytes\n+     */\n+    RAW_BYTES,\n+    /**\n+     * INTEGER is returned when encountering integer value\n+     */\n+    INTEGER,\n+    /**\n+     * LONG is returned when encountering long value\n+     */\n+    LONG,\n+    /**\n+     * FLOAT is returned when encountering float decimal value\n+     */\n+    FLOAT,\n+    /**\n+     * DOUBLE is returned when encountering double decimal value\n+     */\n+    DOUBLE,\n+    /**\n+     * BOOL_TRUE is returned when encountering boolean true value\n+     */\n+    BOOL_TRUE,\n+    /**\n+     * BOOL_FALSE is returned when encountering boolean false value\n+     */\n+    BOOL_FALSE,\n+    /**\n+     * NULL is returned when encountering \"null\" in value context\n+     */\n+    NULL,\n+    /**\n+     * NOT_AVAILABLE can be returned if {@link NonBlockingDataParser} implementation can not currently\n+     * return the requested token (usually next one), or even if any will be available,\n+     * but that may be able to determine this in future. This is the case with non-blocking parsers --", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUwMDk0NA==", "bodyText": "I can remove it, it was just left it there to put more focus on first part that even if it will available but is put in buffer.\nEx. Encountering start object ordinal but not returning it till parsing the size in protobuf.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428500944", "createdAt": "2020-05-21T07:52:26Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    /**\n+     * START_OBJECT is returned when encountering signals starting of an Object/map value.\n+     */\n+    START_OBJECT,\n+    /**\n+     * END_OBJECT is returned when encountering signals ending of an Object/map value\n+     */\n+    END_OBJECT,\n+    /**\n+     * START_ARRAY is returned when encountering signals starting of an Array value\n+     */\n+    START_ARRAY,\n+    /**\n+     * END_ARRAY is returned when encountering signals ending of an Array value\n+     */\n+    END_ARRAY,\n+    /**\n+     * STRING is returned when encountering a string value, field name or reference\n+     */\n+    STRING,\n+    /**\n+     * RAW_BYTES is returned when encountering chunk of raw bytes\n+     */\n+    RAW_BYTES,\n+    /**\n+     * INTEGER is returned when encountering integer value\n+     */\n+    INTEGER,\n+    /**\n+     * LONG is returned when encountering long value\n+     */\n+    LONG,\n+    /**\n+     * FLOAT is returned when encountering float decimal value\n+     */\n+    FLOAT,\n+    /**\n+     * DOUBLE is returned when encountering double decimal value\n+     */\n+    DOUBLE,\n+    /**\n+     * BOOL_TRUE is returned when encountering boolean true value\n+     */\n+    BOOL_TRUE,\n+    /**\n+     * BOOL_FALSE is returned when encountering boolean false value\n+     */\n+    BOOL_FALSE,\n+    /**\n+     * NULL is returned when encountering \"null\" in value context\n+     */\n+    NULL,\n+    /**\n+     * NOT_AVAILABLE can be returned if {@link NonBlockingDataParser} implementation can not currently\n+     * return the requested token (usually next one), or even if any will be available,\n+     * but that may be able to determine this in future. This is the case with non-blocking parsers --", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MTQ2MA=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUzMzEyNw==", "bodyText": "I think it is okay to remove that part as that is implementation detail. Just document that the parser can return this if it doesn't have enough data to return the next token.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428533127", "createdAt": "2020-05-21T09:02:23Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    /**\n+     * START_OBJECT is returned when encountering signals starting of an Object/map value.\n+     */\n+    START_OBJECT,\n+    /**\n+     * END_OBJECT is returned when encountering signals ending of an Object/map value\n+     */\n+    END_OBJECT,\n+    /**\n+     * START_ARRAY is returned when encountering signals starting of an Array value\n+     */\n+    START_ARRAY,\n+    /**\n+     * END_ARRAY is returned when encountering signals ending of an Array value\n+     */\n+    END_ARRAY,\n+    /**\n+     * STRING is returned when encountering a string value, field name or reference\n+     */\n+    STRING,\n+    /**\n+     * RAW_BYTES is returned when encountering chunk of raw bytes\n+     */\n+    RAW_BYTES,\n+    /**\n+     * INTEGER is returned when encountering integer value\n+     */\n+    INTEGER,\n+    /**\n+     * LONG is returned when encountering long value\n+     */\n+    LONG,\n+    /**\n+     * FLOAT is returned when encountering float decimal value\n+     */\n+    FLOAT,\n+    /**\n+     * DOUBLE is returned when encountering double decimal value\n+     */\n+    DOUBLE,\n+    /**\n+     * BOOL_TRUE is returned when encountering boolean true value\n+     */\n+    BOOL_TRUE,\n+    /**\n+     * BOOL_FALSE is returned when encountering boolean false value\n+     */\n+    BOOL_FALSE,\n+    /**\n+     * NULL is returned when encountering \"null\" in value context\n+     */\n+    NULL,\n+    /**\n+     * NOT_AVAILABLE can be returned if {@link NonBlockingDataParser} implementation can not currently\n+     * return the requested token (usually next one), or even if any will be available,\n+     * but that may be able to determine this in future. This is the case with non-blocking parsers --", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MTQ2MA=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODUyMjIzOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjozNDo1MFrOGYn1dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjozNDo1MFrOGYn1dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3MTY2OA==", "bodyText": "update comment?", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428471668", "createdAt": "2020-05-21T06:34:50Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data;\n+\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    /**\n+     * START_OBJECT is returned when encountering signals starting of an Object/map value.\n+     */\n+    START_OBJECT,\n+    /**\n+     * END_OBJECT is returned when encountering signals ending of an Object/map value\n+     */\n+    END_OBJECT,\n+    /**\n+     * START_ARRAY is returned when encountering signals starting of an Array value\n+     */\n+    START_ARRAY,\n+    /**\n+     * END_ARRAY is returned when encountering signals ending of an Array value\n+     */\n+    END_ARRAY,\n+    /**\n+     * STRING is returned when encountering a string value, field name or reference\n+     */\n+    STRING,\n+    /**\n+     * RAW_BYTES is returned when encountering chunk of raw bytes\n+     */\n+    RAW_BYTES,\n+    /**\n+     * INTEGER is returned when encountering integer value\n+     */\n+    INTEGER,\n+    /**\n+     * LONG is returned when encountering long value\n+     */\n+    LONG,\n+    /**\n+     * FLOAT is returned when encountering float decimal value\n+     */\n+    FLOAT,\n+    /**\n+     * DOUBLE is returned when encountering double decimal value\n+     */\n+    DOUBLE,\n+    /**\n+     * BOOL_TRUE is returned when encountering boolean true value\n+     */\n+    BOOL_TRUE,\n+    /**\n+     * BOOL_FALSE is returned when encountering boolean false value\n+     */\n+    BOOL_FALSE,\n+    /**\n+     * NULL is returned when encountering \"null\" in value context\n+     */\n+    NULL,\n+    /**\n+     * NOT_AVAILABLE can be returned if {@link NonBlockingDataParser} implementation can not currently\n+     * return the requested token (usually next one), or even if any will be available,\n+     * but that may be able to determine this in future. This is the case with non-blocking parsers --\n+     * they can not block to wait for more data to parse and must return something.\n+     */\n+    NOT_AVAILABLE,\n+    /**\n+     * Token returned at point when all feed input has been exhausted or\n+     * input feeder has indicated no more input will be forthcoming.\n+     */\n+    EOF_INPUT\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data if {@link #nextToken()} returns {@link Token#NOT_AVAILABLE}\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param len length of bytes to be feed from the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int len) throws IOException;\n+\n+  /**\n+   * Method that should be called after last chunk of data to parse has been fed\n+   * (with {@link #feedInput(byte[], int, int)}). After calling this method,\n+   * no more data can be fed; and parser assumes no more data will be available.\n+   */\n+  void endOfInput();\n+\n+  /**\n+   * Main iteration method, which will advance input enough to determine type of the next token, if any.\n+   * If none remaining (input has no content other than possible white space before ending), null will be returned.\n+   *\n+   * @return Next token from the input, if any found, or null to indicate end-of-input", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODU0MDQyOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjo0MzowNlrOGYoARw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMDowMToyOVrOGYtOjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NDQzOQ==", "bodyText": "This seems odd, why are you popping based on currObjSize? pop() should only be called when END_* tokens are processed.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428474439", "createdAt": "2020-05-21T06:43:06Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);\n+    }\n+    if (_currObjSize == 0)\n+    {\n+      addValue(dataComplex);\n+      return;\n+    }\n+\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+\n+    if (tmp instanceof DataMapBuilder)\n+    {\n+      tmp = ((DataMapBuilder) tmp).convertToDataMap();\n+    }\n+\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (_currObjSizeStack != null)\n+      {\n+        _currObjSize = _currObjSizeStack.pop();\n+      }\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      int currItemSize = -1;\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+        currItemSize = ((DataList) currItem).size();\n+      }\n+      else\n+      {\n+        if (currItem instanceof DataMapBuilder)\n+        {\n+          DataMapBuilder dataMapBuilder = (DataMapBuilder) currItem;\n+          if (dataMapBuilder.smallHashMapThresholdReached())\n+          {\n+            _stack.pop();\n+            DataMap dataMap = dataMapBuilder.convertToDataMap();\n+            _stack.push(dataMap);\n+            CheckedUtil.putWithoutChecking(dataMap, _currField, value);\n+          }\n+          else\n+          {\n+            dataMapBuilder.addKVPair(_currField, value);\n+          }\n+        }\n+        else\n+        {\n+          CheckedUtil.putWithoutChecking((DataMap) currItem, _currField, value);\n+          currItemSize = ((DataMap) currItem).size();\n+        }\n+        _currField = null;\n+      }\n+      if (_currObjSize != -1 && currItemSize != -1 && _currObjSize == currItemSize)\n+      {\n+        pop();\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 348}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUxNDAyNg==", "bodyText": "this is a gray area call which I took to have it here. Look at reply on @karthikrg comment on pop().\nLets discuss offline, I can move this to protobuf parser if extra memory space usage is fine", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428514026", "createdAt": "2020-05-21T08:22:18Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);\n+    }\n+    if (_currObjSize == 0)\n+    {\n+      addValue(dataComplex);\n+      return;\n+    }\n+\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+\n+    if (tmp instanceof DataMapBuilder)\n+    {\n+      tmp = ((DataMapBuilder) tmp).convertToDataMap();\n+    }\n+\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (_currObjSizeStack != null)\n+      {\n+        _currObjSize = _currObjSizeStack.pop();\n+      }\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      int currItemSize = -1;\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+        currItemSize = ((DataList) currItem).size();\n+      }\n+      else\n+      {\n+        if (currItem instanceof DataMapBuilder)\n+        {\n+          DataMapBuilder dataMapBuilder = (DataMapBuilder) currItem;\n+          if (dataMapBuilder.smallHashMapThresholdReached())\n+          {\n+            _stack.pop();\n+            DataMap dataMap = dataMapBuilder.convertToDataMap();\n+            _stack.push(dataMap);\n+            CheckedUtil.putWithoutChecking(dataMap, _currField, value);\n+          }\n+          else\n+          {\n+            dataMapBuilder.addKVPair(_currField, value);\n+          }\n+        }\n+        else\n+        {\n+          CheckedUtil.putWithoutChecking((DataMap) currItem, _currField, value);\n+          currItemSize = ((DataMap) currItem).size();\n+        }\n+        _currField = null;\n+      }\n+      if (_currObjSize != -1 && currItemSize != -1 && _currObjSize == currItemSize)\n+      {\n+        pop();\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NDQzOQ=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 348}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1MTY3OQ==", "bodyText": "My bad, I forgot protobuf codec doesn't have start/end tokens. So ,the current logic tries to do two different types of parsing: one based on start/end tokens and one based on ordinal and size. There is no benefit in mixing them, it would just make the code complicated.\nI think you can implement this by having an abstract decoder that doesn't have any special logic and then extend it to have DataMapBuilder optimizations for start/end token decoders and size based optimizations for ordinal/size decoders.\nYou'd have to make push/pop/addValue overridable. And also have abstract methods to get Map/List container objects.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428551679", "createdAt": "2020-05-21T09:43:00Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);\n+    }\n+    if (_currObjSize == 0)\n+    {\n+      addValue(dataComplex);\n+      return;\n+    }\n+\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+\n+    if (tmp instanceof DataMapBuilder)\n+    {\n+      tmp = ((DataMapBuilder) tmp).convertToDataMap();\n+    }\n+\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (_currObjSizeStack != null)\n+      {\n+        _currObjSize = _currObjSizeStack.pop();\n+      }\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      int currItemSize = -1;\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+        currItemSize = ((DataList) currItem).size();\n+      }\n+      else\n+      {\n+        if (currItem instanceof DataMapBuilder)\n+        {\n+          DataMapBuilder dataMapBuilder = (DataMapBuilder) currItem;\n+          if (dataMapBuilder.smallHashMapThresholdReached())\n+          {\n+            _stack.pop();\n+            DataMap dataMap = dataMapBuilder.convertToDataMap();\n+            _stack.push(dataMap);\n+            CheckedUtil.putWithoutChecking(dataMap, _currField, value);\n+          }\n+          else\n+          {\n+            dataMapBuilder.addKVPair(_currField, value);\n+          }\n+        }\n+        else\n+        {\n+          CheckedUtil.putWithoutChecking((DataMap) currItem, _currField, value);\n+          currItemSize = ((DataMap) currItem).size();\n+        }\n+        _currField = null;\n+      }\n+      if (_currObjSize != -1 && currItemSize != -1 && _currObjSize == currItemSize)\n+      {\n+        pop();\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NDQzOQ=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 348}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MDAxMg==", "bodyText": "Sure, so I will take the 2nd approach from other comment and use abstract methods for initializing dataComplex. won't need to expose push/pop methods", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428560012", "createdAt": "2020-05-21T10:01:29Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);\n+    }\n+    if (_currObjSize == 0)\n+    {\n+      addValue(dataComplex);\n+      return;\n+    }\n+\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+\n+    if (tmp instanceof DataMapBuilder)\n+    {\n+      tmp = ((DataMapBuilder) tmp).convertToDataMap();\n+    }\n+\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (_currObjSizeStack != null)\n+      {\n+        _currObjSize = _currObjSizeStack.pop();\n+      }\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      int currItemSize = -1;\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+        currItemSize = ((DataList) currItem).size();\n+      }\n+      else\n+      {\n+        if (currItem instanceof DataMapBuilder)\n+        {\n+          DataMapBuilder dataMapBuilder = (DataMapBuilder) currItem;\n+          if (dataMapBuilder.smallHashMapThresholdReached())\n+          {\n+            _stack.pop();\n+            DataMap dataMap = dataMapBuilder.convertToDataMap();\n+            _stack.push(dataMap);\n+            CheckedUtil.putWithoutChecking(dataMap, _currField, value);\n+          }\n+          else\n+          {\n+            dataMapBuilder.addKVPair(_currField, value);\n+          }\n+        }\n+        else\n+        {\n+          CheckedUtil.putWithoutChecking((DataMap) currItem, _currField, value);\n+          currItemSize = ((DataMap) currItem).size();\n+        }\n+        _currField = null;\n+      }\n+      if (_currObjSize != -1 && currItemSize != -1 && _currObjSize == currItemSize)\n+      {\n+        pop();\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NDQzOQ=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 348}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODU0OTQxOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjo0NzoyM1rOGYoF4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwODoxOTo0MlrOGYqWeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NTg3NA==", "bodyText": "Is this a bug? this logic is not clear for me. Can you explain?", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428475874", "createdAt": "2020-05-21T06:47:23Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);\n+    }\n+    if (_currObjSize == 0)\n+    {\n+      addValue(dataComplex);\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUxMjg5MA==", "bodyText": "it is an edge case optimization not putting datacomplex object of size 0 onto stack, now moved it out of push method.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428512890", "createdAt": "2020-05-21T08:19:42Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);\n+    }\n+    if (_currObjSize == 0)\n+    {\n+      addValue(dataComplex);\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NTg3NA=="}, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 262}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2ODU1MjY1OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjo0ODo0MlrOGYoH2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwNjo0ODo0MlrOGYoH2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ3NjM3Nw==", "bodyText": "I don't think you'd have to keep the stack of current object sizes. You can just use it as a hint provided by the parser for initializing the datacomplex objects. After that adding/ending the datacomplex should be done only by the parser tokens.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r428476377", "createdAt": "2020-05-21T06:48:42Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.NonBlockingDataParser;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private Deque<Integer> _currObjSizeStack;\n+  private int _currObjSize;\n+  private DataMapBuilder _currDataMapBuilder;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+    _currObjSize = -1;\n+    _currObjSizeStack = null;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(_currObjSize)), false);\n+            }\n+            else\n+            {\n+              // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n+              // need to create a new one.\n+              if (_currDataMapBuilder == null || _currDataMapBuilder.inUse()) {\n+                _currDataMapBuilder = new DataMapBuilder();\n+              }\n+              _currDataMapBuilder.setInUse(true);\n+              push(_currDataMapBuilder, false);\n+            }\n+            break;\n+          case END_OBJECT:\n+            pop();\n+            break;\n+          case START_ARRAY:\n+            _currObjSize = _parser.currentComplexObjSize();\n+            if (_currObjSize >= 0)\n+            {\n+              push(new DataList(_currObjSize), true);\n+            }\n+            else\n+            {\n+              push(new DataList(), true);\n+            }\n+            break;\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null) {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            } else {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (_currObjSize >= 0)\n+    {\n+      _currObjSizeStack.push(_currObjSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11a52aacb7f98d8ce451319628105352cc511557"}, "originalPosition": 256}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjA5ODM3OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/parser/NonBlockingDataParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyMToyN1rOGZK_5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyMToyN1rOGZK_5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0Nzc4Mg==", "bodyText": "Update the doc: EOF_INPUT to indicate end of input", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429047782", "createdAt": "2020-05-22T05:21:27Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/parser/NonBlockingDataParser.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.data.parser;\n+\n+import com.linkedin.data.ByteString;\n+import java.io.IOException;\n+\n+\n+/**\n+ * Data parser interface invoked by non blocking decoder.\n+ *\n+ * This interface contains methods that are invoked when parsing a Data object.\n+ * Each method represents a different kind of event/read action\n+ *\n+ * Methods can throw IOException as a checked exception to indicate parsing error.\n+ *\n+ * @author amgupta1\n+ */\n+public interface NonBlockingDataParser\n+{\n+  /**\n+   * Internal tokens, used to identify types of elements in data during decoding\n+   */\n+  enum Token\n+  {\n+    /**\n+     * START_OBJECT is returned when encountering signals starting of an Object/map value.\n+     */\n+    START_OBJECT,\n+    /**\n+     * END_OBJECT is returned when encountering signals ending of an Object/map value\n+     */\n+    END_OBJECT,\n+    /**\n+     * START_ARRAY is returned when encountering signals starting of an Array value\n+     */\n+    START_ARRAY,\n+    /**\n+     * END_ARRAY is returned when encountering signals ending of an Array value\n+     */\n+    END_ARRAY,\n+    /**\n+     * STRING is returned when encountering a string value, field name or reference\n+     */\n+    STRING,\n+    /**\n+     * RAW_BYTES is returned when encountering chunk of raw bytes\n+     */\n+    RAW_BYTES,\n+    /**\n+     * INTEGER is returned when encountering integer value\n+     */\n+    INTEGER,\n+    /**\n+     * LONG is returned when encountering long value\n+     */\n+    LONG,\n+    /**\n+     * FLOAT is returned when encountering float decimal value\n+     */\n+    FLOAT,\n+    /**\n+     * DOUBLE is returned when encountering double decimal value\n+     */\n+    DOUBLE,\n+    /**\n+     * BOOL_TRUE is returned when encountering boolean true value\n+     */\n+    BOOL_TRUE,\n+    /**\n+     * BOOL_FALSE is returned when encountering boolean false value\n+     */\n+    BOOL_FALSE,\n+    /**\n+     * NULL is returned when encountering \"null\" in value context\n+     */\n+    NULL,\n+    /**\n+     * NOT_AVAILABLE can be returned if {@link NonBlockingDataParser} implementation can not currently\n+     * return the requested token (usually next one), but that may be able to determine this in future.\n+     * non-blocking parsers can not block to wait for more data to parse and must return something.\n+     */\n+    NOT_AVAILABLE,\n+    /**\n+     * Token returned at point when all feed input has been exhausted or\n+     * input feeder has indicated no more input will be forthcoming.\n+     */\n+    EOF_INPUT\n+  }\n+\n+  /**\n+   * Method that can be called to feed more data if {@link #nextToken()} returns {@link Token#NOT_AVAILABLE}\n+   *\n+   * @param data Byte array that contains data to feed: caller must ensure data remains\n+   *    stable until it is fully processed\n+   * @param offset Offset where input data to process starts\n+   * @param len length of bytes to be feed from the input array\n+   *\n+   * @throws IOException if the state is such that this method should not be called\n+   *   (has not yet consumed existing input data, or has been marked as closed)\n+   */\n+  void feedInput(byte[] data, int offset, int len) throws IOException;\n+\n+  /**\n+   * Method that should be called after last chunk of data to parse has been fed\n+   * (with {@link #feedInput(byte[], int, int)}). After calling this method,\n+   * no more data can be fed; and parser assumes no more data will be available.\n+   */\n+  void endOfInput();\n+\n+  /**\n+   * Main iteration method, which will advance input enough to determine type of the next token, if any.\n+   * If none remaining (input has no content other than possible white space before ending),\n+   * {@link Token#EOF_INPUT} will be returned.\n+   *\n+   * @return Next token from the input, if any found, or null to indicate end-of-input", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjA5OTkyOnYy", "diffSide": "LEFT", "path": "data/src/main/java/com/linkedin/data/Data.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyMjozNFrOGZLA1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyMjozNFrOGZLA1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0ODAyMw==", "bodyText": "revert", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429048023", "createdAt": "2020-05-22T05:22:34Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/Data.java", "diffHunk": "@@ -16,7 +16,6 @@\n \n package com.linkedin.data;\n \n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjEwMjc4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNDoyMlrOGZLCfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNDoyMlrOGZLCfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0ODQ0Ng==", "bodyText": "Add javadoc", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429048446", "createdAt": "2020-05-22T05:24:22Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 281}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjEwMzc3OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNTowNFrOGZLDHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNTowNFrOGZLDHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0ODYwNQ==", "bodyText": "Javadoc", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429048605", "createdAt": "2020-05-22T05:25:04Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n+  {\n+    return dataComplex;\n+  }\n+\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+      }\n+      else\n+      {\n+        addEntryToDataObject(currItem, _currField, value);\n+        _currField = null;\n+      }\n+      updateExpected();\n+    }\n+  }\n+\n+  protected void addEntryToDataObject(DataComplex dataObject, String currField, Object currValue)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 304}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjEwNjIxOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNjozN1rOGZLEkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjowODo0NFrOGZLs9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0ODk3OA==", "bodyText": "This can be private? If not add doc", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429048978", "createdAt": "2020-05-22T05:26:37Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n+  {\n+    return dataComplex;\n+  }\n+\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+      }\n+      else\n+      {\n+        addEntryToDataObject(currItem, _currField, value);\n+        _currField = null;\n+      }\n+      updateExpected();\n+    }\n+  }\n+\n+  protected void addEntryToDataObject(DataComplex dataObject, String currField, Object currValue)\n+  {\n+    CheckedUtil.putWithoutChecking((DataMap) dataObject, currField, currValue);\n+  }\n+\n+  protected final void updateObjectStackPeek(DataComplex dataComplex)\n+  {\n+    _stack.pop();\n+    _stack.push(dataComplex);\n+  }\n+  \n+  protected void updateExpected()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 315}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA1OTMxNw==", "bodyText": "added doc, cannot make this private as this was already exposed", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429059317", "createdAt": "2020-05-22T06:08:44Z", "author": {"login": "aman1309"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n+  {\n+    return dataComplex;\n+  }\n+\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+      }\n+      else\n+      {\n+        addEntryToDataObject(currItem, _currField, value);\n+        _currField = null;\n+      }\n+      updateExpected();\n+    }\n+  }\n+\n+  protected void addEntryToDataObject(DataComplex dataObject, String currField, Object currValue)\n+  {\n+    CheckedUtil.putWithoutChecking((DataMap) dataObject, currField, currValue);\n+  }\n+\n+  protected final void updateObjectStackPeek(DataComplex dataComplex)\n+  {\n+    _stack.pop();\n+    _stack.push(dataComplex);\n+  }\n+  \n+  protected void updateExpected()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0ODk3OA=="}, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 315}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjEwNjMyOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNjo0NVrOGZLErA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNToyNjo0NVrOGZLErA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA0OTAwNA==", "bodyText": "Add javadoc\nFor naming: replaceObjectStackTop ?", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429049004", "createdAt": "2020-05-22T05:26:45Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n+  {\n+    return dataComplex;\n+  }\n+\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+      }\n+      else\n+      {\n+        addEntryToDataObject(currItem, _currField, value);\n+        _currField = null;\n+      }\n+      updateExpected();\n+    }\n+  }\n+\n+  protected void addEntryToDataObject(DataComplex dataObject, String currField, Object currValue)\n+  {\n+    CheckedUtil.putWithoutChecking((DataMap) dataObject, currField, currValue);\n+  }\n+\n+  protected final void updateObjectStackPeek(DataComplex dataComplex)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 309}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjExOTA0OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTozNToyMVrOGZLMuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTozNToyMVrOGZLMuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA1MTA2Nw==", "bodyText": "final", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429051067", "createdAt": "2020-05-22T05:35:21Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjEyMTE5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractJacksonDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTozNjo0MlrOGZLN9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTozNjo0MlrOGZLN9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA1MTM4Mw==", "bodyText": "final", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429051383", "createdAt": "2020-05-22T05:36:42Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractJacksonDataDecoder.java", "diffHunk": "@@ -76,328 +65,204 @@\n     }\n   }\n \n-  private static final byte VALUE = (byte) (SIMPLE_VALUE.bitPattern | START_OBJECT.bitPattern | START_ARRAY.bitPattern);\n-  private static final byte NEXT_OBJECT_FIELD = (byte) (FIELD_NAME.bitPattern | END_OBJECT.bitPattern);\n-  private static final byte NEXT_ARRAY_ITEM = (byte) (VALUE | END_ARRAY.bitPattern);\n-\n   private final JsonFactory _jsonFactory;\n-\n-  private CompletableFuture<T> _completable;\n-  private T _result;\n-  private ReadHandle _readHandle;\n-\n-  private JsonParser _jsonParser;\n-  private ByteArrayFeeder _byteArrayFeeder;\n-\n-  private Deque<DataComplex> _stack;\n-  private Deque<String> _currFieldStack;\n-  private String _currField;\n-  // Expected tokens represented by a bit pattern. Every bit represents a token.\n-  private byte _expectedTokens;\n-  private boolean _isCurrList;\n-\n-  private ByteString _currentChunk;\n-  private int _currentChunkIndex = -1;\n-\n   private DataMapBuilder _currDataMapBuilder;\n \n+  /**\n+   * Deprecated, use {@link #AbstractJacksonDataDecoder(JsonFactory, EnumSet)} instead\n+   */\n+  @Deprecated\n   protected AbstractJacksonDataDecoder(JsonFactory jsonFactory, byte expectedFirstToken)\n   {\n+    super();\n     _jsonFactory = jsonFactory;\n-    _completable = new CompletableFuture<>();\n-    _result = null;\n-    _stack = new ArrayDeque<>();\n-    _currFieldStack = new ArrayDeque<>();\n-    _expectedTokens = expectedFirstToken;\n-    _currDataMapBuilder = new DataMapBuilder();\n+    EnumSet<NonBlockingDataParser.Token> expectedDataToken = NONE;\n+    if ((expectedFirstToken & Token.START_OBJECT.bitPattern) != 0) {\n+      expectedDataToken.add(NonBlockingDataParser.Token.START_OBJECT);\n+    }\n+    if ((expectedFirstToken & Token.START_ARRAY.bitPattern) != 0) {\n+      expectedDataToken.add(NonBlockingDataParser.Token.START_ARRAY);\n+    }\n+    _expectedTokens = expectedDataToken;\n   }\n \n   protected AbstractJacksonDataDecoder(JsonFactory jsonFactory)\n   {\n-    this(jsonFactory, (byte) (START_OBJECT.bitPattern | START_ARRAY.bitPattern));\n+    this(jsonFactory, START_TOKENS);\n   }\n \n-  @Override\n-  public void onInit(ReadHandle rh)\n+  protected AbstractJacksonDataDecoder(JsonFactory jsonFactory, EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n   {\n-    _readHandle = rh;\n-\n-    try\n-    {\n-      _jsonParser = _jsonFactory.createNonBlockingByteArrayParser();\n-      _byteArrayFeeder = (ByteArrayFeeder)_jsonParser;\n-    }\n-    catch (IOException e)\n-    {\n-      handleException(e);\n-    }\n-\n-    _readHandle.request(1);\n+    super(expectedFirstTokens);\n+    _jsonFactory = jsonFactory;\n   }\n \n   @Override\n-  public void onDataAvailable(ByteString data)\n+  protected NonBlockingDataParser createDataParser() throws IOException\n   {\n-    // Process chunk incrementally without copying the data in the interest of performance.\n-    _currentChunk = data;\n-    _currentChunkIndex = 0;\n-\n-    processCurrentChunk();\n+    return new JacksonStreamDataParser(_jsonFactory);\n   }\n \n-  private void readNextChunk()\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n   {\n-    if (_currentChunkIndex == -1)\n+    if (_currDataMapBuilder == null || _currDataMapBuilder.inUse())\n     {\n-      _readHandle.request(1);\n-      return;\n+      _currDataMapBuilder = new DataMapBuilder();\n     }\n+    _currDataMapBuilder.setInUse(true);\n+    return _currDataMapBuilder;\n+  }\n \n-    processCurrentChunk();\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList();\n   }\n \n-  private void processCurrentChunk()\n+  @Override\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n   {\n-    try\n-    {\n-      _currentChunkIndex = _currentChunk.feed(_byteArrayFeeder, _currentChunkIndex);\n-      processTokens();\n-    }\n-    catch (IOException e)\n+    if (dataComplex instanceof DataMapBuilder)\n     {\n-      handleException(e);\n+      dataComplex = ((DataMapBuilder) dataComplex).convertToDataMap();\n     }\n+    return dataComplex;\n   }\n \n-  private void processTokens()\n+  @Override\n+  protected void addEntryToDataObject(DataComplex dataComplex, String currField, Object currValue)\n   {\n-    try\n+    if (dataComplex instanceof DataMapBuilder)\n     {\n-      JsonToken token;\n-      while ((token = _jsonParser.nextToken()) != null)\n+      DataMapBuilder dataMapBuilder = (DataMapBuilder) dataComplex;\n+      if (dataMapBuilder.smallHashMapThresholdReached())\n+      {\n+        DataMap dataMap = dataMapBuilder.convertToDataMap();\n+        updateObjectStackPeek(dataMap);\n+        CheckedUtil.putWithoutChecking(dataMap, currField, currValue);\n+      }\n+      else\n       {\n-        switch (token)\n-        {\n-          case START_OBJECT:\n-            validate(START_OBJECT);\n-            // If we are already filling out a DataMap, we cannot reuse _currDataMapBuilder and thus\n-            // need to create a new one.\n-            if (_currDataMapBuilder.inUse())\n-            {\n-              _currDataMapBuilder = new DataMapBuilder();\n-            }\n-            _currDataMapBuilder.setInUse(true);\n-            push(_currDataMapBuilder, false);\n-            break;\n-          case END_OBJECT:\n-            validate(END_OBJECT);\n-            pop();\n-            break;\n-          case START_ARRAY:\n-            validate(START_ARRAY);\n-            push(new DataList(), true);\n-            break;\n-          case END_ARRAY:\n-            validate(END_ARRAY);\n-            pop();\n-            break;\n-          case FIELD_NAME:\n-            validate(FIELD_NAME);\n-            _currField = _jsonParser.getCurrentName();\n-            _expectedTokens = VALUE;\n-            break;\n-          case VALUE_STRING:\n-            validate(SIMPLE_VALUE);\n-            addValue(_jsonParser.getText());\n-            break;\n-          case VALUE_NUMBER_INT:\n-          case VALUE_NUMBER_FLOAT:\n-            validate(SIMPLE_VALUE);\n-            JsonParser.NumberType numberType = _jsonParser.getNumberType();\n-            switch (numberType)\n-            {\n-              case INT:\n-                addValue(_jsonParser.getIntValue());\n-                break;\n-              case LONG:\n-                addValue(_jsonParser.getLongValue());\n-                break;\n-              case FLOAT:\n-                addValue(_jsonParser.getFloatValue());\n-                break;\n-              case DOUBLE:\n-                addValue(_jsonParser.getDoubleValue());\n-                break;\n-              case BIG_INTEGER:\n-              case BIG_DECIMAL:\n-              default:\n-                handleException(new Exception(\"Unexpected number value type \" + numberType + \" at \" + _jsonParser.getTokenLocation()));\n-                break;\n-            }\n-            break;\n-          case VALUE_TRUE:\n-            validate(SIMPLE_VALUE);\n-            addValue(Boolean.TRUE);\n-            break;\n-          case VALUE_FALSE:\n-            validate(SIMPLE_VALUE);\n-            addValue(Boolean.FALSE);\n-            break;\n-          case VALUE_NULL:\n-            validate(SIMPLE_VALUE);\n-            addValue(Data.NULL);\n-            break;\n-          case NOT_AVAILABLE:\n-            readNextChunk();\n-            return;\n-          default:\n-            handleException(new Exception(\"Unexpected token \" + token + \" at \" + _jsonParser.getTokenLocation()));\n-        }\n+        dataMapBuilder.addKVPair(currField, currValue);\n       }\n     }\n-    catch (IOException e)\n+    else\n     {\n-      handleException(e);\n+      CheckedUtil.putWithoutChecking((DataMap) dataComplex, currField, currValue);\n     }\n   }\n \n-  protected final void validate(Token token)\n+  class JacksonStreamDataParser implements NonBlockingDataParser\n   {\n-    if ((_expectedTokens & token.bitPattern) == 0)\n-    {\n-      handleException(new Exception(\"Expecting \" + joinTokens(_expectedTokens) + \" but get \" + token\n-          + \" at \" + _jsonParser.getTokenLocation()));\n-    }\n-  }\n+    private JsonParser _jsonParser;\n+    private ByteArrayFeeder _byteArrayFeeder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 304}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjEyMTk0OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTozNzoxN1rOGZLObw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTozNzoxN1rOGZLObw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA1MTUwMw==", "bodyText": "final", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429051503", "createdAt": "2020-05-22T05:37:17Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private Deque<DataComplex> _stack;\n+  private Deque<String> _currFieldStack;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2f7e32257904da61970edd60a501a78714aa02f"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjIzNDA2OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjozODozMlrOGZMRWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjozODozMlrOGZMRWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2ODYzMg==", "bodyText": "nit: Method invoked..\ns/poped/ popped\nand the method should be named postProcessDataComplex? as DataObject is used for Maps in other places of this class.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429068632", "createdAt": "2020-05-22T06:38:32Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private final CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private final Deque<DataComplex> _stack;\n+  private final Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  /**\n+   * method invoked to do any post processing on complex object/list after its completely parsed and poped from stack\n+   */\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7b6cc4f9fe4a081a7c834e4241005539bda091"}, "originalPosition": 284}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjIzNjM4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjozOTo1MFrOGZMSxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjozOTo1MFrOGZMSxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2ODk5Ng==", "bodyText": "nit: Method..", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429068996", "createdAt": "2020-05-22T06:39:50Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private final CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private final Deque<DataComplex> _stack;\n+  private final Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  /**\n+   * method invoked to do any post processing on complex object/list after its completely parsed and poped from stack\n+   */\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n+  {\n+    return dataComplex;\n+  }\n+\n+  /**\n+   * method invoked to add element to currently pending complex object/list", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7b6cc4f9fe4a081a7c834e4241005539bda091"}, "originalPosition": 290}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjIzODI5OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjo0MDozNVrOGZMT7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNjo0MDozNVrOGZMT7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2OTI5NA==", "bodyText": "Method invoked..\n\"currently pending\" -> the provided data object.", "url": "https://github.com/linkedin/rest.li/pull/292#discussion_r429069294", "createdAt": "2020-05-22T06:40:35Z", "author": {"login": "karthikbalasub"}, "path": "data/src/main/java/com/linkedin/data/codec/entitystream/AbstractDataDecoder.java", "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.Data;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.collections.CheckedUtil;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.entitystream.ReadHandle;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+\n+/**\n+ * A decoder for a {@link DataComplex} object implemented as a\n+ * {@link com.linkedin.entitystream.Reader} reading from an {@link com.linkedin.entitystream.EntityStream} of\n+ * ByteString. The implementation is backed by a non blocking {@link NonBlockingDataParser}\n+ * because the raw bytes are pushed to the decoder, it keeps the partially built data structure in a stack.\n+ * It is not thread safe. Caller must ensure thread safety.\n+ *\n+ * @author kramgopa, xma, amgupta1\n+ */\n+abstract class AbstractDataDecoder<T extends DataComplex> implements DataDecoder<T>\n+{\n+  private static final EnumSet<NonBlockingDataParser.Token> SIMPLE_VALUE =\n+      EnumSet.of(STRING, RAW_BYTES, INTEGER, LONG, FLOAT, DOUBLE, BOOL_TRUE, BOOL_FALSE, NULL);\n+  private static final EnumSet<NonBlockingDataParser.Token> FIELD_NAME = EnumSet.of(STRING);\n+  private static final EnumSet<NonBlockingDataParser.Token> VALUE = EnumSet.of(START_OBJECT, START_ARRAY);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_OBJECT_FIELD = EnumSet.of(END_OBJECT);\n+  private static final EnumSet<NonBlockingDataParser.Token> NEXT_ARRAY_ITEM = EnumSet.of(END_ARRAY);\n+\n+  protected static final EnumSet<NonBlockingDataParser.Token> NONE = EnumSet.noneOf(NonBlockingDataParser.Token.class);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_TOKENS =\n+      EnumSet.of(NonBlockingDataParser.Token.START_OBJECT, NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_ARRAY_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_ARRAY);\n+  protected static final EnumSet<NonBlockingDataParser.Token> START_OBJECT_TOKEN = EnumSet.of(NonBlockingDataParser.Token.START_OBJECT);\n+\n+  static\n+  {\n+    VALUE.addAll(SIMPLE_VALUE);\n+    NEXT_OBJECT_FIELD.addAll(FIELD_NAME);\n+    NEXT_ARRAY_ITEM.addAll(VALUE);\n+  }\n+\n+  private final CompletableFuture<T> _completable;\n+  private T _result;\n+  private ReadHandle _readHandle;\n+  private NonBlockingDataParser _parser;\n+\n+  private final Deque<DataComplex> _stack;\n+  private final Deque<String> _currFieldStack;\n+  private String _currField;\n+  private boolean _isCurrList;\n+  private ByteString _currentChunk;\n+  private int _currentChunkIndex = -1;\n+\n+  protected EnumSet<NonBlockingDataParser.Token> _expectedTokens;\n+\n+  protected AbstractDataDecoder(EnumSet<NonBlockingDataParser.Token> expectedFirstTokens)\n+  {\n+    _completable = new CompletableFuture<>();\n+    _result = null;\n+    _stack = new ArrayDeque<>();\n+    _currFieldStack = new ArrayDeque<>();\n+    _expectedTokens = expectedFirstTokens;\n+  }\n+\n+  protected AbstractDataDecoder()\n+  {\n+    this(START_TOKENS);\n+  }\n+\n+  @Override\n+  public void onInit(ReadHandle rh)\n+  {\n+    _readHandle = rh;\n+\n+    try\n+    {\n+      _parser = createDataParser();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+\n+    _readHandle.request(1);\n+  }\n+\n+  /**\n+   * Interface to create non blocking data object parser that process different kind of event/read operations.\n+   */\n+  protected abstract NonBlockingDataParser createDataParser() throws IOException;\n+\n+  @Override\n+  public void onDataAvailable(ByteString data)\n+  {\n+    // Process chunk incrementally without copying the data in the interest of performance.\n+    _currentChunk = data;\n+    _currentChunkIndex = 0;\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void readNextChunk()\n+  {\n+    if (_currentChunkIndex == -1)\n+    {\n+      _readHandle.request(1);\n+      return;\n+    }\n+\n+    processCurrentChunk();\n+  }\n+\n+  private void processCurrentChunk()\n+  {\n+    try\n+    {\n+      _currentChunkIndex = _currentChunk.feed(_parser, _currentChunkIndex);\n+      processTokens();\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  private void processTokens()\n+  {\n+    try\n+    {\n+      NonBlockingDataParser.Token token;\n+      while ((token = _parser.nextToken()) != EOF_INPUT)\n+      {\n+        validate(token);\n+        switch (token)\n+        {\n+          case START_OBJECT:\n+            push(createDataObject(_parser), false);\n+            break;\n+          case START_ARRAY:\n+            push(createDataList(_parser), true);\n+            break;\n+          case END_OBJECT:\n+          case END_ARRAY:\n+            pop();\n+            break;\n+          case STRING:\n+            if (!_isCurrList && _currField == null)\n+            {\n+              _currField = _parser.getString();\n+              _expectedTokens = VALUE;\n+            }\n+            else\n+            {\n+              addValue(_parser.getString());\n+            }\n+            break;\n+          case RAW_BYTES:\n+            addValue(_parser.getRawBytes());\n+            break;\n+          case INTEGER:\n+            addValue(_parser.getIntValue());\n+            break;\n+          case LONG:\n+            addValue(_parser.getLongValue());\n+            break;\n+          case FLOAT:\n+            addValue(_parser.getFloatValue());\n+            break;\n+          case DOUBLE:\n+            addValue(_parser.getDoubleValue());\n+            break;\n+          case BOOL_TRUE:\n+            addValue(Boolean.TRUE);\n+            break;\n+          case BOOL_FALSE:\n+            addValue(Boolean.FALSE);\n+            break;\n+          case NULL:\n+            addValue(Data.NULL);\n+            break;\n+          case NOT_AVAILABLE:\n+            readNextChunk();\n+            return;\n+          default:\n+            handleException(new Exception(\"Unexpected token \" + token + \" from data parser\"));\n+        }\n+      }\n+    }\n+    catch (IOException e)\n+    {\n+      handleException(e);\n+    }\n+  }\n+\n+  /**\n+   * Interface to new complex object, invoked at the start of parsing object.\n+   */\n+  protected abstract DataComplex createDataObject(NonBlockingDataParser parser);\n+\n+  /**\n+   * Interface to new complex list, invoked at start of parsing array.\n+   */\n+  protected abstract DataComplex createDataList(NonBlockingDataParser parser);\n+\n+  protected final boolean isCurrList()\n+  {\n+    return _isCurrList;\n+  }\n+\n+  protected final void validate(NonBlockingDataParser.Token token)\n+  {\n+    if (!(token == NOT_AVAILABLE || _expectedTokens.contains(token)))\n+    {\n+      handleException(new Exception(\"Expecting \" + _expectedTokens + \" but got \" + token));\n+    }\n+  }\n+\n+  private void push(DataComplex dataComplex, boolean isList)\n+  {\n+    if (!(_isCurrList || _stack.isEmpty()))\n+    {\n+      _currFieldStack.push(_currField);\n+      _currField = null;\n+    }\n+    _stack.push(dataComplex);\n+    _isCurrList = isList;\n+    updateExpected();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private void pop()\n+  {\n+    // The stack should never be empty because of token validation.\n+    assert !_stack.isEmpty() : \"Trying to pop empty stack\";\n+\n+    DataComplex tmp = _stack.pop();\n+    tmp = postProcessDataObject(tmp);\n+    if (_stack.isEmpty())\n+    {\n+      _result = (T) tmp;\n+      // No more tokens is expected.\n+      _expectedTokens = NONE;\n+    }\n+    else\n+    {\n+      _isCurrList = _stack.peek() instanceof DataList;\n+      if (!_isCurrList)\n+      {\n+        _currField = _currFieldStack.pop();\n+      }\n+      addValue(tmp);\n+      updateExpected();\n+    }\n+  }\n+\n+  /**\n+   * method invoked to do any post processing on complex object/list after its completely parsed and poped from stack\n+   */\n+  protected DataComplex postProcessDataObject(DataComplex dataComplex)\n+  {\n+    return dataComplex;\n+  }\n+\n+  /**\n+   * method invoked to add element to currently pending complex object/list\n+   */\n+  protected void addValue(Object value)\n+  {\n+    if (!_stack.isEmpty())\n+    {\n+      DataComplex currItem = _stack.peek();\n+      if (_isCurrList)\n+      {\n+        CheckedUtil.addWithoutChecking((DataList) currItem, value);\n+      }\n+      else\n+      {\n+        addEntryToDataObject(currItem, _currField, value);\n+        _currField = null;\n+      }\n+      updateExpected();\n+    }\n+  }\n+\n+  /**\n+   * method invoked to add element to currently pending complex object", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7b6cc4f9fe4a081a7c834e4241005539bda091"}, "originalPosition": 311}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 437, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}