{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcwMzk3MTY2", "number": 10698, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMTo1OToxNlrODc86mQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMzozNjoxMVrODeBcIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjg0NzYxOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMTo1OToxNlrOFlRjBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMTo1OToxNlrOFlRjBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYyOTEyNg==", "bodyText": "synchronized on what? assume it is always completedWork for all of these variables?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374629126", "createdAt": "2020-02-04T11:59:16Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjg1MjYyOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjowMToxM1rOFlRmKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNDo1MTowM1rOFl6jvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYyOTkyOQ==", "bodyText": "Does a value less than or equal to zero actually mean unlimited batch size?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374629929", "createdAt": "2020-02-04T12:01:13Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY3NzcyNw==", "bodyText": "Yes, I'll add doc for zero being a special value for both maxCommitBatchSize and maxCommitBatchInterval.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374677727", "createdAt": "2020-02-04T13:46:21Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYyOTkyOQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMwMTA1NA==", "bodyText": "zero or less", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375301054", "createdAt": "2020-02-05T14:51:03Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYyOTkyOQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjkwNzMzOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjoyMToyMlrOFlSHZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxMzo0OToxMlrOFmbocA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYzODQzNw==", "bodyText": "huh? Committed is past tense, next message is future??", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374638437", "createdAt": "2020-02-04T12:21:22Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));\n+            outstandingUncommittedWork++;\n+            requestCommit();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Request that done but uncommitted work is committed, either now or in the future\n+     * <p>\n+     * This method will either commit the partition offset now, or schedule it to be done in the future, depending on the values of {@link #maxCommitBatchInterval} and\n+     * {@link #maxCommitBatchSize}.\n+     * <p>\n+     * Calls to this method must be synchronized.\n+     */\n+    private void requestCommit() {\n+        if ((maxCommitBatchSize > 0) && (outstandingUncommittedWork > maxCommitBatchSize)) {\n+            if (pendingCommitTask != null) {\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Cancelling scheduled commit task because we're committing right now\", this);\n+                }\n+                pendingCommitTask.cancel(true);\n+            }\n+            // commit now\n+            commitCompletedWork();\n+        } else {\n+            if ((pendingCommitTask == null) && !maxCommitBatchInterval.isZero()) {\n+                // commit later\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Scheduling deferred commit task\", this);\n+                }\n+                pendingCommitTask = executor.schedule(this::commitCompletedWork, maxCommitBatchInterval.toNanos(), TimeUnit.NANOSECONDS);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to commit the latest block of completed but uncommitted work\n+     * <p>\n+     * We can only commit the offset for a record if all prior records are complete. This method looks through the committed work to see which messages can be committed without\n+     * leaving a gap and then commits them.\n+     */\n+    private void commitCompletedWork() {\n+        synchronized (completedWork) {\n+            if (Thread.interrupted()) {\n+                // Don't do anything if we were cancelled\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Commit task running but has been cancelled\", this);\n+                }\n+                return;\n+            }\n+\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Checking for new work to commit, last committed offset is \" + committedOffset, this);\n+                Tr.debug(this, tc, \"Current completed work\", completedWork);\n+            }\n+\n+            long newCommitOffset = committedOffset;\n+            CompletedWork newestWork = null;\n+            for (CompletedWork work : completedWork) {\n+                if (work.offset < newCommitOffset) {\n+                    // Work that we've already asked to commit, ignore it\n+                } else if (work.offset == newCommitOffset) {\n+                    // Work that should be committed now\n+                    newCommitOffset++;\n+                    newestWork = work;\n+                } else {\n+                    // We've reached the end of a continuous block of completed work\n+                    // Can't commit any further work until everything before it has been committed\n+                    break;\n+                }\n+            }\n+\n+            if (newestWork != null) {\n+                // commit\n+                long originalCommitOffset = committedOffset;\n+                long finalCommitOffset = newCommitOffset;\n+\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isEventEnabled()) {\n+                    Tr.event(this, tc, \"Committing from \" + originalCommitOffset + \" to \" + finalCommitOffset, this);\n+                }\n+                commitUpTo(newestWork).whenCompleteAsync((r, t) -> processCommittedWork(originalCommitOffset, finalCommitOffset, t), executor);\n+            }\n+\n+            outstandingUncommittedWork -= newCommitOffset - committedOffset;\n+            pendingCommitTask = null;\n+            committedOffset = newCommitOffset;\n+        }\n+    }\n+\n+    /**\n+     * Commit the offset up to the offset of the given work\n+     *\n+     * @param work\n+     * @return completion stage which completes with the result of the commit, or completes exceptionally if the commit failed\n+     */\n+    private CompletionStage<Void> commitUpTo(CompletedWork work) {\n+        if (isClosed()) {\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Rejecting commit attempt because partition is closed\", this);\n+            }\n+\n+            CompletableFuture<Void> result = new CompletableFuture<>();\n+            result.completeExceptionally(new Exception(\"Partition is closed\"));\n+            return result;\n+        }\n+\n+        // Note work.offset + 1\n+        // Committed offset is the _next_ message we expect to receive", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY3OTg4MA==", "bodyText": "Does this make more sense:\n        // Note work.offset + 1\n        // In general the committed offset for a partition is the first message which should be received by a new consumer\n        // which starts consuming from that partition.\n        // Therefore, the offset which we are about to commit must be the offset _after_ the last message we have processed.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374679880", "createdAt": "2020-02-04T13:50:19Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));\n+            outstandingUncommittedWork++;\n+            requestCommit();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Request that done but uncommitted work is committed, either now or in the future\n+     * <p>\n+     * This method will either commit the partition offset now, or schedule it to be done in the future, depending on the values of {@link #maxCommitBatchInterval} and\n+     * {@link #maxCommitBatchSize}.\n+     * <p>\n+     * Calls to this method must be synchronized.\n+     */\n+    private void requestCommit() {\n+        if ((maxCommitBatchSize > 0) && (outstandingUncommittedWork > maxCommitBatchSize)) {\n+            if (pendingCommitTask != null) {\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Cancelling scheduled commit task because we're committing right now\", this);\n+                }\n+                pendingCommitTask.cancel(true);\n+            }\n+            // commit now\n+            commitCompletedWork();\n+        } else {\n+            if ((pendingCommitTask == null) && !maxCommitBatchInterval.isZero()) {\n+                // commit later\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Scheduling deferred commit task\", this);\n+                }\n+                pendingCommitTask = executor.schedule(this::commitCompletedWork, maxCommitBatchInterval.toNanos(), TimeUnit.NANOSECONDS);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to commit the latest block of completed but uncommitted work\n+     * <p>\n+     * We can only commit the offset for a record if all prior records are complete. This method looks through the committed work to see which messages can be committed without\n+     * leaving a gap and then commits them.\n+     */\n+    private void commitCompletedWork() {\n+        synchronized (completedWork) {\n+            if (Thread.interrupted()) {\n+                // Don't do anything if we were cancelled\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Commit task running but has been cancelled\", this);\n+                }\n+                return;\n+            }\n+\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Checking for new work to commit, last committed offset is \" + committedOffset, this);\n+                Tr.debug(this, tc, \"Current completed work\", completedWork);\n+            }\n+\n+            long newCommitOffset = committedOffset;\n+            CompletedWork newestWork = null;\n+            for (CompletedWork work : completedWork) {\n+                if (work.offset < newCommitOffset) {\n+                    // Work that we've already asked to commit, ignore it\n+                } else if (work.offset == newCommitOffset) {\n+                    // Work that should be committed now\n+                    newCommitOffset++;\n+                    newestWork = work;\n+                } else {\n+                    // We've reached the end of a continuous block of completed work\n+                    // Can't commit any further work until everything before it has been committed\n+                    break;\n+                }\n+            }\n+\n+            if (newestWork != null) {\n+                // commit\n+                long originalCommitOffset = committedOffset;\n+                long finalCommitOffset = newCommitOffset;\n+\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isEventEnabled()) {\n+                    Tr.event(this, tc, \"Committing from \" + originalCommitOffset + \" to \" + finalCommitOffset, this);\n+                }\n+                commitUpTo(newestWork).whenCompleteAsync((r, t) -> processCommittedWork(originalCommitOffset, finalCommitOffset, t), executor);\n+            }\n+\n+            outstandingUncommittedWork -= newCommitOffset - committedOffset;\n+            pendingCommitTask = null;\n+            committedOffset = newCommitOffset;\n+        }\n+    }\n+\n+    /**\n+     * Commit the offset up to the offset of the given work\n+     *\n+     * @param work\n+     * @return completion stage which completes with the result of the commit, or completes exceptionally if the commit failed\n+     */\n+    private CompletionStage<Void> commitUpTo(CompletedWork work) {\n+        if (isClosed()) {\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Rejecting commit attempt because partition is closed\", this);\n+            }\n+\n+            CompletableFuture<Void> result = new CompletableFuture<>();\n+            result.completeExceptionally(new Exception(\"Partition is closed\"));\n+            return result;\n+        }\n+\n+        // Note work.offset + 1\n+        // Committed offset is the _next_ message we expect to receive", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYzODQzNw=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMwMDQ3MQ==", "bodyText": "If the committed offset is the first message which should be received by a new consumer ... does that mean it's counting from 1? There is no message \"index\" 0?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375300471", "createdAt": "2020-02-05T14:50:09Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));\n+            outstandingUncommittedWork++;\n+            requestCommit();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Request that done but uncommitted work is committed, either now or in the future\n+     * <p>\n+     * This method will either commit the partition offset now, or schedule it to be done in the future, depending on the values of {@link #maxCommitBatchInterval} and\n+     * {@link #maxCommitBatchSize}.\n+     * <p>\n+     * Calls to this method must be synchronized.\n+     */\n+    private void requestCommit() {\n+        if ((maxCommitBatchSize > 0) && (outstandingUncommittedWork > maxCommitBatchSize)) {\n+            if (pendingCommitTask != null) {\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Cancelling scheduled commit task because we're committing right now\", this);\n+                }\n+                pendingCommitTask.cancel(true);\n+            }\n+            // commit now\n+            commitCompletedWork();\n+        } else {\n+            if ((pendingCommitTask == null) && !maxCommitBatchInterval.isZero()) {\n+                // commit later\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Scheduling deferred commit task\", this);\n+                }\n+                pendingCommitTask = executor.schedule(this::commitCompletedWork, maxCommitBatchInterval.toNanos(), TimeUnit.NANOSECONDS);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to commit the latest block of completed but uncommitted work\n+     * <p>\n+     * We can only commit the offset for a record if all prior records are complete. This method looks through the committed work to see which messages can be committed without\n+     * leaving a gap and then commits them.\n+     */\n+    private void commitCompletedWork() {\n+        synchronized (completedWork) {\n+            if (Thread.interrupted()) {\n+                // Don't do anything if we were cancelled\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Commit task running but has been cancelled\", this);\n+                }\n+                return;\n+            }\n+\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Checking for new work to commit, last committed offset is \" + committedOffset, this);\n+                Tr.debug(this, tc, \"Current completed work\", completedWork);\n+            }\n+\n+            long newCommitOffset = committedOffset;\n+            CompletedWork newestWork = null;\n+            for (CompletedWork work : completedWork) {\n+                if (work.offset < newCommitOffset) {\n+                    // Work that we've already asked to commit, ignore it\n+                } else if (work.offset == newCommitOffset) {\n+                    // Work that should be committed now\n+                    newCommitOffset++;\n+                    newestWork = work;\n+                } else {\n+                    // We've reached the end of a continuous block of completed work\n+                    // Can't commit any further work until everything before it has been committed\n+                    break;\n+                }\n+            }\n+\n+            if (newestWork != null) {\n+                // commit\n+                long originalCommitOffset = committedOffset;\n+                long finalCommitOffset = newCommitOffset;\n+\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isEventEnabled()) {\n+                    Tr.event(this, tc, \"Committing from \" + originalCommitOffset + \" to \" + finalCommitOffset, this);\n+                }\n+                commitUpTo(newestWork).whenCompleteAsync((r, t) -> processCommittedWork(originalCommitOffset, finalCommitOffset, t), executor);\n+            }\n+\n+            outstandingUncommittedWork -= newCommitOffset - committedOffset;\n+            pendingCommitTask = null;\n+            committedOffset = newCommitOffset;\n+        }\n+    }\n+\n+    /**\n+     * Commit the offset up to the offset of the given work\n+     *\n+     * @param work\n+     * @return completion stage which completes with the result of the commit, or completes exceptionally if the commit failed\n+     */\n+    private CompletionStage<Void> commitUpTo(CompletedWork work) {\n+        if (isClosed()) {\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Rejecting commit attempt because partition is closed\", this);\n+            }\n+\n+            CompletableFuture<Void> result = new CompletableFuture<>();\n+            result.completeExceptionally(new Exception(\"Partition is closed\"));\n+            return result;\n+        }\n+\n+        // Note work.offset + 1\n+        // Committed offset is the _next_ message we expect to receive", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYzODQzNw=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTg0MjkyOA==", "bodyText": "No, the offset of the first message is 0.\nImagine there are three messages in the partition, none have yet been consumed. The committed offset is 0, the next message to be received has offset 0.\nThe first message is read and processed, the consumer doesn't want to receive this message again. The consumer has processed the record with offset 0, it sets the committed offset to 1, the next message to be received has offset 1.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375842928", "createdAt": "2020-02-06T13:49:12Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));\n+            outstandingUncommittedWork++;\n+            requestCommit();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Request that done but uncommitted work is committed, either now or in the future\n+     * <p>\n+     * This method will either commit the partition offset now, or schedule it to be done in the future, depending on the values of {@link #maxCommitBatchInterval} and\n+     * {@link #maxCommitBatchSize}.\n+     * <p>\n+     * Calls to this method must be synchronized.\n+     */\n+    private void requestCommit() {\n+        if ((maxCommitBatchSize > 0) && (outstandingUncommittedWork > maxCommitBatchSize)) {\n+            if (pendingCommitTask != null) {\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Cancelling scheduled commit task because we're committing right now\", this);\n+                }\n+                pendingCommitTask.cancel(true);\n+            }\n+            // commit now\n+            commitCompletedWork();\n+        } else {\n+            if ((pendingCommitTask == null) && !maxCommitBatchInterval.isZero()) {\n+                // commit later\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Scheduling deferred commit task\", this);\n+                }\n+                pendingCommitTask = executor.schedule(this::commitCompletedWork, maxCommitBatchInterval.toNanos(), TimeUnit.NANOSECONDS);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to commit the latest block of completed but uncommitted work\n+     * <p>\n+     * We can only commit the offset for a record if all prior records are complete. This method looks through the committed work to see which messages can be committed without\n+     * leaving a gap and then commits them.\n+     */\n+    private void commitCompletedWork() {\n+        synchronized (completedWork) {\n+            if (Thread.interrupted()) {\n+                // Don't do anything if we were cancelled\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Commit task running but has been cancelled\", this);\n+                }\n+                return;\n+            }\n+\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Checking for new work to commit, last committed offset is \" + committedOffset, this);\n+                Tr.debug(this, tc, \"Current completed work\", completedWork);\n+            }\n+\n+            long newCommitOffset = committedOffset;\n+            CompletedWork newestWork = null;\n+            for (CompletedWork work : completedWork) {\n+                if (work.offset < newCommitOffset) {\n+                    // Work that we've already asked to commit, ignore it\n+                } else if (work.offset == newCommitOffset) {\n+                    // Work that should be committed now\n+                    newCommitOffset++;\n+                    newestWork = work;\n+                } else {\n+                    // We've reached the end of a continuous block of completed work\n+                    // Can't commit any further work until everything before it has been committed\n+                    break;\n+                }\n+            }\n+\n+            if (newestWork != null) {\n+                // commit\n+                long originalCommitOffset = committedOffset;\n+                long finalCommitOffset = newCommitOffset;\n+\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isEventEnabled()) {\n+                    Tr.event(this, tc, \"Committing from \" + originalCommitOffset + \" to \" + finalCommitOffset, this);\n+                }\n+                commitUpTo(newestWork).whenCompleteAsync((r, t) -> processCommittedWork(originalCommitOffset, finalCommitOffset, t), executor);\n+            }\n+\n+            outstandingUncommittedWork -= newCommitOffset - committedOffset;\n+            pendingCommitTask = null;\n+            committedOffset = newCommitOffset;\n+        }\n+    }\n+\n+    /**\n+     * Commit the offset up to the offset of the given work\n+     *\n+     * @param work\n+     * @return completion stage which completes with the result of the commit, or completes exceptionally if the commit failed\n+     */\n+    private CompletionStage<Void> commitUpTo(CompletedWork work) {\n+        if (isClosed()) {\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Rejecting commit attempt because partition is closed\", this);\n+            }\n+\n+            CompletableFuture<Void> result = new CompletableFuture<>();\n+            result.completeExceptionally(new Exception(\"Partition is closed\"));\n+            return result;\n+        }\n+\n+        // Note work.offset + 1\n+        // Committed offset is the _next_ message we expect to receive", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDYzODQzNw=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 223}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjkxOTE1OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjoyNTo0MFrOFlSOig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjoyNTo0MFrOFlSOig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY0MDI2Ng==", "bodyText": "Maybe use a different local variable name? To avoid confusion with the instance TreeSet variable above.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374640266", "createdAt": "2020-02-04T12:25:40Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));\n+            outstandingUncommittedWork++;\n+            requestCommit();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Request that done but uncommitted work is committed, either now or in the future\n+     * <p>\n+     * This method will either commit the partition offset now, or schedule it to be done in the future, depending on the values of {@link #maxCommitBatchInterval} and\n+     * {@link #maxCommitBatchSize}.\n+     * <p>\n+     * Calls to this method must be synchronized.\n+     */\n+    private void requestCommit() {\n+        if ((maxCommitBatchSize > 0) && (outstandingUncommittedWork > maxCommitBatchSize)) {\n+            if (pendingCommitTask != null) {\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Cancelling scheduled commit task because we're committing right now\", this);\n+                }\n+                pendingCommitTask.cancel(true);\n+            }\n+            // commit now\n+            commitCompletedWork();\n+        } else {\n+            if ((pendingCommitTask == null) && !maxCommitBatchInterval.isZero()) {\n+                // commit later\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Scheduling deferred commit task\", this);\n+                }\n+                pendingCommitTask = executor.schedule(this::commitCompletedWork, maxCommitBatchInterval.toNanos(), TimeUnit.NANOSECONDS);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to commit the latest block of completed but uncommitted work\n+     * <p>\n+     * We can only commit the offset for a record if all prior records are complete. This method looks through the committed work to see which messages can be committed without\n+     * leaving a gap and then commits them.\n+     */\n+    private void commitCompletedWork() {\n+        synchronized (completedWork) {\n+            if (Thread.interrupted()) {\n+                // Don't do anything if we were cancelled\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                    Tr.debug(this, tc, \"Commit task running but has been cancelled\", this);\n+                }\n+                return;\n+            }\n+\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Checking for new work to commit, last committed offset is \" + committedOffset, this);\n+                Tr.debug(this, tc, \"Current completed work\", completedWork);\n+            }\n+\n+            long newCommitOffset = committedOffset;\n+            CompletedWork newestWork = null;\n+            for (CompletedWork work : completedWork) {\n+                if (work.offset < newCommitOffset) {\n+                    // Work that we've already asked to commit, ignore it\n+                } else if (work.offset == newCommitOffset) {\n+                    // Work that should be committed now\n+                    newCommitOffset++;\n+                    newestWork = work;\n+                } else {\n+                    // We've reached the end of a continuous block of completed work\n+                    // Can't commit any further work until everything before it has been committed\n+                    break;\n+                }\n+            }\n+\n+            if (newestWork != null) {\n+                // commit\n+                long originalCommitOffset = committedOffset;\n+                long finalCommitOffset = newCommitOffset;\n+\n+                if (TraceComponent.isAnyTracingEnabled() && tc.isEventEnabled()) {\n+                    Tr.event(this, tc, \"Committing from \" + originalCommitOffset + \" to \" + finalCommitOffset, this);\n+                }\n+                commitUpTo(newestWork).whenCompleteAsync((r, t) -> processCommittedWork(originalCommitOffset, finalCommitOffset, t), executor);\n+            }\n+\n+            outstandingUncommittedWork -= newCommitOffset - committedOffset;\n+            pendingCommitTask = null;\n+            committedOffset = newCommitOffset;\n+        }\n+    }\n+\n+    /**\n+     * Commit the offset up to the offset of the given work\n+     *\n+     * @param work\n+     * @return completion stage which completes with the result of the commit, or completes exceptionally if the commit failed\n+     */\n+    private CompletionStage<Void> commitUpTo(CompletedWork work) {\n+        if (isClosed()) {\n+            if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+                Tr.debug(this, tc, \"Rejecting commit attempt because partition is closed\", this);\n+            }\n+\n+            CompletableFuture<Void> result = new CompletableFuture<>();\n+            result.completeExceptionally(new Exception(\"Partition is closed\"));\n+            return result;\n+        }\n+\n+        // Note work.offset + 1\n+        // Committed offset is the _next_ message we expect to receive\n+        OffsetAndMetadata offsetAndMetadata = factory.newOffsetAndMetadata(work.offset + 1, work.leaderEpoch, null);\n+        return kafkaInput.commitOffsets(topicPartition, offsetAndMetadata);\n+    }\n+\n+    /**\n+     * Complete the CompletionStage associated with work which has been committed\n+     * <p>\n+     * This is called as callback after the partition offset has been committed asynchronously\n+     *\n+     * @param originalOffset the committed offset before this commit\n+     * @param committedOffset the new committed offset\n+     * @param exception the exception which caused the commit to fail, or {@code null} if it was successful\n+     */\n+    private void processCommittedWork(long originalOffset, long committedOffset, Throwable exception) {\n+\n+        if (TraceComponent.isAnyTracingEnabled() && tc.isDebugEnabled()) {\n+            if (exception == null) {\n+                Tr.debug(this, tc, \"Commit from \" + originalOffset + \" to \" + committedOffset + \" completed successfully\", this);\n+            } else {\n+                Tr.debug(this, tc, \"Commit from \" + originalOffset + \" to \" + committedOffset + \" failed\", this, exception);\n+            }\n+        }\n+\n+        // Note: Pull out the list of completed work inside the synchronized block\n+        //       but complete the CompletionStage outside the synchronized block\n+        List<CompletedWork> committedWork = new ArrayList<>();\n+        synchronized (completedWork) {\n+            for (Iterator<CompletedWork> i = completedWork.iterator(); i.hasNext();) {\n+                CompletedWork completedWork = i.next();\n+                if (completedWork.offset < originalOffset) {\n+                    continue;\n+                }\n+                if (completedWork.offset >= committedOffset) {\n+                    break;\n+                }\n+                committedWork.add(completedWork);\n+                i.remove();\n+            }\n+        }\n+\n+        if (exception == null) {\n+            for (CompletedWork completedWork : committedWork) {\n+                completedWork.completion.complete(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 266}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjkyNzc1OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjoyODo0MVrOFlSToA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNjo0OTo0M1rOFlbiEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY0MTU2OA==", "bodyText": "It might never happen (or matter) but... you seem to have code that copes with the same offset being recorded more than once; if that happens should you really increment outstandingUncommittedWork?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374641568", "createdAt": "2020-02-04T12:28:41Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc5MjcyMA==", "bodyText": "We ensure that everything works if commitCompletedWork() is called a second time before the CompletionStage from the previous commit has completed, but we don't actually do anything to protect against recordDone() being called twice with the same offset.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374792720", "createdAt": "2020-02-04T16:49:43Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY0MTU2OA=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNjkyODkwOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjoyOTowNFrOFlSUTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMjoyOTowNFrOFlSUTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY0MTc0Mw==", "bodyText": "synchronized on what?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374641743", "createdAt": "2020-02-04T12:29:04Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/CommittingPartitionTracker.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.OffsetAndMetadata;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks the assignment of a partition to this consumer and commits message offsets to that partition\n+ * <p>\n+ * In addition to the function of {@link PartitionTracker}, this class additionally commits message offsets back to the kafka broker in response to a call to\n+ * {@link #recordDone(long, Optional)}.\n+ * <p>\n+ * For performance, each completed record isn't committed immediately. Instead, this class tries to batch up completed records and commit them together. There are several\n+ * parameters that control how often the message offset is committed:\n+ * <ul>\n+ * <li>{@code maxCommitBatchSize}: sets the maximum number of records to wait for before committing the offset</li>\n+ * <li>{@code maxCommitBatchInterval}: sets the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset</li>\n+ * </ul>\n+ */\n+public class CommittingPartitionTracker extends PartitionTracker {\n+\n+    private static final TraceComponent tc = Tr.register(CommittingPartitionTracker.class);\n+\n+    private final KafkaAdapterFactory factory;\n+    private final ScheduledExecutorService executor;\n+    private final KafkaInput<?, ?> kafkaInput;\n+    private final int maxCommitBatchSize;\n+    private final Duration maxCommitBatchInterval;\n+\n+    /**\n+     * Set of CompletedWork which has either not been committed or has been committed but the commit has not completed yet.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private final SortedSet<CompletedWork> completedWork = new TreeSet<>();\n+\n+    /**\n+     * The count of CompletedWork for which a commit has not been started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private int outstandingUncommittedWork = 0;\n+\n+    /**\n+     * The last offset for which a commit was started\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private long committedOffset;\n+\n+    /**\n+     * The currently scheduled task which will attempt to commit completed work or {@code null} if no task has been scheduled.\n+     * <p>\n+     * Any access to this must be synchronized\n+     */\n+    private Future<?> pendingCommitTask = null;\n+\n+    /**\n+     *\n+     * @param topicPartition the partition to track\n+     * @param factory the KafkaAdaptorFactory\n+     * @param kafkaInput the KafkaInput\n+     * @param initialCommittedOffset the position of the reader when the partition was assigned\n+     * @param executor a ScheduledExecutorService\n+     * @param maxCommitBatchSize the maximum number of records to wait for before committing the offset\n+     * @param maxCommitBatchInterval the maximum time to wait after a {@link #recordDone(long, Optional)} is called before committing the offset\n+     */\n+    public CommittingPartitionTracker(TopicPartition topicPartition,\n+                                      KafkaAdapterFactory factory,\n+                                      KafkaInput<?, ?> kafkaInput,\n+                                      long initialCommittedOffset,\n+                                      ScheduledExecutorService executor,\n+                                      int maxCommitBatchSize,\n+                                      Duration maxCommitBatchInterval) {\n+        super(topicPartition);\n+        this.factory = factory;\n+        this.kafkaInput = kafkaInput;\n+        this.executor = executor;\n+        this.committedOffset = initialCommittedOffset;\n+        this.maxCommitBatchSize = maxCommitBatchSize;\n+        this.maxCommitBatchInterval = maxCommitBatchInterval;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+\n+        CompletableFuture<Void> result = new CompletableFuture<>();\n+        synchronized (completedWork) {\n+            completedWork.add(new CompletedWork(offset, leaderEpoch, result));\n+            outstandingUncommittedWork++;\n+            requestCommit();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Request that done but uncommitted work is committed, either now or in the future\n+     * <p>\n+     * This method will either commit the partition offset now, or schedule it to be done in the future, depending on the values of {@link #maxCommitBatchInterval} and\n+     * {@link #maxCommitBatchSize}.\n+     * <p>\n+     * Calls to this method must be synchronized.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxNzIwODEzOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/KafkaInput.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMzo1OToxMVrOFlU8Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxMzo1OTozMFrOFlU9EQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY4NDc3NQ==", "bodyText": "copyright date", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374684775", "createdAt": "2020-02-04T13:59:11Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/KafkaInput.java", "diffHunk": "@@ -12,11 +12,11 @@\n \n import static com.ibm.websphere.ras.TraceComponent.isAnyTracingEnabled;\n import static java.time.Duration.ZERO;\n-import static org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams.fromIterable;\n \n import java.time.Duration;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.HashMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY4NDk0NQ==", "bodyText": "probably needs checking thoughout", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374684945", "createdAt": "2020-02-04T13:59:30Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/KafkaInput.java", "diffHunk": "@@ -12,11 +12,11 @@\n \n import static com.ibm.websphere.ras.TraceComponent.isAnyTracingEnabled;\n import static java.time.Duration.ZERO;\n-import static org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams.fromIterable;\n \n import java.time.Duration;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.HashMap;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDY4NDc3NQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODAxNjcxOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/PartitionTracker.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzoyNjozN1rOFlc0Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxMzo1Mzo0OVrOFmbyJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxMzczNQ==", "bodyText": "what if isClosed == true?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374813735", "createdAt": "2020-02-04T17:26:37Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/PartitionTracker.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks a particular assignment of a partition to this consumer\n+ * <p>\n+ * This class has two important jobs:\n+ * <ul>\n+ * <li>{@link #recordDone(long, Optional)} can be used to create the result of {@link Message#ack()}</li>\n+ * <li>tracks whether this partition has been revoked (available from {@link #isClosed()})</li>\n+ * </ul>\n+ * <p>\n+ * If a partition is revoked and then later reassigned to this consumer, a new {@code PartitionTracker} instance is created to track that assignment.\n+ */\n+public class PartitionTracker {\n+\n+    protected AtomicBoolean isClosed;\n+    protected final TopicPartition topicPartition;\n+\n+    /**\n+     * @param topicPartition the partition to track\n+     */\n+    public PartitionTracker(TopicPartition topicPartition) {\n+        this.topicPartition = topicPartition;\n+        this.isClosed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Records that the assigned partition has been revoked\n+     */\n+    public void close() {\n+        isClosed.set(true);\n+    }\n+\n+    /**\n+     * Returns whether the assigned partition has been revoked\n+     *\n+     * @return {@code true} if the assigned partition has been revoked\n+     */\n+    public boolean isClosed() {\n+        return isClosed.get();\n+    }\n+\n+    /**\n+     * Record that processing of a record has been completed.\n+     * <p>\n+     * The result of this method can be used as the return value from {@link Message#ack()}\n+     *\n+     * @param offset the record offset\n+     * @param leaderEpoch the record leaderEpoch\n+     * @return a CompletionStage which completes when any associated processing has been completed (e.g. when the message offset has been committed)\n+     */\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTg0NTQxNA==", "bodyText": "I don't think we care, this class is only used when we don't need to do anything in response to ack(), so I don't think we should say that we \"failed\" to do nothing.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375845414", "createdAt": "2020-02-06T13:53:49Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/PartitionTracker.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ * Tracks a particular assignment of a partition to this consumer\n+ * <p>\n+ * This class has two important jobs:\n+ * <ul>\n+ * <li>{@link #recordDone(long, Optional)} can be used to create the result of {@link Message#ack()}</li>\n+ * <li>tracks whether this partition has been revoked (available from {@link #isClosed()})</li>\n+ * </ul>\n+ * <p>\n+ * If a partition is revoked and then later reassigned to this consumer, a new {@code PartitionTracker} instance is created to track that assignment.\n+ */\n+public class PartitionTracker {\n+\n+    protected AtomicBoolean isClosed;\n+    protected final TopicPartition topicPartition;\n+\n+    /**\n+     * @param topicPartition the partition to track\n+     */\n+    public PartitionTracker(TopicPartition topicPartition) {\n+        this.topicPartition = topicPartition;\n+        this.isClosed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Records that the assigned partition has been revoked\n+     */\n+    public void close() {\n+        isClosed.set(true);\n+    }\n+\n+    /**\n+     * Returns whether the assigned partition has been revoked\n+     *\n+     * @return {@code true} if the assigned partition has been revoked\n+     */\n+    public boolean isClosed() {\n+        return isClosed.get();\n+    }\n+\n+    /**\n+     * Record that processing of a record has been completed.\n+     * <p>\n+     * The result of this method can be used as the return value from {@link Message#ack()}\n+     *\n+     * @param offset the record offset\n+     * @param leaderEpoch the record leaderEpoch\n+     * @return a CompletionStage which completes when any associated processing has been completed (e.g. when the message offset has been committed)\n+     */\n+    public CompletionStage<Void> recordDone(long offset, Optional<Integer> leaderEpoch) {\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxMzczNQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODAyMDgyOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/PartitionTrackerFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzoyNzo0OVrOFlc2uA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxNDowMjo0OFrOFmcFvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNDM5Mg==", "bodyText": "Does manualCommit = !autoCommit? It's called autoCommit elsewhere, why the inversion?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374814392", "createdAt": "2020-02-04T17:27:49Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/PartitionTrackerFactory.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ *\n+ */\n+public class PartitionTrackerFactory {\n+\n+    private KafkaAdapterFactory adapterFactory = null;\n+    private ScheduledExecutorService executor = null;\n+    private int commitBatchMaxElements = 500;\n+    private Duration commitBatchMaxInterval = Duration.ofMillis(500);\n+    private boolean manualCommit = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTg1MDQyOQ==", "bodyText": "It made more sense when I was writing it, since the manual case is the one where we have to do stuff. I'll change it.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375850429", "createdAt": "2020-02-06T14:02:48Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/PartitionTrackerFactory.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.time.Duration;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.KafkaAdapterFactory;\n+import com.ibm.ws.microprofile.reactive.messaging.kafka.adapter.TopicPartition;\n+\n+/**\n+ *\n+ */\n+public class PartitionTrackerFactory {\n+\n+    private KafkaAdapterFactory adapterFactory = null;\n+    private ScheduledExecutorService executor = null;\n+    private int commitBatchMaxElements = 500;\n+    private Duration commitBatchMaxInterval = Duration.ofMillis(500);\n+    private boolean manualCommit = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNDM5Mg=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODAzNDYwOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounter.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzozMjowM1rOFlc_dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNDo0NTo0MVrOFl6Weg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNjYyOQ==", "bodyText": "is there a better name than DUMMY?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374816629", "createdAt": "2020-02-04T17:32:03Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounter.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+/**\n+ * A counter which can return a CompletionStage that completes when the counter value drops below a threshold value\n+ */\n+public interface ThresholdCounter {\n+\n+    /**\n+     * Increment the counter\n+     */\n+    void increment();\n+\n+    /**\n+     * Decrement the counter\n+     */\n+    void decrement();\n+\n+    /**\n+     * Returns a completion stage which completes when the counter is less than the threshold value\n+     * <p>\n+     * If the counter is already below the threshold value then a completed completion stage is returned\n+     *\n+     * @return completion stage which completes when the counter is less than the threshold value\n+     */\n+    CompletionStage<Void> waitForBelowThreshold();\n+\n+    ThresholdCounter DUMMY = new ThresholdCounter() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI4NjE4Mg==", "bodyText": "I like DUMMY, though I'm open to other suggestions.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375286182", "createdAt": "2020-02-05T14:27:27Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounter.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+/**\n+ * A counter which can return a CompletionStage that completes when the counter value drops below a threshold value\n+ */\n+public interface ThresholdCounter {\n+\n+    /**\n+     * Increment the counter\n+     */\n+    void increment();\n+\n+    /**\n+     * Decrement the counter\n+     */\n+    void decrement();\n+\n+    /**\n+     * Returns a completion stage which completes when the counter is less than the threshold value\n+     * <p>\n+     * If the counter is already below the threshold value then a completed completion stage is returned\n+     *\n+     * @return completion stage which completes when the counter is less than the threshold value\n+     */\n+    CompletionStage<Void> waitForBelowThreshold();\n+\n+    ThresholdCounter DUMMY = new ThresholdCounter() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNjYyOQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI4NzE0NQ==", "bodyText": "Maybe MAX_THRESHOLD would be appropriate, we're essentially saying that there is no threshold we must stay below.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375287145", "createdAt": "2020-02-05T14:28:55Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounter.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+/**\n+ * A counter which can return a CompletionStage that completes when the counter value drops below a threshold value\n+ */\n+public interface ThresholdCounter {\n+\n+    /**\n+     * Increment the counter\n+     */\n+    void increment();\n+\n+    /**\n+     * Decrement the counter\n+     */\n+    void decrement();\n+\n+    /**\n+     * Returns a completion stage which completes when the counter is less than the threshold value\n+     * <p>\n+     * If the counter is already below the threshold value then a completed completion stage is returned\n+     *\n+     * @return completion stage which completes when the counter is less than the threshold value\n+     */\n+    CompletionStage<Void> waitForBelowThreshold();\n+\n+    ThresholdCounter DUMMY = new ThresholdCounter() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNjYyOQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI5NzY1OA==", "bodyText": "UNLIMITED?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375297658", "createdAt": "2020-02-05T14:45:41Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounter.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+/**\n+ * A counter which can return a CompletionStage that completes when the counter value drops below a threshold value\n+ */\n+public interface ThresholdCounter {\n+\n+    /**\n+     * Increment the counter\n+     */\n+    void increment();\n+\n+    /**\n+     * Decrement the counter\n+     */\n+    void decrement();\n+\n+    /**\n+     * Returns a completion stage which completes when the counter is less than the threshold value\n+     * <p>\n+     * If the counter is already below the threshold value then a completed completion stage is returned\n+     *\n+     * @return completion stage which completes when the counter is less than the threshold value\n+     */\n+    CompletionStage<Void> waitForBelowThreshold();\n+\n+    ThresholdCounter DUMMY = new ThresholdCounter() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgxNjYyOQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA1ODgyOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/flatmap/package-info.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzozOTo1OVrOFldO4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNDoyOTo0NFrOFl5vcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMDU3OA==", "bodyText": "Why is this file needed? If not need then I think there is another one which can be removed too.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374820578", "createdAt": "2020-02-04T17:39:59Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/flatmap/package-info.java", "diffHunk": "@@ -0,0 +1,14 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+/**\n+ *\n+ */\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.flatmap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI4NzY2NA==", "bodyText": "No", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375287664", "createdAt": "2020-02-05T14:29:44Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/flatmap/package-info.java", "diffHunk": "@@ -0,0 +1,14 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+/**\n+ *\n+ */\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.flatmap;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMDU3OA=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA2NzIwOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/KafkaPartitionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0Mjo1MFrOFldUPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNDozMToyM1rOFl5zlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMTk0OA==", "bodyText": "is 2 partitions enough to properly exercise this code?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374821948", "createdAt": "2020-02-04T17:42:50Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/KafkaPartitionTest.java", "diffHunk": "@@ -56,14 +62,20 @@ public static void setup() throws Exception {\n         adminClientProps.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, PlaintextTests.kafkaContainer.getBootstrapServers());\n         AdminClient adminClient = AdminClient.create(adminClientProps);\n \n-        NewTopic newTopic = new NewTopic(PartitionTestReceptionBean.CHANNEL_NAME, 2, (short) 1);\n-        adminClient.createTopics(Collections.singleton(newTopic)).all().get(KafkaTestConstants.DEFAULT_KAFKA_TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        List<NewTopic> newTopics = new ArrayList<>();\n+        newTopics.add(new NewTopic(PartitionTestReceptionBean.CHANNEL_NAME, 2, (short) 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI4ODcyNg==", "bodyText": "I think so. We're really just looking to test the case where a partition is assigned, removed, then assigned again to test that messages are handled correctly. We could add more partitions though we might then need to process more messages and make the test take longer and I'm already a bit concerned about how long this test will take.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375288726", "createdAt": "2020-02-05T14:31:23Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/KafkaPartitionTest.java", "diffHunk": "@@ -56,14 +62,20 @@ public static void setup() throws Exception {\n         adminClientProps.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, PlaintextTests.kafkaContainer.getBootstrapServers());\n         AdminClient adminClient = AdminClient.create(adminClientProps);\n \n-        NewTopic newTopic = new NewTopic(PartitionTestReceptionBean.CHANNEL_NAME, 2, (short) 1);\n-        adminClient.createTopics(Collections.singleton(newTopic)).all().get(KafkaTestConstants.DEFAULT_KAFKA_TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        List<NewTopic> newTopics = new ArrayList<>();\n+        newTopics.add(new NewTopic(PartitionTestReceptionBean.CHANNEL_NAME, 2, (short) 1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMTk0OA=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA2OTk3OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/flatmap/KafkaFlatMapServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0Mzo0N1rOFldWDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNDoyOTozOFrOFl5vLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMjQxMg==", "bodyText": "can you use a constant for the timeouts?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374822412", "createdAt": "2020-02-04T17:43:47Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/flatmap/KafkaFlatMapServlet.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.flatmap;\n+\n+import static java.time.temporal.ChronoUnit.SECONDS;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+\n+import java.time.Duration;\n+import java.util.List;\n+\n+import javax.servlet.annotation.WebServlet;\n+\n+import org.junit.Test;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.AbstractKafkaTestServlet;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.SimpleKafkaReader;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.SimpleKafkaWriter;\n+\n+/**\n+ *\n+ */\n+@WebServlet(\"/flatMapTest\")\n+public class KafkaFlatMapServlet extends AbstractKafkaTestServlet {\n+\n+    /**  */\n+    private static final long serialVersionUID = 1L;\n+\n+    public static final String IN_TOPIC = \"flat-map-in\";\n+    public static final String OUT_TOPIC = \"flat-map-out\";\n+    public static final String APP_GROUPID = \"KafkaFlatMapTest-group\";\n+\n+    @Test\n+    public void testFlatMap() throws InterruptedException {\n+        SimpleKafkaWriter<String> writer = kafkaTestClient.writerFor(IN_TOPIC);\n+        SimpleKafkaReader<String> reader = kafkaTestClient.readerFor(OUT_TOPIC);\n+\n+        long offset = kafkaTestClient.getTopicOffset(IN_TOPIC, APP_GROUPID);\n+\n+        writer.sendMessage(\"abc\");\n+        writer.sendMessage(\"abcd\");\n+        writer.sendMessage(\"abcde\");\n+        writer.sendMessage(\"abcdef\");\n+\n+        List<String> messages = reader.waitForMessages(2, Duration.of(10, SECONDS));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI4NzU5OQ==", "bodyText": "Yes", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375287599", "createdAt": "2020-02-05T14:29:38Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/flatmap/KafkaFlatMapServlet.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.flatmap;\n+\n+import static java.time.temporal.ChronoUnit.SECONDS;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+\n+import java.time.Duration;\n+import java.util.List;\n+\n+import javax.servlet.annotation.WebServlet;\n+\n+import org.junit.Test;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.AbstractKafkaTestServlet;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.SimpleKafkaReader;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.SimpleKafkaWriter;\n+\n+/**\n+ *\n+ */\n+@WebServlet(\"/flatMapTest\")\n+public class KafkaFlatMapServlet extends AbstractKafkaTestServlet {\n+\n+    /**  */\n+    private static final long serialVersionUID = 1L;\n+\n+    public static final String IN_TOPIC = \"flat-map-in\";\n+    public static final String OUT_TOPIC = \"flat-map-out\";\n+    public static final String APP_GROUPID = \"KafkaFlatMapTest-group\";\n+\n+    @Test\n+    public void testFlatMap() throws InterruptedException {\n+        SimpleKafkaWriter<String> writer = kafkaTestClient.writerFor(IN_TOPIC);\n+        SimpleKafkaReader<String> reader = kafkaTestClient.readerFor(OUT_TOPIC);\n+\n+        long offset = kafkaTestClient.getTopicOffset(IN_TOPIC, APP_GROUPID);\n+\n+        writer.sendMessage(\"abc\");\n+        writer.sendMessage(\"abcd\");\n+        writer.sendMessage(\"abcde\");\n+        writer.sendMessage(\"abcdef\");\n+\n+        List<String> messages = reader.waitForMessages(2, Duration.of(10, SECONDS));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMjQxMg=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA3MzAyOnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestBean.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0NDo1NFrOFldYBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0NDo1NFrOFldYBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMjkxOQ==", "bodyText": "Can you use a constant for the number of partitions?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374822919", "createdAt": "2020-02-04T17:44:54Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestBean.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment.Strategy;\n+import org.eclipse.microprofile.reactive.messaging.Incoming;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ *\n+ */\n+@ApplicationScoped\n+public class LivePartitionTestBean {\n+\n+    public static final String CHANNEL_IN = \"live-partition-test-in\";\n+    public static final int WORK_TIME = 100;\n+    public static final int FINAL_MESSAGE_NUMBER = 9999;\n+\n+    ArrayList<ReceivedMessage> messages = new ArrayList<>();\n+    private final CountDownLatch paritionsFinished = new CountDownLatch(2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA3NDM2OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestBean.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0NToyNVrOFldY8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNDozMjowMlrOFl51FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMzE1NQ==", "bodyText": "timeout constant ... based on the number of partitions??", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374823155", "createdAt": "2020-02-04T17:45:25Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestBean.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment.Strategy;\n+import org.eclipse.microprofile.reactive.messaging.Incoming;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ *\n+ */\n+@ApplicationScoped\n+public class LivePartitionTestBean {\n+\n+    public static final String CHANNEL_IN = \"live-partition-test-in\";\n+    public static final int WORK_TIME = 100;\n+    public static final int FINAL_MESSAGE_NUMBER = 9999;\n+\n+    ArrayList<ReceivedMessage> messages = new ArrayList<>();\n+    private final CountDownLatch paritionsFinished = new CountDownLatch(2);\n+\n+    @Incoming(CHANNEL_IN)\n+    @Acknowledgment(Strategy.MANUAL)\n+    public CompletionStage<Void> receive(Message<String> message) throws InterruptedException {\n+\n+        Thread.sleep(WORK_TIME);\n+\n+        ReceivedMessage status = new ReceivedMessage(message.getPayload());\n+        messages.add(status);\n+        System.out.println(\"Bean received message \" + message.getPayload());\n+\n+        message.ack().handle((r, t) -> {\n+            if (t == null) {\n+                status.ackStatus.set(AckStatus.ACK_SUCCESS);\n+                if (status.number == FINAL_MESSAGE_NUMBER) {\n+                    paritionsFinished.countDown();\n+                }\n+            } else {\n+                status.ackStatus.set(AckStatus.ACK_FAILED);\n+            }\n+            return null;\n+        });\n+\n+        return CompletableFuture.completedFuture(null);\n+    }\n+\n+    public void awaitFinish() throws InterruptedException {\n+        assertTrue(\"Test bean did not process all messages within 30 seconds\", paritionsFinished.await(30, TimeUnit.SECONDS));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI4OTEwOQ==", "bodyText": "If anything it should be based on the number of messages.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375289109", "createdAt": "2020-02-05T14:32:02Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestBean.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment.Strategy;\n+import org.eclipse.microprofile.reactive.messaging.Incoming;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ *\n+ */\n+@ApplicationScoped\n+public class LivePartitionTestBean {\n+\n+    public static final String CHANNEL_IN = \"live-partition-test-in\";\n+    public static final int WORK_TIME = 100;\n+    public static final int FINAL_MESSAGE_NUMBER = 9999;\n+\n+    ArrayList<ReceivedMessage> messages = new ArrayList<>();\n+    private final CountDownLatch paritionsFinished = new CountDownLatch(2);\n+\n+    @Incoming(CHANNEL_IN)\n+    @Acknowledgment(Strategy.MANUAL)\n+    public CompletionStage<Void> receive(Message<String> message) throws InterruptedException {\n+\n+        Thread.sleep(WORK_TIME);\n+\n+        ReceivedMessage status = new ReceivedMessage(message.getPayload());\n+        messages.add(status);\n+        System.out.println(\"Bean received message \" + message.getPayload());\n+\n+        message.ack().handle((r, t) -> {\n+            if (t == null) {\n+                status.ackStatus.set(AckStatus.ACK_SUCCESS);\n+                if (status.number == FINAL_MESSAGE_NUMBER) {\n+                    paritionsFinished.countDown();\n+                }\n+            } else {\n+                status.ackStatus.set(AckStatus.ACK_FAILED);\n+            }\n+            return null;\n+        });\n+\n+        return CompletableFuture.completedFuture(null);\n+    }\n+\n+    public void awaitFinish() throws InterruptedException {\n+        assertTrue(\"Test bean did not process all messages within 30 seconds\", paritionsFinished.await(30, TimeUnit.SECONDS));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMzE1NQ=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA3ODk0OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestConsumer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0Njo1NFrOFldb2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0Njo1NFrOFldb2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyMzg5OA==", "bodyText": "number of messages in a constant?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374823898", "createdAt": "2020-02-04T17:46:54Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestConsumer.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Consumer which joins the topic, consumes and commits five messages and then closes\n+ */\n+public class LivePartitionTestConsumer implements Runnable {\n+\n+    private final Map<String, Object> config;\n+    private final String topic;\n+    private final List<String> messagesRecieved;\n+    private final long MAX_DURATION = Duration.ofSeconds(5).toNanos();\n+\n+    public LivePartitionTestConsumer(Map<String, Object> config, String topic) {\n+        this.config = new HashMap<>(config);\n+        this.config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n+        this.topic = topic;\n+        this.messagesRecieved = new ArrayList<>();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try (KafkaConsumer<?, String> consumer = new KafkaConsumer<>(config)) {\n+            consumer.subscribe(Collections.singleton(topic));\n+            ConsumerRecord<?, String> lastRecord = null;\n+\n+            int messages = 0;\n+            long startTime = System.nanoTime();\n+            while (messages < 5 && System.nanoTime() - startTime < MAX_DURATION) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA4MTM2OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestServlet.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0Nzo0N1rOFlddSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQxNjozMDowM1rOFmhtrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyNDI2Nw==", "bodyText": "constant?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374824267", "createdAt": "2020-02-04T17:47:47Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestServlet.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import javax.annotation.Resource;\n+import javax.enterprise.concurrent.ManagedExecutorService;\n+import javax.inject.Inject;\n+import javax.servlet.annotation.WebServlet;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.junit.Test;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.AbstractKafkaTestServlet;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.AckStatus;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.ReceivedMessage;\n+\n+/**\n+ * Tests partition rebalancing in something more akin to a live environment\n+ * <p>\n+ * In particular, we want to test rebalancing while the application is actively processing messages to ensure that none are lost and they're handled correctly.\n+ */\n+@WebServlet(\"/LivePartitionTest\")\n+public class LivePartitionTestServlet extends AbstractKafkaTestServlet {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    public static final String APP_GROUPID = \"kafka-live-partition-test-group\";\n+\n+    @Resource\n+    private ManagedExecutorService executor;\n+\n+    @Inject\n+    private LivePartitionTestBean bean;\n+\n+    @Test\n+    public void testLivePartitionAssignment() throws Exception {\n+\n+        List<String> sentMessages = new ArrayList<>();\n+\n+        Map<String, Object> producerConfig = new HashMap<>();\n+        producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+        producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+\n+        // Load messages into topic\n+        try (KafkaProducer<String, String> producer = new KafkaProducer<>(producerConfig)) {\n+            for (int partition = 0; partition < 2; partition++) {\n+                for (int message = 0; message < 100; message++) {\n+                    String value = partition + \"-\" + message;\n+                    ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                    producer.send(record);\n+                    sentMessages.add(value);\n+                }\n+\n+                // Add a sentinal message to the end of each partition\n+                String value = partition + \"-\" + LivePartitionTestBean.FINAL_MESSAGE_NUMBER;\n+                ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                producer.send(record);\n+                sentMessages.add(value);\n+            }\n+        }\n+\n+        Map<String, Object> consumerConfig = new HashMap<>();\n+        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APP_GROUPID);\n+\n+        // Sleep\n+        Thread.sleep(700);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyNDQzNQ==", "bodyText": "is there any other way than using a sleep?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374824435", "createdAt": "2020-02-04T17:48:09Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestServlet.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import javax.annotation.Resource;\n+import javax.enterprise.concurrent.ManagedExecutorService;\n+import javax.inject.Inject;\n+import javax.servlet.annotation.WebServlet;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.junit.Test;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.AbstractKafkaTestServlet;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.AckStatus;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.ReceivedMessage;\n+\n+/**\n+ * Tests partition rebalancing in something more akin to a live environment\n+ * <p>\n+ * In particular, we want to test rebalancing while the application is actively processing messages to ensure that none are lost and they're handled correctly.\n+ */\n+@WebServlet(\"/LivePartitionTest\")\n+public class LivePartitionTestServlet extends AbstractKafkaTestServlet {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    public static final String APP_GROUPID = \"kafka-live-partition-test-group\";\n+\n+    @Resource\n+    private ManagedExecutorService executor;\n+\n+    @Inject\n+    private LivePartitionTestBean bean;\n+\n+    @Test\n+    public void testLivePartitionAssignment() throws Exception {\n+\n+        List<String> sentMessages = new ArrayList<>();\n+\n+        Map<String, Object> producerConfig = new HashMap<>();\n+        producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+        producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+\n+        // Load messages into topic\n+        try (KafkaProducer<String, String> producer = new KafkaProducer<>(producerConfig)) {\n+            for (int partition = 0; partition < 2; partition++) {\n+                for (int message = 0; message < 100; message++) {\n+                    String value = partition + \"-\" + message;\n+                    ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                    producer.send(record);\n+                    sentMessages.add(value);\n+                }\n+\n+                // Add a sentinal message to the end of each partition\n+                String value = partition + \"-\" + LivePartitionTestBean.FINAL_MESSAGE_NUMBER;\n+                ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                producer.send(record);\n+                sentMessages.add(value);\n+            }\n+        }\n+\n+        Map<String, Object> consumerConfig = new HashMap<>();\n+        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APP_GROUPID);\n+\n+        // Sleep\n+        Thread.sleep(700);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyNDI2Nw=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI5MDUwMg==", "bodyText": "Not really, it's trying to simulate a live environment where other consumers can join and leave the group at random. I'm open to other suggestions.\nThe design of the test is just to provoke a bunch of things to happen and then check that the runtime handled whatever happened in a valid way.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375290502", "createdAt": "2020-02-05T14:34:10Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestServlet.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import javax.annotation.Resource;\n+import javax.enterprise.concurrent.ManagedExecutorService;\n+import javax.inject.Inject;\n+import javax.servlet.annotation.WebServlet;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.junit.Test;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.AbstractKafkaTestServlet;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.AckStatus;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.ReceivedMessage;\n+\n+/**\n+ * Tests partition rebalancing in something more akin to a live environment\n+ * <p>\n+ * In particular, we want to test rebalancing while the application is actively processing messages to ensure that none are lost and they're handled correctly.\n+ */\n+@WebServlet(\"/LivePartitionTest\")\n+public class LivePartitionTestServlet extends AbstractKafkaTestServlet {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    public static final String APP_GROUPID = \"kafka-live-partition-test-group\";\n+\n+    @Resource\n+    private ManagedExecutorService executor;\n+\n+    @Inject\n+    private LivePartitionTestBean bean;\n+\n+    @Test\n+    public void testLivePartitionAssignment() throws Exception {\n+\n+        List<String> sentMessages = new ArrayList<>();\n+\n+        Map<String, Object> producerConfig = new HashMap<>();\n+        producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+        producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+\n+        // Load messages into topic\n+        try (KafkaProducer<String, String> producer = new KafkaProducer<>(producerConfig)) {\n+            for (int partition = 0; partition < 2; partition++) {\n+                for (int message = 0; message < 100; message++) {\n+                    String value = partition + \"-\" + message;\n+                    ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                    producer.send(record);\n+                    sentMessages.add(value);\n+                }\n+\n+                // Add a sentinal message to the end of each partition\n+                String value = partition + \"-\" + LivePartitionTestBean.FINAL_MESSAGE_NUMBER;\n+                ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                producer.send(record);\n+                sentMessages.add(value);\n+            }\n+        }\n+\n+        Map<String, Object> consumerConfig = new HashMap<>();\n+        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APP_GROUPID);\n+\n+        // Sleep\n+        Thread.sleep(700);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyNDI2Nw=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk0MjU3Mw==", "bodyText": "I don't think it's actually helpful to turn this into a constant. The length of time here is pretty arbitrary. It's more important for someone reading the test to be able to see at a glance how long it's sleeping for.", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r375942573", "createdAt": "2020-02-06T16:30:03Z", "author": {"login": "Azquelt"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestServlet.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import javax.annotation.Resource;\n+import javax.enterprise.concurrent.ManagedExecutorService;\n+import javax.inject.Inject;\n+import javax.servlet.annotation.WebServlet;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.junit.Test;\n+\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.framework.AbstractKafkaTestServlet;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.AckStatus;\n+import com.ibm.ws.microprofile.reactive.messaging.fat.kafka.partitions.LivePartitionTestBean.ReceivedMessage;\n+\n+/**\n+ * Tests partition rebalancing in something more akin to a live environment\n+ * <p>\n+ * In particular, we want to test rebalancing while the application is actively processing messages to ensure that none are lost and they're handled correctly.\n+ */\n+@WebServlet(\"/LivePartitionTest\")\n+public class LivePartitionTestServlet extends AbstractKafkaTestServlet {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    public static final String APP_GROUPID = \"kafka-live-partition-test-group\";\n+\n+    @Resource\n+    private ManagedExecutorService executor;\n+\n+    @Inject\n+    private LivePartitionTestBean bean;\n+\n+    @Test\n+    public void testLivePartitionAssignment() throws Exception {\n+\n+        List<String> sentMessages = new ArrayList<>();\n+\n+        Map<String, Object> producerConfig = new HashMap<>();\n+        producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+        producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n+\n+        // Load messages into topic\n+        try (KafkaProducer<String, String> producer = new KafkaProducer<>(producerConfig)) {\n+            for (int partition = 0; partition < 2; partition++) {\n+                for (int message = 0; message < 100; message++) {\n+                    String value = partition + \"-\" + message;\n+                    ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                    producer.send(record);\n+                    sentMessages.add(value);\n+                }\n+\n+                // Add a sentinal message to the end of each partition\n+                String value = partition + \"-\" + LivePartitionTestBean.FINAL_MESSAGE_NUMBER;\n+                ProducerRecord<String, String> record = new ProducerRecord<String, String>(LivePartitionTestBean.CHANNEL_IN, partition, null, value);\n+                producer.send(record);\n+                sentMessages.add(value);\n+            }\n+        }\n+\n+        Map<String, Object> consumerConfig = new HashMap<>();\n+        consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, getKafkaBootstrap());\n+        consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APP_GROUPID);\n+\n+        // Sleep\n+        Thread.sleep(700);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyNDI2Nw=="}, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODA4NjY3OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounterImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0OTozNVrOFldgwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxNzo0OTozNVrOFldgwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDgyNTE1Mg==", "bodyText": "Can this class be unit-tested?", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r374825152", "createdAt": "2020-02-04T17:49:35Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging.kafka/src/com/ibm/ws/microprofile/reactive/messaging/kafka/ThresholdCounterImpl.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*******************************************************************************\n+ * Copyright (c) 2020 IBM Corporation and others.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Public License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/legal/epl-v10.html\n+ *\n+ * Contributors:\n+ *     IBM Corporation - initial API and implementation\n+ *******************************************************************************/\n+package com.ibm.ws.microprofile.reactive.messaging.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import com.ibm.websphere.ras.Tr;\n+import com.ibm.websphere.ras.TraceComponent;\n+\n+/**\n+ *\n+ */\n+public class ThresholdCounterImpl implements ThresholdCounter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7519959b43f01e7cdd1c0a70af9baf272d798a02"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyODA3NDU4OnYy", "diffSide": "RIGHT", "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestConsumer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMzozNjoxMVrOFm9K7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMzozNjoxMVrOFm9K7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM5MjQyOQ==", "bodyText": "use default timeout", "url": "https://github.com/OpenLiberty/open-liberty/pull/10698#discussion_r376392429", "createdAt": "2020-02-07T13:36:11Z", "author": {"login": "tevans78"}, "path": "dev/com.ibm.ws.microprofile.reactive.messaging_fat/fat/src/com/ibm/ws/microprofile/reactive/messaging/fat/kafka/partitions/LivePartitionTestConsumer.java", "diffHunk": "@@ -49,17 +54,17 @@ public void run() {\n \n             int messages = 0;\n             long startTime = System.nanoTime();\n-            while (messages < 5 && System.nanoTime() - startTime < MAX_DURATION) {\n+            while (messages < MESSAGES_TO_CONSUME && System.nanoTime() - startTime < MAX_DURATION) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a1a1cb842597586fa06760b976825ebe8254bdfe"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2203, "cost": 1, "resetAt": "2021-11-13T12:10:21Z"}}}