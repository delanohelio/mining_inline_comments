{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk5OTMyMTQx", "number": 3244, "title": "[STORM-3600] Add caching in Cluster.calculateSharedOffHeapNodeMemory", "bodyText": "This yields a 4 fourfold increase in speed when scheduling large cluster. Tested via TestLargeCluster.\nAnonymized topologies and configurations generated by TestTopologyAnonymizerUtils.", "createdAt": "2020-04-06T22:10:34Z", "url": "https://github.com/apache/storm/pull/3244", "merged": true, "mergeCommit": {"oid": "7ffc44d083707bc60749fe6821f662fc730c6466"}, "closed": true, "closedAt": "2020-04-15T21:10:34Z", "author": {"login": "bipinprasad"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcVGB6ugH2gAyMzk5OTMyMTQxOjdjNWZkYzNjYmFiNDllN2YzZTQyZDlhZTY4YmJkODQxYzJmYTE5MWI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcX5CBKAFqTM5MzgzNDAyMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7c5fdc3cbab49e7f3e42d9ae68bbd841c2fa191b", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/7c5fdc3cbab49e7f3e42d9ae68bbd841c2fa191b", "committedDate": "2020-04-06T22:00:01Z", "message": "[STORM-3600] Add caching in Cluster.calculateSharedOffHeapNodeMemory to yield 4 fourfold increase in speed when scheduling large cluster."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db4182f7f3e2fe1f071b0ba7f0c97005fd61426b", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/db4182f7f3e2fe1f071b0ba7f0c97005fd61426b", "committedDate": "2020-04-08T12:04:50Z", "message": "[STORM-3600] TopologyDetails may not contain stormTopology in some tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee02bb2630d9c506735d0d8336d721936cecb970", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/ee02bb2630d9c506735d0d8336d721936cecb970", "committedDate": "2020-04-09T02:47:59Z", "message": "[STORM-3600] Cache executors as in YSTORM-7378 pull request 1556. Approx 5% speedup."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/4515a40cfeb391d62910f5f32951abe9f7b5277a", "committedDate": "2020-04-09T18:31:18Z", "message": "[STORM-3600] Checkstyle fix and remove commented code."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxMDkzNDA5", "url": "https://github.com/apache/storm/pull/3244#pullrequestreview-391093409", "createdAt": "2020-04-09T20:02:34Z", "commit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDowMjozNFrOGDnd2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDoyNzoyM1rOGDoOFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0NTUyOA==", "bodyText": "We should be able to simplify this with\n        StormTopology topology = td.getTopology();\n        if (topology.is_set_shared_memory()) {\n            for (SharedMemory sharedMemory : topology.get_shared_memory().values()) {\n                double val = sharedMemory.get_off_heap_node();\n                if (val > 0.0) {\n                    topoSharedOffHeapMemoryNodeFlag.put(topoId, true);\n                    return;\n                }\n            }\n        }", "url": "https://github.com/apache/storm/pull/3244#discussion_r406445528", "createdAt": "2020-04-09T20:02:34Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java", "diffHunk": "@@ -709,6 +720,33 @@ public void assign(SchedulerAssignment assignment, boolean ignoreSingleException\n         }\n     }\n \n+    /**\n+     * Initialize the flag to true if specified topology uses SharedOffHeapNodeMemory, false otherwise.\n+     *\n+     * @param td TopologyDetails to examine\n+     */\n+    private void initializeTopoSharedOffHeapNodeMemoryFlag(TopologyDetails td) {\n+        String topoId = td.getId();\n+        topoSharedOffHeapMemoryNodeFlag.put(topoId, false);\n+        StormTopology topology = td.getTopology();\n+        if (topology == null) {\n+            return; // accomodate multitenant_scheduler_test.clj\n+        }\n+        if (topology.is_set_component_to_shared_memory()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ0Njk2Ng==", "bodyText": "Since this is added, Line 808 can be updated to use nodeId instead of slot.getNodeId()", "url": "https://github.com/apache/storm/pull/3244#discussion_r406446966", "createdAt": "2020-04-09T20:05:27Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java", "diffHunk": "@@ -749,12 +792,16 @@ private double calculateSharedOffHeapNodeMemory(\n      */\n     public void freeSlot(WorkerSlot slot) {\n         // remove the slot from the existing assignments\n+        String nodeId = slot.getNodeId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1MDUyOA==", "bodyText": "I was wondering if return within the try-with-resources block will close the resources or not. And I found this https://stackoverflow.com/questions/22947755/try-with-resources-and-return-statements-in-java/22947904\nSo it should be fine.", "url": "https://github.com/apache/storm/pull/3244#discussion_r406450528", "createdAt": "2020-04-09T20:12:23Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NDAwOQ==", "bodyText": "It looks like nm is actually topoId? What does nm stand for?", "url": "https://github.com/apache/storm/pull/3244#discussion_r406454009", "createdAt": "2020-04-09T20:19:29Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();\n+        }\n+    }\n+\n+    public static ClassLoader getContextClassLoader() {\n+        return Thread.currentThread().getContextClassLoader();\n+    }\n+\n+    /**\n+     * Create an array of TopologyDetails by reading serialized files for topology and configuration in the\n+     * resource path.\n+     *\n+     * @param failOnParseError throw exception if there are unmatched files, otherwise ignore unmatched and read errors.\n+     * @return An array of TopologyDetails representing resource files.\n+     * @throws Exception\n+     */\n+    public static TopologyDetails[] createTopoDetailsArray(boolean failOnParseError) throws Exception {\n+        List<TopologyDetails> topoDetailsList = new ArrayList<>();\n+        List<String> errors = new ArrayList<>();\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Map<String, String> codeResourceMap = new TreeMap<>();\n+        Map<String, String> confResourceMap = new HashMap<>();\n+        for (int i = 0 ; i < resources.size() ; i++) {\n+            String resource = resources.get(i);\n+            int idxOfSlash = resource.lastIndexOf(\"/\");\n+            int idxOfDash = resource.lastIndexOf(\"-\");\n+            String nm = idxOfDash > idxOfSlash ? resource.substring(idxOfSlash + 1, idxOfDash) : resource.substring(idxOfSlash + 1, resource.length() - 7);\n+            if (resource.endsWith(\"code.ser\")) {\n+                codeResourceMap.put(nm, resource);\n+            } else if (resource.endsWith(\"conf.ser\")) {\n+                confResourceMap.put(nm, resource);\n+            } else {\n+                LOG.info(\"Ignoring unsupported resource file \" + resource);\n+            }\n+        }\n+        String[] examinedConfParams = {\n+                Config.TOPOLOGY_NAME,\n+                Config.TOPOLOGY_SCHEDULER_STRATEGY,\n+                Config.TOPOLOGY_PRIORITY,\n+                Config.TOPOLOGY_WORKERS,\n+                Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB,\n+                Config.TOPOLOGY_SUBMITTER_USER,\n+                Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT,\n+                Config.TOPOLOGY_ACKER_RESOURCES_OFFHEAP_MEMORY_MB,\n+                Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB,\n+        };\n+\n+        for (String nm : codeResourceMap.keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NTAzMA==", "bodyText": "What does the comment mean? Or can it be removed?", "url": "https://github.com/apache/storm/pull/3244#discussion_r406455030", "createdAt": "2020-04-09T20:21:46Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();\n+        }\n+    }\n+\n+    public static ClassLoader getContextClassLoader() {\n+        return Thread.currentThread().getContextClassLoader();\n+    }\n+\n+    /**\n+     * Create an array of TopologyDetails by reading serialized files for topology and configuration in the\n+     * resource path.\n+     *\n+     * @param failOnParseError throw exception if there are unmatched files, otherwise ignore unmatched and read errors.\n+     * @return An array of TopologyDetails representing resource files.\n+     * @throws Exception\n+     */\n+    public static TopologyDetails[] createTopoDetailsArray(boolean failOnParseError) throws Exception {\n+        List<TopologyDetails> topoDetailsList = new ArrayList<>();\n+        List<String> errors = new ArrayList<>();\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Map<String, String> codeResourceMap = new TreeMap<>();\n+        Map<String, String> confResourceMap = new HashMap<>();\n+        for (int i = 0 ; i < resources.size() ; i++) {\n+            String resource = resources.get(i);\n+            int idxOfSlash = resource.lastIndexOf(\"/\");\n+            int idxOfDash = resource.lastIndexOf(\"-\");\n+            String nm = idxOfDash > idxOfSlash ? resource.substring(idxOfSlash + 1, idxOfDash) : resource.substring(idxOfSlash + 1, resource.length() - 7);\n+            if (resource.endsWith(\"code.ser\")) {\n+                codeResourceMap.put(nm, resource);\n+            } else if (resource.endsWith(\"conf.ser\")) {\n+                confResourceMap.put(nm, resource);\n+            } else {\n+                LOG.info(\"Ignoring unsupported resource file \" + resource);\n+            }\n+        }\n+        String[] examinedConfParams = {\n+                Config.TOPOLOGY_NAME,\n+                Config.TOPOLOGY_SCHEDULER_STRATEGY,\n+                Config.TOPOLOGY_PRIORITY,\n+                Config.TOPOLOGY_WORKERS,\n+                Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB,\n+                Config.TOPOLOGY_SUBMITTER_USER,\n+                Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT,\n+                Config.TOPOLOGY_ACKER_RESOURCES_OFFHEAP_MEMORY_MB,\n+                Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB,\n+        };\n+\n+        for (String nm : codeResourceMap.keySet()) {\n+            String codeResource = codeResourceMap.get(nm);\n+            if (!confResourceMap.containsKey(nm)) {\n+                String err = String.format(\"Ignoring topology file %s because of missing config file for %s\", codeResource, nm);\n+                errors.add(err);\n+                LOG.error(err);\n+                continue;\n+            }\n+            String confResource = confResourceMap.get(nm);\n+            LOG.info(\"Found matching topology and config files: {}, {}\", codeResource, confResource);\n+            StormTopology stormTopology;\n+            try {\n+                stormTopology = Utils.deserialize(getResourceAsBytes(codeResource), StormTopology.class);\n+            } catch (Exception ex) {\n+                String err = String.format(\"Cannot read topology from resource %s\", codeResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+\n+            Map<String, Object> conf;\n+            try {\n+                conf = Utils.fromCompressedJsonConf(getResourceAsBytes(confResource));\n+            } catch (RuntimeException | IOException ex) {\n+                String err = String.format(\"Cannot read configuration from resource %s\", confResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+            // fix 0.10 conf class names\n+            String[] configParamsToFix = {Config.TOPOLOGY_SCHEDULER_STRATEGY, Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN,\n+                    DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY };\n+            for (String configParam: configParamsToFix) {\n+                if (!conf.containsKey(configParam)) {\n+                    continue;\n+                }\n+                String className = (String) conf.get(configParam);\n+                if (className.startsWith(\"backtype\")) {\n+                    className = className.replace(\"backtype\", \"org.apache\");\n+                    conf.put(configParam, className);\n+                }\n+            }\n+            // fix conf params used by ConstraintSolverStrategy\n+            if (!conf.containsKey(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH)) {\n+                conf.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH, 10_000);\n+            }\n+            if (!conf.containsKey(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH)) {\n+                conf.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 10_000);\n+            }\n+\n+            String topoId = nm;\n+            String topoName = (String) conf.getOrDefault(Config.TOPOLOGY_NAME, nm);\n+\n+            // conf\n+            StringBuffer sb = new StringBuffer(\"Config for \" + nm + \": \");\n+            for (String param : examinedConfParams) {\n+                Object val = conf.getOrDefault(param, \"<null>\");\n+                sb.append(param).append(\"=\").append(val).append(\", \");\n+            }\n+            LOG.info(sb.toString());\n+\n+            // topo\n+            Map<ExecutorDetails, String> execToComp = TestUtilsForResourceAwareScheduler.genExecsAndComps(stormTopology);\n+            LOG.info(\"Topology \\\"{}\\\" spouts={}, bolts={}, execToComp size is {}\", topoName,\n+                    stormTopology.get_spouts_size(), stormTopology.get_bolts_size(), execToComp.size());\n+            int numWorkers = Integer.parseInt(\"\" + conf.getOrDefault(Config.TOPOLOGY_WORKERS, \"0\"));\n+            TopologyDetails topo = new TopologyDetails(topoId, conf, stormTopology,  numWorkers,\n+                    execToComp, Time.currentTimeSecs(), \"user\");\n+            topo.getComponents(); // sanity check - normally this should not fail\n+\n+            topoDetailsList.add(topo);\n+        }\n+        if (!errors.isEmpty() && failOnParseError) {\n+            throw new Exception(\"Unable to parse all serialized objects\\n\\t\" + String.join(\"\\n\\t\", errors));\n+        }\n+        return topoDetailsList.toArray(new TopologyDetails[0]);\n+    }\n+\n+    /**\n+     * Check if the files in the resource directory are matched, can be read properly, and code/config files occur\n+     * in matched pairs.\n+     *\n+     * @throws Exception showing bad and unmatched resource files.\n+     */\n+    @Test\n+    public void testReadSerializedTopologiesAndConfigs() throws Exception {\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Assert.assertTrue(\"No resource files found in \" + TEST_RESOURCE_PATH, !resources.isEmpty());\n+        TopologyDetails[] topoDetailsArray = createTopoDetailsArray(true);\n+    }\n+\n+    /**\n+     * Create one supervisor and add to the supervisors list.\n+     *\n+     * @param rack rack-number\n+     * @param superInRack supervisor number in the rack\n+     * @param cpu percentage\n+     * @param mem in megabytes\n+     * @param numPorts number of ports on this supervisor\n+     * @param sups returned map os supervisors\n+     */\n+    private static void createAndAddOneSupervisor(\n+            int rack, int superInRack, double cpu, double mem, int numPorts,\n+            Map<String, SupervisorDetails> sups) {\n+\n+        List<Number> ports = new LinkedList<>();\n+        for (int p = 0; p < numPorts; p++) {\n+            ports.add(p);\n+        }\n+        String superId = String.format(\"r%03ds%03d\", rack, superInRack);\n+        String hostId  = String.format(\"host-%03d-rack-%03d\", superInRack, rack);\n+        Map<String, Double> resourceMap = new HashMap<>();\n+        resourceMap.put(Config.SUPERVISOR_CPU_CAPACITY, cpu);\n+        resourceMap.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, mem);\n+        resourceMap.put(\"network.resource.units\", 50.0);\n+        SupervisorDetails sup = new SupervisorDetails(superId,\n+                hostId, null, ports,\n+                NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceMap));\n+        sups.put(sup.getId(), sup);\n+    }\n+\n+    /**\n+     * Create supervisors.\n+     *\n+     * @param uniformSupervisors true if all supervisors are of the same size, false otherwise.\n+     * @return supervisor details indexed by id\n+     */\n+    private static Map<String, SupervisorDetails> createSupervisors(boolean uniformSupervisors) {\n+        Map<String, SupervisorDetails> retVal;\n+        if (uniformSupervisors) {\n+            int numRacks = 16;\n+            int numSupersPerRack = 82;\n+            int numPorts = 50; // scheduling will fill up 2-6, leaving of 90% workerslots unused - does this cause slow scheduling?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 318}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NTQ5Ng==", "bodyText": "IridiumBlue doesn't mean anything in the community code. Maybe reword it.\nTypo: closelt", "url": "https://github.com/apache/storm/pull/3244#discussion_r406455496", "createdAt": "2020-04-09T20:22:42Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();\n+        }\n+    }\n+\n+    public static ClassLoader getContextClassLoader() {\n+        return Thread.currentThread().getContextClassLoader();\n+    }\n+\n+    /**\n+     * Create an array of TopologyDetails by reading serialized files for topology and configuration in the\n+     * resource path.\n+     *\n+     * @param failOnParseError throw exception if there are unmatched files, otherwise ignore unmatched and read errors.\n+     * @return An array of TopologyDetails representing resource files.\n+     * @throws Exception\n+     */\n+    public static TopologyDetails[] createTopoDetailsArray(boolean failOnParseError) throws Exception {\n+        List<TopologyDetails> topoDetailsList = new ArrayList<>();\n+        List<String> errors = new ArrayList<>();\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Map<String, String> codeResourceMap = new TreeMap<>();\n+        Map<String, String> confResourceMap = new HashMap<>();\n+        for (int i = 0 ; i < resources.size() ; i++) {\n+            String resource = resources.get(i);\n+            int idxOfSlash = resource.lastIndexOf(\"/\");\n+            int idxOfDash = resource.lastIndexOf(\"-\");\n+            String nm = idxOfDash > idxOfSlash ? resource.substring(idxOfSlash + 1, idxOfDash) : resource.substring(idxOfSlash + 1, resource.length() - 7);\n+            if (resource.endsWith(\"code.ser\")) {\n+                codeResourceMap.put(nm, resource);\n+            } else if (resource.endsWith(\"conf.ser\")) {\n+                confResourceMap.put(nm, resource);\n+            } else {\n+                LOG.info(\"Ignoring unsupported resource file \" + resource);\n+            }\n+        }\n+        String[] examinedConfParams = {\n+                Config.TOPOLOGY_NAME,\n+                Config.TOPOLOGY_SCHEDULER_STRATEGY,\n+                Config.TOPOLOGY_PRIORITY,\n+                Config.TOPOLOGY_WORKERS,\n+                Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB,\n+                Config.TOPOLOGY_SUBMITTER_USER,\n+                Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT,\n+                Config.TOPOLOGY_ACKER_RESOURCES_OFFHEAP_MEMORY_MB,\n+                Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB,\n+        };\n+\n+        for (String nm : codeResourceMap.keySet()) {\n+            String codeResource = codeResourceMap.get(nm);\n+            if (!confResourceMap.containsKey(nm)) {\n+                String err = String.format(\"Ignoring topology file %s because of missing config file for %s\", codeResource, nm);\n+                errors.add(err);\n+                LOG.error(err);\n+                continue;\n+            }\n+            String confResource = confResourceMap.get(nm);\n+            LOG.info(\"Found matching topology and config files: {}, {}\", codeResource, confResource);\n+            StormTopology stormTopology;\n+            try {\n+                stormTopology = Utils.deserialize(getResourceAsBytes(codeResource), StormTopology.class);\n+            } catch (Exception ex) {\n+                String err = String.format(\"Cannot read topology from resource %s\", codeResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+\n+            Map<String, Object> conf;\n+            try {\n+                conf = Utils.fromCompressedJsonConf(getResourceAsBytes(confResource));\n+            } catch (RuntimeException | IOException ex) {\n+                String err = String.format(\"Cannot read configuration from resource %s\", confResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+            // fix 0.10 conf class names\n+            String[] configParamsToFix = {Config.TOPOLOGY_SCHEDULER_STRATEGY, Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN,\n+                    DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY };\n+            for (String configParam: configParamsToFix) {\n+                if (!conf.containsKey(configParam)) {\n+                    continue;\n+                }\n+                String className = (String) conf.get(configParam);\n+                if (className.startsWith(\"backtype\")) {\n+                    className = className.replace(\"backtype\", \"org.apache\");\n+                    conf.put(configParam, className);\n+                }\n+            }\n+            // fix conf params used by ConstraintSolverStrategy\n+            if (!conf.containsKey(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH)) {\n+                conf.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH, 10_000);\n+            }\n+            if (!conf.containsKey(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH)) {\n+                conf.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 10_000);\n+            }\n+\n+            String topoId = nm;\n+            String topoName = (String) conf.getOrDefault(Config.TOPOLOGY_NAME, nm);\n+\n+            // conf\n+            StringBuffer sb = new StringBuffer(\"Config for \" + nm + \": \");\n+            for (String param : examinedConfParams) {\n+                Object val = conf.getOrDefault(param, \"<null>\");\n+                sb.append(param).append(\"=\").append(val).append(\", \");\n+            }\n+            LOG.info(sb.toString());\n+\n+            // topo\n+            Map<ExecutorDetails, String> execToComp = TestUtilsForResourceAwareScheduler.genExecsAndComps(stormTopology);\n+            LOG.info(\"Topology \\\"{}\\\" spouts={}, bolts={}, execToComp size is {}\", topoName,\n+                    stormTopology.get_spouts_size(), stormTopology.get_bolts_size(), execToComp.size());\n+            int numWorkers = Integer.parseInt(\"\" + conf.getOrDefault(Config.TOPOLOGY_WORKERS, \"0\"));\n+            TopologyDetails topo = new TopologyDetails(topoId, conf, stormTopology,  numWorkers,\n+                    execToComp, Time.currentTimeSecs(), \"user\");\n+            topo.getComponents(); // sanity check - normally this should not fail\n+\n+            topoDetailsList.add(topo);\n+        }\n+        if (!errors.isEmpty() && failOnParseError) {\n+            throw new Exception(\"Unable to parse all serialized objects\\n\\t\" + String.join(\"\\n\\t\", errors));\n+        }\n+        return topoDetailsList.toArray(new TopologyDetails[0]);\n+    }\n+\n+    /**\n+     * Check if the files in the resource directory are matched, can be read properly, and code/config files occur\n+     * in matched pairs.\n+     *\n+     * @throws Exception showing bad and unmatched resource files.\n+     */\n+    @Test\n+    public void testReadSerializedTopologiesAndConfigs() throws Exception {\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Assert.assertTrue(\"No resource files found in \" + TEST_RESOURCE_PATH, !resources.isEmpty());\n+        TopologyDetails[] topoDetailsArray = createTopoDetailsArray(true);\n+    }\n+\n+    /**\n+     * Create one supervisor and add to the supervisors list.\n+     *\n+     * @param rack rack-number\n+     * @param superInRack supervisor number in the rack\n+     * @param cpu percentage\n+     * @param mem in megabytes\n+     * @param numPorts number of ports on this supervisor\n+     * @param sups returned map os supervisors\n+     */\n+    private static void createAndAddOneSupervisor(\n+            int rack, int superInRack, double cpu, double mem, int numPorts,\n+            Map<String, SupervisorDetails> sups) {\n+\n+        List<Number> ports = new LinkedList<>();\n+        for (int p = 0; p < numPorts; p++) {\n+            ports.add(p);\n+        }\n+        String superId = String.format(\"r%03ds%03d\", rack, superInRack);\n+        String hostId  = String.format(\"host-%03d-rack-%03d\", superInRack, rack);\n+        Map<String, Double> resourceMap = new HashMap<>();\n+        resourceMap.put(Config.SUPERVISOR_CPU_CAPACITY, cpu);\n+        resourceMap.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, mem);\n+        resourceMap.put(\"network.resource.units\", 50.0);\n+        SupervisorDetails sup = new SupervisorDetails(superId,\n+                hostId, null, ports,\n+                NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceMap));\n+        sups.put(sup.getId(), sup);\n+    }\n+\n+    /**\n+     * Create supervisors.\n+     *\n+     * @param uniformSupervisors true if all supervisors are of the same size, false otherwise.\n+     * @return supervisor details indexed by id\n+     */\n+    private static Map<String, SupervisorDetails> createSupervisors(boolean uniformSupervisors) {\n+        Map<String, SupervisorDetails> retVal;\n+        if (uniformSupervisors) {\n+            int numRacks = 16;\n+            int numSupersPerRack = 82;\n+            int numPorts = 50; // scheduling will fill up 2-6, leaving of 90% workerslots unused - does this cause slow scheduling?\n+            int rackStart = 0;\n+            int superInRackStart = 1;\n+            double cpu = 7200; // %percent\n+            double mem = 356_000; // MB\n+            Map<String, Double> miscResources = new HashMap<>();\n+            miscResources.put(\"network.resource.units\", 100.0);\n+\n+            return TestUtilsForResourceAwareScheduler.genSupervisorsWithRacks(\n+                    numRacks, numSupersPerRack, numPorts, rackStart, superInRackStart, cpu, mem, miscResources);\n+\n+        } else {\n+            // this non-uniform supervisor distribution closelt (but not exactly) mimics IridiumBlue cluster", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 330}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1NjU5OQ==", "bodyText": "Which one we want to keep?  iTopo = i or random.nextInt(topoDetailsArray.length);", "url": "https://github.com/apache/storm/pull/3244#discussion_r406456599", "createdAt": "2020-04-09T20:24:53Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();\n+        }\n+    }\n+\n+    public static ClassLoader getContextClassLoader() {\n+        return Thread.currentThread().getContextClassLoader();\n+    }\n+\n+    /**\n+     * Create an array of TopologyDetails by reading serialized files for topology and configuration in the\n+     * resource path.\n+     *\n+     * @param failOnParseError throw exception if there are unmatched files, otherwise ignore unmatched and read errors.\n+     * @return An array of TopologyDetails representing resource files.\n+     * @throws Exception\n+     */\n+    public static TopologyDetails[] createTopoDetailsArray(boolean failOnParseError) throws Exception {\n+        List<TopologyDetails> topoDetailsList = new ArrayList<>();\n+        List<String> errors = new ArrayList<>();\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Map<String, String> codeResourceMap = new TreeMap<>();\n+        Map<String, String> confResourceMap = new HashMap<>();\n+        for (int i = 0 ; i < resources.size() ; i++) {\n+            String resource = resources.get(i);\n+            int idxOfSlash = resource.lastIndexOf(\"/\");\n+            int idxOfDash = resource.lastIndexOf(\"-\");\n+            String nm = idxOfDash > idxOfSlash ? resource.substring(idxOfSlash + 1, idxOfDash) : resource.substring(idxOfSlash + 1, resource.length() - 7);\n+            if (resource.endsWith(\"code.ser\")) {\n+                codeResourceMap.put(nm, resource);\n+            } else if (resource.endsWith(\"conf.ser\")) {\n+                confResourceMap.put(nm, resource);\n+            } else {\n+                LOG.info(\"Ignoring unsupported resource file \" + resource);\n+            }\n+        }\n+        String[] examinedConfParams = {\n+                Config.TOPOLOGY_NAME,\n+                Config.TOPOLOGY_SCHEDULER_STRATEGY,\n+                Config.TOPOLOGY_PRIORITY,\n+                Config.TOPOLOGY_WORKERS,\n+                Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB,\n+                Config.TOPOLOGY_SUBMITTER_USER,\n+                Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT,\n+                Config.TOPOLOGY_ACKER_RESOURCES_OFFHEAP_MEMORY_MB,\n+                Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB,\n+        };\n+\n+        for (String nm : codeResourceMap.keySet()) {\n+            String codeResource = codeResourceMap.get(nm);\n+            if (!confResourceMap.containsKey(nm)) {\n+                String err = String.format(\"Ignoring topology file %s because of missing config file for %s\", codeResource, nm);\n+                errors.add(err);\n+                LOG.error(err);\n+                continue;\n+            }\n+            String confResource = confResourceMap.get(nm);\n+            LOG.info(\"Found matching topology and config files: {}, {}\", codeResource, confResource);\n+            StormTopology stormTopology;\n+            try {\n+                stormTopology = Utils.deserialize(getResourceAsBytes(codeResource), StormTopology.class);\n+            } catch (Exception ex) {\n+                String err = String.format(\"Cannot read topology from resource %s\", codeResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+\n+            Map<String, Object> conf;\n+            try {\n+                conf = Utils.fromCompressedJsonConf(getResourceAsBytes(confResource));\n+            } catch (RuntimeException | IOException ex) {\n+                String err = String.format(\"Cannot read configuration from resource %s\", confResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+            // fix 0.10 conf class names\n+            String[] configParamsToFix = {Config.TOPOLOGY_SCHEDULER_STRATEGY, Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN,\n+                    DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY };\n+            for (String configParam: configParamsToFix) {\n+                if (!conf.containsKey(configParam)) {\n+                    continue;\n+                }\n+                String className = (String) conf.get(configParam);\n+                if (className.startsWith(\"backtype\")) {\n+                    className = className.replace(\"backtype\", \"org.apache\");\n+                    conf.put(configParam, className);\n+                }\n+            }\n+            // fix conf params used by ConstraintSolverStrategy\n+            if (!conf.containsKey(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH)) {\n+                conf.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH, 10_000);\n+            }\n+            if (!conf.containsKey(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH)) {\n+                conf.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 10_000);\n+            }\n+\n+            String topoId = nm;\n+            String topoName = (String) conf.getOrDefault(Config.TOPOLOGY_NAME, nm);\n+\n+            // conf\n+            StringBuffer sb = new StringBuffer(\"Config for \" + nm + \": \");\n+            for (String param : examinedConfParams) {\n+                Object val = conf.getOrDefault(param, \"<null>\");\n+                sb.append(param).append(\"=\").append(val).append(\", \");\n+            }\n+            LOG.info(sb.toString());\n+\n+            // topo\n+            Map<ExecutorDetails, String> execToComp = TestUtilsForResourceAwareScheduler.genExecsAndComps(stormTopology);\n+            LOG.info(\"Topology \\\"{}\\\" spouts={}, bolts={}, execToComp size is {}\", topoName,\n+                    stormTopology.get_spouts_size(), stormTopology.get_bolts_size(), execToComp.size());\n+            int numWorkers = Integer.parseInt(\"\" + conf.getOrDefault(Config.TOPOLOGY_WORKERS, \"0\"));\n+            TopologyDetails topo = new TopologyDetails(topoId, conf, stormTopology,  numWorkers,\n+                    execToComp, Time.currentTimeSecs(), \"user\");\n+            topo.getComponents(); // sanity check - normally this should not fail\n+\n+            topoDetailsList.add(topo);\n+        }\n+        if (!errors.isEmpty() && failOnParseError) {\n+            throw new Exception(\"Unable to parse all serialized objects\\n\\t\" + String.join(\"\\n\\t\", errors));\n+        }\n+        return topoDetailsList.toArray(new TopologyDetails[0]);\n+    }\n+\n+    /**\n+     * Check if the files in the resource directory are matched, can be read properly, and code/config files occur\n+     * in matched pairs.\n+     *\n+     * @throws Exception showing bad and unmatched resource files.\n+     */\n+    @Test\n+    public void testReadSerializedTopologiesAndConfigs() throws Exception {\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Assert.assertTrue(\"No resource files found in \" + TEST_RESOURCE_PATH, !resources.isEmpty());\n+        TopologyDetails[] topoDetailsArray = createTopoDetailsArray(true);\n+    }\n+\n+    /**\n+     * Create one supervisor and add to the supervisors list.\n+     *\n+     * @param rack rack-number\n+     * @param superInRack supervisor number in the rack\n+     * @param cpu percentage\n+     * @param mem in megabytes\n+     * @param numPorts number of ports on this supervisor\n+     * @param sups returned map os supervisors\n+     */\n+    private static void createAndAddOneSupervisor(\n+            int rack, int superInRack, double cpu, double mem, int numPorts,\n+            Map<String, SupervisorDetails> sups) {\n+\n+        List<Number> ports = new LinkedList<>();\n+        for (int p = 0; p < numPorts; p++) {\n+            ports.add(p);\n+        }\n+        String superId = String.format(\"r%03ds%03d\", rack, superInRack);\n+        String hostId  = String.format(\"host-%03d-rack-%03d\", superInRack, rack);\n+        Map<String, Double> resourceMap = new HashMap<>();\n+        resourceMap.put(Config.SUPERVISOR_CPU_CAPACITY, cpu);\n+        resourceMap.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, mem);\n+        resourceMap.put(\"network.resource.units\", 50.0);\n+        SupervisorDetails sup = new SupervisorDetails(superId,\n+                hostId, null, ports,\n+                NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceMap));\n+        sups.put(sup.getId(), sup);\n+    }\n+\n+    /**\n+     * Create supervisors.\n+     *\n+     * @param uniformSupervisors true if all supervisors are of the same size, false otherwise.\n+     * @return supervisor details indexed by id\n+     */\n+    private static Map<String, SupervisorDetails> createSupervisors(boolean uniformSupervisors) {\n+        Map<String, SupervisorDetails> retVal;\n+        if (uniformSupervisors) {\n+            int numRacks = 16;\n+            int numSupersPerRack = 82;\n+            int numPorts = 50; // scheduling will fill up 2-6, leaving of 90% workerslots unused - does this cause slow scheduling?\n+            int rackStart = 0;\n+            int superInRackStart = 1;\n+            double cpu = 7200; // %percent\n+            double mem = 356_000; // MB\n+            Map<String, Double> miscResources = new HashMap<>();\n+            miscResources.put(\"network.resource.units\", 100.0);\n+\n+            return TestUtilsForResourceAwareScheduler.genSupervisorsWithRacks(\n+                    numRacks, numSupersPerRack, numPorts, rackStart, superInRackStart, cpu, mem, miscResources);\n+\n+        } else {\n+            // this non-uniform supervisor distribution closelt (but not exactly) mimics IridiumBlue cluster\n+            int numSupersPerRack = 82;\n+            int numPorts = 50;\n+\n+            Map<String, SupervisorDetails> retList = new HashMap<>();\n+\n+            for (int rack = 0 ; rack < 12 ; rack++) {\n+                double cpu = 3600; // %percent\n+                double mem = 178_000; // MB\n+                for (int superInRack = 0; superInRack < numSupersPerRack ; superInRack++) {\n+                    createAndAddOneSupervisor(rack, superInRack, cpu - 100 * (superInRack % 2), mem, numPorts, retList);\n+                }\n+            }\n+            for (int rack = 12 ; rack < 14 ; rack++) {\n+                double cpu = 2400; // %percent\n+                double mem = 118_100; // MB\n+                for (int superInRack = 0; superInRack < numSupersPerRack ; superInRack++) {\n+                    createAndAddOneSupervisor(rack, superInRack, cpu - 100 * (superInRack % 2), mem, numPorts, retList);\n+                }\n+            }\n+            for (int rack = 14 ; rack < 16 ; rack++) {\n+                double cpu = 1200; // %percent\n+                double mem = 42_480; // MB\n+                for (int superInRack = 0; superInRack < numSupersPerRack ; superInRack++) {\n+                    createAndAddOneSupervisor(rack, superInRack, cpu - 100 * (superInRack % 2), mem, numPorts, retList);\n+                }\n+            }\n+            return retList;\n+        }\n+    }\n+\n+    /**\n+     * Create a large cluster, read topologies and configuration from resource directory and schedule.\n+     *\n+     * @throws Exception\n+     */\n+    @Test\n+    public void testLargeCluster() throws Exception {\n+        boolean uniformSupervisors = false; // false means non-uniform supervisor distribution\n+\n+        Map<String, SupervisorDetails> supervisors = createSupervisors(uniformSupervisors);\n+\n+        TopologyDetails[] topoDetailsArray = createTopoDetailsArray(false);\n+        Assert.assertTrue(\"No topologies found\", topoDetailsArray.length > 0);\n+        Topologies topologies = new Topologies(topoDetailsArray);\n+\n+        Config confWithDefaultStrategy = new Config();\n+        confWithDefaultStrategy.putAll(topoDetailsArray[0].getConf());\n+        confWithDefaultStrategy.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, DefaultResourceAwareStrategy.class.getName());\n+\n+        INimbus iNimbus = new INimbusTest();\n+        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supervisors, new HashMap<>(),\n+                topologies, confWithDefaultStrategy);\n+\n+        scheduler = new ResourceAwareScheduler();\n+\n+        List<Class> classesToDebug = Arrays.asList(DefaultResourceAwareStrategy.class,\n+                GenericResourceAwareStrategy.class, ResourceAwareScheduler.class,\n+                Cluster.class // count calls to calculateSharedOffHeapNodeMemory()\n+        );\n+        Level logLevel = Level.INFO ; // switch to Level.DEBUG for verbose otherwise Level.INFO\n+        classesToDebug.forEach(x -> Configurator.setLevel(x.getName(), logLevel));\n+        long startTime = System.currentTimeMillis();\n+        scheduler.prepare(confWithDefaultStrategy, new StormMetricsRegistry());\n+        scheduler.schedule(topologies, cluster);\n+        long endTime = System.currentTimeMillis();\n+        LOG.info(\"Scheduling Time: {} topologies in {} seconds\", topoDetailsArray.length, (endTime - startTime) / 1000.0);\n+\n+        for (TopologyDetails td : topoDetailsArray) {\n+            TestUtilsForResourceAwareScheduler.assertTopologiesFullyScheduled(cluster, td.getName());\n+        }\n+\n+        // Remove topology and reschedule it\n+        java.util.Random random = new java.util.Random();\n+        for (int i = 0 ; i < topoDetailsArray.length ; i++) {\n+            startTime = System.currentTimeMillis();\n+            int iTopo = i; // random.nextInt(topoDetailsArray.length);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 406}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1Njk1Nw==", "bodyText": "What does the comment mean here?", "url": "https://github.com/apache/storm/pull/3244#discussion_r406456957", "createdAt": "2020-04-09T20:25:36Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();\n+        }\n+    }\n+\n+    public static ClassLoader getContextClassLoader() {\n+        return Thread.currentThread().getContextClassLoader();\n+    }\n+\n+    /**\n+     * Create an array of TopologyDetails by reading serialized files for topology and configuration in the\n+     * resource path.\n+     *\n+     * @param failOnParseError throw exception if there are unmatched files, otherwise ignore unmatched and read errors.\n+     * @return An array of TopologyDetails representing resource files.\n+     * @throws Exception\n+     */\n+    public static TopologyDetails[] createTopoDetailsArray(boolean failOnParseError) throws Exception {\n+        List<TopologyDetails> topoDetailsList = new ArrayList<>();\n+        List<String> errors = new ArrayList<>();\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Map<String, String> codeResourceMap = new TreeMap<>();\n+        Map<String, String> confResourceMap = new HashMap<>();\n+        for (int i = 0 ; i < resources.size() ; i++) {\n+            String resource = resources.get(i);\n+            int idxOfSlash = resource.lastIndexOf(\"/\");\n+            int idxOfDash = resource.lastIndexOf(\"-\");\n+            String nm = idxOfDash > idxOfSlash ? resource.substring(idxOfSlash + 1, idxOfDash) : resource.substring(idxOfSlash + 1, resource.length() - 7);\n+            if (resource.endsWith(\"code.ser\")) {\n+                codeResourceMap.put(nm, resource);\n+            } else if (resource.endsWith(\"conf.ser\")) {\n+                confResourceMap.put(nm, resource);\n+            } else {\n+                LOG.info(\"Ignoring unsupported resource file \" + resource);\n+            }\n+        }\n+        String[] examinedConfParams = {\n+                Config.TOPOLOGY_NAME,\n+                Config.TOPOLOGY_SCHEDULER_STRATEGY,\n+                Config.TOPOLOGY_PRIORITY,\n+                Config.TOPOLOGY_WORKERS,\n+                Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB,\n+                Config.TOPOLOGY_SUBMITTER_USER,\n+                Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT,\n+                Config.TOPOLOGY_ACKER_RESOURCES_OFFHEAP_MEMORY_MB,\n+                Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB,\n+        };\n+\n+        for (String nm : codeResourceMap.keySet()) {\n+            String codeResource = codeResourceMap.get(nm);\n+            if (!confResourceMap.containsKey(nm)) {\n+                String err = String.format(\"Ignoring topology file %s because of missing config file for %s\", codeResource, nm);\n+                errors.add(err);\n+                LOG.error(err);\n+                continue;\n+            }\n+            String confResource = confResourceMap.get(nm);\n+            LOG.info(\"Found matching topology and config files: {}, {}\", codeResource, confResource);\n+            StormTopology stormTopology;\n+            try {\n+                stormTopology = Utils.deserialize(getResourceAsBytes(codeResource), StormTopology.class);\n+            } catch (Exception ex) {\n+                String err = String.format(\"Cannot read topology from resource %s\", codeResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+\n+            Map<String, Object> conf;\n+            try {\n+                conf = Utils.fromCompressedJsonConf(getResourceAsBytes(confResource));\n+            } catch (RuntimeException | IOException ex) {\n+                String err = String.format(\"Cannot read configuration from resource %s\", confResource);\n+                errors.add(err);\n+                LOG.error(err, ex);\n+                continue;\n+            }\n+            // fix 0.10 conf class names\n+            String[] configParamsToFix = {Config.TOPOLOGY_SCHEDULER_STRATEGY, Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN,\n+                    DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY };\n+            for (String configParam: configParamsToFix) {\n+                if (!conf.containsKey(configParam)) {\n+                    continue;\n+                }\n+                String className = (String) conf.get(configParam);\n+                if (className.startsWith(\"backtype\")) {\n+                    className = className.replace(\"backtype\", \"org.apache\");\n+                    conf.put(configParam, className);\n+                }\n+            }\n+            // fix conf params used by ConstraintSolverStrategy\n+            if (!conf.containsKey(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH)) {\n+                conf.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_STATE_SEARCH, 10_000);\n+            }\n+            if (!conf.containsKey(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH)) {\n+                conf.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 10_000);\n+            }\n+\n+            String topoId = nm;\n+            String topoName = (String) conf.getOrDefault(Config.TOPOLOGY_NAME, nm);\n+\n+            // conf\n+            StringBuffer sb = new StringBuffer(\"Config for \" + nm + \": \");\n+            for (String param : examinedConfParams) {\n+                Object val = conf.getOrDefault(param, \"<null>\");\n+                sb.append(param).append(\"=\").append(val).append(\", \");\n+            }\n+            LOG.info(sb.toString());\n+\n+            // topo\n+            Map<ExecutorDetails, String> execToComp = TestUtilsForResourceAwareScheduler.genExecsAndComps(stormTopology);\n+            LOG.info(\"Topology \\\"{}\\\" spouts={}, bolts={}, execToComp size is {}\", topoName,\n+                    stormTopology.get_spouts_size(), stormTopology.get_bolts_size(), execToComp.size());\n+            int numWorkers = Integer.parseInt(\"\" + conf.getOrDefault(Config.TOPOLOGY_WORKERS, \"0\"));\n+            TopologyDetails topo = new TopologyDetails(topoId, conf, stormTopology,  numWorkers,\n+                    execToComp, Time.currentTimeSecs(), \"user\");\n+            topo.getComponents(); // sanity check - normally this should not fail\n+\n+            topoDetailsList.add(topo);\n+        }\n+        if (!errors.isEmpty() && failOnParseError) {\n+            throw new Exception(\"Unable to parse all serialized objects\\n\\t\" + String.join(\"\\n\\t\", errors));\n+        }\n+        return topoDetailsList.toArray(new TopologyDetails[0]);\n+    }\n+\n+    /**\n+     * Check if the files in the resource directory are matched, can be read properly, and code/config files occur\n+     * in matched pairs.\n+     *\n+     * @throws Exception showing bad and unmatched resource files.\n+     */\n+    @Test\n+    public void testReadSerializedTopologiesAndConfigs() throws Exception {\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Assert.assertTrue(\"No resource files found in \" + TEST_RESOURCE_PATH, !resources.isEmpty());\n+        TopologyDetails[] topoDetailsArray = createTopoDetailsArray(true);\n+    }\n+\n+    /**\n+     * Create one supervisor and add to the supervisors list.\n+     *\n+     * @param rack rack-number\n+     * @param superInRack supervisor number in the rack\n+     * @param cpu percentage\n+     * @param mem in megabytes\n+     * @param numPorts number of ports on this supervisor\n+     * @param sups returned map os supervisors\n+     */\n+    private static void createAndAddOneSupervisor(\n+            int rack, int superInRack, double cpu, double mem, int numPorts,\n+            Map<String, SupervisorDetails> sups) {\n+\n+        List<Number> ports = new LinkedList<>();\n+        for (int p = 0; p < numPorts; p++) {\n+            ports.add(p);\n+        }\n+        String superId = String.format(\"r%03ds%03d\", rack, superInRack);\n+        String hostId  = String.format(\"host-%03d-rack-%03d\", superInRack, rack);\n+        Map<String, Double> resourceMap = new HashMap<>();\n+        resourceMap.put(Config.SUPERVISOR_CPU_CAPACITY, cpu);\n+        resourceMap.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, mem);\n+        resourceMap.put(\"network.resource.units\", 50.0);\n+        SupervisorDetails sup = new SupervisorDetails(superId,\n+                hostId, null, ports,\n+                NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceMap));\n+        sups.put(sup.getId(), sup);\n+    }\n+\n+    /**\n+     * Create supervisors.\n+     *\n+     * @param uniformSupervisors true if all supervisors are of the same size, false otherwise.\n+     * @return supervisor details indexed by id\n+     */\n+    private static Map<String, SupervisorDetails> createSupervisors(boolean uniformSupervisors) {\n+        Map<String, SupervisorDetails> retVal;\n+        if (uniformSupervisors) {\n+            int numRacks = 16;\n+            int numSupersPerRack = 82;\n+            int numPorts = 50; // scheduling will fill up 2-6, leaving of 90% workerslots unused - does this cause slow scheduling?\n+            int rackStart = 0;\n+            int superInRackStart = 1;\n+            double cpu = 7200; // %percent\n+            double mem = 356_000; // MB\n+            Map<String, Double> miscResources = new HashMap<>();\n+            miscResources.put(\"network.resource.units\", 100.0);\n+\n+            return TestUtilsForResourceAwareScheduler.genSupervisorsWithRacks(\n+                    numRacks, numSupersPerRack, numPorts, rackStart, superInRackStart, cpu, mem, miscResources);\n+\n+        } else {\n+            // this non-uniform supervisor distribution closelt (but not exactly) mimics IridiumBlue cluster\n+            int numSupersPerRack = 82;\n+            int numPorts = 50;\n+\n+            Map<String, SupervisorDetails> retList = new HashMap<>();\n+\n+            for (int rack = 0 ; rack < 12 ; rack++) {\n+                double cpu = 3600; // %percent\n+                double mem = 178_000; // MB\n+                for (int superInRack = 0; superInRack < numSupersPerRack ; superInRack++) {\n+                    createAndAddOneSupervisor(rack, superInRack, cpu - 100 * (superInRack % 2), mem, numPorts, retList);\n+                }\n+            }\n+            for (int rack = 12 ; rack < 14 ; rack++) {\n+                double cpu = 2400; // %percent\n+                double mem = 118_100; // MB\n+                for (int superInRack = 0; superInRack < numSupersPerRack ; superInRack++) {\n+                    createAndAddOneSupervisor(rack, superInRack, cpu - 100 * (superInRack % 2), mem, numPorts, retList);\n+                }\n+            }\n+            for (int rack = 14 ; rack < 16 ; rack++) {\n+                double cpu = 1200; // %percent\n+                double mem = 42_480; // MB\n+                for (int superInRack = 0; superInRack < numSupersPerRack ; superInRack++) {\n+                    createAndAddOneSupervisor(rack, superInRack, cpu - 100 * (superInRack % 2), mem, numPorts, retList);\n+                }\n+            }\n+            return retList;\n+        }\n+    }\n+\n+    /**\n+     * Create a large cluster, read topologies and configuration from resource directory and schedule.\n+     *\n+     * @throws Exception\n+     */\n+    @Test\n+    public void testLargeCluster() throws Exception {\n+        boolean uniformSupervisors = false; // false means non-uniform supervisor distribution\n+\n+        Map<String, SupervisorDetails> supervisors = createSupervisors(uniformSupervisors);\n+\n+        TopologyDetails[] topoDetailsArray = createTopoDetailsArray(false);\n+        Assert.assertTrue(\"No topologies found\", topoDetailsArray.length > 0);\n+        Topologies topologies = new Topologies(topoDetailsArray);\n+\n+        Config confWithDefaultStrategy = new Config();\n+        confWithDefaultStrategy.putAll(topoDetailsArray[0].getConf());\n+        confWithDefaultStrategy.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, DefaultResourceAwareStrategy.class.getName());\n+\n+        INimbus iNimbus = new INimbusTest();\n+        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supervisors, new HashMap<>(),\n+                topologies, confWithDefaultStrategy);\n+\n+        scheduler = new ResourceAwareScheduler();\n+\n+        List<Class> classesToDebug = Arrays.asList(DefaultResourceAwareStrategy.class,\n+                GenericResourceAwareStrategy.class, ResourceAwareScheduler.class,\n+                Cluster.class // count calls to calculateSharedOffHeapNodeMemory()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 388}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1Nzg3Nw==", "bodyText": "Maybe clean up the comments if they are not needed", "url": "https://github.com/apache/storm/pull/3244#discussion_r406457877", "createdAt": "2020-04-09T20:27:23Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestTopologyAnonymizerUtils.java", "diffHunk": "@@ -0,0 +1,355 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.Bolt;\n+import org.apache.storm.generated.GlobalStreamId;\n+import org.apache.storm.generated.SpoutSpec;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.serialization.GzipThriftSerializationDelegate;\n+import org.apache.storm.utils.Utils;\n+import org.junit.jupiter.api.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Anonymize Serialized Topologies and Configs with the goal of taking internally developed topologies and configuration\n+ * and make them publicly available for testing.\n+ *\n+ * Assume that topologies and configurations exist in the specified resource directory with names ending in stormcode.ser\n+ * and stormconf.ser respectively as they exist in blobstore. Also, stormconf.ser file shares the same name prefix with\n+ * the topology serialized file.\n+ *\n+ * <li> Rename topologies and its corresponding configuration (as identified by its resource name). Ensure that renamed\n+ * configuration file for a topology retains the proper linkage so that:\n+ *     <p>&lt;old-topo-name&gt;-stormcode.ser -&gt; &lt;new-topo-name&gt;-stormcode.ser</p> and its old conf\n+ *     <p>&lt;old-topo-name&gt;-stormconf.ser -&gt; &lt;new-topo-name&gt;-stormconf.ser</p>\n+ * </li>\n+ *\n+ * <li>Rename components in each of the topologies.</li>\n+ *\n+ * The new converted resource files can be copied to a resource directory under \"clusterconf\" and made available for use\n+ * in TestLargeCluster class.\n+ */\n+public class TestTopologyAnonymizerUtils {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestTopologyAnonymizerUtils.class);\n+\n+    private static final String DEFAULT_ORIGINAL_RESOURCES_PATH = \"clusterconf/iridiumblue\";\n+    private static final String DEFAULT_ANONYMIZED_RESOURCES_OUTDIR = \"src/test/resources/clusterconf/largeCluster01\";\n+\n+    private String originalResourcePath;\n+    private String outputDirPath;\n+\n+    public TestTopologyAnonymizerUtils() {\n+        this.originalResourcePath = DEFAULT_ORIGINAL_RESOURCES_PATH;\n+        this.outputDirPath = DEFAULT_ANONYMIZED_RESOURCES_OUTDIR;\n+    }\n+\n+    //public TestTopologyAnonymizerUtils(String originalResourcePath, String outputDirPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 79}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "772af366c158e84093552dec53399d70a9655c85", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/772af366c158e84093552dec53399d70a9655c85", "committedDate": "2020-04-10T00:54:23Z", "message": "[STORM-3600] Use shared_memory instead of component_shared_memory."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b3eff3645b403df22fba92ecf0bb566d21824e7", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/9b3eff3645b403df22fba92ecf0bb566d21824e7", "committedDate": "2020-04-10T00:56:10Z", "message": "[STORM-3600] Comment changes and cleanup."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxMjMzMjUz", "url": "https://github.com/apache/storm/pull/3244#pullrequestreview-391233253", "createdAt": "2020-04-10T02:08:44Z", "commit": {"oid": "9b3eff3645b403df22fba92ecf0bb566d21824e7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyMzU2MjE0", "url": "https://github.com/apache/storm/pull/3244#pullrequestreview-392356214", "createdAt": "2020-04-13T18:55:51Z", "commit": {"oid": "9b3eff3645b403df22fba92ecf0bb566d21824e7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNDUyMDMy", "url": "https://github.com/apache/storm/pull/3244#pullrequestreview-392452032", "createdAt": "2020-04-13T21:26:58Z", "commit": {"oid": "9b3eff3645b403df22fba92ecf0bb566d21824e7"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMToyNjo1OFrOGE19IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMToyOTowOVrOGE2BLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczMTQ4OQ==", "bodyText": "Nit: move these file extensions as constants to the head of the file", "url": "https://github.com/apache/storm/pull/3244#discussion_r407731489", "createdAt": "2020-04-13T21:26:58Z", "author": {"login": "govind-menon"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,456 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b3eff3645b403df22fba92ecf0bb566d21824e7"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczMjI4NQ==", "bodyText": "Agreed - try with resources will close as part of the \"destructor\"", "url": "https://github.com/apache/storm/pull/3244#discussion_r407732285", "createdAt": "2020-04-13T21:28:41Z", "author": {"login": "govind-menon"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ1MDUyOA=="}, "originalCommit": {"oid": "4515a40cfeb391d62910f5f32951abe9f7b5277a"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczMjUyNg==", "bodyText": "Nit: See above about extracting file extension as constants", "url": "https://github.com/apache/storm/pull/3244#discussion_r407732526", "createdAt": "2020-04-13T21:29:09Z", "author": {"login": "govind-menon"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -0,0 +1,456 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.scheduler.resource.strategies.scheduling;\n+\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.core.config.Configurator;\n+import org.apache.storm.Config;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.generated.StormTopology;\n+import org.apache.storm.metric.StormMetricsRegistry;\n+import org.apache.storm.scheduler.Cluster;\n+import org.apache.storm.scheduler.ExecutorDetails;\n+import org.apache.storm.scheduler.INimbus;\n+import org.apache.storm.scheduler.IScheduler;\n+import org.apache.storm.scheduler.SupervisorDetails;\n+import org.apache.storm.scheduler.Topologies;\n+import org.apache.storm.scheduler.TopologyDetails;\n+import org.apache.storm.scheduler.WorkerSlot;\n+import org.apache.storm.scheduler.resource.ResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.TestUtilsForResourceAwareScheduler;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;\n+import org.apache.storm.scheduler.resource.normalization.NormalizedResourcesExtension;\n+import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.apache.storm.utils.Utils;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+@ExtendWith({NormalizedResourcesExtension.class})\n+public class TestLargeCluster {\n+    private static final Logger LOG = LoggerFactory.getLogger(TestLargeCluster.class);\n+\n+    public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n+    public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+\n+    private static IScheduler scheduler = null;\n+\n+    @AfterEach\n+    public void cleanup() {\n+        if (scheduler != null) {\n+            scheduler.cleanup();\n+            scheduler = null;\n+        }\n+    }\n+\n+    /**\n+     * Get the list of serialized topology (*code.ser) and configuration (*conf.ser)\n+     * resource files in the path. The resources are sorted so that paired topology and conf\n+     * files are sequential. Unpaired files may be ignored by the caller.\n+     *\n+     * @param path directory in which resources exist.\n+     * @return\n+     * @throws IOException\n+     */\n+    public static List<String> getResourceFiles(String path) throws IOException {\n+        List<String> fileNames = new ArrayList<>();\n+\n+        try (\n+                InputStream in = getResourceAsStream(path);\n+                BufferedReader br = new BufferedReader(new InputStreamReader(in))\n+        ) {\n+            String resource;\n+\n+            while ((resource = br.readLine()) != null) {\n+                if (resource.endsWith(\"code.ser\") || resource.endsWith(\"conf.ser\")) {\n+                    fileNames.add(path + \"/\" + resource);\n+                }\n+            }\n+            Collections.sort(fileNames);\n+        }\n+        return fileNames;\n+    }\n+\n+    /**\n+     * InputStream to read the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     */\n+    public static InputStream getResourceAsStream(String resource) {\n+        final InputStream in = getContextClassLoader().getResourceAsStream(resource);\n+        return in == null ? ClassLoader.getSystemClassLoader().getResourceAsStream(resource) : in;\n+    }\n+\n+    /**\n+     * Read the contents of the fully qualified resource path.\n+     *\n+     * @param resource\n+     * @return\n+     * @throws Exception\n+     */\n+    public static byte[] getResourceAsBytes(String resource) throws Exception {\n+        InputStream in = getResourceAsStream(resource);\n+        if (in == null) {\n+            return null;\n+        }\n+        try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+            while (in.available() > 0) {\n+                out.write(in.read());\n+            }\n+            return out.toByteArray();\n+        }\n+    }\n+\n+    public static ClassLoader getContextClassLoader() {\n+        return Thread.currentThread().getContextClassLoader();\n+    }\n+\n+    /**\n+     * Create an array of TopologyDetails by reading serialized files for topology and configuration in the\n+     * resource path.\n+     *\n+     * @param failOnParseError throw exception if there are unmatched files, otherwise ignore unmatched and read errors.\n+     * @return An array of TopologyDetails representing resource files.\n+     * @throws Exception\n+     */\n+    public static TopologyDetails[] createTopoDetailsArray(boolean failOnParseError) throws Exception {\n+        List<TopologyDetails> topoDetailsList = new ArrayList<>();\n+        List<String> errors = new ArrayList<>();\n+        List<String> resources = getResourceFiles(TEST_RESOURCE_PATH);\n+        Map<String, String> codeResourceMap = new TreeMap<>();\n+        Map<String, String> confResourceMap = new HashMap<>();\n+        for (int i = 0 ; i < resources.size() ; i++) {\n+            String resource = resources.get(i);\n+            int idxOfSlash = resource.lastIndexOf(\"/\");\n+            int idxOfDash = resource.lastIndexOf(\"-\");\n+            String nm = idxOfDash > idxOfSlash ? resource.substring(idxOfSlash + 1, idxOfDash) : resource.substring(idxOfSlash + 1, resource.length() - 7);\n+            if (resource.endsWith(\"code.ser\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b3eff3645b403df22fba92ecf0bb566d21824e7"}, "originalPosition": 166}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c9d467efaf1c52647c24963571be594f6f4cebc", "author": {"user": null}, "url": "https://github.com/apache/storm/commit/0c9d467efaf1c52647c24963571be594f6f4cebc", "committedDate": "2020-04-15T04:15:05Z", "message": "[STORM-3600] Introduce constants for file name endings."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkzODM0MDIy", "url": "https://github.com/apache/storm/pull/3244#pullrequestreview-393834022", "createdAt": "2020-04-15T14:33:07Z", "commit": {"oid": "0c9d467efaf1c52647c24963571be594f6f4cebc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxNDozMzowN1rOGF8opA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxNDozMzowN1rOGF8opA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODg4OTUwOA==", "bodyText": "There is a variable with the same name in TestTopologyAnonymizerUtils.java but with different meanings. What's the difference here?", "url": "https://github.com/apache/storm/pull/3244#discussion_r408889508", "createdAt": "2020-04-15T14:33:07Z", "author": {"login": "Ethanlm"}, "path": "storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java", "diffHunk": "@@ -70,6 +70,8 @@\n \n     public static final String TEST_CLUSTER_NAME = \"largeCluster01\";\n     public static final String TEST_RESOURCE_PATH = \"clusterconf/\" + TEST_CLUSTER_NAME;\n+    public static final String COMPRESSED_SERIALIZED_TOPOLOGY_FILENAME_ENDING = \"code.ser\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c9d467efaf1c52647c24963571be594f6f4cebc"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4550, "cost": 1, "resetAt": "2021-11-01T11:59:11Z"}}}