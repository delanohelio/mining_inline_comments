{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyMzMwNDE5", "number": 336, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxODo0MzowNFrODxV8sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxODo1NDo0NlrODxWM-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDY2NDE3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxODo0MzowNFrOGEwr7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMTo0MDowMFrOGE2U1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng==", "bodyText": "maybe declare \"IGNORE_INVALID_INPUT\" as a constant on RequestContext ?", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407645166", "createdAt": "2020-04-13T18:43:04Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java", "diffHunk": "@@ -64,7 +66,16 @@ public void initialize(RequestContext context) {\n     @Override\n     public List<Fragment> getFragments() throws Exception {\n         Path path = new Path(hcfsType.getDataUri(jobConf, context));\n-        List<InputSplit> splits = getSplits(path);\n+        List<InputSplit> splits;\n+        try {\n+            splits = getSplits(path);\n+        } catch (InvalidInputException e) {\n+            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(\"IGNORE_INVALID_INPUT\"))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0Nzc5OA==", "bodyText": "can we think of a better name ? \"Invalid input\" is not smth exposed to end-users defining path in LOCATION Url, for them it is either \"path\" or \"resource\", in fact we call it in our docs \"path to data\" LOCATION('pxf://<path-to-data>?PROFILE=<profile_name>, so maybe at least IGNORE_INVALID_PATH. It is also not that the path is invalid (like in violation of syntax rules) but more as non-existent, but IGNORE_NON_EXISTENT_PATH is too long and prone to typing errors. I'd prefer 1-2 simple words max.", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407647798", "createdAt": "2020-04-13T18:47:44Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java", "diffHunk": "@@ -64,7 +66,16 @@ public void initialize(RequestContext context) {\n     @Override\n     public List<Fragment> getFragments() throws Exception {\n         Path path = new Path(hcfsType.getDataUri(jobConf, context));\n-        List<InputSplit> splits = getSplits(path);\n+        List<InputSplit> splits;\n+        try {\n+            splits = getSplits(path);\n+        } catch (InvalidInputException e) {\n+            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(\"IGNORE_INVALID_INPUT\"))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng=="}, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY5ODY3MA==", "bodyText": "maybe something like IGNORE_MISSING_PATH could also work", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407698670", "createdAt": "2020-04-13T20:23:35Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java", "diffHunk": "@@ -64,7 +66,16 @@ public void initialize(RequestContext context) {\n     @Override\n     public List<Fragment> getFragments() throws Exception {\n         Path path = new Path(hcfsType.getDataUri(jobConf, context));\n-        List<InputSplit> splits = getSplits(path);\n+        List<InputSplit> splits;\n+        try {\n+            splits = getSplits(path);\n+        } catch (InvalidInputException e) {\n+            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(\"IGNORE_INVALID_INPUT\"))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng=="}, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczNzU1OA==", "bodyText": "Using IGNORE_MISSING_PATH", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407737558", "createdAt": "2020-04-13T21:40:00Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java", "diffHunk": "@@ -64,7 +66,16 @@ public void initialize(RequestContext context) {\n     @Override\n     public List<Fragment> getFragments() throws Exception {\n         Path path = new Path(hcfsType.getDataUri(jobConf, context));\n-        List<InputSplit> splits = getSplits(path);\n+        List<InputSplit> splits;\n+        try {\n+            splits = getSplits(path);\n+        } catch (InvalidInputException e) {\n+            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(\"IGNORE_INVALID_INPUT\"))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng=="}, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDcwNTg0OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxODo1NDo0NlrOGExFnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTo0NzozNFrOGE7NXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA==", "bodyText": "what about other profiles ? If we introduce such an option, users might expect it to work similarly for other profiles (Hive, JDBC, etc). So maybe have other profiles proactively throw an error if they encounter this setting ? Or will this be too restrictive, since they will essentially ignore the option and error out if they determine actually missing path.", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407651740", "createdAt": "2020-04-13T18:54:46Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java", "diffHunk": "@@ -74,4 +74,37 @@ public void testFragmenterWilcardPath() throws Exception {\n         assertNotNull(fragmentList);\n         assertEquals(4, fragmentList.size());\n     }\n+\n+    @Test\n+    public void testInvalidInputPath() throws Exception {\n+        expectedException.expect(InvalidInputException.class);\n+        expectedException.expectMessage(\"Input Pattern file:/tmp/non-existent-path-on-disk/*.csv matches 0 files\");\n+\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+        fragmenter.getFragments();\n+    }\n+\n+    @Test\n+    public void testInvalidInputPathIgnored() throws Exception {\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.addOption(\"IGNORE_INVALID_INPUT\", \"true\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+\n+        List<Fragment> fragmentList = fragmenter.getFragments();\n+        assertNotNull(fragmentList);\n+        assertEquals(0, fragmentList.size());\n+    }\n }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NTc1OQ==", "bodyText": "I thought about that, but I think this concept of PATH does not apply to other profiles. In essence, this feature allows supporting partitioned tables. Not in the Hive partitioned tables sense, but rather Greenplum partitioned tables backed by external tables accessing Hadoop Compatible Filesystems.", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407675759", "createdAt": "2020-04-13T19:39:53Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java", "diffHunk": "@@ -74,4 +74,37 @@ public void testFragmenterWilcardPath() throws Exception {\n         assertNotNull(fragmentList);\n         assertEquals(4, fragmentList.size());\n     }\n+\n+    @Test\n+    public void testInvalidInputPath() throws Exception {\n+        expectedException.expect(InvalidInputException.class);\n+        expectedException.expectMessage(\"Input Pattern file:/tmp/non-existent-path-on-disk/*.csv matches 0 files\");\n+\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+        fragmenter.getFragments();\n+    }\n+\n+    @Test\n+    public void testInvalidInputPathIgnored() throws Exception {\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.addOption(\"IGNORE_INVALID_INPUT\", \"true\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+\n+        List<Fragment> fragmentList = fragmenter.getFragments();\n+        assertNotNull(fragmentList);\n+        assertEquals(0, fragmentList.size());\n+    }\n }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA=="}, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc5ODAxOA==", "bodyText": "Is it fair to say that the only profiles that don't use Hdfs{Data,File}Fragmenter are Hive and JDBC?", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407798018", "createdAt": "2020-04-14T00:37:01Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java", "diffHunk": "@@ -74,4 +74,37 @@ public void testFragmenterWilcardPath() throws Exception {\n         assertNotNull(fragmentList);\n         assertEquals(4, fragmentList.size());\n     }\n+\n+    @Test\n+    public void testInvalidInputPath() throws Exception {\n+        expectedException.expect(InvalidInputException.class);\n+        expectedException.expectMessage(\"Input Pattern file:/tmp/non-existent-path-on-disk/*.csv matches 0 files\");\n+\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+        fragmenter.getFragments();\n+    }\n+\n+    @Test\n+    public void testInvalidInputPathIgnored() throws Exception {\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.addOption(\"IGNORE_INVALID_INPUT\", \"true\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+\n+        List<Fragment> fragmentList = fragmenter.getFragments();\n+        assertNotNull(fragmentList);\n+        assertEquals(0, fragmentList.size());\n+    }\n }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA=="}, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNzU2Nw==", "bodyText": "we also have Hbase and S3 Select", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407817567", "createdAt": "2020-04-14T01:47:34Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java", "diffHunk": "@@ -74,4 +74,37 @@ public void testFragmenterWilcardPath() throws Exception {\n         assertNotNull(fragmentList);\n         assertEquals(4, fragmentList.size());\n     }\n+\n+    @Test\n+    public void testInvalidInputPath() throws Exception {\n+        expectedException.expect(InvalidInputException.class);\n+        expectedException.expectMessage(\"Input Pattern file:/tmp/non-existent-path-on-disk/*.csv matches 0 files\");\n+\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+        fragmenter.getFragments();\n+    }\n+\n+    @Test\n+    public void testInvalidInputPathIgnored() throws Exception {\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.addOption(\"IGNORE_INVALID_INPUT\", \"true\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+\n+        List<Fragment> fragmentList = fragmenter.getFragments();\n+        assertNotNull(fragmentList);\n+        assertEquals(0, fragmentList.size());\n+    }\n }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA=="}, "originalCommit": {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f"}, "originalPosition": 37}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3619, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}