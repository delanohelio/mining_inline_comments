{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1ODEyMDU1", "number": 456, "title": "Add support for Network File Storage (NFS) ", "bodyText": "The NFS profile allows PXF reading data from Network File Systems. The\nNFS has to be mounted on every segment host where PXF is running from.\nPXF will then read/write data from the mounted NFS file.\nFor NFS, the pxf.fs.basePath property must be set to a valid path. The\npxf.fs.basePath property represents the base path to be used for\ntables accessing that server.\nAdd automation test for the NFS profile using a multinode cluster", "createdAt": "2020-09-30T21:58:46Z", "url": "https://github.com/greenplum-db/pxf/pull/456", "merged": true, "mergeCommit": {"oid": "c2aae95b09205553342e5efadf1d8d340dcf8494"}, "closed": true, "closedAt": "2020-10-07T17:18:23Z", "author": {"login": "frankgh"}, "timelineItems": {"totalCount": 45, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNZmjJgH2gAyNDk1ODEyMDU1OmYzNzgzYzgyNTQ2YjAxNTRkMmUzODQ1Yzg5OWIzNWQ5NDQxZDI0YWQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQQPGiAH2gAyNDk1ODEyMDU1OjJkNzZhMGI3YzVjZmQ2N2ZlZTBiZjI4OTI0N2IxOGQ2MjE0Yjc4YWE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/f3783c82546b0154d2e3845c899b35d9441d24ad", "committedDate": "2020-09-28T20:27:59Z", "message": "Productize NFS (localfile) profile\n\nThe NFS profile allows PXF reading data from Network File Systems. The\nNFS has to be mounted on every segment host where PXF is running from.\nPXF will then read/write data from the mounted NFS file.\n\nFor NFS, the `pxf.fs.basePath` property must be set to a valid path. The\n`pxf.fs.basePath` property represents the base path to be used for\ntables accessing that server.\n\nFor example, if the `pxf.fs.basePath` property is set to `/mount/path`\nand the external table definition is the following:\n\n```sql\nCREATE EXTERNAL TABLE nfs_table()\nLOCATION ('pxf://path/to/data?PROFILE=nfs:avro&SERVER=nfs-server')\nFORMAT 'CUSTOM' (formatter='pxfwritable_import');\n\nThen PXF will use the :///mount/path/path/to/data URI to read AVRO\nfiles.\n\nIn this commit, PXF also prevents relative paths to be specified in the\nlocation of any of the hadoop-compatible file systems (S3, GS, HDFS,\nNFS, etc). This is to prevent users from accessing data under relative\npaths to the `basePath`. For example, assume the mount path above, a\nmalicious user can attempt to read data from `/etc/passwd` by creating\nan external table like this:\n\n```sql\nCREATE EXTERNAL TABLE hack_etc_passwd (entry text)\nLOCATION ('pxf://../../etc/passwd?PROFILE=nfs:text&SERVER=nfs')\nFORMAT 'text';\n```\n\nPXF now prevents users from accessing relative paths in the path defined\nin the LOCATION URI. PXF also prevents using `$` to disallow resolution\nof environment variables for example:\n\n```sql\nCREATE EXTERNAL TABLE hack_home_dir (entry text)\nLOCATION\n('pxf://$HOME/secret-files-in-gpadmin-home?PROFILE=nfs:text&SERVER=nfs')\nFORMAT 'text';\n```\n\nPXF now will throw an exception in cases where users attempt to use\nrelative paths or $ in the path."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e73d3e784842efa5d1973fa70b6f433061b1c5f", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/2e73d3e784842efa5d1973fa70b6f433061b1c5f", "committedDate": "2020-09-28T20:41:25Z", "message": "Deprecate LOCALFILE\n\nLOCALFILE is deprecated in favor of NFS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/0ab9a53952a65fa7776c62555e81ef5aa13f4503", "committedDate": "2020-09-30T21:56:02Z", "message": "Add automation tests for NFS\n\nAdd multinode tests for NFS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/b9fc21cd0a10ee5af16bd21d36c553d2c92589f7", "committedDate": "2020-09-30T22:48:54Z", "message": "Mount NFS on the running container, configure the NFS server on the cluster"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44a81852b24bcb49d2e3e363c7ccd548b61d9ee4", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/44a81852b24bcb49d2e3e363c7ccd548b61d9ee4", "committedDate": "2020-09-30T23:09:54Z", "message": "Fix sed"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODU4Nzkx", "url": "https://github.com/greenplum-db/pxf/pull/456#pullrequestreview-499858791", "createdAt": "2020-09-30T22:35:15Z", "commit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozNToxNVrOHaxqaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo1NzoyNVrOHayHOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzODY5Nw==", "bodyText": "It looks like this check does not allow any occurrence of .. in the effective base path. This is probably the safest check to do, but doesn't it prevent the use of relative paths that stay within the configured base path, e.g. /mnt/data/dir1/../dir2?", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497838697", "createdAt": "2020-09-30T22:35:15Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +213,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");\n+\n+        if (\"..\".equals(effectiveBasePath) || StringUtils.contains(effectiveBasePath, \"../\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYzNg==", "bodyText": "You can have pathnames that contain $ that are not part of an environment variable. As long as we don't expand the path or pass it to a shell unescaped, we should be able to safely allow $ in paths, right?", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497839636", "createdAt": "2020-09-30T22:38:00Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +213,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");\n+\n+        if (\"..\".equals(effectiveBasePath) || StringUtils.contains(effectiveBasePath, \"../\")) {\n+            // Disallow relative paths\n+            throw new IllegalArgumentException(String\n+                    .format(\"the provided path '%s' is invalid. Relative paths are not allowed by PXF\", effectiveBasePath));\n+        }\n+\n+        if (StringUtils.contains(effectiveBasePath, \"$\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NDYwNA==", "bodyText": "The showmount command is provided by nfs-utils which is being installed after we try to use it.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497844604", "createdAt": "2020-09-30T22:52:37Z", "author": {"login": "bradfordb-vmware"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NTQ0MQ==", "bodyText": "Minor nit: should we include -t nfs? Without it, mount will try to guess it.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497845441", "createdAt": "2020-09-30T22:55:22Z", "author": {"login": "bradfordb-vmware"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw\n+\n+echo \"install the NFS client\"\n+yum install -y -q -e 0 nfs-utils\n+\n+echo \"create mount point and mount it\"\n+mkdir -p ${BASE_PATH}\n+mount mdw:/var/nfs ${BASE_PATH}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NjA3Mg==", "bodyText": "Is there a reason we don't externalize this and scp to the master host? Would save us from having to prefix every line with sudo.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497846072", "createdAt": "2020-09-30T22:57:25Z", "author": {"login": "bradfordb-vmware"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw\n+\n+echo \"install the NFS client\"\n+yum install -y -q -e 0 nfs-utils\n+\n+echo \"create mount point and mount it\"\n+mkdir -p ${BASE_PATH}\n+mount mdw:/var/nfs ${BASE_PATH}\n+chown gpadmin:gpadmin ${BASE_PATH}\n+chmod 755 ${BASE_PATH}\n+\n+echo \"verify the mount worked\"\n+mount | grep nfs\n+df -hT\n+\n+echo \"write a test file to make sure it worked\"\n+sudo runuser -l gpadmin -c \"touch ${BASE_PATH}/$(hostname)-test\"\n+ls -l ${BASE_PATH}\n+\n+EOFF\n+\n+  chmod +x /tmp/install_and_configure_nfs_client.sh\n+  scp /tmp/install_and_configure_nfs_client.sh \"${MASTER_HOSTNAME}:~gpadmin\"\n+}\n+\n+# assumes only two segment hosts sdw1 and sdw2\n+function run_nfs_installation() {\n+\n+  # install and configure the NFS server on master\n+  ssh \"centos@${MASTER_HOSTNAME}\" \"\n+    echo 'install NFS server'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODgxNjg1", "url": "https://github.com/greenplum-db/pxf/pull/456#pullrequestreview-499881685", "createdAt": "2020-09-30T23:34:02Z", "commit": {"oid": "44a81852b24bcb49d2e3e363c7ccd548b61d9ee4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozNDowMlrOHayzUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozNDowMlrOHayzUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1NzM2MQ==", "bodyText": "effectiveDataSource", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497857361", "createdAt": "2020-09-30T23:34:02Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +215,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44a81852b24bcb49d2e3e363c7ccd548b61d9ee4"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5ODgxOTk3", "url": "https://github.com/greenplum-db/pxf/pull/456#pullrequestreview-499881997", "createdAt": "2020-09-30T23:35:00Z", "commit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozNTowMFrOHay0SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo1Mzo0MFrOHazI3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1NzYwOQ==", "bodyText": "this logic will still be correct for \"/\" input, right ? Then I think comparing is a premature optimization.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497857609", "createdAt": "2020-09-30T23:35:00Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -212,20 +243,37 @@ protected String getDataUriForPrefix(Configuration configuration, String dataSou\n         URI defaultFS = FileSystem.getDefaultUri(configuration);\n \n         String uri;\n-        String normalizedDataSource = normalizeDataSource(dataSource);\n+        String basePath = validateAndNormalizeBasePath(configuration.get(CONFIG_KEY_BASE_PATH));\n+        String normalizedDataSource = validateAndNormalizeDataSource(dataSource);\n \n         if (FILE_SCHEME.equals(defaultFS.getScheme())) {\n             // if the defaultFS is file://, but enum is not FILE, use enum scheme only\n-            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + basePath + normalizedDataSource;\n         } else {\n             // if the defaultFS is not file://, use it, instead of enum scheme and append user's path\n-            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + \"/\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + basePath + \"/\" + normalizedDataSource;\n         }\n \n         disableSecureTokenRenewal(uri, configuration);\n         return uri;\n     }\n \n+    /**\n+     * Validates the basePath and normalizes it for the appropriate filesystem\n+     *\n+     * @param basePath the basePath as configured by the user\n+     * @return the normalized basePath\n+     */\n+    protected String validateAndNormalizeBasePath(String basePath) {\n+        if (StringUtils.isNotBlank(basePath)) {\n+            if (\"/\".equals(basePath))\n+                return \"/\";\n+            return StringUtils.removeEnd(StringUtils.removeStart(basePath, \"/\"), \"/\") + \"/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MDIzMw==", "bodyText": "this seems to be the same as the previous one", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497860233", "createdAt": "2020-09-30T23:44:27Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HcfsTypeTest.java", "diffHunk": "@@ -373,4 +374,144 @@ public void testHcfsGlobPattern() {\n         assertEquals(\"0.0.0.0\", configuration.get(MRJobConfig.JOB_NAMENODES_TOKEN_RENEWAL_EXCLUDE));\n     }\n \n+    @Test\n+    public void testFailureOnNFSWhenBasePathIsNotConfigured() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the 'pxf.fs.basePath' configuration is required to access NFS filesystems. Configure a valid 'pxf.fs.basePath' property to access this server\");\n+\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testFailureOnNFSWhenInvalidDefaultFSIsProvided() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"profile protocol (nfs) is not compatible with server filesystem (s3a)\");\n+\n+        configuration.set(\"fs.defaultFS\", \"s3a://abc/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToRootDirectory() {\n+        configuration.set(\"pxf.fs.basePath\", \"/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToAFixedBucket() {\n+        configuration.set(\"pxf.fs.basePath\", \"some-bucket\");\n+        context.setProfileScheme(\"s3a\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"s3a://some-bucket/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToASingleCharPath() {\n+        configuration.set(\"pxf.fs.basePath\", \"p\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing and preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToSomeValue() {\n+        configuration.set(\"pxf.fs.basePath\", \"/my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / preceding the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / trailing the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MTAzNw==", "bodyText": "let's also test ../ and a/..", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497861037", "createdAt": "2020-09-30T23:46:59Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HcfsTypeTest.java", "diffHunk": "@@ -373,4 +374,144 @@ public void testHcfsGlobPattern() {\n         assertEquals(\"0.0.0.0\", configuration.get(MRJobConfig.JOB_NAMENODES_TOKEN_RENEWAL_EXCLUDE));\n     }\n \n+    @Test\n+    public void testFailureOnNFSWhenBasePathIsNotConfigured() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the 'pxf.fs.basePath' configuration is required to access NFS filesystems. Configure a valid 'pxf.fs.basePath' property to access this server\");\n+\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testFailureOnNFSWhenInvalidDefaultFSIsProvided() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"profile protocol (nfs) is not compatible with server filesystem (s3a)\");\n+\n+        configuration.set(\"fs.defaultFS\", \"s3a://abc/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToRootDirectory() {\n+        configuration.set(\"pxf.fs.basePath\", \"/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToAFixedBucket() {\n+        configuration.set(\"pxf.fs.basePath\", \"some-bucket\");\n+        context.setProfileScheme(\"s3a\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"s3a://some-bucket/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToASingleCharPath() {\n+        configuration.set(\"pxf.fs.basePath\", \"p\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing and preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToSomeValue() {\n+        configuration.set(\"pxf.fs.basePath\", \"/my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / preceding the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / trailing the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // preceding and trailing / in the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/my/base/path/\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testFailsWhenARelativeDataSourceIsProvided1() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the provided path '../../../etc/passwd' is invalid. Relative paths are not allowed by PXF\");\n+\n+        configuration.set(\"pxf.fs.basePath\", \"/some/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        context.setDataSource(\"../../../etc/passwd\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        nfs.getDataUri(configuration, context);\n+    }\n+\n+    @Test\n+    public void testFailsWhenARelativeDataSourceIsProvided2() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the provided path '..' is invalid. Relative paths are not allowed by PXF\");\n+\n+        configuration.set(\"pxf.fs.basePath\", \"/some/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        context.setDataSource(\"..\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        nfs.getDataUri(configuration, context);\n+    }\n+\n+    @Test\n+    public void testDataSourceWithTwoDotsInName() {\n+        configuration.set(\"pxf.fs.basePath\", \"/some/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        context.setDataSource(\"a..txt\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MTYxMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <!-- NFS (Network File System profiles -->\n          \n          \n            \n                <!-- NFS (Network File System) profiles -->", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497861610", "createdAt": "2020-09-30T23:49:17Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,106 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- NFS (Network File System profiles -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MjE0OA==", "bodyText": "I wonder if anyone is using these SequenceFile profiles ? These are for MR jobs, I think. I don't think they will be relevant for NFS at all.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497862148", "createdAt": "2020-09-30T23:51:05Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,106 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- NFS (Network File System profiles -->\n+    <profile>\n+        <name>nfs:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non\n+            parallel) and slower than HdfsTextSimple.\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsFileFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.QuotedLineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:parquet</name>\n+        <description>A profile for reading and writing Parquet data from Google Cloud Storage\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.ParquetResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:avro</name>\n+        <description>This profile is suitable for using when reading Avro files (i.e\n+            fileName.avro)\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.AvroFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.AvroResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:json</name>\n+        <description>\n+            Access JSON data either as:\n+            * one JSON record per line (default)\n+            * or multiline JSON records with an IDENTIFIER parameter indicating a member name used\n+            to determine the encapsulating json object to return\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.json.JsonAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.json.JsonResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:AvroSequenceFile</name>\n+        <description>\n+            Read an Avro format stored in sequence file, with separated schema file from Google\n+            Cloud Storage\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.SequenceFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.AvroResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:SequenceFile</name>\n+        <description>\n+            Profile for accessing Sequence files serialized with a custom Writable class\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.SequenceFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.WritableResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2Mjg3OA==", "bodyText": "I think this is too much, especially the CDATA block. One paragraph should be enough, the rest can be found in documentation.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497862878", "createdAt": "2020-09-30T23:53:40Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "diffHunk": "@@ -35,4 +35,35 @@\n         </description>\n     </property>\n     -->\n+\n+    <!--\n+    <property>\n+        <name>pxf.fs.basePath</name>\n+        <value></value>\n+        <description>\n+        <![CDATA[\n+            Sets the base path when constructing a URI for read and write\n+            operations. This property must be set in the server configuration\n+            for NFS-based (Network File System) profiles. This property is\n+            useful when setting the path to a mount point for a given\n+            filesystem. This prevents users from creating tables accessing\n+            data outside of the mount point.\n+\n+            For example, if the pxf.fs.basePath property is set to /mount/path/\n+            and an external table definition is the following:\n+\n+            CREATE EXTERNAL TABLE nfs_table ()\n+            LOCATION ('pxf://path/to/data?PROFILE=nfs:avro&SERVER=nfs-server')\n+            FORMAT 'CUSTOM' (formatter='pxfwritable_import')\n+\n+            Then PXF will use the :///mount/path/path/to/data URI to read\n+            AVRO files. Users won't be able to access data at the root\n+            directory of the filesystem.\n+\n+            This property can also be used for any Hadoop compatible file\n+            system, for example S3.\n+        ]]>\n+        </description>\n+    </property>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8a71180642fd7c67cef9cd4a3a0cb491c64f7e1", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/b8a71180642fd7c67cef9cd4a3a0cb491c64f7e1", "committedDate": "2020-10-01T10:27:52Z", "message": " is the if the issue?"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcda74048057f7aedf0cfe90ea969ae6d4a33e7b", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/fcda74048057f7aedf0cfe90ea969ae6d4a33e7b", "committedDate": "2020-10-01T10:47:37Z", "message": "Install NFS first then use showmount"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "358b5d7bbc1e4b749d878df23f183c6bb5429580", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/358b5d7bbc1e4b749d878df23f183c6bb5429580", "committedDate": "2020-10-01T10:52:37Z", "message": "fix sed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5abf435b1fb0989d0c05227aa572722026647039", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/5abf435b1fb0989d0c05227aa572722026647039", "committedDate": "2020-10-01T14:03:03Z", "message": "start services inside container. do not install nfs-utils, as it is already installed in the VMs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2dcd4a0951b2bc5ffa2da140f6886e0c5e75bb0", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/d2dcd4a0951b2bc5ffa2da140f6886e0c5e75bb0", "committedDate": "2020-10-01T14:14:02Z", "message": "fixup! Productize NFS (localfile) profile"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c62cb21c214be4d9145ec87eddfaa2e5e46f0843", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/c62cb21c214be4d9145ec87eddfaa2e5e46f0843", "committedDate": "2020-10-01T15:03:59Z", "message": "Fix mounting of NFS inside the container"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebc6785591fd3e19e3b1d225b75dcab7a20d9ff8", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/ebc6785591fd3e19e3b1d225b75dcab7a20d9ff8", "committedDate": "2020-10-01T15:31:45Z", "message": "Address PR feedback from Alex: f3783c82546b0154d2e3845c899b35d9441d24ad"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e11a451cac6bf53bf9a0a58b075d0e0579579f33", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/e11a451cac6bf53bf9a0a58b075d0e0579579f33", "committedDate": "2020-10-01T15:32:00Z", "message": "Run as privileged"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0567abdb46cd1f352952fda7871b9ec1bab8f9f9", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/0567abdb46cd1f352952fda7871b9ec1bab8f9f9", "committedDate": "2020-10-01T15:43:04Z", "message": "Fix issues with chown"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7809c1a176e9c3a80a14e8cb4beb6ad7f3193bc7", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/7809c1a176e9c3a80a14e8cb4beb6ad7f3193bc7", "committedDate": "2020-10-01T19:39:57Z", "message": "CI: When the profile is NFS, remove the basePath during external table creation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/e9a76fcd23c4d968da812105d435dd1a54be4ac0", "committedDate": "2020-10-01T22:07:35Z", "message": "NFS -> FILE"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bcb9c0c432844abc1a2a2eb652fad5568e94e2c", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/2bcb9c0c432844abc1a2a2eb652fad5568e94e2c", "committedDate": "2020-10-01T22:37:04Z", "message": "make SecureLogin.isUserImpersonationEnabled static"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e30f06a57af4ee8f8da464605e498e531b4b3ad", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/4e30f06a57af4ee8f8da464605e498e531b4b3ad", "committedDate": "2020-10-01T22:51:47Z", "message": "Error out when impersonation is enabled for the FILE protocol"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3df36444cf0865c46b742acb2e71692ad4de2f10", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/3df36444cf0865c46b742acb2e71692ad4de2f10", "committedDate": "2020-10-01T23:01:34Z", "message": "Configure pxf-profiles for file protocol to include file:AvroSequenceFile and file:SequenceFile"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNzcwNDA0", "url": "https://github.com/greenplum-db/pxf/pull/456#pullrequestreview-500770404", "createdAt": "2020-10-01T22:55:35Z", "commit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMjo1NTozNVrOHbdIoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMzowMzo0OFrOHbdRCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU1MDk0NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    String.format(\"the '%1$s' configuration is required to access locally mounted file systems. Configure a valid '%1$s' property to access this server\",\n          \n          \n            \n                                    String.format(\"Configure a valid value for '%s' property for this server to access the filesystem\",", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498550944", "createdAt": "2020-10-01T22:55:35Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -26,20 +26,24 @@ public String getDataUri(Configuration configuration, RequestContext context) {\n     },\n     FILE {\n         @Override\n-        public String getDataUri(Configuration configuration, RequestContext context) {\n-            throw new IllegalStateException(\"core-site.xml is missing or using unsupported file:// as default filesystem\");\n-        }\n+        protected String validateAndNormalizeBasePath(String basePath) {\n+            if (StringUtils.isBlank(basePath))\n+                throw new IllegalArgumentException(\n+                        String.format(\"the '%1$s' configuration is required to access locally mounted file systems. Configure a valid '%1$s' property to access this server\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU1MzA5OA==", "bodyText": "normalized base path already has \"/\" at the end, unless it is empty, so will you end up with \"//\" here ?", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498553098", "createdAt": "2020-10-01T23:03:48Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -212,20 +234,34 @@ protected String getDataUriForPrefix(Configuration configuration, String dataSou\n         URI defaultFS = FileSystem.getDefaultUri(configuration);\n \n         String uri;\n-        String normalizedDataSource = normalizeDataSource(dataSource);\n+        String basePath = validateAndNormalizeBasePath(configuration.get(CONFIG_KEY_BASE_PATH));\n+        String normalizedDataSource = validateAndNormalizeDataSource(dataSource);\n \n         if (FILE_SCHEME.equals(defaultFS.getScheme())) {\n             // if the defaultFS is file://, but enum is not FILE, use enum scheme only\n-            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + basePath + normalizedDataSource;\n         } else {\n             // if the defaultFS is not file://, use it, instead of enum scheme and append user's path\n-            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + \"/\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + basePath + \"/\" + normalizedDataSource;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0"}, "originalPosition": 100}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7ba39e5d8260541031bab5148b5d800bce98f37a", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/7ba39e5d8260541031bab5148b5d800bce98f37a", "committedDate": "2020-10-01T23:21:39Z", "message": "Address PR feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "859fb8ee7dbf8c7ac1939af572def13c0d441104", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/859fb8ee7dbf8c7ac1939af572def13c0d441104", "committedDate": "2020-10-01T23:28:42Z", "message": "fix impersonation, address PR feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30a0ba4c50028c39d4ab712ad207a950a3999c13", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/30a0ba4c50028c39d4ab712ad207a950a3999c13", "committedDate": "2020-10-02T09:42:14Z", "message": "fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4d1f224a2f826d6fc9c0e50fe34ec138dcbd026", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/b4d1f224a2f826d6fc9c0e50fe34ec138dcbd026", "committedDate": "2020-10-02T10:51:19Z", "message": "More fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93aeb8b1f8a3f3892e990a7c0825fe802e5a44dd", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/93aeb8b1f8a3f3892e990a7c0825fe802e5a44dd", "committedDate": "2020-10-02T14:15:52Z", "message": "Skip workspace creation for write tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c2410e7c84f952d426e625d47f46a88cbf8f2b0", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/4c2410e7c84f952d426e625d47f46a88cbf8f2b0", "committedDate": "2020-10-02T14:42:01Z", "message": "Allow writes from all segments on the basepath"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8556d28a28e027aa499d6d08a963692296fe8811", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/8556d28a28e027aa499d6d08a963692296fe8811", "committedDate": "2020-10-02T15:48:30Z", "message": "More fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecaac700abc1ea0bf7d451d4f18f2b6d38f3b545", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/ecaac700abc1ea0bf7d451d4f18f2b6d38f3b545", "committedDate": "2020-10-02T17:14:42Z", "message": "Use 777 for mount points to see if write works correctly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9d59d78454e74580b66d7c2476791b095a5e5fa", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/d9d59d78454e74580b66d7c2476791b095a5e5fa", "committedDate": "2020-10-02T18:33:21Z", "message": "more fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "902ed7fe31c011b99d833d04c0085357979a291b", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/902ed7fe31c011b99d833d04c0085357979a291b", "committedDate": "2020-10-02T18:58:16Z", "message": "Another attempt at fixing permissions for write"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00ea5e305811d42942f79d6fc19011e26e6889f1", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/00ea5e305811d42942f79d6fc19011e26e6889f1", "committedDate": "2020-10-02T20:04:35Z", "message": "fix HdfsReadableTextTest.limit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "80786a007f3800b1ef86337dbe97913ace1d8216", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/80786a007f3800b1ef86337dbe97913ace1d8216", "committedDate": "2020-10-02T20:59:38Z", "message": "fix parquet tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24eea1cf571c3c53955a3375d7d3f26a016388b4", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/24eea1cf571c3c53955a3375d7d3f26a016388b4", "committedDate": "2020-10-02T21:44:01Z", "message": "match user and group id with VMs gpadmin user/group ids"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "641bf22560c29c17a286af26270cceb5686bb217", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/641bf22560c29c17a286af26270cceb5686bb217", "committedDate": "2020-10-02T22:01:00Z", "message": "Configure remote access to gpdb then change ids"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47761eb4f1df5255daf788c4c786b34b219085aa", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/47761eb4f1df5255daf788c4c786b34b219085aa", "committedDate": "2020-10-02T23:23:13Z", "message": "some reverts after write works"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5988e132fe9e94df70f4a65120fe354822351a81", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/5988e132fe9e94df70f4a65120fe354822351a81", "committedDate": "2020-10-02T23:23:17Z", "message": "Revert \"make SecureLogin.isUserImpersonationEnabled static\"\n\nThis reverts commit 2bcb9c0c432844abc1a2a2eb652fad5568e94e2c."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b496a1be45e64fc96f94b130773f3c89229b013", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/9b496a1be45e64fc96f94b130773f3c89229b013", "committedDate": "2020-10-03T11:40:02Z", "message": "Fix parquet write tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82f901fe8a11c9f3d073e0490612f0340a4c0036", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/82f901fe8a11c9f3d073e0490612f0340a4c0036", "committedDate": "2020-10-03T12:39:41Z", "message": "do not sleep for HDFS and FILE. Fixes to HdfsWritableTextTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bd7525c120775e07f35196c30f1a126f02daf57", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/2bd7525c120775e07f35196c30f1a126f02daf57", "committedDate": "2020-10-04T00:43:56Z", "message": "fix HdfsWritableTextTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/540d5e085b5db11cd1bc8a45ef60cc95bd245916", "committedDate": "2020-10-04T15:40:32Z", "message": "Fix HdfsWritableSequenceTest"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MDg1MjI3", "url": "https://github.com/greenplum-db/pxf/pull/456#pullrequestreview-504085227", "createdAt": "2020-10-07T16:57:52Z", "commit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "state": "APPROVED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1Nzo1MlrOHd84Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNzowMjo0NVrOHd9Ehw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODE5OA==", "bodyText": "ADL here is not needed, I guess copy & paste", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168198", "createdAt": "2020-10-07T16:57:52Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODMzNQ==", "bodyText": "same", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168335", "createdAt": "2020-10-07T16:58:03Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODYxNA==", "bodyText": "HDFS here not needed", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168614", "createdAt": "2020-10-07T16:58:27Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODgyMw==", "bodyText": "Google here", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168823", "createdAt": "2020-10-07T16:58:46Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non\n+            parallel) and slower than HdfsTextSimple.\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsFileFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.QuotedLineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:parquet</name>\n+        <description>A profile for reading and writing Parquet data from Google Cloud Storage", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE3MTMzNQ==", "bodyText": "just a thought whether pxf.fs.basePath is the best user-visible name we can come up with. Not sure whether pxf.file.base is any better. In reality, it seems this property will impact other profiles like hdfs, s3, etc (although it is not mandatory there)", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501171335", "createdAt": "2020-10-07T17:02:45Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "diffHunk": "@@ -35,4 +35,16 @@\n         </description>\n     </property>\n     -->\n+\n+    <!--\n+    <property>\n+        <name>pxf.fs.basePath</name>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 7}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d76a0b7c5cfd67fee0bf289247b18d6214b78aa", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/2d76a0b7c5cfd67fee0bf289247b18d6214b78aa", "committedDate": "2020-10-07T17:15:00Z", "message": "Address PR feedback"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4897, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}