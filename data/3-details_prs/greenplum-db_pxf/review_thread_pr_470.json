{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4NDM1MjQz", "number": 470, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOTowMzoyNlrOE8WvsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDo0NjoxOFrOFB-rxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxNzIyNjcyOnYy", "diffSide": "RIGHT", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOTowMzoyNlrOH4bX9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDowNDozOVrOH4dbPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDgwNA==", "bodyText": "Could we convert these to guard clauses with early returns?\nif (!(node instandeof OperatorNode)) {\n    return node\n}\n\nif (operatorNode.getOperator() != Operator.IN || !(operatorNode.getLeft() instanceof ColumnIndexOperationNode) || !(operatorNode.getRight() instanceof CollectionOperandNode) {\n    return node;\n}\nThat way the bulk of the method isn't doubly indented.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528930804", "createdAt": "2020-11-23T19:03:26Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+\n+import java.util.List;\n+\n+/**\n+ * Transforms IN operator into a chain of OR operators. This transformer is\n+ * useful for predicate builders that do not support the IN operator.\n+ */\n+public class InOperatorTransformer implements TreeVisitor {\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk2NDQxMw==", "bodyText": "I originally had the code like that, but then other implementations of TreeVisitor follow this pattern. So, I preferred to keep it like that for consistency across the code. I also prefer early returns.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528964413", "createdAt": "2020-11-23T20:04:39Z", "author": {"login": "frankgh"}, "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+\n+import java.util.List;\n+\n+/**\n+ * Transforms IN operator into a chain of OR operators. This transformer is\n+ * useful for predicate builders that do not support the IN operator.\n+ */\n+public class InOperatorTransformer implements TreeVisitor {\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDgwNA=="}, "originalCommit": {"oid": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxNzI1NTg3OnYy", "diffSide": "RIGHT", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOToxMDo1NFrOH4bonA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDowNjoyNFrOH4de0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzNTA2OA==", "bodyText": "is data.size() guaranteed to have size 2? If not, doesn't the transform tree look more like\n      (or)\n      /  \\\n   (or)  (eq)\n   /  \\\n(eq)  (eq)", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528935068", "createdAt": "2020-11-23T19:10:54Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+\n+import java.util.List;\n+\n+/**\n+ * Transforms IN operator into a chain of OR operators. This transformer is\n+ * useful for predicate builders that do not support the IN operator.\n+ */\n+public class InOperatorTransformer implements TreeVisitor {\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+\n+            if (operatorNode.getOperator() == Operator.IN\n+                    && operatorNode.getLeft() instanceof ColumnIndexOperandNode\n+                    && operatorNode.getRight() instanceof CollectionOperandNode) {\n+\n+                ColumnIndexOperandNode columnNode = (ColumnIndexOperandNode) operatorNode.getLeft();\n+                CollectionOperandNode collectionOperandNode = (CollectionOperandNode) operatorNode.getRight();\n+                List<String> data = collectionOperandNode.getData();\n+                DataType type = collectionOperandNode.getDataType().getTypeElem() != null\n+                        ? collectionOperandNode.getDataType().getTypeElem()\n+                        : collectionOperandNode.getDataType();\n+\n+                // Transform the IN operator into a chain of ORs", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk2NTMyOQ==", "bodyText": "data size can be at least 1 up to n. When the size is 1 then we just transform in to eq, otherwise we return a chain of ORs", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528965329", "createdAt": "2020-11-23T20:06:24Z", "author": {"login": "frankgh"}, "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/filter/InOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.api.filter;\n+\n+import org.greenplum.pxf.api.io.DataType;\n+\n+import java.util.List;\n+\n+/**\n+ * Transforms IN operator into a chain of OR operators. This transformer is\n+ * useful for predicate builders that do not support the IN operator.\n+ */\n+public class InOperatorTransformer implements TreeVisitor {\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+\n+            if (operatorNode.getOperator() == Operator.IN\n+                    && operatorNode.getLeft() instanceof ColumnIndexOperandNode\n+                    && operatorNode.getRight() instanceof CollectionOperandNode) {\n+\n+                ColumnIndexOperandNode columnNode = (ColumnIndexOperandNode) operatorNode.getLeft();\n+                CollectionOperandNode collectionOperandNode = (CollectionOperandNode) operatorNode.getRight();\n+                List<String> data = collectionOperandNode.getData();\n+                DataType type = collectionOperandNode.getDataType().getTypeElem() != null\n+                        ? collectionOperandNode.getDataType().getTypeElem()\n+                        : collectionOperandNode.getDataType();\n+\n+                // Transform the IN operator into a chain of ORs", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzNTA2OA=="}, "originalCommit": {"oid": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxNzI3MzQyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOToxNTo1N1rOH4bzKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOToxNTo1N1rOH4bzKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzNzc2OQ==", "bodyText": "Same comment as above about converting to guard clauses with early returns.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r528937769", "createdAt": "2020-11-23T19:15:57Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.plugins.hdfs.filter;\n+\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.ScalarOperandNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.Utilities;\n+\n+/**\n+ * Transforms non-logical operator nodes that have scalar operand nodes as its\n+ * children of BPCHAR type and which values have whitespace at the end of the\n+ * string.\n+ */\n+public class BPCharOperatorTransformer implements TreeVisitor {\n+    @Override\n+    public Node before(Node node, final int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5df3a3a3bcc87ab1faee77fb63a7c30616958a09"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NTg0MDcxOnYy", "diffSide": "RIGHT", "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/model/BasePlugin.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxOToyNDoxMlrOIA2ueg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxOToyNDoxMlrOIA2ueg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2NzU0Ng==", "bodyText": "speed is usually measured in units/units of time, like records/second, so you will need to divide the other way and update the unit label", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537767546", "createdAt": "2020-12-07T19:24:12Z", "author": {"login": "denalex"}, "path": "server/pxf-api/src/main/java/org/greenplum/pxf/api/model/BasePlugin.java", "diffHunk": "@@ -31,4 +33,27 @@ public void setRequestContext(RequestContext context) {\n     @Override\n     public void afterPropertiesSet() {\n     }\n+\n+    /**\n+     * When DEBUG mode is enabled, logs the total number of rows read, the\n+     * amount of time it took to read the file, and the average read speed\n+     * in nanoseconds\n+     *\n+     * @param totalRowsRead        the total number of rows read\n+     * @param totalReadTimeInNanos the total nanoseconds it took to read the file\n+     */\n+    protected void logReadStats(long totalRowsRead, long totalReadTimeInNanos) {\n+        if (LOG.isDebugEnabled()) {\n+            final long millis = TimeUnit.NANOSECONDS.toMillis(totalReadTimeInNanos);\n+            long average = totalRowsRead == 0 ? 0 : totalReadTimeInNanos / totalRowsRead;\n+            LOG.debug(\"{}-{}: Read TOTAL of {} rows from file {} on server {} in {} ms. Average speed: {} nanoseconds\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NTg0NjI3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/build.gradle", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxOToyNTozMFrOIA2xmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo1NzoxM1rOIA-mzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2ODM0Ng==", "bodyText": "do we really need to pin protobuf version here ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537768346", "createdAt": "2020-12-07T19:25:30Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/build.gradle", "diffHunk": "@@ -62,6 +62,13 @@ dependencies {\n     implementation(\"com.fasterxml.woodstox:woodstox-core:5.0.3\") { transitive = false }\n     implementation(\"org.codehaus.woodstox:stax2-api:3.1.4\")      { transitive = false }\n \n+    // ORC dependencies\n+    implementation(\"org.apache.orc:orc-core:${orcVersion}\")      { transitive = false }\n+    implementation(\"org.apache.orc:orc-shims:${orcVersion}\")     { transitive = false }\n+    implementation(\"org.apache.hive:hive-common:${hiveVersion}\") { transitive = false }\n+    implementation(\"com.google.protobuf:protobuf-java:2.5.0\")    { transitive = false }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5NjY1Mg==", "bodyText": "where do you suggest we pin it?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537896652", "createdAt": "2020-12-07T22:57:13Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/build.gradle", "diffHunk": "@@ -62,6 +62,13 @@ dependencies {\n     implementation(\"com.fasterxml.woodstox:woodstox-core:5.0.3\") { transitive = false }\n     implementation(\"org.codehaus.woodstox:stax2-api:3.1.4\")      { transitive = false }\n \n+    // ORC dependencies\n+    implementation(\"org.apache.orc:orc-core:${orcVersion}\")      { transitive = false }\n+    implementation(\"org.apache.orc:orc-shims:${orcVersion}\")     { transitive = false }\n+    implementation(\"org.apache.hive:hive-common:${hiveVersion}\") { transitive = false }\n+    implementation(\"com.google.protobuf:protobuf-java:2.5.0\")    { transitive = false }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2ODM0Ng=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NTg1MTAxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxOToyNjozOVrOIA20Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo1Mzo1M1rOIA-gJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2OTA1MA==", "bodyText": "ideally we should not be touching Parquet in ORC PR.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537769050", "createdAt": "2020-12-07T19:26:39Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -322,16 +315,17 @@ public void closeForWrite() throws IOException, InterruptedException {\n         List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n         ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n                 tupleDescription, originalFieldsMap);\n-        TreeVisitor pruner = new ParquetOperatorPrunerAndTransformer(\n+        TreeVisitor pruner = new ParquetOperatorPruner(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5NDk1MA==", "bodyText": "I will open a separate PR for the changes here", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537894950", "createdAt": "2020-12-07T22:53:53Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -322,16 +315,17 @@ public void closeForWrite() throws IOException, InterruptedException {\n         List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n         ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n                 tupleDescription, originalFieldsMap);\n-        TreeVisitor pruner = new ParquetOperatorPrunerAndTransformer(\n+        TreeVisitor pruner = new ParquetOperatorPruner(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc2OTA1MA=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NTg1ODkxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxOToyODoyMVrOIA24zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDowOTozNFrOIBeHKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc3MDE4OA==", "bodyText": "why adding BPCHAR transformer is needed here ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537770188", "createdAt": "2020-12-07T19:28:21Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -322,16 +315,17 @@ public void closeForWrite() throws IOException, InterruptedException {\n         List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n         ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n                 tupleDescription, originalFieldsMap);\n-        TreeVisitor pruner = new ParquetOperatorPrunerAndTransformer(\n+        TreeVisitor pruner = new ParquetOperatorPruner(\n                 tupleDescription, originalFieldsMap, SUPPORTED_OPERATORS);\n \n         try {\n             // Parse the filter string into a expression tree Node\n             Node root = new FilterParser().parse(filterString);\n-            // Prune the parsed tree with valid supported operators and then\n+            // Transform IN operators into a chain of ORs, then\n+            // prune the parsed tree with valid supported operators and then\n             // traverse the pruned tree with the ParquetRecordFilterBuilder to\n             // produce a record filter for parquet\n-            TRAVERSER.traverse(root, pruner, filterBuilder);\n+            TRAVERSER.traverse(root, IN_OPERATOR_TRANSFORMER, pruner, BPCHAR_TRANSFORMER, filterBuilder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQxMjg0MA==", "bodyText": "The logic in ParquetOperatorPrunerAndTransformer was split into ParquetOperatorPruner and BPCharOperatorTransformer. This effectively preserves the same logic as before. We need the BPCHAR transformer here because, depending on the implementation, the char fields in parquet files will either be padded to the width of the field or the second case is when the char field is trimmed. So for that reason we need BPCHAR_TRANSFORMER", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r538412840", "createdAt": "2020-12-08T14:09:34Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -322,16 +315,17 @@ public void closeForWrite() throws IOException, InterruptedException {\n         List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n         ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n                 tupleDescription, originalFieldsMap);\n-        TreeVisitor pruner = new ParquetOperatorPrunerAndTransformer(\n+        TreeVisitor pruner = new ParquetOperatorPruner(\n                 tupleDescription, originalFieldsMap, SUPPORTED_OPERATORS);\n \n         try {\n             // Parse the filter string into a expression tree Node\n             Node root = new FilterParser().parse(filterString);\n-            // Prune the parsed tree with valid supported operators and then\n+            // Transform IN operators into a chain of ORs, then\n+            // prune the parsed tree with valid supported operators and then\n             // traverse the pruned tree with the ParquetRecordFilterBuilder to\n             // produce a record filter for parquet\n-            TRAVERSER.traverse(root, pruner, filterBuilder);\n+            TRAVERSER.traverse(root, IN_OPERATOR_TRANSFORMER, pruner, BPCHAR_TRANSFORMER, filterBuilder);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc3MDE4OA=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjAyNDU3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDowNDozN1rOIA4aIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxODoxNDo1OVrOIBuohQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NTEwNQ==", "bodyText": "don't we need to do this multiple times if there are multiple blank space paddings ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537795105", "createdAt": "2020-12-07T20:04:37Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.plugins.hdfs.filter;\n+\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.ScalarOperandNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.Utilities;\n+\n+/**\n+ * Transforms non-logical operator nodes that have scalar operand nodes as its\n+ * children of BPCHAR type and which values have whitespace at the end of the\n+ * string.\n+ */\n+public class BPCharOperatorTransformer implements TreeVisitor {\n+    @Override\n+    public Node before(Node node, final int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()\n+                    && operatorNode.getLeft() instanceof ColumnIndexOperandNode\n+                    && operatorNode.getRight() instanceof ScalarOperandNode) {\n+                ScalarOperandNode scalarOperandNode = (ScalarOperandNode) operatorNode.getRight();\n+\n+                if (scalarOperandNode.getDataType() == DataType.BPCHAR) {\n+                    String value = scalarOperandNode.getValue();\n+\n+                    /* Determine whether the string has whitespace at the end */\n+                    if (value.length() > 0 && value.charAt(value.length() - 1) == ' ') {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY4MzUyNQ==", "bodyText": "I have addressed this in a separate PR.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r538683525", "createdAt": "2020-12-08T18:14:59Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/filter/BPCharOperatorTransformer.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package org.greenplum.pxf.plugins.hdfs.filter;\n+\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.ScalarOperandNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.Utilities;\n+\n+/**\n+ * Transforms non-logical operator nodes that have scalar operand nodes as its\n+ * children of BPCHAR type and which values have whitespace at the end of the\n+ * string.\n+ */\n+public class BPCharOperatorTransformer implements TreeVisitor {\n+    @Override\n+    public Node before(Node node, final int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()\n+                    && operatorNode.getLeft() instanceof ColumnIndexOperandNode\n+                    && operatorNode.getRight() instanceof ScalarOperandNode) {\n+                ScalarOperandNode scalarOperandNode = (ScalarOperandNode) operatorNode.getRight();\n+\n+                if (scalarOperandNode.getDataType() == DataType.BPCHAR) {\n+                    String value = scalarOperandNode.getValue();\n+\n+                    /* Determine whether the string has whitespace at the end */\n+                    if (value.length() > 0 && value.charAt(value.length() - 1) == ' ') {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NTEwNQ=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjAzMjg4OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDowNjo0MFrOIA4e1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoxMjo1MlrOIA9IPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NjMwOQ==", "bodyText": "so I wonder why HiveVectorizedORC is not using filters, just omission ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537796309", "createdAt": "2020-12-07T20:06:40Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3MjQ0NA==", "bodyText": "most likely, good question. I guess we can support it.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537872444", "createdAt": "2020-12-07T22:12:52Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NjMwOQ=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjA0ODIwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDoxMDowM1rOIA4nJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMzowMDozMFrOIA-s9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5ODQzOQ==", "bodyText": "not sure how valuable timekeeping is here, as the reader might not actually read the whole batch in memory in nextBatch() or will it ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537798439", "createdAt": "2020-12-07T20:10:03Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});\n+\n+        // Read the row data\n+        recordReader = fileReader.rows(options);\n+        batch = readSchema.createRowBatch();\n+        context.setMetadata(readSchema);\n+        return true;\n+    }\n+\n+    /**\n+     * Reads the next batch for the current fragment\n+     *\n+     * @return the next batch in OneRow format, the key is the batch number, and data is the batch\n+     * @throws IOException when reading of the next batch occurs\n+     */\n+    @Override\n+    public OneRow readNextObject() throws IOException {\n+        final Instant start = Instant.now();\n+        final boolean hasNextBatch = recordReader.nextBatch(batch);\n+        totalReadTimeInNanos += Duration.between(start, Instant.now()).toNanos();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5ODIzMQ==", "bodyText": "there are two cases, when the stripe needs to be read, and when the stripe is in memory. Depending on the case, timing will vary, but I think it's still worth keeping track. What would an alternative be for this?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537898231", "createdAt": "2020-12-07T23:00:30Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});\n+\n+        // Read the row data\n+        recordReader = fileReader.rows(options);\n+        batch = readSchema.createRowBatch();\n+        context.setMetadata(readSchema);\n+        return true;\n+    }\n+\n+    /**\n+     * Reads the next batch for the current fragment\n+     *\n+     * @return the next batch in OneRow format, the key is the batch number, and data is the batch\n+     * @throws IOException when reading of the next batch occurs\n+     */\n+    @Override\n+    public OneRow readNextObject() throws IOException {\n+        final Instant start = Instant.now();\n+        final boolean hasNextBatch = recordReader.nextBatch(batch);\n+        totalReadTimeInNanos += Duration.between(start, Instant.now()).toNanos();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5ODQzOQ=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjA2NjQxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDoxNDozMlrOIA4xpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoxODoxOFrOIA9UeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMTEyNQ==", "bodyText": "so, will we support the cases where GP has more columns than ORC and will fill with NULL and the other one where ORC has more columns and we will ignore them ? What about when diff ORC files have a bit different schemas ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537801125", "createdAt": "2020-12-07T20:14:32Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});\n+\n+        // Read the row data\n+        recordReader = fileReader.rows(options);\n+        batch = readSchema.createRowBatch();\n+        context.setMetadata(readSchema);\n+        return true;\n+    }\n+\n+    /**\n+     * Reads the next batch for the current fragment\n+     *\n+     * @return the next batch in OneRow format, the key is the batch number, and data is the batch\n+     * @throws IOException when reading of the next batch occurs\n+     */\n+    @Override\n+    public OneRow readNextObject() throws IOException {\n+        final Instant start = Instant.now();\n+        final boolean hasNextBatch = recordReader.nextBatch(batch);\n+        totalReadTimeInNanos += Duration.between(start, Instant.now()).toNanos();\n+        if (hasNextBatch) {\n+            totalRowsRead += batch.size;\n+            return new OneRow(new LongWritable(batchIndex++), batch);\n+        }\n+        return null; // all batches are exhausted\n+    }\n+\n+    @Override\n+    public void closeForRead() throws IOException {\n+        logReadStats(totalRowsRead, totalReadTimeInNanos);\n+        if (recordReader != null) {\n+            recordReader.close();\n+        }\n+        if (fileReader != null) {\n+            fileReader.close();\n+        }\n+    }\n+\n+    @Override\n+    public boolean openForWrite() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public boolean writeNextObject(OneRow onerow) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void closeForWrite() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    /**\n+     * Given a filter string, builds the SearchArgument object to perform\n+     * predicated pushdown for ORC\n+     *\n+     * @param filterString   the serialized filter string from the query predicate\n+     * @param originalSchema the original schema for the ORC file\n+     * @return null if filter string is null, the built SearchArgument otherwise\n+     * @throws IOException when a filter parsing error occurs\n+     */\n+    private SearchArgument getSearchArgument(String filterString, TypeDescription originalSchema) throws IOException {\n+        if (StringUtils.isBlank(filterString)) {\n+            return null;\n+        }\n+\n+        List<ColumnDescriptor> descriptors = columnDescriptors;\n+\n+        if (positionalAccess) {\n+            // We need to adjust the descriptors to match the column names\n+            // in the ORC schema to support predicate push down\n+            descriptors = new ArrayList<>();\n+            for (int i = 0; i < columnDescriptors.size() && i < originalSchema.getFieldNames().size(); i++) {\n+                ColumnDescriptor columnDescriptor = columnDescriptors.get(i);\n+                String columnName = originalSchema.getFieldNames().get(i);\n+                ColumnDescriptor copyDescriptor = new ColumnDescriptor(\n+                        columnName, // the name of the column in the ORC schema\n+                        columnDescriptor.columnTypeCode(),\n+                        columnDescriptor.columnIndex(),\n+                        columnDescriptor.columnTypeName(),\n+                        columnDescriptor.columnTypeModifiers(),\n+                        columnDescriptor.isProjected());\n+                descriptors.add(copyDescriptor);\n+            }\n+        }\n+\n+        SearchArgumentBuilder searchArgumentBuilder =\n+                new SearchArgumentBuilder(descriptors, configuration);\n+\n+        // Parse the filter string into a expression tree Node\n+        Node root = new FilterParser().parse(filterString);\n+        // Prune the parsed tree with valid supported operators and then\n+        // traverse the pruned tree with the searchArgumentBuilder to produce a\n+        // SearchArgument for ORC\n+        TRAVERSER.traverse(root, PRUNER, BPCHAR_TRANSFORMER, searchArgumentBuilder);\n+\n+        // Build the SearchArgument object\n+        return searchArgumentBuilder.getFilterBuilder().build();\n+    }\n+\n+    /**\n+     * Given the column descriptors that we receive from Greenplum, builds\n+     * the read schema that will perform column projection\n+     *\n+     * @param originalSchema the original schema for the ORC file\n+     * @return the read schema\n+     */\n+    private TypeDescription buildReadSchema(TypeDescription originalSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3NTU3Nw==", "bodyText": "we have tests where ORC have different schemas, so we support both GP has more columns than ORC and we fill with null: https://github.com/greenplum-db/pxf/pull/470/files#diff-f4945312d8b51ad817a8d7882795a2e32575ac346d2d8567a6dda9f4a4334a1dR74\nWe also support reading a subset of columns from an ORC file (column projection).", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537875577", "createdAt": "2020-12-07T22:18:18Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedAccessor.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.Reader;\n+import org.apache.orc.RecordReader;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.filter.BPCharOperatorTransformer;\n+import org.greenplum.pxf.plugins.hdfs.filter.SearchArgumentBuilder;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+public class ORCVectorizedAccessor extends BasePlugin implements Accessor {\n+\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS =\n+            EnumSet.of(\n+                    Operator.NOOP,\n+                    Operator.LESS_THAN,\n+                    Operator.GREATER_THAN,\n+                    Operator.LESS_THAN_OR_EQUAL,\n+                    Operator.GREATER_THAN_OR_EQUAL,\n+                    Operator.EQUALS,\n+                    Operator.NOT_EQUALS,\n+                    // Operator.LIKE,\n+                    Operator.IS_NULL,\n+                    Operator.IS_NOT_NULL,\n+                    Operator.IN,\n+                    Operator.AND,\n+                    Operator.OR,\n+                    Operator.NOT\n+            );\n+    private static final TreeVisitor PRUNER = new SupportedOperatorPruner(SUPPORTED_OPERATORS);\n+    private static final TreeVisitor BPCHAR_TRANSFORMER = new BPCharOperatorTransformer();\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    static final String MAP_BY_POSITION_OPTION = \"MAP_BY_POSITION\";\n+\n+    /**\n+     * True if the accessor accesses the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+    private int batchIndex;\n+    private long totalRowsRead;\n+    private long totalReadTimeInNanos;\n+    private Reader fileReader;\n+    private RecordReader recordReader;\n+    private VectorizedRowBatch batch;\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    @Override\n+    public boolean openForRead() throws IOException {\n+        Path file = new Path(context.getDataSource());\n+        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context.getDataSource(), context.getFragmentMetadata());\n+\n+        fileReader = OrcFile.createReader(file, OrcFile\n+                .readerOptions(configuration)\n+                .filesystem(file.getFileSystem(configuration)));\n+\n+        // The original schema from the file\n+        TypeDescription schema = fileReader.getSchema();\n+        // Add column projection to the Reader.Options\n+        TypeDescription readSchema = buildReadSchema(schema);\n+        // Get the record filter in case of predicate push-down\n+        SearchArgument searchArgument = getSearchArgument(context.getFilterString(), schema);\n+\n+        // Build the reader options\n+        Reader.Options options = fileReader\n+                .options()\n+                .schema(readSchema)\n+                .positionalEvolutionLevel(0)\n+                .range(fileSplit.getStart(), fileSplit.getLength())\n+                .searchArgument(searchArgument, new String[]{});\n+\n+        // Read the row data\n+        recordReader = fileReader.rows(options);\n+        batch = readSchema.createRowBatch();\n+        context.setMetadata(readSchema);\n+        return true;\n+    }\n+\n+    /**\n+     * Reads the next batch for the current fragment\n+     *\n+     * @return the next batch in OneRow format, the key is the batch number, and data is the batch\n+     * @throws IOException when reading of the next batch occurs\n+     */\n+    @Override\n+    public OneRow readNextObject() throws IOException {\n+        final Instant start = Instant.now();\n+        final boolean hasNextBatch = recordReader.nextBatch(batch);\n+        totalReadTimeInNanos += Duration.between(start, Instant.now()).toNanos();\n+        if (hasNextBatch) {\n+            totalRowsRead += batch.size;\n+            return new OneRow(new LongWritable(batchIndex++), batch);\n+        }\n+        return null; // all batches are exhausted\n+    }\n+\n+    @Override\n+    public void closeForRead() throws IOException {\n+        logReadStats(totalRowsRead, totalReadTimeInNanos);\n+        if (recordReader != null) {\n+            recordReader.close();\n+        }\n+        if (fileReader != null) {\n+            fileReader.close();\n+        }\n+    }\n+\n+    @Override\n+    public boolean openForWrite() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public boolean writeNextObject(OneRow onerow) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void closeForWrite() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    /**\n+     * Given a filter string, builds the SearchArgument object to perform\n+     * predicated pushdown for ORC\n+     *\n+     * @param filterString   the serialized filter string from the query predicate\n+     * @param originalSchema the original schema for the ORC file\n+     * @return null if filter string is null, the built SearchArgument otherwise\n+     * @throws IOException when a filter parsing error occurs\n+     */\n+    private SearchArgument getSearchArgument(String filterString, TypeDescription originalSchema) throws IOException {\n+        if (StringUtils.isBlank(filterString)) {\n+            return null;\n+        }\n+\n+        List<ColumnDescriptor> descriptors = columnDescriptors;\n+\n+        if (positionalAccess) {\n+            // We need to adjust the descriptors to match the column names\n+            // in the ORC schema to support predicate push down\n+            descriptors = new ArrayList<>();\n+            for (int i = 0; i < columnDescriptors.size() && i < originalSchema.getFieldNames().size(); i++) {\n+                ColumnDescriptor columnDescriptor = columnDescriptors.get(i);\n+                String columnName = originalSchema.getFieldNames().get(i);\n+                ColumnDescriptor copyDescriptor = new ColumnDescriptor(\n+                        columnName, // the name of the column in the ORC schema\n+                        columnDescriptor.columnTypeCode(),\n+                        columnDescriptor.columnIndex(),\n+                        columnDescriptor.columnTypeName(),\n+                        columnDescriptor.columnTypeModifiers(),\n+                        columnDescriptor.isProjected());\n+                descriptors.add(copyDescriptor);\n+            }\n+        }\n+\n+        SearchArgumentBuilder searchArgumentBuilder =\n+                new SearchArgumentBuilder(descriptors, configuration);\n+\n+        // Parse the filter string into a expression tree Node\n+        Node root = new FilterParser().parse(filterString);\n+        // Prune the parsed tree with valid supported operators and then\n+        // traverse the pruned tree with the searchArgumentBuilder to produce a\n+        // SearchArgument for ORC\n+        TRAVERSER.traverse(root, PRUNER, BPCHAR_TRANSFORMER, searchArgumentBuilder);\n+\n+        // Build the SearchArgument object\n+        return searchArgumentBuilder.getFilterBuilder().build();\n+    }\n+\n+    /**\n+     * Given the column descriptors that we receive from Greenplum, builds\n+     * the read schema that will perform column projection\n+     *\n+     * @param originalSchema the original schema for the ORC file\n+     * @return the read schema\n+     */\n+    private TypeDescription buildReadSchema(TypeDescription originalSchema) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMTEyNQ=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjA4OTAzOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDoxOTo1OVrOIA4-vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoyMToyNlrOIA9arw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNDQ3OA==", "bodyText": "repeating means all the values of this column in a batch are the same ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537804478", "createdAt": "2020-12-07T20:19:59Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3NzE2Nw==", "bodyText": "correct, this is from java docs\n\nTrue if same value repeats for whole column vector.\nIf so, vector[0] holds the repeating value.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537877167", "createdAt": "2020-12-07T22:21:26Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNDQ3OA=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjA5OTI3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDoyMjowMlrOIA5Ecw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoyMjoyM1rOIA9dMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNTkzOQ==", "bodyText": "the body of these methods is all the same, other than types, would generics work here: Mapper<T>, DoubleMapper extends Mapper<Double>, etc ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537805939", "createdAt": "2020-12-07T20:22:02Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Float value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? (float) dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] doubleMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Double value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3NzgwOA==", "bodyText": "I didn't want to address this here, I wanted to solve this at the refactor level, where we determine what type we need for the wire format.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537877808", "createdAt": "2020-12-07T22:22:23Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Float value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? (float) dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] doubleMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Double value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNTkzOQ=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjExMjc0OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDoyNToxNVrOIA5MEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoyNToxN1rOIA9j0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNzg5MA==", "bodyText": "we can pre-define OneField(oid, null) as singletons for a given oid and then use Arrays.fill to fill the array. That will save a lot of object creation on null values.", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537807890", "createdAt": "2020-12-07T20:25:15Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Float value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? (float) dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] doubleMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Double value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] textMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        BytesColumnVector bcv = (BytesColumnVector) columnVector;\n+        if (bcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = bcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        String value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+\n+            value = bcv.noNulls || !bcv.isNull[rowId] ?\n+                    new String(bcv.vector[rowIndex], bcv.start[rowIndex],\n+                            bcv.length[rowIndex], StandardCharsets.UTF_8) : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] decimalMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        DecimalColumnVector dcv = (DecimalColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        HiveDecimalWritable value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] binaryMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        BytesColumnVector bcv = (BytesColumnVector) columnVector;\n+        if (bcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = bcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        byte[] value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            if (bcv.noNulls || !bcv.isNull[rowId]) {\n+                value = new byte[bcv.length[rowId]];\n+                System.arraycopy(bcv.vector[rowId], bcv.start[rowId], value, 0, bcv.length[rowId]);\n+            } else {\n+                value = null;\n+            }\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    // DateWritable is no longer deprecated in newer versions of storage api \u00af\\_(\u30c4)_/\u00af\n+    @SuppressWarnings(\"deprecation\")\n+    public static OneField[] dateMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Date value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? Date.valueOf(LocalDate.ofEpochDay(lcv.vector[rowIndex]))\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] timestampMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        TimestampColumnVector tcv = (TimestampColumnVector) columnVector;\n+        if (tcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = tcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        String value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (tcv.noNulls || !tcv.isNull[rowId])\n+                    ? timestampToString(tcv.asScratchTimestamp(rowId))\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] getNullResultSet(int oid, int size) {\n+        OneField[] result = new OneField[size];\n+        for (int i = 0; i < result.length; i++)\n+            result[i] = new OneField(oid, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 267}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3OTUwNA==", "bodyText": "good idea!", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537879504", "createdAt": "2020-12-07T22:25:17Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedMappingFunctions.java", "diffHunk": "@@ -0,0 +1,286 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.greenplum.pxf.api.GreenplumDateTime;\n+import org.greenplum.pxf.api.OneField;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.ZoneId;\n+\n+/**\n+ * Maps vectors of ORC types to a list of OneFields.\n+ * ORC provides a rich set of scalar and compound types. The mapping is as\n+ * follows\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Integer          |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Integer          |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Integer          |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Integer          |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Integer          |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Integer          |  date              |  DATE          |  1082         |\n+ * |  Floating point   |  float             |  REAL          |  700          |\n+ * |  Floating point   |  double            |  FLOAT8        |  701          |\n+ * |  String           |  string            |  TEXT          |  25           |\n+ * |  String           |  char              |  BPCHAR        |  1042         |\n+ * |  String           |  varchar           |  VARCHAR       |  1043         |\n+ * |  Byte[]           |  binary            |  BYTEA         |  17           |\n+ * ---------------------------------------------------------------------------\n+ */\n+class ORCVectorizedMappingFunctions {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ORCVectorizedMappingFunctions.class);\n+\n+    public static OneField[] booleanMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Boolean value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId] == 1\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] shortMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Short value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (short) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] integerMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Integer value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? (int) lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] longMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Long value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? lcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] floatMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Float value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? (float) dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] doubleMapper(VectorizedRowBatch batch, ColumnVector columnVector, int oid) {\n+        DoubleColumnVector dcv = (DoubleColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Double value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] textMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        BytesColumnVector bcv = (BytesColumnVector) columnVector;\n+        if (bcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = bcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        String value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+\n+            value = bcv.noNulls || !bcv.isNull[rowId] ?\n+                    new String(bcv.vector[rowIndex], bcv.start[rowIndex],\n+                            bcv.length[rowIndex], StandardCharsets.UTF_8) : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] decimalMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        DecimalColumnVector dcv = (DecimalColumnVector) columnVector;\n+        if (dcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = dcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        HiveDecimalWritable value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (dcv.noNulls || !dcv.isNull[rowId])\n+                    ? dcv.vector[rowId]\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] binaryMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        BytesColumnVector bcv = (BytesColumnVector) columnVector;\n+        if (bcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = bcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        byte[] value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            if (bcv.noNulls || !bcv.isNull[rowId]) {\n+                value = new byte[bcv.length[rowId]];\n+                System.arraycopy(bcv.vector[rowId], bcv.start[rowId], value, 0, bcv.length[rowId]);\n+            } else {\n+                value = null;\n+            }\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    // DateWritable is no longer deprecated in newer versions of storage api \u00af\\_(\u30c4)_/\u00af\n+    @SuppressWarnings(\"deprecation\")\n+    public static OneField[] dateMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        LongColumnVector lcv = (LongColumnVector) columnVector;\n+        if (lcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = lcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        Date value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (lcv.noNulls || !lcv.isNull[rowId])\n+                    ? Date.valueOf(LocalDate.ofEpochDay(lcv.vector[rowIndex]))\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] timestampMapper(VectorizedRowBatch batch, ColumnVector columnVector, Integer oid) {\n+        TimestampColumnVector tcv = (TimestampColumnVector) columnVector;\n+        if (tcv == null)\n+            return getNullResultSet(oid, batch.size);\n+\n+        OneField[] result = new OneField[batch.size];\n+        int m = tcv.isRepeating ? 0 : 1;\n+        int rowId;\n+        String value;\n+        for (int rowIndex = 0; rowIndex < batch.size; rowIndex++) {\n+            rowId = m * rowIndex;\n+            value = (tcv.noNulls || !tcv.isNull[rowId])\n+                    ? timestampToString(tcv.asScratchTimestamp(rowId))\n+                    : null;\n+            result[rowIndex] = new OneField(oid, value);\n+        }\n+        return result;\n+    }\n+\n+    public static OneField[] getNullResultSet(int oid, int size) {\n+        OneField[] result = new OneField[size];\n+        for (int i = 0; i < result.length; i++)\n+            result[i] = new OneField(oid, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwNzg5MA=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 267}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjEzNzk3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDozMToxMFrOIA5aRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo0ODozNlrOIA-V-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMTUyNg==", "bodyText": "wonder if we can reuse these between batches", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537811526", "createdAt": "2020-12-07T20:31:10Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5MjM0NQ==", "bodyText": "good point", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537892345", "createdAt": "2020-12-07T22:48:36Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMTUyNg=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjE0ODI4OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDozMzo0MFrOIA5gJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDozMzo0MFrOIA5gJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMzAzMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        for (int i = 0; i < batchSize; i++) {\n          \n          \n            \n                        for (int row = 0; row < batchSize; row++) {", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537813030", "createdAt": "2020-12-07T20:33:40Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));\n+        }\n+\n+        // index to the projected columns\n+        int columnIndex = 0;\n+        OneField[] oneFields;\n+        for (ColumnDescriptor columnDescriptor : columnDescriptors) {\n+            if (!columnDescriptor.isProjected()) {\n+                oneFields = ORCVectorizedMappingFunctions\n+                        .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+            } else {\n+                TypeDescription orcColumn = positionalAccess\n+                        ? columnIndex < readSchema.getChildren().size() ? readSchema.getChildren().get(columnIndex) : null\n+                        : readFields.get(columnDescriptor.columnName());\n+                if (orcColumn == null) {\n+                    // this column is missing in the underlying ORC file, but\n+                    // it is defined in the Greenplum table. This can happen\n+                    // when a schema evolves, for example the original\n+                    // ORC-backed table had 4 columns, and at a later point in\n+                    // time a fifth column was added. Files written before the\n+                    // column was added will have 4 columns, and new files\n+                    // will have 5 columns\n+                    oneFields = ORCVectorizedMappingFunctions\n+                            .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+                } else if (orcColumn.getCategory().isPrimitive()) {\n+                    oneFields = functions[columnIndex]\n+                            .apply(vectorizedBatch, vectorizedBatch.cols[columnIndex], typeOidMappings[columnIndex]);\n+                    columnIndex++;\n+                } else {\n+                    throw new UnsupportedTypeException(\n+                            String.format(\"Unable to resolve column '%s' with category '%s'. Only primitive types are supported.\",\n+                                    readSchema.getFieldNames().get(columnIndex), orcColumn.getCategory()));\n+                }\n+            }\n+\n+            // oneFields is the array of fields for the current column we are\n+            // processing. We need to add it to the corresponding list\n+            for (int i = 0; i < batchSize; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 163}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjE2MDQxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDozNjozOVrOIA5nPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDozNjozOVrOIA5nPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNDg0Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            resolvedBatch.get(i).add(oneFields[i]);\n          \n          \n            \n                            resolvedBatch.get(row).add(oneFieldsForRow[row]);", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537814847", "createdAt": "2020-12-07T20:36:39Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/orc/ORCVectorizedResolver.java", "diffHunk": "@@ -0,0 +1,272 @@\n+package org.greenplum.pxf.plugins.hdfs.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.ReadVectorizedResolver;\n+import org.greenplum.pxf.api.error.PxfRuntimeException;\n+import org.greenplum.pxf.api.error.UnsupportedTypeException;\n+import org.greenplum.pxf.api.function.TriFunction;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+\n+import static org.greenplum.pxf.api.io.DataType.BIGINT;\n+import static org.greenplum.pxf.api.io.DataType.BOOLEAN;\n+import static org.greenplum.pxf.api.io.DataType.BPCHAR;\n+import static org.greenplum.pxf.api.io.DataType.BYTEA;\n+import static org.greenplum.pxf.api.io.DataType.DATE;\n+import static org.greenplum.pxf.api.io.DataType.FLOAT8;\n+import static org.greenplum.pxf.api.io.DataType.INTEGER;\n+import static org.greenplum.pxf.api.io.DataType.NUMERIC;\n+import static org.greenplum.pxf.api.io.DataType.REAL;\n+import static org.greenplum.pxf.api.io.DataType.SMALLINT;\n+import static org.greenplum.pxf.api.io.DataType.TEXT;\n+import static org.greenplum.pxf.api.io.DataType.TIMESTAMP;\n+import static org.greenplum.pxf.api.io.DataType.VARCHAR;\n+import static org.greenplum.pxf.plugins.hdfs.orc.ORCVectorizedAccessor.MAP_BY_POSITION_OPTION;\n+\n+/**\n+ * Resolves ORC VectorizedRowBatch into lists of List<OneField>. Only primitive\n+ * types are supported. Currently, Timestamp and Timestamp with TimeZone are\n+ * not supported. The supported mapping is as follows:\n+ * <p>\n+ * ---------------------------------------------------------------------------\n+ * | ORC Physical Type | ORC Logical Type   | Greenplum Type | Greenplum OID |\n+ * ---------------------------------------------------------------------------\n+ * |  Long             |  boolean  (1 bit)  |  BOOLEAN       |  16           |\n+ * |  Long             |  tinyint  (8 bit)  |  SMALLINT      |  21           |\n+ * |  Long             |  smallint (16 bit) |  SMALLINT      |  21           |\n+ * |  Long             |  int      (32 bit) |  INTEGER       |  23           |\n+ * |  Long             |  bigint   (64 bit) |  BIGINT        |  20           |\n+ * |  Double           |  float             |  REAL          |  700          |\n+ * |  Double           |  double            |  FLOAT8        |  701          |\n+ * |  byte[]           |  string            |  TEXT          |  25           |\n+ * |  byte[]           |  char              |  BPCHAR        |  1042         |\n+ * |  byte[]           |  varchar           |  VARCHAR       |  1043         |\n+ * |  byte[]           |  binary            |  BYTEA         |  17           |\n+ * |  Long             |  date              |  DATE          |  1082         |\n+ * |  binary           |  decimal           |  NUMERIC       |  1700         |\n+ * |  binary           |  timestamp         |  TIMESTAMP     |  1114         |\n+ * ---------------------------------------------------------------------------\n+ */\n+public class ORCVectorizedResolver extends BasePlugin implements ReadVectorizedResolver, Resolver {\n+\n+    /**\n+     * The schema used to read the ORC file.\n+     */\n+    private TypeDescription readSchema;\n+\n+    /**\n+     * An array of functions that resolve ColumnVectors into Lists of OneFields\n+     * The array has the same size as the readSchema, and the functions depend\n+     * on the type of the elements in the schema.\n+     */\n+    private TriFunction<VectorizedRowBatch, ColumnVector, Integer, OneField[]>[] functions;\n+\n+    /**\n+     * An array of types that map from the readSchema types to Greenplum OIDs.\n+     */\n+    private int[] typeOidMappings;\n+\n+    private Map<String, TypeDescription> readFields;\n+\n+    /**\n+     * True if the resolver resolves the columns defined in the\n+     * ORC file in the same order they were defined in the Greenplum table,\n+     * otherwise the columns are matches by name. (Defaults to false)\n+     */\n+    private boolean positionalAccess;\n+\n+    /**\n+     * A local copy of the column descriptors coming from the RequestContext.\n+     * We make this variable local to improve performance while accessing the\n+     * descriptors.\n+     */\n+    private List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * {@inheritDoc}\n+     */\n+    @Override\n+    public void afterPropertiesSet() {\n+        super.afterPropertiesSet();\n+        columnDescriptors = context.getTupleDescription();\n+        positionalAccess = context.getOption(MAP_BY_POSITION_OPTION, false);\n+    }\n+\n+    /**\n+     * Returns the resolved list of list of OneFields given a\n+     * VectorizedRowBatch\n+     *\n+     * @param batch unresolved batch\n+     * @return the resolved batch mapped to the Greenplum type\n+     */\n+    @Override\n+    public List<List<OneField>> getFieldsForBatch(OneRow batch) {\n+        ensureFunctionsAreInitialized();\n+        VectorizedRowBatch vectorizedBatch = (VectorizedRowBatch) batch.getData();\n+        int batchSize = vectorizedBatch.size;\n+\n+        // The resolved batch returns a list of the list of OneField that\n+        // matches the size of the batch. Every internal list, has a list of\n+        // OneFields with size the number of columns\n+        List<List<OneField>> resolvedBatch = new ArrayList<>(batchSize);\n+\n+        // Initialize the internal lists\n+        for (int i = 0; i < batchSize; i++) {\n+            resolvedBatch.add(new ArrayList<>(columnDescriptors.size()));\n+        }\n+\n+        // index to the projected columns\n+        int columnIndex = 0;\n+        OneField[] oneFields;\n+        for (ColumnDescriptor columnDescriptor : columnDescriptors) {\n+            if (!columnDescriptor.isProjected()) {\n+                oneFields = ORCVectorizedMappingFunctions\n+                        .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+            } else {\n+                TypeDescription orcColumn = positionalAccess\n+                        ? columnIndex < readSchema.getChildren().size() ? readSchema.getChildren().get(columnIndex) : null\n+                        : readFields.get(columnDescriptor.columnName());\n+                if (orcColumn == null) {\n+                    // this column is missing in the underlying ORC file, but\n+                    // it is defined in the Greenplum table. This can happen\n+                    // when a schema evolves, for example the original\n+                    // ORC-backed table had 4 columns, and at a later point in\n+                    // time a fifth column was added. Files written before the\n+                    // column was added will have 4 columns, and new files\n+                    // will have 5 columns\n+                    oneFields = ORCVectorizedMappingFunctions\n+                            .getNullResultSet(columnDescriptor.columnTypeCode(), batchSize);\n+                } else if (orcColumn.getCategory().isPrimitive()) {\n+                    oneFields = functions[columnIndex]\n+                            .apply(vectorizedBatch, vectorizedBatch.cols[columnIndex], typeOidMappings[columnIndex]);\n+                    columnIndex++;\n+                } else {\n+                    throw new UnsupportedTypeException(\n+                            String.format(\"Unable to resolve column '%s' with category '%s'. Only primitive types are supported.\",\n+                                    readSchema.getFieldNames().get(columnIndex), orcColumn.getCategory()));\n+                }\n+            }\n+\n+            // oneFields is the array of fields for the current column we are\n+            // processing. We need to add it to the corresponding list\n+            for (int i = 0; i < batchSize; i++) {\n+                resolvedBatch.get(i).add(oneFields[i]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjE4NzgyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/apache/hadoop/hive/ql/io/orc/PxfRecordReaderImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDo0MzozMVrOIA52kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo1MzoyMlrOIA-fPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxODc3MQ==", "bodyText": "will this be fixed in ORC, is there a JIRA to refer to ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537818771", "createdAt": "2020-12-07T20:43:31Z", "author": {"login": "denalex"}, "path": "server/pxf-hive/src/main/java/org/apache/hadoop/hive/ql/io/orc/PxfRecordReaderImpl.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DateColumnVector;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+\n+import java.io.IOException;\n+\n+/**\n+ * This class fixes an issue introduced in ORC core library version 1.5.9", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5NDcxOA==", "bodyText": "as the time of this fix, there's no jira. But I assume that this will be addressed at some point since newer ORC libraries break Hive functionality", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537894718", "createdAt": "2020-12-07T22:53:22Z", "author": {"login": "frankgh"}, "path": "server/pxf-hive/src/main/java/org/apache/hadoop/hive/ql/io/orc/PxfRecordReaderImpl.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.hadoop.hive.ql.io.orc;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DateColumnVector;\n+import org.apache.orc.Reader;\n+import org.apache.orc.TypeDescription;\n+\n+import java.io.IOException;\n+\n+/**\n+ * This class fixes an issue introduced in ORC core library version 1.5.9", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxODc3MQ=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjE5OTA5OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/templates/conf/pxf-log4j2.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMDo0NjoxOFrOIA59LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoxNTowNFrOIA9Naw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgyMDQ2MA==", "bodyText": "some excessive WARN messages during normal use ?", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537820460", "createdAt": "2020-12-07T20:46:18Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/templates/conf/pxf-log4j2.xml", "diffHunk": "@@ -34,6 +34,7 @@\n     <Logger name=\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase\" level=\"warn\"/>\n     <Logger name=\"org.apache.hadoop\" level=\"warn\"/>\n     <Logger name=\"org.apache.parquet\" level=\"warn\"/>\n+    <Logger name=\"org.apache.orc\" level=\"error\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3Mzc3MQ==", "bodyText": "yes, I had to quiet down the log messages, otherwise my log was getting filled up with orc warn messages", "url": "https://github.com/greenplum-db/pxf/pull/470#discussion_r537873771", "createdAt": "2020-12-07T22:15:04Z", "author": {"login": "frankgh"}, "path": "server/pxf-service/src/templates/conf/pxf-log4j2.xml", "diffHunk": "@@ -34,6 +34,7 @@\n     <Logger name=\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase\" level=\"warn\"/>\n     <Logger name=\"org.apache.hadoop\" level=\"warn\"/>\n     <Logger name=\"org.apache.parquet\" level=\"warn\"/>\n+    <Logger name=\"org.apache.orc\" level=\"error\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgyMDQ2MA=="}, "originalCommit": {"oid": "39587e272832b9d1c32a657a3173b14612a355ce"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3501, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}