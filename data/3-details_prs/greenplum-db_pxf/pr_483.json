{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2MzgxNDU3", "number": 483, "title": "docs - Hive profile supports column projection and pushdown", "bodyText": "misc doc updates for the recent Hive* work.  in this PR:\n\nadd a \"choosing the profile\" topic to main hdfs page that mentions which data types have more than one profile, and some criteria for choosing\nadd info about pxf.ppd.hive property in pxf-site.xml\nadd column projection info\nadd filter pushdown info - I AM NOT SURE IF I AM REPRESENTING THIS CORRECTLY IN THE TABLE\nhive topic - discuss new PPD option, remove limitation.\n\nnote:  wanted to get this part out for review; i am still determining if other edits are required to the hive topic.\ndoc review site links:\n\nchoosing the profile - http://docs-lisa-hive-colppd.cfapps.io/pxf/5-16/using/access_hdfs.html#choose_profile\npxf.ppd.hive property description - http://docs-lisa-hive-colppd.cfapps.io/pxf/5-16/using/cfg_server.html#pxf-site\nfilter pushdown - http://docs-lisa-hive-colppd.cfapps.io/pxf/5-16/using/filter_push.html\nhive topic PPD - http://docs-lisa-hive-colppd.cfapps.io/pxf/5-16/using/hive_pxf.html#hive_queryextdata", "createdAt": "2020-11-05T22:38:29Z", "url": "https://github.com/greenplum-db/pxf/pull/483", "merged": true, "mergeCommit": {"oid": "bb71f6d015382e2b1a85b930a3bda529bba77a41"}, "closed": true, "closedAt": "2020-11-06T18:18:52Z", "author": {"login": "lisakowen"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZplHBgH2gAyNTE2MzgxNDU3OjYxMzFhY2FjY2IyZGMwYTEwOTE3MzcwYTI5ZGY0YzNmZmI1NmMwNzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdZ7BA_gH2gAyNTE2MzgxNDU3OjEwOTBlMmI4Y2NjODQyZDU1ZmQ5NDRhODU4MTU5N2IzYjNkMjIxNDA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6131acaccb2dc0a10917370a29df4c3ffb56c077", "author": {"user": {"login": "lisakowen", "name": "Lisa Owen"}}, "url": "https://github.com/greenplum-db/pxf/commit/6131acaccb2dc0a10917370a29df4c3ffb56c077", "committedDate": "2020-11-05T21:51:59Z", "message": "docs - Hive profile supports column projection and pushdown"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0NzQ0Mzc4", "url": "https://github.com/greenplum-db/pxf/pull/483#pullrequestreview-524744378", "createdAt": "2020-11-05T23:11:13Z", "commit": {"oid": "6131acaccb2dc0a10917370a29df4c3ffb56c077"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMzoxMToxM1rOHuaNvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMzoxNTo1MlrOHuaVCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNjA0Nw==", "bodyText": "I think line should be merged with line 59,  but let's confirm with Alex.", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518426047", "createdAt": "2020-11-05T23:11:13Z", "author": {"login": "frankgh"}, "path": "docs/content/filter_push.html.md.erb", "diffHunk": "@@ -57,8 +57,10 @@ PXF accesses data sources using profiles exposed by different connectors, and fi\n | s3:parquet and s3:text with S3-Select | Y |  N | Y | Y | Y | Y | Y |\n | HBase | Y | N | Y | N | Y | Y | N |\n | Hive | Y<sup>2</sup> | N | N | N | Y<sup>2</sup> | Y<sup>2</sup> | N |\n+| Hive (accessing stored as Parquet) | Y, Y<sup>2</sup> | N | N | Y | Y, Y<sup>2</sup> | Y, Y<sup>2</sup> | Y |\n+| Hive (accessing stored as RCFile or ORC) | Y, Y<sup>2</sup> | N | Y | Y | Y, Y<sup>2</sup> | Y, Y<sup>2</sup> | Y |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6131acaccb2dc0a10917370a29df4c3ffb56c077"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNzkxMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - The data resides in a Hive table, and you do not know the type of data up front.\n          \n          \n            \n            - The data resides in a Hive table, and you do not know the underlying file type of the table up front.", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518427913", "createdAt": "2020-11-05T23:15:52Z", "author": {"login": "frankgh"}, "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -119,7 +119,25 @@ The PXF Hadoop connectors expose the following profiles to read, and in many cas\n | [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n | [HBase](hbase_pxf.html) | Any | HBase | n/a |\n \n-You provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file, directory, or table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text`:\n+\n+### <a id=\"choose_profile\"></a>Choosing the Profile\n+\n+PXF provides more than one profile to access text and Parquet data on Hadoop. Here are some things to consider as you determine which profile to choose.\n+\n+Choose the `Hive` profile when:\n+\n+- The data resides in a Hive table, and you do not know the type of data up front.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6131acaccb2dc0a10917370a29df4c3ffb56c077"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2", "author": {"user": {"login": "lisakowen", "name": "Lisa Owen"}}, "url": "https://github.com/greenplum-db/pxf/commit/c9b07319da168e25cb1fa2722c664ea28e0417e2", "committedDate": "2020-11-05T23:36:48Z", "message": "underlying file type of the table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0Nzg4Mzc5", "url": "https://github.com/greenplum-db/pxf/pull/483#pullrequestreview-524788379", "createdAt": "2020-11-06T01:06:14Z", "commit": {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNjoxNFrOHucgKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMToxNjoxNVrOHucrzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzUzMQ==", "bodyText": "also for tables stored as text files", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518463531", "createdAt": "2020-11-06T01:06:14Z", "author": {"login": "denalex"}, "path": "docs/content/col_project.html.md.erb", "diffHunk": "@@ -11,7 +11,7 @@ Column projection is automatically enabled for the `pxf` external table protocol\n | Data Source | Connector | Profile(s) |\n |-------------|---------------|---------|\n | External SQL database | JDBC Connector | Jdbc |\n-| Hive | Hive Connector | Hive, HiveRC, HiveORC, HiveVectorizedORC |\n+| Hive | Hive Connector | Hive (accessing tables stored as Parquet, RCFile, and ORC), HiveRC, HiveORC, HiveVectorizedORC |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NjUxMA==", "bodyText": "this is tricky -- Hive itself does not make sense, the pushdown is different based on the file storage format for a table partition, each partition can have different pushdown eve when accessed by the same Hive profile. I'd remove Hive line altogether. Also HiveText does not support any pushdown (other than partition pruning). And I think VectorizedORC does not support pushdown at all.", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518466510", "createdAt": "2020-11-06T01:16:15Z", "author": {"login": "denalex"}, "path": "docs/content/filter_push.html.md.erb", "diffHunk": "@@ -57,8 +57,10 @@ PXF accesses data sources using profiles exposed by different connectors, and fi\n | s3:parquet and s3:text with S3-Select | Y |  N | Y | Y | Y | Y | Y |\n | HBase | Y | N | Y | N | Y | Y | N |\n | Hive | Y<sup>2</sup> | N | N | N | Y<sup>2</sup> | Y<sup>2</sup> | N |\n+| Hive (accessing stored as Parquet) | Y, Y<sup>2</sup> | N | N | Y | Y, Y<sup>2</sup> | Y, Y<sup>2</sup> | Y |\n+| Hive (accessing stored as RCFile or ORC) | Y, Y<sup>2</sup> | N | Y | Y | Y, Y<sup>2</sup> | Y, Y<sup>2</sup> | Y |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQyNjA0Nw=="}, "originalCommit": {"oid": "6131acaccb2dc0a10917370a29df4c3ffb56c077"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MjQ5OTE5", "url": "https://github.com/greenplum-db/pxf/pull/483#pullrequestreview-525249919", "createdAt": "2020-11-06T15:20:15Z", "commit": {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNToyMDoxNVrOHuyQbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNToyMjoyOFrOHuyWiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgxOTk0OA==", "bodyText": "Mixing both of these in the intro sentences was a little confusing to me.  Doesn't the first bullet always go with hdfs:text and the second one with hdfss:parquet?  If so I'd just make each a separate paragraph \"Choose the hdfs:text profile when you know the file type and you know the location of the file in the HDFS file system.\" \"Choose the hdfs:parquet profile when the file e is Parquet, you know ..., \"", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518819948", "createdAt": "2020-11-06T15:20:15Z", "author": {"login": "dyozie"}, "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -119,7 +119,25 @@ The PXF Hadoop connectors expose the following profiles to read, and in many cas\n | [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n | [HBase](hbase_pxf.html) | Any | HBase | n/a |\n \n-You provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file, directory, or table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text`:\n+\n+### <a id=\"choose_profile\"></a>Choosing the Profile\n+\n+PXF provides more than one profile to access text and Parquet data on Hadoop. Here are some things to consider as you determine which profile to choose.\n+\n+Choose the `Hive` profile when:\n+\n+- The data resides in a Hive table, and you do not know the underlying file type of the table up front.\n+- The data resides in a Hive table, and the Hive table is partitioned.\n+\n+Choose the `hdfs:text` or `hdfs:parquet` profile when:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgyMTUxMg==", "bodyText": "on a -> in a\n, or HBase or Hive table -> , HBase table, or Hive table", "url": "https://github.com/greenplum-db/pxf/pull/483#discussion_r518821512", "createdAt": "2020-11-06T15:22:28Z", "author": {"login": "dyozie"}, "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -119,7 +119,25 @@ The PXF Hadoop connectors expose the following profiles to read, and in many cas\n | [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n | [HBase](hbase_pxf.html) | Any | HBase | n/a |\n \n-You provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file, directory, or table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text`:\n+\n+### <a id=\"choose_profile\"></a>Choosing the Profile\n+\n+PXF provides more than one profile to access text and Parquet data on Hadoop. Here are some things to consider as you determine which profile to choose.\n+\n+Choose the `Hive` profile when:\n+\n+- The data resides in a Hive table, and you do not know the underlying file type of the table up front.\n+- The data resides in a Hive table, and the Hive table is partitioned.\n+\n+Choose the `hdfs:text` or `hdfs:parquet` profile when:\n+\n+- You know the file type, and you know the location of the file in the HDFS file system.\n+- The file is Parquet, you know the location of the file in the HDFS file system, and you want to take advantage of extended filter pushdown support for additional data types and operators.\n+\n+\n+### <a id=\"specify_profile\"></a>Specifying the Profile\n+\n+You must provide the profile name when you specify the `pxf` protocol on a `CREATE EXTERNAL TABLE` command to create a Greenplum Database external table that references a Hadoop file or directory, or HBase or Hive table. For example, the following command creates an external table that uses the default server and specifies the profile named `hdfs:text` to access the HDFS file `/data/pxf_examples/pxf_hdfs_simple.txt`:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9b07319da168e25cb1fa2722c664ea28e0417e2"}, "originalPosition": 23}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3bcd7996b6312f782c9691b0118bd0628a894bef", "author": {"user": {"login": "lisakowen", "name": "Lisa Owen"}}, "url": "https://github.com/greenplum-db/pxf/commit/3bcd7996b6312f782c9691b0118bd0628a894bef", "committedDate": "2020-11-06T17:50:04Z", "message": "Hive profile accessing text supports column projection"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5a1d283b442704dd7f63e9456615b76ad979233", "author": {"user": {"login": "lisakowen", "name": "Lisa Owen"}}, "url": "https://github.com/greenplum-db/pxf/commit/c5a1d283b442704dd7f63e9456615b76ad979233", "committedDate": "2020-11-06T17:54:47Z", "message": "edits requested by david"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e15a53914fc23b5dce5bcaa1c19db2a2e833ce10", "author": {"user": {"login": "lisakowen", "name": "Lisa Owen"}}, "url": "https://github.com/greenplum-db/pxf/commit/e15a53914fc23b5dce5bcaa1c19db2a2e833ce10", "committedDate": "2020-11-06T18:08:20Z", "message": "(hopefully) correctly address comments on filter pushdown page"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1090e2b8ccc842d55fd944a8581597b3b3d22140", "author": {"user": {"login": "lisakowen", "name": "Lisa Owen"}}, "url": "https://github.com/greenplum-db/pxf/commit/1090e2b8ccc842d55fd944a8581597b3b3d22140", "committedDate": "2020-11-06T18:10:51Z", "message": "use partition pruning terminology in topic heading"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4939, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}