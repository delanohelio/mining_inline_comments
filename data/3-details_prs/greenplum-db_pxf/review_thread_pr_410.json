{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3OTkxNjgz", "number": 410, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODo1MjoyOVrOEN8lGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzowODowMlrOEOCM4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMDU5NDgwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxODo1MjoyOVrOGw1e3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMToyNToxM1rOGw6cRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTA4Ng==", "bodyText": "I think we can not rely on fragment index because:\n\nnot sure if fragment list coming in JSON guarantees order of splits\nisn't index maintained from 0 for each segment ?\nwhen reading multiple files, some beginning splits will not be the first fragments as other files would be read before them\n\nI think instead we should be looking at the offset in the fragment metadata, if the offset is 0, then we're reading the beginning of the file and can apply the header logic.", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r453861086", "createdAt": "2020-07-13T18:52:29Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java", "diffHunk": "@@ -56,17 +57,20 @@ public LineBreakAccessor() {\n     }\n \n     @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n+    public void initialize(RequestContext context) {\n+        super.initialize(context);\n         ((TextInputFormat) inputFormat).configure(jobConf);\n+        skipHeaderCount = context.getFragmentIndex() != 0 ? 0 :", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d0a4382e9ace6e232aa20c7d1e88860e8cbfc0c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk0MjM0MA==", "bodyText": "agreed, pushing that change now.", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r453942340", "createdAt": "2020-07-13T21:25:13Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java", "diffHunk": "@@ -56,17 +57,20 @@ public LineBreakAccessor() {\n     }\n \n     @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n+    public void initialize(RequestContext context) {\n+        super.initialize(context);\n         ((TextInputFormat) inputFormat).configure(jobConf);\n+        skipHeaderCount = context.getFragmentIndex() != 0 ? 0 :", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTA4Ng=="}, "originalCommit": {"oid": "4d0a4382e9ace6e232aa20c7d1e88860e8cbfc0c"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTUxNTg1OnYy", "diffSide": "LEFT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzowODowMlrOGw9y5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzo1MzowNlrOGw-2mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NzI4Ng==", "bodyText": "this is confusing, if openForRead is used only for 1 Fragment and each Fragment is represented by a FileSplit, why are there iterators in this old logic ? Asking because if we can process multiple splits as the current logic implies, then we have to implement skipping lines inside readNextObject method.", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r453997286", "createdAt": "2020-07-13T23:08:02Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java", "diffHunk": "@@ -80,7 +84,6 @@ public void initialize(RequestContext requestContext) {\n     @Override\n     public boolean openForRead() throws Exception {\n         LinkedList<InputSplit> requestSplits = new LinkedList<>();\n-        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4841abdfa1b9113f3962560ebd57d769912f372"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxMTE1NA==", "bodyText": "Yes, this is confusing. I have seen this code before when we did the refactor, and I wasn't sure why it was there. I took at stab at removing this code in a subsequent commit and it looks like it was only being used by Avro, but there wasn't anything in avro that would add splits to the iterator. So it looks safe to remove. You can take a look at this commit for the cleanup: 24003f9", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r454011154", "createdAt": "2020-07-13T23:41:50Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java", "diffHunk": "@@ -80,7 +84,6 @@ public void initialize(RequestContext requestContext) {\n     @Override\n     public boolean openForRead() throws Exception {\n         LinkedList<InputSplit> requestSplits = new LinkedList<>();\n-        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NzI4Ng=="}, "originalCommit": {"oid": "c4841abdfa1b9113f3962560ebd57d769912f372"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxNDYxOQ==", "bodyText": "now waiting for pipelines to go green: https://ud.ci.gpdb.pivotal.io/teams/main/pipelines/dev:fguerrero-support-header-text", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r454014619", "createdAt": "2020-07-13T23:53:06Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java", "diffHunk": "@@ -80,7 +84,6 @@ public void initialize(RequestContext requestContext) {\n     @Override\n     public boolean openForRead() throws Exception {\n         LinkedList<InputSplit> requestSplits = new LinkedList<>();\n-        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NzI4Ng=="}, "originalCommit": {"oid": "c4841abdfa1b9113f3962560ebd57d769912f372"}, "originalPosition": 22}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3407, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}