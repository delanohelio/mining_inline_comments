{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2ODQ0NTgw", "number": 389, "title": "Avro: add compression", "bodyText": "With this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). For XZ and BZIP2 we cannot currently\nsupport as it requires adding the Apache Commons Compression jar[0].\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n[0] https://commons.apache.org/proper/commons-compress/\nAuthored-by: Oliver Albertini oalbertini@vmware.com", "createdAt": "2020-06-19T01:55:21Z", "url": "https://github.com/greenplum-db/pxf/pull/389", "merged": true, "mergeCommit": {"oid": "749291163e8b7f42463dcc6ad61eaef6a8f40d98"}, "closed": true, "closedAt": "2020-06-27T01:18:09Z", "author": {"login": "oliverralbertini"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcspt3GgBqjM0NjA3MTY5MTY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcvNZsOgBqjM0ODg1MzIwNDc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b267c73aedb38ed6e9cc76c8bd1fcb437849cde1", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/b267c73aedb38ed6e9cc76c8bd1fcb437849cde1", "committedDate": "2020-06-19T01:54:12Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). For XZ and BZIP2 we cannot currently\nsupport as it requires adding the Apache Commons Compression jar[0].\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}, "afterCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/5f8ed4f1eeb016af64d603c84c31dc408918f515", "committedDate": "2020-06-19T02:34:35Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). For XZ and BZIP2 we cannot currently\nsupport as it requires adding the Apache Commons Compression jar[0].\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNzY1NjA3", "url": "https://github.com/greenplum-db/pxf/pull/389#pullrequestreview-433765607", "createdAt": "2020-06-19T02:36:01Z", "commit": {"oid": "b267c73aedb38ed6e9cc76c8bd1fcb437849cde1"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjozODoxNVrOGmGLxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo1MTowM1rOGmGXwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDM4OA==", "bodyText": "not sure what this does, but we don't want to write to stderr from the app", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600388", "createdAt": "2020-06-19T02:38:15Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);\n+\n+        try (PrintStream printStream = new PrintStream(new FileOutputStream(new File(pathToMetadata)))) {\n+            tool.run(null, printStream, System.err, args);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDU1NQ==", "bodyText": "is Tool thread safe and stateless ? If yes, we should be using a single (static) instance of it", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600555", "createdAt": "2020-06-19T02:39:02Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDkwNg==", "bodyText": "List<String> args = Arrays.asList(pathToFile)", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600906", "createdAt": "2020-06-19T02:40:33Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTUyOA==", "bodyText": "what's the point, it will throw NPE in either case (before and after the change) ? This is mostly used for checking input params to the functions are not nulls.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442601528", "createdAt": "2020-06-19T02:43:03Z", "author": {"login": "denalex"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -60,7 +61,7 @@\n     public void beforeClass() throws Exception {\n         // path for storing data on HDFS (for processing by PXF)\n         hdfsPath = hdfs.getWorkingDirectory() + \"/writableAvro/\";\n-        String absolutePath = getClass().getClassLoader().getResource(\"data\").getPath();\n+        String absolutePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"data\")).getPath();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTg1Ng==", "bodyText": "should we use the value uncompressed (like in other profiles, I believe) instead of null ?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442601856", "createdAt": "2020-06-19T02:44:26Z", "author": {"login": "denalex"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -96,12 +97,43 @@ public void generateSchemaPrimitive() throws Exception {\n         publicStage += \"generateSchemaPrimitive/\";\n         // fetch all the segment-generated avro files and make them into json records\n         // confirm that the lines generated by the segments match what we expect\n-        fetchAndVerifyAvroHcfsFiles(\"primitives.json\");\n+        fetchAndVerifyAvroHcfsFiles(\"primitives.json\", \"deflate\");\n \n         // check using GPDB readable external table that what went into HCFS is correct\n         runTincTest(\"pxf.features.hdfs.writable.avro.primitives_generate_schema.runTest\");\n     }\n \n+    @Test(groups = {\"features\", \"gpdb\", \"hcfs\", \"security\"})\n+    public void generateSchemaPrimitive_withNoCompression() throws Exception {\n+        gpdbTable = \"writable_avro_primitive_no_compression\";\n+        fullTestPath = hdfsPath + \"generate_schema_primitive_types_with_no_compression\";\n+        exTable = new WritableExternalTable(gpdbTable + \"_writable\", avroPrimitiveTableCols, fullTestPath, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_export\");\n+        exTable.setProfile(protocol.value() + \":avro\");\n+        exTable.setUserParameters(new String[]{\"COMPRESSION_CODEC=null\"});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMjY0OQ==", "bodyText": "I suggest to make all these as constants", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442602649", "createdAt": "2020-06-19T02:47:43Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzAwMQ==", "bodyText": "the default case should handle all unsupported ones and throw exception, while no-compression case should be one of the explicit choices above", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442603001", "createdAt": "2020-06-19T02:49:06Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":\n+                writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n+                break;\n+            case \"xz\":\n+                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");\n+                // writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));\n+                // break;\n+            case \"bzip2\":\n+                throw new RuntimeException(\"Avro Compression codec bzip2 currently not supported\");\n+                // writer.setCodec(CodecFactory.bzip2Codec());\n+                // break;\n+            case \"snappy\":\n+                writer.setCodec(CodecFactory.snappyCodec());\n+                break;\n+            default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw==", "bodyText": "I don't like going back to profile and relying on its name, let's think if this can be done in a better way", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442603457", "createdAt": "2020-06-19T02:51:03Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MDgwNjAw", "url": "https://github.com/greenplum-db/pxf/pull/389#pullrequestreview-434080600", "createdAt": "2020-06-19T13:33:24Z", "commit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozMzoyNFrOGmU_iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozODo0NlrOGmVLHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MzAxNg==", "bodyText": "why are these not supported? sometimes some compressions are supported only when you have hadoop native tools in your ld_library path, which is a likely scenario in customer environments", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442843016", "createdAt": "2020-06-19T13:33:24Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":\n+                writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n+                break;\n+            case \"xz\":\n+                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NTcwMA==", "bodyText": "Let's avoid using regex. This will fail for legacy profiles ie Avro. This will also fail if the user creates the table as\ncreate external table avro ()\nlocation('pxf://foo?PROFILE=HDFS:aVrO')\nformat ....\n\nIt's not a case sensitive comparison", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442845700", "createdAt": "2020-06-19T13:38:13Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {\n+            return isParquetCompressionThreadSafe(compCodec);\n+        }\n+        if (context.getProfile().matches(\"^.*:avro$\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NTk4Mw==", "bodyText": "yeah, this needs some more thought. But I don't see where else the information might come from, without accessing the file. I believe this happens at the beginning of the bridge call to determine if the access will need locking", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442845983", "createdAt": "2020-06-19T13:38:46Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0Mzk5MDQz", "url": "https://github.com/greenplum-db/pxf/pull/389#pullrequestreview-434399043", "createdAt": "2020-06-20T03:25:39Z", "commit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQwMzoyNTo0MFrOGmkfDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQwMzoyNzowNlrOGmkfTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5Njg0Ng==", "bodyText": "I am not sure this logic is equivalent (or a superset) or what was removed, but it might as well be. Actually, I prefer the way comparison was done before: BZip2Codec.class.isAssignableFrom(..) instead of comparing class names. There was also getting Hadoop mapping compressionCodecName.getHadoopCompressionCodecClass() which is no longer done here.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443096846", "createdAt": "2020-06-20T03:25:40Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        // bzip2 for Avro, and BZip2Codec for Parquet\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(\"org.apache.hadoop.io.compress.BZip2Codec\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5NjkxMA==", "bodyText": "what if datasource points to directory full of bz2 files (and not an individual file), will this detect it ?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443096910", "createdAt": "2020-06-20T03:27:06Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0Mzc4NDUw", "url": "https://github.com/greenplum-db/pxf/pull/389#pullrequestreview-434378450", "createdAt": "2020-06-19T23:23:31Z", "commit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzoyMzozMVrOGmjNwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzoyMzozMVrOGmjNwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3NjAzNA==", "bodyText": "should we call it NO_CODEC?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443076034", "createdAt": "2020-06-19T23:23:31Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -142,26 +147,20 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n-        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, DEFLATE_CODEC).toLowerCase();\n         int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n         switch (codec) {\n-            case \"deflate\":\n+            case DEFLATE_CODEC:\n                 writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n                 break;\n-            case \"xz\":\n-                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");\n-                // writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));\n-                // break;\n-            case \"bzip2\":\n-                throw new RuntimeException(\"Avro Compression codec bzip2 currently not supported\");\n-                // writer.setCodec(CodecFactory.bzip2Codec());\n-                // break;\n-            case \"snappy\":\n+            case SNAPPY_CODEC:\n                 writer.setCodec(CodecFactory.snappyCodec());\n                 break;\n-            default:\n+            case NULL_CODEC:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 43}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/5f0181f7d17d38d69ce10e362f5661585d5bf948", "committedDate": "2020-06-19T22:50:48Z", "message": "Incorporate PR feedback\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}, "afterCommit": {"oid": "322f966be914b6d30c62a335e97cac4b20346aba", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/322f966be914b6d30c62a335e97cac4b20346aba", "committedDate": "2020-06-22T19:01:55Z", "message": "Make tests pass after rebasing master\n\n(smallint changes were merged to master)\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NTAyODQw", "url": "https://github.com/greenplum-db/pxf/pull/389#pullrequestreview-438502840", "createdAt": "2020-06-26T18:17:42Z", "commit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODoxNzo0MlrOGpqZwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODoyNTo0NVrOGpqnkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzOTUyMg==", "bodyText": "I was suggesting to collapse into 1 return statement, like this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (compCodec == null) {\n          \n          \n            \n                        // check for file extensions indicating bzip2 (Text only)\n          \n          \n            \n                        // currently doesn't check for bzip2 in .avro files\n          \n          \n            \n                        Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n          \n          \n            \n                        return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n          \n          \n            \n                    }\n          \n          \n            \n                    return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());\n          \n          \n            \n                    Class<? extends CompressionCodec> codecClass = null;\n          \n          \n            \n                    if (compCodec == null) {\n          \n          \n            \n                        // check for file extensions indicating bzip2 (Text only)\n          \n          \n            \n                        // currently doesn't check for bzip2 in .avro files\n          \n          \n            \n                        codecClass = getCodecClassByPath(configuration, dataSource);\n          \n          \n            \n                    }\n          \n          \n            \n                    return !( \"bzip2\".equalsIgnoreCase(compCodec) ||\n          \n          \n            \n                                   BZip2Codec.class.getName().equalsIgnoreCase(compCodec) ||\n          \n          \n            \n                                   (codecClass != null && BZip2Codec.class.isAssignableFrom(codecClass))\n          \n          \n            \n                                  );", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446339522", "createdAt": "2020-06-26T18:17:42Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/CodecFactory.java", "diffHunk": "@@ -80,6 +81,24 @@ public CompressionCodec getCodec(String name, Configuration conf) {\n         return codecClass;\n     }\n \n+    /**\n+     * Determine whether a given compression codec is safe for multiple concurrent threads\n+     *\n+     * @param compCodec     the user-given COMPRESSION_CODEC, may be null\n+     * @param dataSource    the file that we are accessing\n+     * @param configuration HDFS config\n+     * @return true only if it's thread safe\n+     */\n+    public boolean isCodecThreadSafe(String compCodec, String dataSource, Configuration configuration) {\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Text only)\n+            // currently doesn't check for bzip2 in .avro files\n+            Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MzA1Ng==", "bodyText": "should we have context.getCompressionCodec() method not to deal with strings here ? That method can also deal with short-name vs. fully qualified class (if needed).", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446343056", "createdAt": "2020-06-26T18:25:45Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        return codecFactory.isCodecThreadSafe(context.getOption(\"COMPRESSION_CODEC\"), context.getDataSource(), configuration);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "originalPosition": 18}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/5e20e30348703dabfd80788ed668f1caac8558aa", "committedDate": "2020-06-26T17:59:00Z", "message": "Push thread safe check down to CodecFactory class\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}, "afterCommit": {"oid": "fcbe7d2be740e019407317417aa45b7b9615639d", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/fcbe7d2be740e019407317417aa45b7b9615639d", "committedDate": "2020-06-26T18:59:30Z", "message": "Push thread safe check down to CodecFactory class\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NTcwNzQz", "url": "https://github.com/greenplum-db/pxf/pull/389#pullrequestreview-438570743", "createdAt": "2020-06-26T20:18:59Z", "commit": {"oid": "fcbe7d2be740e019407317417aa45b7b9615639d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fcbe7d2be740e019407317417aa45b7b9615639d", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/fcbe7d2be740e019407317417aa45b7b9615639d", "committedDate": "2020-06-26T18:59:30Z", "message": "Push thread safe check down to CodecFactory class\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}, "afterCommit": {"oid": "cd506be15ad5f333981afe2add5f06745a0084a0", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/cd506be15ad5f333981afe2add5f06745a0084a0", "committedDate": "2020-06-26T22:09:02Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cd506be15ad5f333981afe2add5f06745a0084a0", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/cd506be15ad5f333981afe2add5f06745a0084a0", "committedDate": "2020-06-26T22:09:02Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}, "afterCommit": {"oid": "bb3931557f185a157233378587bbc585a6b77c5c", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/bb3931557f185a157233378587bbc585a6b77c5c", "committedDate": "2020-06-26T22:11:04Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "749291163e8b7f42463dcc6ad61eaef6a8f40d98", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/749291163e8b7f42463dcc6ad61eaef6a8f40d98", "committedDate": "2020-06-27T01:16:09Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bb3931557f185a157233378587bbc585a6b77c5c", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/bb3931557f185a157233378587bbc585a6b77c5c", "committedDate": "2020-06-26T22:11:04Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}, "afterCommit": {"oid": "749291163e8b7f42463dcc6ad61eaef6a8f40d98", "author": {"user": {"login": "oliverralbertini", "name": "Oliver Albertini"}}, "url": "https://github.com/greenplum-db/pxf/commit/749291163e8b7f42463dcc6ad61eaef6a8f40d98", "committedDate": "2020-06-27T01:16:09Z", "message": "Avro: add compression\n\nWith this commit we can support writing (and reading back) Avro data\nwith Snappy, Deflate compressions along with no compression (by\nspecifying COMPRESSION_CODEC=null). XZ and BZIP2 support will be added\nlater.\n\nFor Deflate and in the future XZ you can supply a compression level\nbetween 1-9 inclusive, where higher is better compression, but slower\nperformance.\n\n[0] https://commons.apache.org/proper/commons-compress/\n\nAuthored-by: Oliver Albertini <oalbertini@vmware.com>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 99, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}