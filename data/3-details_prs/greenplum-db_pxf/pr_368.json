{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3OTkzODYy", "number": 368, "title": "Migrate pxf-hbase to Spring Boot", "bodyText": "Update top level build.gradle to include pxf-hbase\nAdd new build.gradle for pxf-hbase\nall existing unit tests should be ported to junit 5\nall automation tests for HBase should be working\n\nIn this PR, we intentionally comment out the automation tests for Hive, which should be restored with the inclusion of the pxf-hive profile.", "createdAt": "2020-06-04T17:51:04Z", "url": "https://github.com/greenplum-db/pxf/pull/368", "merged": true, "mergeCommit": {"oid": "92384e1d9e46f721e0df0778e479fff74f3f2d32"}, "closed": true, "closedAt": "2020-06-04T20:12:35Z", "author": {"login": "frankgh"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoByzfgH2gAyNDI3OTkzODYyOjE2YzY0YmMzYzU2ZWQyZjBhNGFhNGU4MWVhZjEzNjY3ZDlhNDdmYjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcoD1GXAH2gAyNDI3OTkzODYyOmVhOWQ5MzRkZDE1ZjQzOGVkOTllY2U2ZWQ3N2ZlMDEzMTU0MWYwMjI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9", "committedDate": "2020-06-04T17:48:27Z", "message": "Migrate pxf-hbase to Spring Boot\n\n- Update top level build.gradle to include pxf-hbase\n- Add new build.gradle for pxf-hbase\n- all existing unit tests should be ported to junit 5\n- all automation tests for HBase should be working"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0NzI1NzEy", "url": "https://github.com/greenplum-db/pxf/pull/368#pullrequestreview-424725712", "createdAt": "2020-06-04T18:34:05Z", "commit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozNDowNVrOGfS1kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo1MjoxMFrOGfTnBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2NzY2NQ==", "bodyText": "when will this be uncommented ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435467665", "createdAt": "2020-06-04T18:34:05Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/MultipleHiveFragmentsPerFileFragmenter.java", "diffHunk": "@@ -1,122 +1,122 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n-import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n-import org.apache.hadoop.mapred.FileInputFormat;\n-import org.apache.hadoop.mapred.FileSplit;\n-import org.apache.hadoop.mapred.InputFormat;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.greenplum.pxf.api.model.BaseFragmenter;\n-import org.greenplum.pxf.api.model.Fragment;\n-import org.greenplum.pxf.api.model.Metadata;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveClientWrapper;\n-import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n-import org.greenplum.pxf.plugins.hive.HiveUserData;\n-\n-import java.io.ByteArrayOutputStream;\n-import java.io.ObjectOutputStream;\n-import java.util.List;\n-import java.util.Properties;\n-\n-\n-/**\n- * Fragmenter which splits one file into multiple fragments. Helps to simulate a\n- * case of big files.\n- * <p>\n- * inputData has to have following parameters:\n- * TEST-FRAGMENTS-NUM - defines how many fragments will be returned for current file\n- */\n-public class MultipleHiveFragmentsPerFileFragmenter extends BaseFragmenter {\n-    private static final Log LOG = LogFactory.getLog(MultipleHiveFragmentsPerFileFragmenter.class);\n-\n-    private static final long SPLIT_SIZE = 1024;\n-    private JobConf jobConf;\n-    private IMetaStoreClient client;\n-    private HiveClientWrapper hiveClientWrapper;\n-\n-    public MultipleHiveFragmentsPerFileFragmenter() {\n-        hiveClientWrapper = HiveClientWrapper.getInstance();\n-    }\n-\n-    @Override\n-    public void initialize(RequestContext context) {\n-        super.initialize(context);\n-        jobConf = new JobConf(configuration, MultipleHiveFragmentsPerFileFragmenter.class);\n-        client = hiveClientWrapper.initHiveClient(context, configuration);\n-    }\n-\n-    @Override\n-    public List<Fragment> getFragments() throws Exception {\n-        String localhostname = java.net.InetAddress.getLocalHost().getHostName();\n-        String[] localHosts = new String[]{localhostname, localhostname};\n-\n-        // TODO whitelist property\n-        int fragmentsNum = Integer.parseInt(context.getOption(\"TEST-FRAGMENTS-NUM\"));\n-        Metadata.Item tblDesc = hiveClientWrapper.extractTableFromName(context.getDataSource());\n-        Table tbl = hiveClientWrapper.getHiveTable(client, tblDesc);\n-        Properties properties = getSchema(tbl);\n-\n-        for (int i = 0; i < fragmentsNum; i++) {\n-\n-            String userData = \"inputFormatName\" + HiveUserData.HIVE_UD_DELIM\n-                    + tbl.getSd().getSerdeInfo().getSerializationLib()\n-                    + HiveUserData.HIVE_UD_DELIM + \"propertiesString\"\n-                    + HiveUserData.HIVE_UD_DELIM + HiveDataFragmenter.HIVE_NO_PART_TBL\n-                    + HiveUserData.HIVE_UD_DELIM + \"filterInFragmenter\"\n-                    + HiveUserData.HIVE_UD_DELIM + \"delimiter\"\n-                    + HiveUserData.HIVE_UD_DELIM + properties.getProperty(\"columns.types\");\n-\n-            ByteArrayOutputStream bas = new ByteArrayOutputStream();\n-            ObjectOutputStream os = new ObjectOutputStream(bas);\n-            os.writeLong(i * SPLIT_SIZE); // start\n-            os.writeLong(SPLIT_SIZE); // length\n-            os.writeObject(localHosts); // hosts\n-            os.close();\n-\n-            String filePath = getFilePath(tbl);\n-\n-            fragments.add(new Fragment(filePath, localHosts, bas.toByteArray(), userData.getBytes()));\n-        }\n-\n-        return fragments;\n-    }\n-\n-\n-    private static Properties getSchema(Table table) {\n-        return MetaStoreUtils.getSchema(table.getSd(), table.getSd(),\n-                table.getParameters(), table.getDbName(), table.getTableName(),\n-                table.getPartitionKeys());\n-    }\n-\n-    private String getFilePath(Table tbl) throws Exception {\n-\n-        StorageDescriptor descTable = tbl.getSd();\n-\n-        InputFormat<?, ?> fformat = HiveDataFragmenter.makeInputFormat(descTable.getInputFormat(), jobConf);\n-\n-        FileInputFormat.setInputPaths(jobConf, new Path(descTable.getLocation()));\n-\n-        InputSplit[] splits;\n-        try {\n-            splits = fformat.getSplits(jobConf, 1);\n-        } catch (org.apache.hadoop.mapred.InvalidInputException e) {\n-            LOG.debug(\"getSplits failed on \" + e.getMessage());\n-            throw new RuntimeException(\"Unable to get file path for table.\");\n-        }\n-\n-        for (InputSplit split : splits) {\n-            FileSplit fsp = (FileSplit) split;\n-            String[] hosts = fsp.getLocations();\n-            String filepath = fsp.getPath().toString();\n-            return filepath;\n-        }\n-        throw new RuntimeException(\"Unable to get file path for table.\");\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.apache.hadoop.fs.Path;\n+//import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+//import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n+//import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+//import org.apache.hadoop.hive.metastore.api.Table;\n+//import org.apache.hadoop.mapred.FileInputFormat;\n+//import org.apache.hadoop.mapred.FileSplit;\n+//import org.apache.hadoop.mapred.InputFormat;\n+//import org.apache.hadoop.mapred.InputSplit;\n+//import org.apache.hadoop.mapred.JobConf;\n+//import org.greenplum.pxf.api.model.BaseFragmenter;\n+//import org.greenplum.pxf.api.model.Fragment;\n+//import org.greenplum.pxf.api.model.Metadata;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveClientWrapper;\n+//import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n+//import org.greenplum.pxf.plugins.hive.HiveUserData;\n+//\n+//import java.io.ByteArrayOutputStream;\n+//import java.io.ObjectOutputStream;\n+//import java.util.List;\n+//import java.util.Properties;\n+//\n+//\n+///**\n+// * Fragmenter which splits one file into multiple fragments. Helps to simulate a\n+// * case of big files.\n+// * <p>\n+// * inputData has to have following parameters:\n+// * TEST-FRAGMENTS-NUM - defines how many fragments will be returned for current file\n+// */\n+//public class MultipleHiveFragmentsPerFileFragmenter extends BaseFragmenter {\n+//    private static final Log LOG = LogFactory.getLog(MultipleHiveFragmentsPerFileFragmenter.class);\n+//\n+//    private static final long SPLIT_SIZE = 1024;\n+//    private JobConf jobConf;\n+//    private IMetaStoreClient client;\n+//    private HiveClientWrapper hiveClientWrapper;\n+//\n+//    public MultipleHiveFragmentsPerFileFragmenter() {\n+//        hiveClientWrapper = HiveClientWrapper.getInstance();\n+//    }\n+//\n+//    @Override\n+//    public void initialize(RequestContext context) {\n+//        super.initialize(context);\n+//        jobConf = new JobConf(configuration, MultipleHiveFragmentsPerFileFragmenter.class);\n+//        client = hiveClientWrapper.initHiveClient(context, configuration);\n+//    }\n+//\n+//    @Override\n+//    public List<Fragment> getFragments() throws Exception {\n+//        String localhostname = java.net.InetAddress.getLocalHost().getHostName();\n+//        String[] localHosts = new String[]{localhostname, localhostname};\n+//\n+//        // TODO whitelist property\n+//        int fragmentsNum = Integer.parseInt(context.getOption(\"TEST-FRAGMENTS-NUM\"));\n+//        Metadata.Item tblDesc = hiveClientWrapper.extractTableFromName(context.getDataSource());\n+//        Table tbl = hiveClientWrapper.getHiveTable(client, tblDesc);\n+//        Properties properties = getSchema(tbl);\n+//\n+//        for (int i = 0; i < fragmentsNum; i++) {\n+//\n+//            String userData = \"inputFormatName\" + HiveUserData.HIVE_UD_DELIM\n+//                    + tbl.getSd().getSerdeInfo().getSerializationLib()\n+//                    + HiveUserData.HIVE_UD_DELIM + \"propertiesString\"\n+//                    + HiveUserData.HIVE_UD_DELIM + HiveDataFragmenter.HIVE_NO_PART_TBL\n+//                    + HiveUserData.HIVE_UD_DELIM + \"filterInFragmenter\"\n+//                    + HiveUserData.HIVE_UD_DELIM + \"delimiter\"\n+//                    + HiveUserData.HIVE_UD_DELIM + properties.getProperty(\"columns.types\");\n+//\n+//            ByteArrayOutputStream bas = new ByteArrayOutputStream();\n+//            ObjectOutputStream os = new ObjectOutputStream(bas);\n+//            os.writeLong(i * SPLIT_SIZE); // start\n+//            os.writeLong(SPLIT_SIZE); // length\n+//            os.writeObject(localHosts); // hosts\n+//            os.close();\n+//\n+//            String filePath = getFilePath(tbl);\n+//\n+//            fragments.add(new Fragment(filePath, localHosts, bas.toByteArray(), userData.getBytes()));\n+//        }\n+//\n+//        return fragments;\n+//    }\n+//\n+//\n+//    private static Properties getSchema(Table table) {\n+//        return MetaStoreUtils.getSchema(table.getSd(), table.getSd(),\n+//                table.getParameters(), table.getDbName(), table.getTableName(),\n+//                table.getPartitionKeys());\n+//    }\n+//\n+//    private String getFilePath(Table tbl) throws Exception {\n+//\n+//        StorageDescriptor descTable = tbl.getSd();\n+//\n+//        InputFormat<?, ?> fformat = HiveDataFragmenter.makeInputFormat(descTable.getInputFormat(), jobConf);\n+//\n+//        FileInputFormat.setInputPaths(jobConf, new Path(descTable.getLocation()));\n+//\n+//        InputSplit[] splits;\n+//        try {\n+//            splits = fformat.getSplits(jobConf, 1);\n+//        } catch (org.apache.hadoop.mapred.InvalidInputException e) {\n+//            LOG.debug(\"getSplits failed on \" + e.getMessage());\n+//            throw new RuntimeException(\"Unable to get file path for table.\");\n+//        }\n+//\n+//        for (InputSplit split : splits) {\n+//            FileSplit fsp = (FileSplit) split;\n+//            String[] hosts = fsp.getLocations();\n+//            String filepath = fsp.getPath().toString();\n+//            return filepath;\n+//        }\n+//        throw new RuntimeException(\"Unable to get file path for table.\");\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2Nzc4Ng==", "bodyText": "when will this be uncommented ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435467786", "createdAt": "2020-06-04T18:34:15Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveInputFormatFragmenterWithFilter.java", "diffHunk": "@@ -1,35 +1,35 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveInputFormatFragmenter;\n-\n-public class HiveInputFormatFragmenterWithFilter extends HiveInputFormatFragmenter {\n-\n-    private static final Log LOG = LogFactory.getLog(HiveInputFormatFragmenterWithFilter.class);\n-\n-    @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n-        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n-    }\n-\n-    /*\n-     *  Ignores filter from gpdb, use user defined filter\n-     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n-     */\n-    private void addFilters() {\n-\n-        // TODO: whitelist the option\n-        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n-        LOG.debug(\"user defined filter: \" + filterStr);\n-        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n-            return;\n-\n-            context.setFilterString(filterStr);\n-            LOG.debug(\"User defined filter: \" + context.getFilterString());\n-\n-            LOG.debug(\"User defined filter: \" + context.hasFilter());\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveInputFormatFragmenter;\n+//\n+//public class HiveInputFormatFragmenterWithFilter extends HiveInputFormatFragmenter {\n+//\n+//    private static final Log LOG = LogFactory.getLog(HiveInputFormatFragmenterWithFilter.class);\n+//\n+//    @Override\n+//    public void initialize(RequestContext requestContext) {\n+//        super.initialize(requestContext);\n+//        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n+//    }\n+//\n+//    /*\n+//     *  Ignores filter from gpdb, use user defined filter\n+//     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n+//     */\n+//    private void addFilters() {\n+//\n+//        // TODO: whitelist the option\n+//        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n+//        LOG.debug(\"user defined filter: \" + filterStr);\n+//        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n+//            return;\n+//\n+//            context.setFilterString(filterStr);\n+//            LOG.debug(\"User defined filter: \" + context.getFilterString());\n+//\n+//            LOG.debug(\"User defined filter: \" + context.hasFilter());\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2Nzg3MQ==", "bodyText": "when will this be uncommented ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435467871", "createdAt": "2020-06-04T18:34:21Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveDataFragmenterWithFilter.java", "diffHunk": "@@ -1,35 +1,35 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n-\n-public class HiveDataFragmenterWithFilter extends HiveDataFragmenter {\n-\n-    private static final Log LOG = LogFactory.getLog(HiveDataFragmenterWithFilter.class);\n-\n-    @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n-        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n-    }\n-\n-    /*\n-     *  Ignores filter from gpdb, use user defined filter\n-     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n-     */\n-    private void addFilters() {\n-\n-        //TODO whitelist the option\n-        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n-        LOG.debug(\"user defined filter: \" + filterStr);\n-        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n-            return;\n-\n-        context.setFilterString(filterStr);\n-        LOG.debug(\"User defined filter: \" + context.getFilterString());\n-\n-        LOG.debug(\"User defined filter: \" + context.hasFilter());\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n+//\n+//public class HiveDataFragmenterWithFilter extends HiveDataFragmenter {\n+//\n+//    private static final Log LOG = LogFactory.getLog(HiveDataFragmenterWithFilter.class);\n+//\n+//    @Override\n+//    public void initialize(RequestContext requestContext) {\n+//        super.initialize(requestContext);\n+//        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n+//    }\n+//\n+//    /*\n+//     *  Ignores filter from gpdb, use user defined filter\n+//     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n+//     */\n+//    private void addFilters() {\n+//\n+//        //TODO whitelist the option\n+//        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n+//        LOG.debug(\"user defined filter: \" + filterStr);\n+//        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n+//            return;\n+//\n+//        context.setFilterString(filterStr);\n+//        LOG.debug(\"User defined filter: \" + context.getFilterString());\n+//\n+//        LOG.debug(\"User defined filter: \" + context.hasFilter());\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2ODYzNg==", "bodyText": "this does not need @JsonCreator because it has a single argument constructor and there's only 1 property ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435468636", "createdAt": "2020-06-04T18:35:17Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/FilterVerifyFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package org.greenplum.pxf.automation.testplugin;\n+\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+public class FilterVerifyFragmentMetadata implements FragmentMetadata {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3MjEzMw==", "bodyText": "wonder why we need the whole netty and the whole zookeeper (where only client jars would suffice) ? also is yammer still needed with Boot ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435472133", "createdAt": "2020-06-04T18:39:56Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/build.gradle", "diffHunk": "@@ -0,0 +1,38 @@\n+apply plugin: \"java-library\"\n+\n+jar {\n+    enabled = true\n+}\n+\n+dependencies {\n+    /*******************************\n+     * API Dependencies\n+     *******************************/\n+\n+    /*******************************\n+     * Implementation Dependencies\n+     *******************************/\n+\n+    implementation(project(':pxf-api'))\n+    implementation(\"org.apache.hbase:hbase-client:${hbaseVersion}\")\n+    implementation(\"org.apache.hbase:hbase-common:${hbaseVersion}\")\n+    implementation(\"org.apache.hbase:hbase-protocol:${hbaseVersion}\")\n+    implementation(\"org.apache.htrace:htrace-core:3.1.0-incubating\")\n+    implementation(\"io.netty:netty-all:4.0.23.Final\")\n+    implementation(\"org.apache.zookeeper:zookeeper:3.4.6\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NDQ2NQ==", "bodyText": "wonder how serialized byte[] looks like in JSON ? Is it a base64 encoded string ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435474465", "createdAt": "2020-06-04T18:42:50Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/main/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import lombok.Getter;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.codehaus.jackson.annotate.JsonCreator;\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+import java.util.Map;\n+\n+public class HBaseFragmentMetadata implements FragmentMetadata {\n+\n+    @Getter\n+    private final byte[] startKey;\n+\n+    @Getter\n+    private final byte[] endKey;\n+\n+    @Getter\n+    private final Map<String, byte[]> columnMapping;\n+\n+    public HBaseFragmentMetadata(HRegionInfo region, Map<String, byte[]> columnMapping) {\n+        this(region.getStartKey(), region.getEndKey(), columnMapping);\n+    }\n+\n+    @JsonCreator\n+    public HBaseFragmentMetadata(\n+            @JsonProperty(\"startKey\") byte[] startKey,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NzM2Nw==", "bodyText": "the more I see this, the more it reminds me of https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/beans/factory/InitializingBean.html -- wonder if we should take out explicit initialization in plugins and have them implement afterPropertiesSet() instead and have it being managed by Spring and not the Bridge. The tests will still have to do it, though, like here. Just food for thought.", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435477367", "createdAt": "2020-06-04T18:46:45Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "diffHunk": "@@ -73,47 +67,44 @@ public void tearDown() throws Exception {\n         accessor = null;\n     }\n \n-\t/*\n-\t * Test construction of HBaseAccessor.\n-\t * Actually no need for this as it is tested in all other tests\n-\t * constructing HBaseAccessor but it serves as a simple example\n-\t * of mocking\n-\t *\n-\t * HBaseAccessor is created and then HBaseTupleDescriptioncreation\n-\t * is verified\n-\t */\n+    /*\n+     * Test construction of HBaseAccessor.\n+     * Actually no need for this as it is tested in all other tests\n+     * constructing HBaseAccessor but it serves as a simple example\n+     * of mocking\n+     *\n+     * HBaseAccessor is created and then HBaseTupleDescriptioncreation\n+     * is verified\n+     */\n     @Test\n-    public void construction() throws Exception {\n+    public void construction() {\n         prepareConstruction();\n         HBaseAccessor accessor = new HBaseAccessor();\n-        accessor.initialize(context);\n-        PowerMockito.verifyNew(HBaseTupleDescription.class).withArguments(context);\n+        accessor.setRequestContext(context);\n+        accessor.initialize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3Nzc4Mg==", "bodyText": "remove ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435477782", "createdAt": "2020-06-04T18:47:34Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "diffHunk": "@@ -156,11 +142,10 @@ private void prepareTableOpen() throws Exception {\n      * Helper for test setup.\n      * Sets zero columns (not realistic) and no filter\n      */\n-    private void prepareEmptyScanner() throws Exception {\n+    private void prepareEmptyScanner() {\n         scanDetails = mock(Scan.class);\n-        PowerMockito.whenNew(Scan.class).withNoArguments().thenReturn(scanDetails);\n \n-        when(tupleDescription.columns()).thenReturn(0);\n+        // when(tupleDescription.columns()).thenReturn(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3ODMxNg==", "bodyText": "did behavior switch from returning null to throwing NPE ? Is it Ok ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435478316", "createdAt": "2020-06-04T18:48:35Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFilterBuilderTest.java", "diffHunk": "@@ -94,12 +90,11 @@ public void parseNotExpressionIgnored() throws Exception {\n \n     @Test\n     public void parseNotOpCodeInConstant() throws Exception {\n-        thrown.expect(NullPointerException.class);\n \n         String filter = \"a1c25s2dl2o1a1c20s1d2o2l0\";\n         // Testing that we get past the parsing stage\n         // Very crude but it avoids instantiating all the necessary dependencies\n-        assertNull(helper(filter, null));\n+        assertThrows(NullPointerException.class, () -> helper(filter, null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3OTU3NA==", "bodyText": "maybe also have a few tests to serialize / deserialize the POJO with ObjectMapper to make sure @JsonCreator and others are configured correctly ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435479574", "createdAt": "2020-06-04T18:50:50Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadataTest.java", "diffHunk": "@@ -0,0 +1,52 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertSame;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class HBaseFragmentMetadataTest {\n+\n+    @Test\n+    public void testHRegionInfoConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HRegionInfo hRegionInfo = mock(HRegionInfo.class);\n+        when(hRegionInfo.getStartKey()).thenReturn(startKey);\n+        when(hRegionInfo.getEndKey()).thenReturn(endKey);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(hRegionInfo, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+\n+    @Test\n+    public void testConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(startKey, endKey, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4MDMyNA==", "bodyText": "wonder why the change", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435480324", "createdAt": "2020-06-04T18:52:10Z", "author": {"login": "denalex"}, "path": "server/pxf-jdbc/build.gradle", "diffHunk": "@@ -9,12 +9,11 @@ dependencies {\n      * API Dependencies\n      *******************************/\n \n-    api(project(':pxf-api'))\n-\n     /*******************************\n      * Implementation Dependencies\n      *******************************/\n \n+    implementation(project(':pxf-api'))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 10}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05d299bbc02fa99faaa531a4e96843d1b17a9871", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/05d299bbc02fa99faaa531a4e96843d1b17a9871", "committedDate": "2020-06-04T19:54:38Z", "message": "Address PR feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea9d934dd15f438ed99ece6ed77fe0131541f022", "author": {"user": null}, "url": "https://github.com/greenplum-db/pxf/commit/ea9d934dd15f438ed99ece6ed77fe0131541f022", "committedDate": "2020-06-04T20:10:46Z", "message": "Address PR feedback"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 77, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}