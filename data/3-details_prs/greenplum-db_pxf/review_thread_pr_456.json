{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1ODEyMDU1", "number": 456, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozNToxNVrOEpOmGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNzowMjo0NVrOErRjjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjY2MjAyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozNToxNVrOHaxqaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo1NToyM1rOHazK4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzODY5Nw==", "bodyText": "It looks like this check does not allow any occurrence of .. in the effective base path. This is probably the safest check to do, but doesn't it prevent the use of relative paths that stay within the configured base path, e.g. /mnt/data/dir1/../dir2?", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497838697", "createdAt": "2020-09-30T22:35:15Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +213,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");\n+\n+        if (\"..\".equals(effectiveBasePath) || StringUtils.contains(effectiveBasePath, \"../\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MzM5Mg==", "bodyText": "it does, but we don't want to be super smart here as that location is reachable without using ..", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497863392", "createdAt": "2020-09-30T23:55:23Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +213,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");\n+\n+        if (\"..\".equals(effectiveBasePath) || StringUtils.contains(effectiveBasePath, \"../\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzODY5Nw=="}, "originalCommit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjY2ODE3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozODowMFrOHaxuFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzoyNTo1M1rOHayqLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYzNg==", "bodyText": "You can have pathnames that contain $ that are not part of an environment variable. As long as we don't expand the path or pass it to a shell unescaped, we should be able to safely allow $ in paths, right?", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497839636", "createdAt": "2020-09-30T22:38:00Z", "author": {"login": "bradfordb-vmware"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +213,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");\n+\n+        if (\"..\".equals(effectiveBasePath) || StringUtils.contains(effectiveBasePath, \"../\")) {\n+            // Disallow relative paths\n+            throw new IllegalArgumentException(String\n+                    .format(\"the provided path '%s' is invalid. Relative paths are not allowed by PXF\", effectiveBasePath));\n+        }\n+\n+        if (StringUtils.contains(effectiveBasePath, \"$\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1NTAyMg==", "bodyText": "yeah, but we are being conservative here. We make a big bet that nobody will actually have a path with $ in the path name.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497855022", "createdAt": "2020-09-30T23:25:53Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +213,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");\n+\n+        if (\"..\".equals(effectiveBasePath) || StringUtils.contains(effectiveBasePath, \"../\")) {\n+            // Disallow relative paths\n+            throw new IllegalArgumentException(String\n+                    .format(\"the provided path '%s' is invalid. Relative paths are not allowed by PXF\", effectiveBasePath));\n+        }\n+\n+        if (StringUtils.contains(effectiveBasePath, \"$\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYzNg=="}, "originalCommit": {"oid": "f3783c82546b0154d2e3845c899b35d9441d24ad"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjcwMTY5OnYy", "diffSide": "RIGHT", "path": "concourse/scripts/install_nfs.bash", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo1MjozN1rOHayBfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo1MjozN1rOHayBfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NDYwNA==", "bodyText": "The showmount command is provided by nfs-utils which is being installed after we try to use it.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497844604", "createdAt": "2020-09-30T22:52:37Z", "author": {"login": "bradfordb-vmware"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjcwNzQ3OnYy", "diffSide": "RIGHT", "path": "concourse/scripts/install_nfs.bash", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo1NToyMlrOHayEwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo1NToyMlrOHayEwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NTQ0MQ==", "bodyText": "Minor nit: should we include -t nfs? Without it, mount will try to guess it.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497845441", "createdAt": "2020-09-30T22:55:22Z", "author": {"login": "bradfordb-vmware"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw\n+\n+echo \"install the NFS client\"\n+yum install -y -q -e 0 nfs-utils\n+\n+echo \"create mount point and mount it\"\n+mkdir -p ${BASE_PATH}\n+mount mdw:/var/nfs ${BASE_PATH}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjcxMTc3OnYy", "diffSide": "RIGHT", "path": "concourse/scripts/install_nfs.bash", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo1NzoyNVrOHayHOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNTowODozNVrOHbO-uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NjA3Mg==", "bodyText": "Is there a reason we don't externalize this and scp to the master host? Would save us from having to prefix every line with sudo.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497846072", "createdAt": "2020-09-30T22:57:25Z", "author": {"login": "bradfordb-vmware"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw\n+\n+echo \"install the NFS client\"\n+yum install -y -q -e 0 nfs-utils\n+\n+echo \"create mount point and mount it\"\n+mkdir -p ${BASE_PATH}\n+mount mdw:/var/nfs ${BASE_PATH}\n+chown gpadmin:gpadmin ${BASE_PATH}\n+chmod 755 ${BASE_PATH}\n+\n+echo \"verify the mount worked\"\n+mount | grep nfs\n+df -hT\n+\n+echo \"write a test file to make sure it worked\"\n+sudo runuser -l gpadmin -c \"touch ${BASE_PATH}/$(hostname)-test\"\n+ls -l ${BASE_PATH}\n+\n+EOFF\n+\n+  chmod +x /tmp/install_and_configure_nfs_client.sh\n+  scp /tmp/install_and_configure_nfs_client.sh \"${MASTER_HOSTNAME}:~gpadmin\"\n+}\n+\n+# assumes only two segment hosts sdw1 and sdw2\n+function run_nfs_installation() {\n+\n+  # install and configure the NFS server on master\n+  ssh \"centos@${MASTER_HOSTNAME}\" \"\n+    echo 'install NFS server'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMxOTAzNQ==", "bodyText": "I think this is a pattern we've been using for commands we run on master.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498319035", "createdAt": "2020-10-01T15:08:35Z", "author": {"login": "frankgh"}, "path": "concourse/scripts/install_nfs.bash", "diffHunk": "@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+GPHOME=/usr/local/greenplum-db-devel\n+# we need word boundary in case of standby master (smdw)\n+MASTER_HOSTNAME=$(grep < cluster_env_files/etc_hostfile '\\bmdw' | awk '{print $2}')\n+BASE_PATH=${BASE_PATH:-/mnt/nfs/var/nfsshare}\n+\n+cat << EOF\n+  ############################\n+  #                          #\n+  #     NFS Installation     #\n+  #                          #\n+  ############################\n+EOF\n+\n+function create_nfs_installer_scripts() {\n+  cat > /tmp/install_and_configure_nfs_client.sh <<-EOFF\n+#!/bin/bash\n+\n+set -euxo pipefail\n+\n+echo \"check available NFS shares in mdw\"\n+showmount -e mdw\n+\n+echo \"install the NFS client\"\n+yum install -y -q -e 0 nfs-utils\n+\n+echo \"create mount point and mount it\"\n+mkdir -p ${BASE_PATH}\n+mount mdw:/var/nfs ${BASE_PATH}\n+chown gpadmin:gpadmin ${BASE_PATH}\n+chmod 755 ${BASE_PATH}\n+\n+echo \"verify the mount worked\"\n+mount | grep nfs\n+df -hT\n+\n+echo \"write a test file to make sure it worked\"\n+sudo runuser -l gpadmin -c \"touch ${BASE_PATH}/$(hostname)-test\"\n+ls -l ${BASE_PATH}\n+\n+EOFF\n+\n+  chmod +x /tmp/install_and_configure_nfs_client.sh\n+  scp /tmp/install_and_configure_nfs_client.sh \"${MASTER_HOSTNAME}:~gpadmin\"\n+}\n+\n+# assumes only two segment hosts sdw1 and sdw2\n+function run_nfs_installation() {\n+\n+  # install and configure the NFS server on master\n+  ssh \"centos@${MASTER_HOSTNAME}\" \"\n+    echo 'install NFS server'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0NjA3Mg=="}, "originalCommit": {"oid": "0ab9a53952a65fa7776c62555e81ef5aa13f4503"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjc4NjI2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozNDowMlrOHayzUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozNDowMlrOHayzUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1NzM2MQ==", "bodyText": "effectiveDataSource", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497857361", "createdAt": "2020-09-30T23:34:02Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -199,8 +215,23 @@ public String getDataUri(Configuration configuration, String path) {\n      * @param dataSource The path to the data source\n      * @return the normalized path to the data source\n      */\n-    public String normalizeDataSource(String dataSource) {\n-        return StringUtils.removeStart(dataSource, \"/\");\n+    public String validateAndNormalizeDataSource(String dataSource) {\n+\n+        String effectiveBasePath = StringUtils.removeStart(dataSource, \"/\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44a81852b24bcb49d2e3e363c7ccd548b61d9ee4"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjc4ODAzOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzozNTowMFrOHay0SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNToxMTowNVrOHbPFyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1NzYwOQ==", "bodyText": "this logic will still be correct for \"/\" input, right ? Then I think comparing is a premature optimization.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497857609", "createdAt": "2020-09-30T23:35:00Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -212,20 +243,37 @@ protected String getDataUriForPrefix(Configuration configuration, String dataSou\n         URI defaultFS = FileSystem.getDefaultUri(configuration);\n \n         String uri;\n-        String normalizedDataSource = normalizeDataSource(dataSource);\n+        String basePath = validateAndNormalizeBasePath(configuration.get(CONFIG_KEY_BASE_PATH));\n+        String normalizedDataSource = validateAndNormalizeDataSource(dataSource);\n \n         if (FILE_SCHEME.equals(defaultFS.getScheme())) {\n             // if the defaultFS is file://, but enum is not FILE, use enum scheme only\n-            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + basePath + normalizedDataSource;\n         } else {\n             // if the defaultFS is not file://, use it, instead of enum scheme and append user's path\n-            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + \"/\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + basePath + \"/\" + normalizedDataSource;\n         }\n \n         disableSecureTokenRenewal(uri, configuration);\n         return uri;\n     }\n \n+    /**\n+     * Validates the basePath and normalizes it for the appropriate filesystem\n+     *\n+     * @param basePath the basePath as configured by the user\n+     * @return the normalized basePath\n+     */\n+    protected String validateAndNormalizeBasePath(String basePath) {\n+        if (StringUtils.isNotBlank(basePath)) {\n+            if (\"/\".equals(basePath))\n+                return \"/\";\n+            return StringUtils.removeEnd(StringUtils.removeStart(basePath, \"/\"), \"/\") + \"/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMyMDg0MQ==", "bodyText": "yes, this is something I was doing somewhere else", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498320841", "createdAt": "2020-10-01T15:11:05Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -212,20 +243,37 @@ protected String getDataUriForPrefix(Configuration configuration, String dataSou\n         URI defaultFS = FileSystem.getDefaultUri(configuration);\n \n         String uri;\n-        String normalizedDataSource = normalizeDataSource(dataSource);\n+        String basePath = validateAndNormalizeBasePath(configuration.get(CONFIG_KEY_BASE_PATH));\n+        String normalizedDataSource = validateAndNormalizeDataSource(dataSource);\n \n         if (FILE_SCHEME.equals(defaultFS.getScheme())) {\n             // if the defaultFS is file://, but enum is not FILE, use enum scheme only\n-            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + basePath + normalizedDataSource;\n         } else {\n             // if the defaultFS is not file://, use it, instead of enum scheme and append user's path\n-            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + \"/\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + basePath + \"/\" + normalizedDataSource;\n         }\n \n         disableSecureTokenRenewal(uri, configuration);\n         return uri;\n     }\n \n+    /**\n+     * Validates the basePath and normalizes it for the appropriate filesystem\n+     *\n+     * @param basePath the basePath as configured by the user\n+     * @return the normalized basePath\n+     */\n+    protected String validateAndNormalizeBasePath(String basePath) {\n+        if (StringUtils.isNotBlank(basePath)) {\n+            if (\"/\".equals(basePath))\n+                return \"/\";\n+            return StringUtils.removeEnd(StringUtils.removeStart(basePath, \"/\"), \"/\") + \"/\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1NzYwOQ=="}, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjgwNTg3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HcfsTypeTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo0NDoyN1rOHay-iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo0NDoyN1rOHay-iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MDIzMw==", "bodyText": "this seems to be the same as the previous one", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497860233", "createdAt": "2020-09-30T23:44:27Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HcfsTypeTest.java", "diffHunk": "@@ -373,4 +374,144 @@ public void testHcfsGlobPattern() {\n         assertEquals(\"0.0.0.0\", configuration.get(MRJobConfig.JOB_NAMENODES_TOKEN_RENEWAL_EXCLUDE));\n     }\n \n+    @Test\n+    public void testFailureOnNFSWhenBasePathIsNotConfigured() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the 'pxf.fs.basePath' configuration is required to access NFS filesystems. Configure a valid 'pxf.fs.basePath' property to access this server\");\n+\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testFailureOnNFSWhenInvalidDefaultFSIsProvided() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"profile protocol (nfs) is not compatible with server filesystem (s3a)\");\n+\n+        configuration.set(\"fs.defaultFS\", \"s3a://abc/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToRootDirectory() {\n+        configuration.set(\"pxf.fs.basePath\", \"/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToAFixedBucket() {\n+        configuration.set(\"pxf.fs.basePath\", \"some-bucket\");\n+        context.setProfileScheme(\"s3a\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"s3a://some-bucket/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToASingleCharPath() {\n+        configuration.set(\"pxf.fs.basePath\", \"p\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing and preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToSomeValue() {\n+        configuration.set(\"pxf.fs.basePath\", \"/my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / preceding the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / trailing the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjgxMTUzOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HcfsTypeTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo0Njo1OVrOHazBrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo0Njo1OVrOHazBrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MTAzNw==", "bodyText": "let's also test ../ and a/..", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497861037", "createdAt": "2020-09-30T23:46:59Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HcfsTypeTest.java", "diffHunk": "@@ -373,4 +374,144 @@ public void testHcfsGlobPattern() {\n         assertEquals(\"0.0.0.0\", configuration.get(MRJobConfig.JOB_NAMENODES_TOKEN_RENEWAL_EXCLUDE));\n     }\n \n+    @Test\n+    public void testFailureOnNFSWhenBasePathIsNotConfigured() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the 'pxf.fs.basePath' configuration is required to access NFS filesystems. Configure a valid 'pxf.fs.basePath' property to access this server\");\n+\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testFailureOnNFSWhenInvalidDefaultFSIsProvided() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"profile protocol (nfs) is not compatible with server filesystem (s3a)\");\n+\n+        configuration.set(\"fs.defaultFS\", \"s3a://abc/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType.getHcfsType(configuration, context);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToRootDirectory() {\n+        configuration.set(\"pxf.fs.basePath\", \"/\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToAFixedBucket() {\n+        configuration.set(\"pxf.fs.basePath\", \"some-bucket\");\n+        context.setProfileScheme(\"s3a\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"s3a://some-bucket/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToASingleCharPath() {\n+        configuration.set(\"pxf.fs.basePath\", \"p\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+\n+        // trailing and preceding / in basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/p/\");\n+        context.setProfileScheme(\"nfs\");\n+        nfs = HcfsType.getHcfsType(configuration, context);\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///p/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testBasePathIsConfiguredToSomeValue() {\n+        configuration.set(\"pxf.fs.basePath\", \"/my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        String uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / preceding the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // no / trailing the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"my/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+\n+        // preceding and trailing / in the basePath\n+        configuration.set(\"pxf.fs.basePath\", \"/my/base/path/\");\n+        context.setProfileScheme(\"nfs\");\n+        uri = nfs.getDataUri(configuration, context);\n+        assertEquals(\"file:///my/base/path/foo/bar.txt\", uri);\n+    }\n+\n+    @Test\n+    public void testFailsWhenARelativeDataSourceIsProvided1() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the provided path '../../../etc/passwd' is invalid. Relative paths are not allowed by PXF\");\n+\n+        configuration.set(\"pxf.fs.basePath\", \"/some/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        context.setDataSource(\"../../../etc/passwd\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        nfs.getDataUri(configuration, context);\n+    }\n+\n+    @Test\n+    public void testFailsWhenARelativeDataSourceIsProvided2() {\n+        thrown.expect(IllegalArgumentException.class);\n+        thrown.expectMessage(\"the provided path '..' is invalid. Relative paths are not allowed by PXF\");\n+\n+        configuration.set(\"pxf.fs.basePath\", \"/some/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        context.setDataSource(\"..\");\n+        HcfsType nfs = HcfsType.getHcfsType(configuration, context);\n+        nfs.getDataUri(configuration, context);\n+    }\n+\n+    @Test\n+    public void testDataSourceWithTwoDotsInName() {\n+        configuration.set(\"pxf.fs.basePath\", \"/some/base/path\");\n+        context.setProfileScheme(\"nfs\");\n+        context.setDataSource(\"a..txt\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjgxNTMyOnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo0OToxN1rOHazD6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo0OToxN1rOHazD6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MTYxMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <!-- NFS (Network File System profiles -->\n          \n          \n            \n                <!-- NFS (Network File System) profiles -->", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497861610", "createdAt": "2020-09-30T23:49:17Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,106 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- NFS (Network File System profiles -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjgxOTM2OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo1MTowNVrOHazGBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNToyNTowNFrOHbPtmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MjE0OA==", "bodyText": "I wonder if anyone is using these SequenceFile profiles ? These are for MR jobs, I think. I don't think they will be relevant for NFS at all.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497862148", "createdAt": "2020-09-30T23:51:05Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,106 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- NFS (Network File System profiles -->\n+    <profile>\n+        <name>nfs:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non\n+            parallel) and slower than HdfsTextSimple.\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsFileFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.QuotedLineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:parquet</name>\n+        <description>A profile for reading and writing Parquet data from Google Cloud Storage\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.ParquetResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:avro</name>\n+        <description>This profile is suitable for using when reading Avro files (i.e\n+            fileName.avro)\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.AvroFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.AvroResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:json</name>\n+        <description>\n+            Access JSON data either as:\n+            * one JSON record per line (default)\n+            * or multiline JSON records with an IDENTIFIER parameter indicating a member name used\n+            to determine the encapsulating json object to return\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.json.JsonAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.json.JsonResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:AvroSequenceFile</name>\n+        <description>\n+            Read an Avro format stored in sequence file, with separated schema file from Google\n+            Cloud Storage\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.SequenceFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.AvroResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:SequenceFile</name>\n+        <description>\n+            Profile for accessing Sequence files serialized with a custom Writable class\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.SequenceFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.WritableResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMzMTAzMw==", "bodyText": "yeah, that makes sense", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498331033", "createdAt": "2020-10-01T15:25:04Z", "author": {"login": "frankgh"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,106 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- NFS (Network File System profiles -->\n+    <profile>\n+        <name>nfs:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non\n+            parallel) and slower than HdfsTextSimple.\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsFileFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.QuotedLineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:parquet</name>\n+        <description>A profile for reading and writing Parquet data from Google Cloud Storage\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.ParquetResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:avro</name>\n+        <description>This profile is suitable for using when reading Avro files (i.e\n+            fileName.avro)\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.AvroFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.AvroResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:json</name>\n+        <description>\n+            Access JSON data either as:\n+            * one JSON record per line (default)\n+            * or multiline JSON records with an IDENTIFIER parameter indicating a member name used\n+            to determine the encapsulating json object to return\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.json.JsonAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.json.JsonResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:AvroSequenceFile</name>\n+        <description>\n+            Read an Avro format stored in sequence file, with separated schema file from Google\n+            Cloud Storage\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.SequenceFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.AvroResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>\n+    <profile>\n+        <name>nfs:SequenceFile</name>\n+        <description>\n+            Profile for accessing Sequence files serialized with a custom Writable class\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.SequenceFileAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.WritableResolver</resolver>\n+        </plugins>\n+        <protocol>nfs</protocol>\n+    </profile>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2MjE0OA=="}, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjgyNDI4OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMzo1Mzo0MFrOHazI3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNToyODo0OFrOHbP4Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2Mjg3OA==", "bodyText": "I think this is too much, especially the CDATA block. One paragraph should be enough, the rest can be found in documentation.", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r497862878", "createdAt": "2020-09-30T23:53:40Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "diffHunk": "@@ -35,4 +35,35 @@\n         </description>\n     </property>\n     -->\n+\n+    <!--\n+    <property>\n+        <name>pxf.fs.basePath</name>\n+        <value></value>\n+        <description>\n+        <![CDATA[\n+            Sets the base path when constructing a URI for read and write\n+            operations. This property must be set in the server configuration\n+            for NFS-based (Network File System) profiles. This property is\n+            useful when setting the path to a mount point for a given\n+            filesystem. This prevents users from creating tables accessing\n+            data outside of the mount point.\n+\n+            For example, if the pxf.fs.basePath property is set to /mount/path/\n+            and an external table definition is the following:\n+\n+            CREATE EXTERNAL TABLE nfs_table ()\n+            LOCATION ('pxf://path/to/data?PROFILE=nfs:avro&SERVER=nfs-server')\n+            FORMAT 'CUSTOM' (formatter='pxfwritable_import')\n+\n+            Then PXF will use the :///mount/path/path/to/data URI to read\n+            AVRO files. Users won't be able to access data at the root\n+            directory of the filesystem.\n+\n+            This property can also be used for any Hadoop compatible file\n+            system, for example S3.\n+        ]]>\n+        </description>\n+    </property>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMzMzc3MA==", "bodyText": "yeah, I figured this was going to be a lot, maybe @lisakowen can use it as a base for documentation", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498333770", "createdAt": "2020-10-01T15:28:48Z", "author": {"login": "frankgh"}, "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "diffHunk": "@@ -35,4 +35,35 @@\n         </description>\n     </property>\n     -->\n+\n+    <!--\n+    <property>\n+        <name>pxf.fs.basePath</name>\n+        <value></value>\n+        <description>\n+        <![CDATA[\n+            Sets the base path when constructing a URI for read and write\n+            operations. This property must be set in the server configuration\n+            for NFS-based (Network File System) profiles. This property is\n+            useful when setting the path to a mount point for a given\n+            filesystem. This prevents users from creating tables accessing\n+            data outside of the mount point.\n+\n+            For example, if the pxf.fs.basePath property is set to /mount/path/\n+            and an external table definition is the following:\n+\n+            CREATE EXTERNAL TABLE nfs_table ()\n+            LOCATION ('pxf://path/to/data?PROFILE=nfs:avro&SERVER=nfs-server')\n+            FORMAT 'CUSTOM' (formatter='pxfwritable_import')\n+\n+            Then PXF will use the :///mount/path/path/to/data URI to read\n+            AVRO files. Users won't be able to access data at the root\n+            directory of the filesystem.\n+\n+            This property can also be used for any Hadoop compatible file\n+            system, for example S3.\n+        ]]>\n+        </description>\n+    </property>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg2Mjg3OA=="}, "originalCommit": {"oid": "b9fc21cd0a10ee5af16bd21d36c553d2c92589f7"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMTExNjk3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMjo1NTozNVrOHbdIoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMjo1NTozNVrOHbdIoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU1MDk0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    String.format(\"the '%1$s' configuration is required to access locally mounted file systems. Configure a valid '%1$s' property to access this server\",\n          \n          \n            \n                                    String.format(\"Configure a valid value for '%s' property for this server to access the filesystem\",", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498550944", "createdAt": "2020-10-01T22:55:35Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -26,20 +26,24 @@ public String getDataUri(Configuration configuration, RequestContext context) {\n     },\n     FILE {\n         @Override\n-        public String getDataUri(Configuration configuration, RequestContext context) {\n-            throw new IllegalStateException(\"core-site.xml is missing or using unsupported file:// as default filesystem\");\n-        }\n+        protected String validateAndNormalizeBasePath(String basePath) {\n+            if (StringUtils.isBlank(basePath))\n+                throw new IllegalArgumentException(\n+                        String.format(\"the '%1$s' configuration is required to access locally mounted file systems. Configure a valid '%1$s' property to access this server\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMTEzMDM2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMzowMzo0OFrOHbdRCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMzoxOTozMVrOHbdg5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU1MzA5OA==", "bodyText": "normalized base path already has \"/\" at the end, unless it is empty, so will you end up with \"//\" here ?", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498553098", "createdAt": "2020-10-01T23:03:48Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -212,20 +234,34 @@ protected String getDataUriForPrefix(Configuration configuration, String dataSou\n         URI defaultFS = FileSystem.getDefaultUri(configuration);\n \n         String uri;\n-        String normalizedDataSource = normalizeDataSource(dataSource);\n+        String basePath = validateAndNormalizeBasePath(configuration.get(CONFIG_KEY_BASE_PATH));\n+        String normalizedDataSource = validateAndNormalizeDataSource(dataSource);\n \n         if (FILE_SCHEME.equals(defaultFS.getScheme())) {\n             // if the defaultFS is file://, but enum is not FILE, use enum scheme only\n-            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + basePath + normalizedDataSource;\n         } else {\n             // if the defaultFS is not file://, use it, instead of enum scheme and append user's path\n-            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + \"/\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + basePath + \"/\" + normalizedDataSource;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU1NzE1OA==", "bodyText": "yes, this is incorrect", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r498557158", "createdAt": "2020-10-01T23:19:31Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsType.java", "diffHunk": "@@ -212,20 +234,34 @@ protected String getDataUriForPrefix(Configuration configuration, String dataSou\n         URI defaultFS = FileSystem.getDefaultUri(configuration);\n \n         String uri;\n-        String normalizedDataSource = normalizeDataSource(dataSource);\n+        String basePath = validateAndNormalizeBasePath(configuration.get(CONFIG_KEY_BASE_PATH));\n+        String normalizedDataSource = validateAndNormalizeDataSource(dataSource);\n \n         if (FILE_SCHEME.equals(defaultFS.getScheme())) {\n             // if the defaultFS is file://, but enum is not FILE, use enum scheme only\n-            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(scheme, \"://\") + \"://\" + basePath + normalizedDataSource;\n         } else {\n             // if the defaultFS is not file://, use it, instead of enum scheme and append user's path\n-            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + \"/\" + normalizedDataSource;\n+            uri = StringUtils.removeEnd(defaultFS.toString(), \"/\") + basePath + \"/\" + normalizedDataSource;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODU1MzA5OA=="}, "originalCommit": {"oid": "e9a76fcd23c4d968da812105d435dd1a54be4ac0"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODA5ODg2OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1Nzo1MlrOHd84Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNzoxMDoxNFrOHd9WBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODE5OA==", "bodyText": "ADL here is not needed, I guess copy & paste", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168198", "createdAt": "2020-10-07T16:57:52Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE3NTgxMg==", "bodyText": "good catch! thanks", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501175812", "createdAt": "2020-10-07T17:10:14Z", "author": {"login": "frankgh"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODE5OA=="}, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODA5OTc4OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1ODowM1rOHd84zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1ODowM1rOHd84zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODMzNQ==", "bodyText": "same", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168335", "createdAt": "2020-10-07T16:58:03Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODEwMTM5OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1ODoyN1rOHd855g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1ODoyN1rOHd855g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODYxNA==", "bodyText": "HDFS here not needed", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168614", "createdAt": "2020-10-07T16:58:27Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODEwMjc5OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1ODo0NlrOHd86tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo1ODo0NlrOHd86tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2ODgyMw==", "bodyText": "Google here", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501168823", "createdAt": "2020-10-07T16:58:46Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/main/resources/pxf-profiles-default.xml", "diffHunk": "@@ -749,4 +749,75 @@ under the License.\n         </plugins>\n         <protocol>wasbs</protocol>\n     </profile>\n+\n+    <!-- File profiles for file systems mounted on the Greenplum cluster -->\n+    <profile>\n+        <name>file:text</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text, tab-delimited, files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:csv</name>\n+        <description>This profile is suitable for using when reading delimited single line records\n+            from plain text CSV files on Azure Data Lake\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsDataFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.LineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:text:multi</name>\n+        <description>This profile is suitable for using when reading delimited single or multi line\n+            records (with quoted linefeeds) from plain text files on HDFS. It is not splittable (non\n+            parallel) and slower than HdfsTextSimple.\n+        </description>\n+        <plugins>\n+            <fragmenter>org.greenplum.pxf.plugins.hdfs.HdfsFileFragmenter</fragmenter>\n+            <accessor>org.greenplum.pxf.plugins.hdfs.QuotedLineBreakAccessor</accessor>\n+            <resolver>org.greenplum.pxf.plugins.hdfs.StringPassResolver</resolver>\n+        </plugins>\n+    </profile>\n+    <profile>\n+        <name>file:parquet</name>\n+        <description>A profile for reading and writing Parquet data from Google Cloud Storage", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODExODU1OnYy", "diffSide": "RIGHT", "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNzowMjo0NVrOHd9Ehw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNzowMjo0NVrOHd9Ehw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE3MTMzNQ==", "bodyText": "just a thought whether pxf.fs.basePath is the best user-visible name we can come up with. Not sure whether pxf.file.base is any better. In reality, it seems this property will impact other profiles like hdfs, s3, etc (although it is not mandatory there)", "url": "https://github.com/greenplum-db/pxf/pull/456#discussion_r501171335", "createdAt": "2020-10-07T17:02:45Z", "author": {"login": "denalex"}, "path": "server/pxf-service/src/templates/user/templates/pxf-site.xml", "diffHunk": "@@ -35,4 +35,16 @@\n         </description>\n     </property>\n     -->\n+\n+    <!--\n+    <property>\n+        <name>pxf.fs.basePath</name>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540d5e085b5db11cd1bc8a45ef60cc95bd245916"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3482, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}