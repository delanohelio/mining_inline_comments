{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2ODQ0NTgw", "number": 389, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjozODoxNVrOEG-OPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODoyNTo0NVrOEJOdrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ2MzY3OnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjozODoxNVrOGmGLxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMTo0NzowMlrOGmh-ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDM4OA==", "bodyText": "not sure what this does, but we don't want to write to stderr from the app", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600388", "createdAt": "2020-06-19T02:38:15Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);\n+\n+        try (PrintStream printStream = new PrintStream(new FileOutputStream(new File(pathToMetadata)))) {\n+            tool.run(null, printStream, System.err, args);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NTcxNw==", "bodyText": "Thanks for the review. This is used in automation test for Writable Avro tables, in which we write the contents of PXF-generated Avro data/metadata into files to make sure that what ends up in external is what we think (avoids potential garbage in/garbage out problem when testing with writable/readable tables).\nThe API for these Avro tools takes a stream for stdout and stderr, but mostly we care about stdout in this case which will go to a file.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443055717", "createdAt": "2020-06-19T21:47:02Z", "author": {"login": "oliverralbertini"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);\n+\n+        try (PrintStream printStream = new PrintStream(new FileOutputStream(new File(pathToMetadata)))) {\n+            tool.run(null, printStream, System.err, args);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDM4OA=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ2NDgwOnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjozOTowMlrOGmGMaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMTo0ODoxMFrOGmh_1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDU1NQ==", "bodyText": "is Tool thread safe and stateless ? If yes, we should be using a single (static) instance of it", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600555", "createdAt": "2020-06-19T02:39:02Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NjA4Nw==", "bodyText": "Since it's not being used in the app, we can ignore that.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443056087", "createdAt": "2020-06-19T21:48:10Z", "author": {"login": "oliverralbertini"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDU1NQ=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ2Njc1OnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0MDozM1rOGmGNyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMTo1MDoyN1rOGmiCIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDkwNg==", "bodyText": "List<String> args = Arrays.asList(pathToFile)", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442600906", "createdAt": "2020-06-19T02:40:33Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NjY3NQ==", "bodyText": "with editor help, ended up with\nList<String> args = Collections.singletonList(pathToFile);", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443056675", "createdAt": "2020-06-19T21:50:27Z", "author": {"login": "oliverralbertini"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/components/hdfs/Hdfs.java", "diffHunk": "@@ -365,6 +366,17 @@ public void writeJsonFileFromAvro(String pathToFile, String pathToJson)\n         }\n     }\n \n+    public void writeAvroMetadata(String pathToFile, String pathToMetadata)\n+            throws Exception {\n+        Tool tool = new DataFileGetMetaTool();\n+        List<String> args = new ArrayList<>();\n+        args.add(pathToFile);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMDkwNg=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ3MDg1OnYy", "diffSide": "RIGHT", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0MzowM1rOGmGQOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMTo0ODozM1rOGmiARA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTUyOA==", "bodyText": "what's the point, it will throw NPE in either case (before and after the change) ? This is mostly used for checking input params to the functions are not nulls.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442601528", "createdAt": "2020-06-19T02:43:03Z", "author": {"login": "denalex"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -60,7 +61,7 @@\n     public void beforeClass() throws Exception {\n         // path for storing data on HDFS (for processing by PXF)\n         hdfsPath = hdfs.getWorkingDirectory() + \"/writableAvro/\";\n-        String absolutePath = getClass().getClassLoader().getResource(\"data\").getPath();\n+        String absolutePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"data\")).getPath();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1NjE5Ng==", "bodyText": "yeah, this was suggested by the editor for some reason.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443056196", "createdAt": "2020-06-19T21:48:33Z", "author": {"login": "oliverralbertini"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -60,7 +61,7 @@\n     public void beforeClass() throws Exception {\n         // path for storing data on HDFS (for processing by PXF)\n         hdfsPath = hdfs.getWorkingDirectory() + \"/writableAvro/\";\n-        String absolutePath = getClass().getClassLoader().getResource(\"data\").getPath();\n+        String absolutePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"data\")).getPath();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTUyOA=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ3MzA0OnYy", "diffSide": "RIGHT", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0NDoyNlrOGmGRgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0NDoyNlrOGmGRgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMTg1Ng==", "bodyText": "should we use the value uncompressed (like in other profiles, I believe) instead of null ?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442601856", "createdAt": "2020-06-19T02:44:26Z", "author": {"login": "denalex"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/avro/HdfsWritableAvroTest.java", "diffHunk": "@@ -96,12 +97,43 @@ public void generateSchemaPrimitive() throws Exception {\n         publicStage += \"generateSchemaPrimitive/\";\n         // fetch all the segment-generated avro files and make them into json records\n         // confirm that the lines generated by the segments match what we expect\n-        fetchAndVerifyAvroHcfsFiles(\"primitives.json\");\n+        fetchAndVerifyAvroHcfsFiles(\"primitives.json\", \"deflate\");\n \n         // check using GPDB readable external table that what went into HCFS is correct\n         runTincTest(\"pxf.features.hdfs.writable.avro.primitives_generate_schema.runTest\");\n     }\n \n+    @Test(groups = {\"features\", \"gpdb\", \"hcfs\", \"security\"})\n+    public void generateSchemaPrimitive_withNoCompression() throws Exception {\n+        gpdbTable = \"writable_avro_primitive_no_compression\";\n+        fullTestPath = hdfsPath + \"generate_schema_primitive_types_with_no_compression\";\n+        exTable = new WritableExternalTable(gpdbTable + \"_writable\", avroPrimitiveTableCols, fullTestPath, \"custom\");\n+        exTable.setHost(pxfHost);\n+        exTable.setPort(pxfPort);\n+        exTable.setFormatter(\"pxfwritable_export\");\n+        exTable.setProfile(protocol.value() + \":avro\");\n+        exTable.setUserParameters(new String[]{\"COMPRESSION_CODEC=null\"});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ3ODIxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0Nzo0M1rOGmGUmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0Nzo0M1rOGmGUmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMjY0OQ==", "bodyText": "I suggest to make all these as constants", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442602649", "createdAt": "2020-06-19T02:47:43Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ4MDMxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0OTowNlrOGmGV-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo0OTowNlrOGmGV-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzAwMQ==", "bodyText": "the default case should handle all unsupported ones and throw exception, while no-compression case should be one of the explicit choices above", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442603001", "createdAt": "2020-06-19T02:49:06Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":\n+                writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n+                break;\n+            case \"xz\":\n+                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");\n+                // writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));\n+                // break;\n+            case \"bzip2\":\n+                throw new RuntimeException(\"Avro Compression codec bzip2 currently not supported\");\n+                // writer.setCodec(CodecFactory.bzip2Codec());\n+                // break;\n+            case \"snappy\":\n+                writer.setCodec(CodecFactory.snappyCodec());\n+                break;\n+            default:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NzQ4MzIwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMjo1MTowM1rOGmGXwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMjoyNDoyM1rOGmihng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw==", "bodyText": "I don't like going back to profile and relying on its name, let's think if this can be done in a better way", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442603457", "createdAt": "2020-06-19T02:51:03Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NTk4Mw==", "bodyText": "yeah, this needs some more thought. But I don't see where else the information might come from, without accessing the file. I believe this happens at the beginning of the bridge call to determine if the access will need locking", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442845983", "createdAt": "2020-06-19T13:38:46Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2NDczNA==", "bodyText": "Simplest approach is just to make sure that COMPRESSION_CODEC != \"bzip2\", this works regardless of the type of data (Avro, Parquet, or TEXT).", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443064734", "createdAt": "2020-06-19T22:24:23Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYwMzQ1Nw=="}, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODk4ODIyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozMzoyNFrOGmU_iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozMzoyNFrOGmU_iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MzAxNg==", "bodyText": "why are these not supported? sometimes some compressions are supported only when you have hadoop native tools in your ld_library path, which is a likely scenario in customer environments", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442843016", "createdAt": "2020-06-19T13:33:24Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -138,6 +142,28 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n+        switch (codec) {\n+            case \"deflate\":\n+                writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n+                break;\n+            case \"xz\":\n+                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1OTAwNTI3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozODoxM1rOGmVKBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozODoxM1rOGmVKBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NTcwMA==", "bodyText": "Let's avoid using regex. This will fail for legacy profiles ie Avro. This will also fail if the user creates the table as\ncreate external table avro ()\nlocation('pxf://foo?PROFILE=HDFS:aVrO')\nformat ....\n\nIt's not a case sensitive comparison", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r442845700", "createdAt": "2020-06-19T13:38:13Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        if (context.getProfile() == null) {\n+            return true;\n+        }\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\", null);\n+        if (context.getProfile().matches(\"^.*:parquet$\")) {\n+            return isParquetCompressionThreadSafe(compCodec);\n+        }\n+        if (context.getProfile().matches(\"^.*:avro$\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f8ed4f1eeb016af64d603c84c31dc408918f515"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MDQ1NDY2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzoyMzozMVrOGmjNwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMzoyMzozMVrOGmjNwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3NjAzNA==", "bodyText": "should we call it NO_CODEC?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443076034", "createdAt": "2020-06-19T23:23:31Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java", "diffHunk": "@@ -142,26 +147,20 @@ public OneRow readNextObject() throws IOException {\n     public boolean openForWrite() throws Exception {\n         // make writer\n         writer = new DataFileWriter<>(new GenericDatumWriter<>(schema));\n-        String codec = context.getOption(COMPRESSION_CODEC_OPTION, \"deflate\");\n+        String codec = context.getOption(COMPRESSION_CODEC_OPTION, DEFLATE_CODEC).toLowerCase();\n         int codecCompressionLevel = context.getOption(CODEC_COMPRESSION_LEVEL_OPTION, DEFAULT_CODEC_COMPRESSION_LEVEL);\n         switch (codec) {\n-            case \"deflate\":\n+            case DEFLATE_CODEC:\n                 writer.setCodec(CodecFactory.deflateCodec(codecCompressionLevel));\n                 break;\n-            case \"xz\":\n-                throw new RuntimeException(\"Avro Compression codec xz currently not supported\");\n-                // writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));\n-                // break;\n-            case \"bzip2\":\n-                throw new RuntimeException(\"Avro Compression codec bzip2 currently not supported\");\n-                // writer.setCodec(CodecFactory.bzip2Codec());\n-                // break;\n-            case \"snappy\":\n+            case SNAPPY_CODEC:\n                 writer.setCodec(CodecFactory.snappyCodec());\n                 break;\n-            default:\n+            case NULL_CODEC:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MDYwNDg3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQwMzoyNTo0MFrOGmkfDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQxNjoxOToxNlrOGmnQrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5Njg0Ng==", "bodyText": "I am not sure this logic is equivalent (or a superset) or what was removed, but it might as well be. Actually, I prefer the way comparison was done before: BZip2Codec.class.isAssignableFrom(..) instead of comparing class names. There was also getting Hadoop mapping compressionCodecName.getHadoopCompressionCodecClass() which is no longer done here.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443096846", "createdAt": "2020-06-20T03:25:40Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        // bzip2 for Avro, and BZip2Codec for Parquet\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(\"org.apache.hadoop.io.compress.BZip2Codec\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE0MjMxNw==", "bodyText": "That brings us right back around to the Parquet vs Avro issue. How do we know which type of compression we are dealing with?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443142317", "createdAt": "2020-06-20T16:19:16Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        // bzip2 for Avro, and BZip2Codec for Parquet\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(\"org.apache.hadoop.io.compress.BZip2Codec\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5Njg0Ng=="}, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MDYwNTM4OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQwMzoyNzowNlrOGmkfTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQxNjoxNjo1M1rOGmnQGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5NjkxMA==", "bodyText": "what if datasource points to directory full of bz2 files (and not an individual file), will this detect it ?", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443096910", "createdAt": "2020-06-20T03:27:06Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE0MjE3MA==", "bodyText": "Good catch, no it will not. This isn't an issue with my change, however, this was always the case. On the other hand, I think that the Accessor stage in PXF should only see files, not directories. If this were happening during fragmentation of the external resource, it would be more concerning.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r443142170", "createdAt": "2020-06-20T16:16:53Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        String compCodec = context.getOption(\"COMPRESSION_CODEC\");\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Parquet only)\n+            Class<? extends CompressionCodec> codecClass = codecFactory.getCodecClassByPath(configuration, context.getDataSource());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5NjkxMA=="}, "originalCommit": {"oid": "5f0181f7d17d38d69ce10e362f5661585d5bf948"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTA3NDYxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/CodecFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODoxNzo0MlrOGpqZwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODo1NTowMFrOGpralg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzOTUyMg==", "bodyText": "I was suggesting to collapse into 1 return statement, like this:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (compCodec == null) {\n          \n          \n            \n                        // check for file extensions indicating bzip2 (Text only)\n          \n          \n            \n                        // currently doesn't check for bzip2 in .avro files\n          \n          \n            \n                        Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n          \n          \n            \n                        return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n          \n          \n            \n                    }\n          \n          \n            \n                    return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());\n          \n          \n            \n                    Class<? extends CompressionCodec> codecClass = null;\n          \n          \n            \n                    if (compCodec == null) {\n          \n          \n            \n                        // check for file extensions indicating bzip2 (Text only)\n          \n          \n            \n                        // currently doesn't check for bzip2 in .avro files\n          \n          \n            \n                        codecClass = getCodecClassByPath(configuration, dataSource);\n          \n          \n            \n                    }\n          \n          \n            \n                    return !( \"bzip2\".equalsIgnoreCase(compCodec) ||\n          \n          \n            \n                                   BZip2Codec.class.getName().equalsIgnoreCase(compCodec) ||\n          \n          \n            \n                                   (codecClass != null && BZip2Codec.class.isAssignableFrom(codecClass))\n          \n          \n            \n                                  );", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446339522", "createdAt": "2020-06-26T18:17:42Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/CodecFactory.java", "diffHunk": "@@ -80,6 +81,24 @@ public CompressionCodec getCodec(String name, Configuration conf) {\n         return codecClass;\n     }\n \n+    /**\n+     * Determine whether a given compression codec is safe for multiple concurrent threads\n+     *\n+     * @param compCodec     the user-given COMPRESSION_CODEC, may be null\n+     * @param dataSource    the file that we are accessing\n+     * @param configuration HDFS config\n+     * @return true only if it's thread safe\n+     */\n+    public boolean isCodecThreadSafe(String compCodec, String dataSource, Configuration configuration) {\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Text only)\n+            // currently doesn't check for bzip2 in .avro files\n+            Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1NjExOA==", "bodyText": "This looks equivalent, I will give it a try.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446356118", "createdAt": "2020-06-26T18:55:00Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/CodecFactory.java", "diffHunk": "@@ -80,6 +81,24 @@ public CompressionCodec getCodec(String name, Configuration conf) {\n         return codecClass;\n     }\n \n+    /**\n+     * Determine whether a given compression codec is safe for multiple concurrent threads\n+     *\n+     * @param compCodec     the user-given COMPRESSION_CODEC, may be null\n+     * @param dataSource    the file that we are accessing\n+     * @param configuration HDFS config\n+     * @return true only if it's thread safe\n+     */\n+    public boolean isCodecThreadSafe(String compCodec, String dataSource, Configuration configuration) {\n+        if (compCodec == null) {\n+            // check for file extensions indicating bzip2 (Text only)\n+            // currently doesn't check for bzip2 in .avro files\n+            Class<? extends CompressionCodec> codecClass = getCodecClassByPath(configuration, dataSource);\n+            return (codecClass == null || !BZip2Codec.class.isAssignableFrom(codecClass));\n+        }\n+        return !compCodec.equalsIgnoreCase(\"bzip2\") && !compCodec.equalsIgnoreCase(BZip2Codec.class.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzOTUyMg=="}, "originalCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTA5NjEzOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODoyNTo0NVrOGpqnkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODo0ODo1MlrOGprPrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MzA1Ng==", "bodyText": "should we have context.getCompressionCodec() method not to deal with strings here ? That method can also deal with short-name vs. fully qualified class (if needed).", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446343056", "createdAt": "2020-06-26T18:25:45Z", "author": {"login": "denalex"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        return codecFactory.isCodecThreadSafe(context.getOption(\"COMPRESSION_CODEC\"), context.getDataSource(), configuration);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1MzMyNA==", "bodyText": "This would be a great thing to tackle in #393 . That's where we start to allow short names in the text profiles.", "url": "https://github.com/greenplum-db/pxf/pull/389#discussion_r446353324", "createdAt": "2020-06-26T18:48:52Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HcfsBaseAccessor.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.hadoop.io.compress.BZip2Codec;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.BasePlugin;\n+\n+public abstract class HcfsBaseAccessor extends BasePlugin implements Accessor {\n+    private static final CodecFactory codecFactory = CodecFactory.getInstance();\n+\n+    /**\n+     * Checks if requests should be handled in a single thread or not.\n+     *\n+     * @return if the request can be run in multi-threaded mode.\n+     */\n+    @Override\n+    public boolean isThreadSafe() {\n+        return codecFactory.isCodecThreadSafe(context.getOption(\"COMPRESSION_CODEC\"), context.getDataSource(), configuration);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MzA1Ng=="}, "originalCommit": {"oid": "5e20e30348703dabfd80788ed668f1caac8558aa"}, "originalPosition": 18}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3688, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}