{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU5NzI2OTcx", "number": 286, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMDoyNDozMVrODaBVMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTo1MjoxNFrODaCLUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjExMzc3OnYy", "diffSide": "RIGHT", "path": "concourse/scripts/pxf-perf-multi-node.bash", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMDoyNDozMVrOFgvhew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowNTozNFrOFgxvsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg3NzM3MQ==", "bodyText": "Seems that we weren't using run_id previously, but now we are. What's the reason?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369877371", "createdAt": "2020-01-23T00:24:31Z", "author": {"login": "oliverralbertini"}, "path": "concourse/scripts/pxf-perf-multi-node.bash", "diffHunk": "@@ -205,10 +205,20 @@ EOF\n     sync_configuration\n }\n \n-function create_gcs_tables() {\n+function create_gcs_text_tables() {\n     local name=${1}\n+    local run_id=${2}\n+    # create text tables\n     readable_external_table_text_query \"${name}\" \"pxf://data-gpdb-ud-tpch/${SCALE}/lineitem_data/?PROFILE=gs:text&SERVER=gsbenchmark\"\n-    writable_external_table_text_query \"${name}\" \"pxf://data-gpdb-ud-pxf-benchmark/output/${SCALE}/${UUID}/?PROFILE=gs:text&SERVER=gsbenchmark\"\n+    writable_external_table_text_query \"${name}\" \"pxf://data-gpdb-ud-pxf-benchmark/output/${SCALE}/${UUID}-${run_id}/?PROFILE=gs:text&SERVER=gsbenchmark\"\n+}\n+\n+function create_gcs_parquet_tables() {\n+    local name=${1}\n+    local run_id=${2}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMzc3OQ==", "bodyText": "we didn't run concurrency for GCS, but this allows us to run concurrency. The run_id is used to get a unique path for the execution of multiple queries", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369913779", "createdAt": "2020-01-23T03:05:34Z", "author": {"login": "frankgh"}, "path": "concourse/scripts/pxf-perf-multi-node.bash", "diffHunk": "@@ -205,10 +205,20 @@ EOF\n     sync_configuration\n }\n \n-function create_gcs_tables() {\n+function create_gcs_text_tables() {\n     local name=${1}\n+    local run_id=${2}\n+    # create text tables\n     readable_external_table_text_query \"${name}\" \"pxf://data-gpdb-ud-tpch/${SCALE}/lineitem_data/?PROFILE=gs:text&SERVER=gsbenchmark\"\n-    writable_external_table_text_query \"${name}\" \"pxf://data-gpdb-ud-pxf-benchmark/output/${SCALE}/${UUID}/?PROFILE=gs:text&SERVER=gsbenchmark\"\n+    writable_external_table_text_query \"${name}\" \"pxf://data-gpdb-ud-pxf-benchmark/output/${SCALE}/${UUID}-${run_id}/?PROFILE=gs:text&SERVER=gsbenchmark\"\n+}\n+\n+function create_gcs_parquet_tables() {\n+    local name=${1}\n+    local run_id=${2}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg3NzM3MQ=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjEzODUyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMDo0MDoyNFrOFgvwzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowNzoxNlrOFgxxEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4MTI5NA==", "bodyText": "Can we call this something else? I am thinking columnProjectedSchema.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369881294", "createdAt": "2020-01-23T00:40:24Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -86,53 +98,66 @@\n         }\n     }\n \n-    private ParquetFileReader fileReader;\n-    private MessageColumnIO columnIO;\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS = EnumSet.of(\n+            Operator.NOOP,\n+            Operator.LESS_THAN,\n+            Operator.GREATER_THAN,\n+            Operator.LESS_THAN_OR_EQUAL,\n+            Operator.GREATER_THAN_OR_EQUAL,\n+            Operator.EQUALS,\n+            Operator.NOT_EQUALS,\n+            Operator.IS_NULL,\n+            Operator.IS_NOT_NULL,\n+            // Operator.IN,\n+            Operator.OR,\n+            Operator.AND,\n+            Operator.NOT\n+    );\n+\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    private ParquetReader<Group> fileReader;\n     private CompressionCodecName codecName;\n     private ParquetWriter<Group> parquetWriter;\n-    private RecordReader<Group> recordReader;\n-    private GroupRecordConverter groupRecordConverter;\n     private GroupWriteSupport groupWriteSupport;\n     private FileSystem fs;\n     private Path file;\n     private String filePrefix;\n-    private int fileIndex, pageSize, rowgroupSize, dictionarySize;\n+    private int fileIndex, pageSize, rowGroupSize, dictionarySize;\n     private long rowsRead, rowsWritten, totalRowsRead, totalRowsWritten;\n-    private long rowsInRowGroup, rowGroupsReadCount;\n     private WriterVersion parquetVersion;\n     private CodecFactory codecFactory = CodecFactory.getInstance();\n \n+    private long totalReadTimeInNanos;\n+\n     /**\n      * Opens the resource for read.\n      *\n      * @throws IOException if opening the resource failed\n      */\n     @Override\n     public boolean openForRead() throws IOException {\n-        MessageType schema, readSchema;\n-\n         file = new Path(context.getDataSource());\n         FileSplit fileSplit = HdfsUtilities.parseFileSplit(context);\n-        // Create reader for a given split, read a range in file\n-        MetadataFilter filter = ParquetMetadataConverter.range(\n-                fileSplit.getStart(), fileSplit.getStart() + fileSplit.getLength());\n-        fileReader = new ParquetFileReader(configuration, file, filter);\n-        try {\n-            ParquetMetadata metadata = fileReader.getFooter();\n-            schema = metadata.getFileMetaData().getSchema();\n-            readSchema = buildReadSchema(schema);\n \n-            columnIO = new ColumnIOFactory().getColumnIO(readSchema, schema);\n-            groupRecordConverter = new GroupRecordConverter(readSchema);\n-            if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"Reading file {} with {} records in {} rowgroups\",\n-                        file.getName(), fileReader.getRecordCount(),\n-                        fileReader.getRowGroups().size());\n-            }\n-        } catch (Exception e) {\n-            fileReader.close();\n-            throw new IOException(e);\n-        }\n+        // Read the original schema from the parquet file\n+        MessageType originalSchema = getSchema(file, fileSplit);\n+        // Get a map of the column name to Types for the given schema\n+        Map<String, Type> originalFieldsMap = getOriginalFieldsMap(originalSchema);\n+        // Get the read schema in case of column projection\n+        MessageType readSchema = buildReadSchema(originalFieldsMap, originalSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDEzMA==", "bodyText": "I'd prefer to keep it as readSchema because it's the schema we are reading. If there's no projection, it will be end up being all the columns of the Greenplum schema. I realize that the comment might not be accurate though. What do you think?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369914130", "createdAt": "2020-01-23T03:07:16Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -86,53 +98,66 @@\n         }\n     }\n \n-    private ParquetFileReader fileReader;\n-    private MessageColumnIO columnIO;\n+    public static final EnumSet<Operator> SUPPORTED_OPERATORS = EnumSet.of(\n+            Operator.NOOP,\n+            Operator.LESS_THAN,\n+            Operator.GREATER_THAN,\n+            Operator.LESS_THAN_OR_EQUAL,\n+            Operator.GREATER_THAN_OR_EQUAL,\n+            Operator.EQUALS,\n+            Operator.NOT_EQUALS,\n+            Operator.IS_NULL,\n+            Operator.IS_NOT_NULL,\n+            // Operator.IN,\n+            Operator.OR,\n+            Operator.AND,\n+            Operator.NOT\n+    );\n+\n+    private static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    private ParquetReader<Group> fileReader;\n     private CompressionCodecName codecName;\n     private ParquetWriter<Group> parquetWriter;\n-    private RecordReader<Group> recordReader;\n-    private GroupRecordConverter groupRecordConverter;\n     private GroupWriteSupport groupWriteSupport;\n     private FileSystem fs;\n     private Path file;\n     private String filePrefix;\n-    private int fileIndex, pageSize, rowgroupSize, dictionarySize;\n+    private int fileIndex, pageSize, rowGroupSize, dictionarySize;\n     private long rowsRead, rowsWritten, totalRowsRead, totalRowsWritten;\n-    private long rowsInRowGroup, rowGroupsReadCount;\n     private WriterVersion parquetVersion;\n     private CodecFactory codecFactory = CodecFactory.getInstance();\n \n+    private long totalReadTimeInNanos;\n+\n     /**\n      * Opens the resource for read.\n      *\n      * @throws IOException if opening the resource failed\n      */\n     @Override\n     public boolean openForRead() throws IOException {\n-        MessageType schema, readSchema;\n-\n         file = new Path(context.getDataSource());\n         FileSplit fileSplit = HdfsUtilities.parseFileSplit(context);\n-        // Create reader for a given split, read a range in file\n-        MetadataFilter filter = ParquetMetadataConverter.range(\n-                fileSplit.getStart(), fileSplit.getStart() + fileSplit.getLength());\n-        fileReader = new ParquetFileReader(configuration, file, filter);\n-        try {\n-            ParquetMetadata metadata = fileReader.getFooter();\n-            schema = metadata.getFileMetaData().getSchema();\n-            readSchema = buildReadSchema(schema);\n \n-            columnIO = new ColumnIOFactory().getColumnIO(readSchema, schema);\n-            groupRecordConverter = new GroupRecordConverter(readSchema);\n-            if (LOG.isDebugEnabled()) {\n-                LOG.debug(\"Reading file {} with {} records in {} rowgroups\",\n-                        file.getName(), fileReader.getRecordCount(),\n-                        fileReader.getRowGroups().size());\n-            }\n-        } catch (Exception e) {\n-            fileReader.close();\n-            throw new IOException(e);\n-        }\n+        // Read the original schema from the parquet file\n+        MessageType originalSchema = getSchema(file, fileSplit);\n+        // Get a map of the column name to Types for the given schema\n+        Map<String, Type> originalFieldsMap = getOriginalFieldsMap(originalSchema);\n+        // Get the read schema in case of column projection\n+        MessageType readSchema = buildReadSchema(originalFieldsMap, originalSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4MTI5NA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjE0MjEwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMDo0MjozNFrOFgvzAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzowODowMlrOFgxxmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4MTg1Nw==", "bodyText": "this is a nice improvement in the accessor logic .", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369881857", "createdAt": "2020-01-23T00:42:34Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -145,36 +170,16 @@ public boolean openForRead() throws IOException {\n      */\n     @Override\n     public OneRow readNextObject() throws IOException {\n-\n-        if (rowsRead == rowsInRowGroup && !readNextRowGroup())\n-            return null;\n-        Group group = recordReader.read();\n-        rowsRead++;\n-        return new OneRow(null, group);\n-    }\n-\n-    private boolean readNextRowGroup() throws IOException {\n-\n-        PageReadStore currentRowGroup = fileReader.readNextRowGroup();\n-        if (currentRowGroup == null) {\n-            LOG.debug(\"All rowgroups have been exhausted for {}\", file.getName());\n-            return false;\n+        final long then = System.nanoTime();\n+        Group group = fileReader.read();\n+        final long nanos = System.nanoTime() - then;\n+        totalReadTimeInNanos += nanos;\n+\n+        if (group != null) {\n+            rowsRead++;\n+            return new OneRow(null, group);\n         }\n-\n-        rowGroupsReadCount++;\n-        totalRowsRead += rowsRead;\n-        // Reset rows read\n-        rowsRead = 0;\n-        recordReader = columnIO.getRecordReader(currentRowGroup, groupRecordConverter);\n-        rowsInRowGroup = currentRowGroup.getRowCount();\n-\n-        LOG.debug(\"Reading {} rows (rowgroup {})\", rowsInRowGroup, rowGroupsReadCount);\n-        return true;\n-    }\n-\n-    private int getOption(String optionName, int defaultValue) {\n-        String optionStr = context.getOption(optionName);\n-        return optionStr != null ? Integer.parseInt(optionStr) : defaultValue;\n+        return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDI2NQ==", "bodyText": "definitely, the new Parquet APIs help!", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369914265", "createdAt": "2020-01-23T03:08:02Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -145,36 +170,16 @@ public boolean openForRead() throws IOException {\n      */\n     @Override\n     public OneRow readNextObject() throws IOException {\n-\n-        if (rowsRead == rowsInRowGroup && !readNextRowGroup())\n-            return null;\n-        Group group = recordReader.read();\n-        rowsRead++;\n-        return new OneRow(null, group);\n-    }\n-\n-    private boolean readNextRowGroup() throws IOException {\n-\n-        PageReadStore currentRowGroup = fileReader.readNextRowGroup();\n-        if (currentRowGroup == null) {\n-            LOG.debug(\"All rowgroups have been exhausted for {}\", file.getName());\n-            return false;\n+        final long then = System.nanoTime();\n+        Group group = fileReader.read();\n+        final long nanos = System.nanoTime() - then;\n+        totalReadTimeInNanos += nanos;\n+\n+        if (group != null) {\n+            rowsRead++;\n+            return new OneRow(null, group);\n         }\n-\n-        rowGroupsReadCount++;\n-        totalRowsRead += rowsRead;\n-        // Reset rows read\n-        rowsRead = 0;\n-        recordReader = columnIO.getRecordReader(currentRowGroup, groupRecordConverter);\n-        rowsInRowGroup = currentRowGroup.getRowCount();\n-\n-        LOG.debug(\"Reading {} rows (rowgroup {})\", rowsInRowGroup, rowGroupsReadCount);\n-        return true;\n-    }\n-\n-    private int getOption(String optionName, int defaultValue) {\n-        String optionStr = context.getOption(optionName);\n-        return optionStr != null ? Integer.parseInt(optionStr) : defaultValue;\n+        return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4MTg1Nw=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjE0NTY1OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMDo0NDo1N1rOFgv1Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMToyOFrOFgxztg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4MjQyNg==", "bodyText": "It might be time to start thinking about how to make this logging more generic. I have something similar in the Avro code.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369882426", "createdAt": "2020-01-23T00:44:57Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -186,7 +191,19 @@ private int getOption(String optionName, int defaultValue) {\n     public void closeForRead() throws IOException {\n \n         totalRowsRead += rowsRead;\n-        LOG.debug(\"Read TOTAL of {} rows in {} rowgroups\", totalRowsRead, rowGroupsReadCount);\n+\n+        if (LOG.isDebugEnabled()) {\n+            final long millis = TimeUnit.NANOSECONDS.toMillis(totalReadTimeInNanos);\n+            long average = totalReadTimeInNanos / totalRowsRead;\n+            LOG.debug(\"{}-{}: Read TOTAL of {} rows from file {} on server {} in {} ms. Average speed: {} nanoseconds\",\n+                    context.getTransactionId(),\n+                    context.getSegmentId(),\n+                    totalRowsRead,\n+                    context.getDataSource(),\n+                    context.getServerName(),\n+                    millis,\n+                    average);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDgwNg==", "bodyText": "yeah, I've been thinking about this. This definitely helps debug things when you have a large greenplum cluster with concurrent queries. We definitely need better instrumentation to trace the source of the error.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369914806", "createdAt": "2020-01-23T03:11:28Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -186,7 +191,19 @@ private int getOption(String optionName, int defaultValue) {\n     public void closeForRead() throws IOException {\n \n         totalRowsRead += rowsRead;\n-        LOG.debug(\"Read TOTAL of {} rows in {} rowgroups\", totalRowsRead, rowGroupsReadCount);\n+\n+        if (LOG.isDebugEnabled()) {\n+            final long millis = TimeUnit.NANOSECONDS.toMillis(totalReadTimeInNanos);\n+            long average = totalReadTimeInNanos / totalRowsRead;\n+            LOG.debug(\"{}-{}: Read TOTAL of {} rows from file {} on server {} in {} ms. Average speed: {} nanoseconds\",\n+                    context.getTransactionId(),\n+                    context.getSegmentId(),\n+                    totalRowsRead,\n+                    context.getDataSource(),\n+                    context.getServerName(),\n+                    millis,\n+                    average);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4MjQyNg=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjE1Njc1OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMDo1MjowOFrOFgv8gQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxMjozNVrOFgx0cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4NDI4OQ==", "bodyText": "why do we need the if?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369884289", "createdAt": "2020-01-23T00:52:08Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -269,19 +287,94 @@ public void closeForWrite() throws IOException {\n             parquetWriter.close();\n             totalRowsWritten += rowsWritten;\n         }\n-        LOG.debug(\"Segment {}: writer closed, wrote a TOTAL of {} rows to {} on server {}\",\n+        LOG.debug(\"{}-{}: writer closed, wrote a TOTAL of {} rows to {} on server {}\",\n+                context.getTransactionId(),\n                 context.getSegmentId(),\n                 totalRowsWritten,\n                 context.getDataSource(),\n                 context.getServerName());\n     }\n \n     /**\n-     * Generates a read schema when there is column projection\n+     * Returns the parquet record filter for the given filter string\n      *\n-     * @param originalSchema the original read schema\n+     * @param filterString      the filter string\n+     * @param originalFieldsMap a map of field names to types\n+     * @param schema            the parquet schema\n+     * @return the parquet record filter for the given filter string\n      */\n-    private MessageType buildReadSchema(MessageType originalSchema) {\n+    private FilterCompat.Filter getRecordFilter(String filterString, Map<String, Type> originalFieldsMap, MessageType schema) {\n+        if (StringUtils.isBlank(filterString)) {\n+            return FilterCompat.NOOP;\n+        }\n+\n+        ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n+                context.getTupleDescription(), originalFieldsMap);\n+        TreeVisitor pruner = new SupportedParquetPrimitiveTypePruner(\n+                context.getTupleDescription(), originalFieldsMap, SUPPORTED_OPERATORS);\n+\n+        try {\n+            // Parse the filter string into a expression tree Node\n+            Node root = new FilterParser().parse(filterString);\n+            // Prune the parsed tree with valid supported operators and then\n+            // traverse the pruned tree with the ParquetRecordFilterBuilder to\n+            // produce a record filter for parquet\n+            TRAVERSER.traverse(root, pruner, filterBuilder);\n+            return filterBuilder.getRecordFilter();\n+        } catch (Exception e) {\n+            LOG.error(String.format(\"%s-%d: %s--%s Unable to generate Parquet Record Filter for filter\",\n+                    context.getTransactionId(),\n+                    context.getSegmentId(),\n+                    context.getDataSource(),\n+                    context.getFilterString()), e);\n+            return FilterCompat.NOOP;\n+        }\n+    }\n+\n+    /**\n+     * Reads the original schema from the parquet file.\n+     *\n+     * @param parquetFile the path to the parquet file\n+     * @param fileSplit   the file split we are accessing\n+     * @return the original schema from the parquet file\n+     * @throws IOException when there's an IOException while reading the schema\n+     */\n+    private MessageType getSchema(Path parquetFile, FileSplit fileSplit) throws IOException {\n+\n+        final long then = System.nanoTime();\n+        ParquetMetadataConverter.MetadataFilter filter = ParquetMetadataConverter.range(\n+                fileSplit.getStart(), fileSplit.getStart() + fileSplit.getLength());\n+        ParquetReadOptions parquetReadOptions = HadoopReadOptions\n+                .builder(configuration)\n+                .withMetadataFilter(filter)\n+                .build();\n+        HadoopInputFile inputFile = HadoopInputFile.fromPath(parquetFile, configuration);\n+        try (ParquetFileReader parquetFileReader =\n+                     ParquetFileReader.open(inputFile, parquetReadOptions)) {\n+            FileMetaData metadata = parquetFileReader.getFileMetaData();\n+            if (LOG.isDebugEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4NDU5Nw==", "bodyText": "are we worried that those get*() will be called?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369884597", "createdAt": "2020-01-23T00:53:18Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -269,19 +287,94 @@ public void closeForWrite() throws IOException {\n             parquetWriter.close();\n             totalRowsWritten += rowsWritten;\n         }\n-        LOG.debug(\"Segment {}: writer closed, wrote a TOTAL of {} rows to {} on server {}\",\n+        LOG.debug(\"{}-{}: writer closed, wrote a TOTAL of {} rows to {} on server {}\",\n+                context.getTransactionId(),\n                 context.getSegmentId(),\n                 totalRowsWritten,\n                 context.getDataSource(),\n                 context.getServerName());\n     }\n \n     /**\n-     * Generates a read schema when there is column projection\n+     * Returns the parquet record filter for the given filter string\n      *\n-     * @param originalSchema the original read schema\n+     * @param filterString      the filter string\n+     * @param originalFieldsMap a map of field names to types\n+     * @param schema            the parquet schema\n+     * @return the parquet record filter for the given filter string\n      */\n-    private MessageType buildReadSchema(MessageType originalSchema) {\n+    private FilterCompat.Filter getRecordFilter(String filterString, Map<String, Type> originalFieldsMap, MessageType schema) {\n+        if (StringUtils.isBlank(filterString)) {\n+            return FilterCompat.NOOP;\n+        }\n+\n+        ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n+                context.getTupleDescription(), originalFieldsMap);\n+        TreeVisitor pruner = new SupportedParquetPrimitiveTypePruner(\n+                context.getTupleDescription(), originalFieldsMap, SUPPORTED_OPERATORS);\n+\n+        try {\n+            // Parse the filter string into a expression tree Node\n+            Node root = new FilterParser().parse(filterString);\n+            // Prune the parsed tree with valid supported operators and then\n+            // traverse the pruned tree with the ParquetRecordFilterBuilder to\n+            // produce a record filter for parquet\n+            TRAVERSER.traverse(root, pruner, filterBuilder);\n+            return filterBuilder.getRecordFilter();\n+        } catch (Exception e) {\n+            LOG.error(String.format(\"%s-%d: %s--%s Unable to generate Parquet Record Filter for filter\",\n+                    context.getTransactionId(),\n+                    context.getSegmentId(),\n+                    context.getDataSource(),\n+                    context.getFilterString()), e);\n+            return FilterCompat.NOOP;\n+        }\n+    }\n+\n+    /**\n+     * Reads the original schema from the parquet file.\n+     *\n+     * @param parquetFile the path to the parquet file\n+     * @param fileSplit   the file split we are accessing\n+     * @return the original schema from the parquet file\n+     * @throws IOException when there's an IOException while reading the schema\n+     */\n+    private MessageType getSchema(Path parquetFile, FileSplit fileSplit) throws IOException {\n+\n+        final long then = System.nanoTime();\n+        ParquetMetadataConverter.MetadataFilter filter = ParquetMetadataConverter.range(\n+                fileSplit.getStart(), fileSplit.getStart() + fileSplit.getLength());\n+        ParquetReadOptions parquetReadOptions = HadoopReadOptions\n+                .builder(configuration)\n+                .withMetadataFilter(filter)\n+                .build();\n+        HadoopInputFile inputFile = HadoopInputFile.fromPath(parquetFile, configuration);\n+        try (ParquetFileReader parquetFileReader =\n+                     ParquetFileReader.open(inputFile, parquetReadOptions)) {\n+            FileMetaData metadata = parquetFileReader.getFileMetaData();\n+            if (LOG.isDebugEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4NDI4OQ=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 336}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDk5NQ==", "bodyText": "I assume that getRecordCount() and getRowGroups() might be expensive calls, so I prefer to avoid calling them at debug level, when debugging is not on", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369914995", "createdAt": "2020-01-23T03:12:35Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -269,19 +287,94 @@ public void closeForWrite() throws IOException {\n             parquetWriter.close();\n             totalRowsWritten += rowsWritten;\n         }\n-        LOG.debug(\"Segment {}: writer closed, wrote a TOTAL of {} rows to {} on server {}\",\n+        LOG.debug(\"{}-{}: writer closed, wrote a TOTAL of {} rows to {} on server {}\",\n+                context.getTransactionId(),\n                 context.getSegmentId(),\n                 totalRowsWritten,\n                 context.getDataSource(),\n                 context.getServerName());\n     }\n \n     /**\n-     * Generates a read schema when there is column projection\n+     * Returns the parquet record filter for the given filter string\n      *\n-     * @param originalSchema the original read schema\n+     * @param filterString      the filter string\n+     * @param originalFieldsMap a map of field names to types\n+     * @param schema            the parquet schema\n+     * @return the parquet record filter for the given filter string\n      */\n-    private MessageType buildReadSchema(MessageType originalSchema) {\n+    private FilterCompat.Filter getRecordFilter(String filterString, Map<String, Type> originalFieldsMap, MessageType schema) {\n+        if (StringUtils.isBlank(filterString)) {\n+            return FilterCompat.NOOP;\n+        }\n+\n+        ParquetRecordFilterBuilder filterBuilder = new ParquetRecordFilterBuilder(\n+                context.getTupleDescription(), originalFieldsMap);\n+        TreeVisitor pruner = new SupportedParquetPrimitiveTypePruner(\n+                context.getTupleDescription(), originalFieldsMap, SUPPORTED_OPERATORS);\n+\n+        try {\n+            // Parse the filter string into a expression tree Node\n+            Node root = new FilterParser().parse(filterString);\n+            // Prune the parsed tree with valid supported operators and then\n+            // traverse the pruned tree with the ParquetRecordFilterBuilder to\n+            // produce a record filter for parquet\n+            TRAVERSER.traverse(root, pruner, filterBuilder);\n+            return filterBuilder.getRecordFilter();\n+        } catch (Exception e) {\n+            LOG.error(String.format(\"%s-%d: %s--%s Unable to generate Parquet Record Filter for filter\",\n+                    context.getTransactionId(),\n+                    context.getSegmentId(),\n+                    context.getDataSource(),\n+                    context.getFilterString()), e);\n+            return FilterCompat.NOOP;\n+        }\n+    }\n+\n+    /**\n+     * Reads the original schema from the parquet file.\n+     *\n+     * @param parquetFile the path to the parquet file\n+     * @param fileSplit   the file split we are accessing\n+     * @return the original schema from the parquet file\n+     * @throws IOException when there's an IOException while reading the schema\n+     */\n+    private MessageType getSchema(Path parquetFile, FileSplit fileSplit) throws IOException {\n+\n+        final long then = System.nanoTime();\n+        ParquetMetadataConverter.MetadataFilter filter = ParquetMetadataConverter.range(\n+                fileSplit.getStart(), fileSplit.getStart() + fileSplit.getLength());\n+        ParquetReadOptions parquetReadOptions = HadoopReadOptions\n+                .builder(configuration)\n+                .withMetadataFilter(filter)\n+                .build();\n+        HadoopInputFile inputFile = HadoopInputFile.fromPath(parquetFile, configuration);\n+        try (ParquetFileReader parquetFileReader =\n+                     ParquetFileReader.open(inputFile, parquetReadOptions)) {\n+            FileMetaData metadata = parquetFileReader.getFileMetaData();\n+            if (LOG.isDebugEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4NDI4OQ=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 336}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjE3MDY0OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTowMDoyMVrOFgwE6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxNToxNFrOFgx2Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4NjQ0MQ==", "bodyText": "How do we know all the nodes have been visited?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369886441", "createdAt": "2020-01-23T01:00:21Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilder.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.filter2.predicate.FilterApi;\n+import org.apache.parquet.filter2.predicate.FilterPredicate;\n+import org.apache.parquet.filter2.predicate.Operators;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.OperandNode;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDate;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Deque;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiFunction;\n+\n+import static org.apache.parquet.filter2.predicate.FilterApi.and;\n+import static org.apache.parquet.filter2.predicate.FilterApi.binaryColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.booleanColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.doubleColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.floatColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.intColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.longColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.not;\n+import static org.apache.parquet.filter2.predicate.FilterApi.or;\n+\n+/**\n+ * This is the implementation of {@link TreeVisitor} for Parquet.\n+ * <p>\n+ * The class visits all the {@link Node}s from the expression tree,\n+ * and builds a simple (single {@link FilterCompat.Filter} class) for\n+ * {@link org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor} to use for its\n+ * scan.\n+ */\n+public class ParquetRecordFilterBuilder implements TreeVisitor {\n+\n+    protected final Logger LOG = LoggerFactory.getLogger(this.getClass());\n+\n+    private final Map<String, Type> fields;\n+    private final List<ColumnDescriptor> columnDescriptors;\n+    private final Deque<FilterPredicate> filterQueue;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors the list of column descriptors\n+     * @param originalFields    a map of field names to types\n+     */\n+    public ParquetRecordFilterBuilder(List<ColumnDescriptor> columnDescriptors, Map<String, Type> originalFields) {\n+        this.columnDescriptors = columnDescriptors;\n+        this.filterQueue = new LinkedList<>();\n+        this.fields = originalFields;\n+    }\n+\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()) {\n+                processSimpleColumnOperator(operatorNode);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    @Override\n+    public Node after(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+            if (operator.isLogical()) {\n+                processLogicalOperator(operator);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    /**\n+     * Returns the built record filter\n+     *\n+     * @return the built record filter\n+     */\n+    public FilterCompat.Filter getRecordFilter() {\n+        FilterPredicate predicate = filterQueue.poll();\n+        if (!filterQueue.isEmpty()) {\n+            throw new IllegalStateException(\"Filter queue is not empty after visiting all nodes\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTQwNg==", "bodyText": "We rely on TreeTraverser to be correct. TreeTraverser has a set of unit tests that should give us some degree of guarantee that we'll visit all the nodes. In theory, there shouldn't be a situation where the queue is not empty after visiting all nodes", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369915406", "createdAt": "2020-01-23T03:15:14Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilder.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.filter2.predicate.FilterApi;\n+import org.apache.parquet.filter2.predicate.FilterPredicate;\n+import org.apache.parquet.filter2.predicate.Operators;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.OperandNode;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDate;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Deque;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiFunction;\n+\n+import static org.apache.parquet.filter2.predicate.FilterApi.and;\n+import static org.apache.parquet.filter2.predicate.FilterApi.binaryColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.booleanColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.doubleColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.floatColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.intColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.longColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.not;\n+import static org.apache.parquet.filter2.predicate.FilterApi.or;\n+\n+/**\n+ * This is the implementation of {@link TreeVisitor} for Parquet.\n+ * <p>\n+ * The class visits all the {@link Node}s from the expression tree,\n+ * and builds a simple (single {@link FilterCompat.Filter} class) for\n+ * {@link org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor} to use for its\n+ * scan.\n+ */\n+public class ParquetRecordFilterBuilder implements TreeVisitor {\n+\n+    protected final Logger LOG = LoggerFactory.getLogger(this.getClass());\n+\n+    private final Map<String, Type> fields;\n+    private final List<ColumnDescriptor> columnDescriptors;\n+    private final Deque<FilterPredicate> filterQueue;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors the list of column descriptors\n+     * @param originalFields    a map of field names to types\n+     */\n+    public ParquetRecordFilterBuilder(List<ColumnDescriptor> columnDescriptors, Map<String, Type> originalFields) {\n+        this.columnDescriptors = columnDescriptors;\n+        this.filterQueue = new LinkedList<>();\n+        this.fields = originalFields;\n+    }\n+\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()) {\n+                processSimpleColumnOperator(operatorNode);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    @Override\n+    public Node after(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+            if (operator.isLogical()) {\n+                processLogicalOperator(operator);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    /**\n+     * Returns the built record filter\n+     *\n+     * @return the built record filter\n+     */\n+    public FilterCompat.Filter getRecordFilter() {\n+        FilterPredicate predicate = filterQueue.poll();\n+        if (!filterQueue.isEmpty()) {\n+            throw new IllegalStateException(\"Filter queue is not empty after visiting all nodes\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4NjQ0MQ=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjE4NTY3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMToxMDowNlrOFgwOGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxNjozMFrOFgx3CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4ODc5Mg==", "bodyText": "what?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369888792", "createdAt": "2020-01-23T01:10:06Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilder.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.filter2.predicate.FilterApi;\n+import org.apache.parquet.filter2.predicate.FilterPredicate;\n+import org.apache.parquet.filter2.predicate.Operators;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.OperandNode;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDate;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Deque;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiFunction;\n+\n+import static org.apache.parquet.filter2.predicate.FilterApi.and;\n+import static org.apache.parquet.filter2.predicate.FilterApi.binaryColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.booleanColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.doubleColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.floatColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.intColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.longColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.not;\n+import static org.apache.parquet.filter2.predicate.FilterApi.or;\n+\n+/**\n+ * This is the implementation of {@link TreeVisitor} for Parquet.\n+ * <p>\n+ * The class visits all the {@link Node}s from the expression tree,\n+ * and builds a simple (single {@link FilterCompat.Filter} class) for\n+ * {@link org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor} to use for its\n+ * scan.\n+ */\n+public class ParquetRecordFilterBuilder implements TreeVisitor {\n+\n+    protected final Logger LOG = LoggerFactory.getLogger(this.getClass());\n+\n+    private final Map<String, Type> fields;\n+    private final List<ColumnDescriptor> columnDescriptors;\n+    private final Deque<FilterPredicate> filterQueue;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors the list of column descriptors\n+     * @param originalFields    a map of field names to types\n+     */\n+    public ParquetRecordFilterBuilder(List<ColumnDescriptor> columnDescriptors, Map<String, Type> originalFields) {\n+        this.columnDescriptors = columnDescriptors;\n+        this.filterQueue = new LinkedList<>();\n+        this.fields = originalFields;\n+    }\n+\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()) {\n+                processSimpleColumnOperator(operatorNode);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    @Override\n+    public Node after(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+            if (operator.isLogical()) {\n+                processLogicalOperator(operator);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    /**\n+     * Returns the built record filter\n+     *\n+     * @return the built record filter\n+     */\n+    public FilterCompat.Filter getRecordFilter() {\n+        FilterPredicate predicate = filterQueue.poll();\n+        if (!filterQueue.isEmpty()) {\n+            throw new IllegalStateException(\"Filter queue is not empty after visiting all nodes\");\n+        }\n+        return predicate != null ? FilterCompat.get(predicate) : FilterCompat.NOOP;\n+    }\n+\n+    private void processLogicalOperator(Operator operator) {\n+        FilterPredicate right = filterQueue.poll();\n+        FilterPredicate left = null;\n+\n+        if (right == null) {\n+            throw new IllegalStateException(\"Unable to process logical operator \" + operator.toString());\n+        }\n+\n+        if (operator == Operator.AND || operator == Operator.OR) {\n+            left = filterQueue.poll();\n+\n+            if (left == null) {\n+                throw new IllegalStateException(\"Unable to process logical operator \" + operator.toString());\n+            }\n+        }\n+\n+        switch (operator) {\n+            case AND:\n+                filterQueue.push(and(left, right));\n+                break;\n+            case OR:\n+                filterQueue.push(or(left, right));\n+                break;\n+            case NOT:\n+                filterQueue.push(not(right));\n+                break;\n+        }\n+    }\n+\n+    /**\n+     * Handles simple column-operator-constant expressions.\n+     *\n+     * @param operatorNode the operator node\n+     */\n+    private void processSimpleColumnOperator(OperatorNode operatorNode) {\n+\n+        Operator operator = operatorNode.getOperator();\n+        ColumnIndexOperandNode columnIndexOperand = operatorNode.getColumnIndexOperand();\n+        OperandNode valueOperand = null;\n+\n+        if (operator != Operator.IS_NULL && operator != Operator.IS_NOT_NULL) {\n+            valueOperand = operatorNode.getValueOperand();\n+            if (valueOperand == null) {\n+                throw new IllegalArgumentException(\n+                        String.format(\"Operator %s does not contain an operand\", operator));\n+            }\n+        }\n+\n+        ColumnDescriptor columnDescriptor = columnDescriptors.get(columnIndexOperand.index());\n+        String filterColumnName = columnDescriptor.columnName();\n+        Type type = fields.get(filterColumnName);\n+\n+        // INT96 and FIXED_LEN_BYTE_ARRAY cannot be pushed down\n+        // for more details look at org.apache.parquet.filter2.dictionarylevel.DictionaryFilter#expandDictionary\n+        // where INT96 and FIXED_LEN_BYTE_ARRAY are not dictionary values\n+        FilterPredicate simpleFilter;\n+        switch (type.asPrimitiveType().getPrimitiveTypeName()) {\n+            case INT32:\n+                simpleFilter = ParquetRecordFilterBuilder.<Integer, Operators.IntColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(intColumn(type.getName()), getIntegerForINT32(type.getOriginalType(), valueOperand));\n+                break;\n+\n+            case INT64:\n+                simpleFilter = ParquetRecordFilterBuilder.<Long, Operators.LongColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(longColumn(type.getName()), valueOperand == null ? null : Long.parseLong(valueOperand.toString()));\n+                break;\n+\n+            case BINARY:\n+                simpleFilter = ParquetRecordFilterBuilder.<Binary, Operators.BinaryColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(binaryColumn(type.getName()), valueOperand == null ? null : Binary.fromString(valueOperand.toString()));\n+                break;\n+\n+            case BOOLEAN:\n+                // Boolean does not SupportsLtGt\n+                simpleFilter = ParquetRecordFilterBuilder.<Boolean, Operators.BooleanColumn>getOperatorWithEqNotEqSupport(operator)\n+                        .apply(booleanColumn(type.getName()), valueOperand == null ? null : Boolean.parseBoolean(valueOperand.toString()));\n+                break;\n+\n+            case FLOAT:\n+                simpleFilter = ParquetRecordFilterBuilder.<Float, Operators.FloatColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(floatColumn(type.getName()), valueOperand == null ? null : Float.parseFloat(valueOperand.toString()));\n+                break;\n+\n+            case DOUBLE:\n+                simpleFilter = ParquetRecordFilterBuilder.<Double, Operators.DoubleColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(doubleColumn(type.getName()), valueOperand == null ? null : Double.parseDouble(valueOperand.toString()));\n+                break;\n+\n+            default:\n+                throw new UnsupportedOperationException(String.format(\"Column %s of type %s is not supported\",\n+                        type.getName(), type.asPrimitiveType().getPrimitiveTypeName()));\n+        }\n+\n+        filterQueue.push(simpleFilter);\n+    }\n+\n+    /**\n+     * Returns the FilterPredicate function that supports equals and not equals\n+     * for the given operator\n+     *\n+     * @param operator the operator\n+     * @param <T>      the type\n+     * @param <C>      the column type\n+     * @return the FilterPredicate function\n+     */\n+    private static <T extends Comparable<T>, C extends Operators.Column<T> & Operators.SupportsEqNotEq> BiFunction<C, T, FilterPredicate> getOperatorWithEqNotEqSupport(Operator operator) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTY1Nw==", "bodyText": ":) generics. This was a refactoring I did because the code was a bit messy when Alex and I reviewed it. Basically, this will give you the predicate for the given operator", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369915657", "createdAt": "2020-01-23T03:16:30Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilder.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.filter2.predicate.FilterApi;\n+import org.apache.parquet.filter2.predicate.FilterPredicate;\n+import org.apache.parquet.filter2.predicate.Operators;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.OperandNode;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.TreeVisitor;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.LocalDate;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Deque;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiFunction;\n+\n+import static org.apache.parquet.filter2.predicate.FilterApi.and;\n+import static org.apache.parquet.filter2.predicate.FilterApi.binaryColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.booleanColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.doubleColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.floatColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.intColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.longColumn;\n+import static org.apache.parquet.filter2.predicate.FilterApi.not;\n+import static org.apache.parquet.filter2.predicate.FilterApi.or;\n+\n+/**\n+ * This is the implementation of {@link TreeVisitor} for Parquet.\n+ * <p>\n+ * The class visits all the {@link Node}s from the expression tree,\n+ * and builds a simple (single {@link FilterCompat.Filter} class) for\n+ * {@link org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor} to use for its\n+ * scan.\n+ */\n+public class ParquetRecordFilterBuilder implements TreeVisitor {\n+\n+    protected final Logger LOG = LoggerFactory.getLogger(this.getClass());\n+\n+    private final Map<String, Type> fields;\n+    private final List<ColumnDescriptor> columnDescriptors;\n+    private final Deque<FilterPredicate> filterQueue;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors the list of column descriptors\n+     * @param originalFields    a map of field names to types\n+     */\n+    public ParquetRecordFilterBuilder(List<ColumnDescriptor> columnDescriptors, Map<String, Type> originalFields) {\n+        this.columnDescriptors = columnDescriptors;\n+        this.filterQueue = new LinkedList<>();\n+        this.fields = originalFields;\n+    }\n+\n+    @Override\n+    public Node before(Node node, int level) {\n+        return node;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical()) {\n+                processSimpleColumnOperator(operatorNode);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    @Override\n+    public Node after(Node node, int level) {\n+        if (node instanceof OperatorNode) {\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+            if (operator.isLogical()) {\n+                processLogicalOperator(operator);\n+            }\n+        }\n+        return node;\n+    }\n+\n+    /**\n+     * Returns the built record filter\n+     *\n+     * @return the built record filter\n+     */\n+    public FilterCompat.Filter getRecordFilter() {\n+        FilterPredicate predicate = filterQueue.poll();\n+        if (!filterQueue.isEmpty()) {\n+            throw new IllegalStateException(\"Filter queue is not empty after visiting all nodes\");\n+        }\n+        return predicate != null ? FilterCompat.get(predicate) : FilterCompat.NOOP;\n+    }\n+\n+    private void processLogicalOperator(Operator operator) {\n+        FilterPredicate right = filterQueue.poll();\n+        FilterPredicate left = null;\n+\n+        if (right == null) {\n+            throw new IllegalStateException(\"Unable to process logical operator \" + operator.toString());\n+        }\n+\n+        if (operator == Operator.AND || operator == Operator.OR) {\n+            left = filterQueue.poll();\n+\n+            if (left == null) {\n+                throw new IllegalStateException(\"Unable to process logical operator \" + operator.toString());\n+            }\n+        }\n+\n+        switch (operator) {\n+            case AND:\n+                filterQueue.push(and(left, right));\n+                break;\n+            case OR:\n+                filterQueue.push(or(left, right));\n+                break;\n+            case NOT:\n+                filterQueue.push(not(right));\n+                break;\n+        }\n+    }\n+\n+    /**\n+     * Handles simple column-operator-constant expressions.\n+     *\n+     * @param operatorNode the operator node\n+     */\n+    private void processSimpleColumnOperator(OperatorNode operatorNode) {\n+\n+        Operator operator = operatorNode.getOperator();\n+        ColumnIndexOperandNode columnIndexOperand = operatorNode.getColumnIndexOperand();\n+        OperandNode valueOperand = null;\n+\n+        if (operator != Operator.IS_NULL && operator != Operator.IS_NOT_NULL) {\n+            valueOperand = operatorNode.getValueOperand();\n+            if (valueOperand == null) {\n+                throw new IllegalArgumentException(\n+                        String.format(\"Operator %s does not contain an operand\", operator));\n+            }\n+        }\n+\n+        ColumnDescriptor columnDescriptor = columnDescriptors.get(columnIndexOperand.index());\n+        String filterColumnName = columnDescriptor.columnName();\n+        Type type = fields.get(filterColumnName);\n+\n+        // INT96 and FIXED_LEN_BYTE_ARRAY cannot be pushed down\n+        // for more details look at org.apache.parquet.filter2.dictionarylevel.DictionaryFilter#expandDictionary\n+        // where INT96 and FIXED_LEN_BYTE_ARRAY are not dictionary values\n+        FilterPredicate simpleFilter;\n+        switch (type.asPrimitiveType().getPrimitiveTypeName()) {\n+            case INT32:\n+                simpleFilter = ParquetRecordFilterBuilder.<Integer, Operators.IntColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(intColumn(type.getName()), getIntegerForINT32(type.getOriginalType(), valueOperand));\n+                break;\n+\n+            case INT64:\n+                simpleFilter = ParquetRecordFilterBuilder.<Long, Operators.LongColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(longColumn(type.getName()), valueOperand == null ? null : Long.parseLong(valueOperand.toString()));\n+                break;\n+\n+            case BINARY:\n+                simpleFilter = ParquetRecordFilterBuilder.<Binary, Operators.BinaryColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(binaryColumn(type.getName()), valueOperand == null ? null : Binary.fromString(valueOperand.toString()));\n+                break;\n+\n+            case BOOLEAN:\n+                // Boolean does not SupportsLtGt\n+                simpleFilter = ParquetRecordFilterBuilder.<Boolean, Operators.BooleanColumn>getOperatorWithEqNotEqSupport(operator)\n+                        .apply(booleanColumn(type.getName()), valueOperand == null ? null : Boolean.parseBoolean(valueOperand.toString()));\n+                break;\n+\n+            case FLOAT:\n+                simpleFilter = ParquetRecordFilterBuilder.<Float, Operators.FloatColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(floatColumn(type.getName()), valueOperand == null ? null : Float.parseFloat(valueOperand.toString()));\n+                break;\n+\n+            case DOUBLE:\n+                simpleFilter = ParquetRecordFilterBuilder.<Double, Operators.DoubleColumn>getOperatorWithLtGtSupport(operator)\n+                        .apply(doubleColumn(type.getName()), valueOperand == null ? null : Double.parseDouble(valueOperand.toString()));\n+                break;\n+\n+            default:\n+                throw new UnsupportedOperationException(String.format(\"Column %s of type %s is not supported\",\n+                        type.getName(), type.asPrimitiveType().getPrimitiveTypeName()));\n+        }\n+\n+        filterQueue.push(simpleFilter);\n+    }\n+\n+    /**\n+     * Returns the FilterPredicate function that supports equals and not equals\n+     * for the given operator\n+     *\n+     * @param operator the operator\n+     * @param <T>      the type\n+     * @param <C>      the column type\n+     * @return the FilterPredicate function\n+     */\n+    private static <T extends Comparable<T>, C extends Operators.Column<T> & Operators.SupportsEqNotEq> BiFunction<C, T, FilterPredicate> getOperatorWithEqNotEqSupport(Operator operator) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4ODc5Mg=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 214}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjE5MzUwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/resources/parquet/generate_parquet_types.bash", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMToxNDozMVrOFgwSqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxODoyNFrOFgx4Lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4OTk2Mg==", "bodyText": "HDFS_CMD and HIVE_CMD should all be quoted here (lines 14, 15, 17, 19, 23).", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369889962", "createdAt": "2020-01-23T01:14:31Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/resources/parquet/generate_parquet_types.bash", "diffHunk": "@@ -0,0 +1,23 @@\n+#!/bin/bash\n+\n+# Requires Hive 2.3+\n+\n+set -euxo pipefail\n+\n+# Run this command to generate the parquet_types.parquet file\n+\n+SRC_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+HDFS_CMD=${HDFS_CMD:-~/workspace/singlecluster/bin/hdfs}\n+HIVE_CMD=${HIVE_CMD:-~/workspace/singlecluster/bin/hive}\n+HDFS_DIR=${HDFS_DIR:-/tmp/parquet_types/csv}\n+\n+$HDFS_CMD dfs -rm -r -f \"$HDFS_DIR\"\n+$HDFS_CMD dfs -mkdir -p \"$HDFS_DIR\"\n+# Copy source CSV file to HDFS\n+$HDFS_CMD dfs -copyFromLocal \"$SRC_DIR/parquet_types.csv\" \"$HDFS_DIR\"\n+# Run the HQL file\n+$HIVE_CMD -f \"$SRC_DIR/generate_parquet_types.hql\"\n+\n+rm -f \"$SRC_DIR/parquet_types.parquet\"\n+# Copy file to the directory where this script resides\n+$HDFS_CMD dfs -copyToLocal /hive/warehouse/parquet_types/000000_0 \"$SRC_DIR/parquet_types.parquet\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5MDQyNw==", "bodyText": "I think the point of this script is to re-create parquet_types.parquet using PXF. Should we do a diff somewhere?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369890427", "createdAt": "2020-01-23T01:16:22Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/resources/parquet/generate_parquet_types.bash", "diffHunk": "@@ -0,0 +1,23 @@\n+#!/bin/bash\n+\n+# Requires Hive 2.3+\n+\n+set -euxo pipefail\n+\n+# Run this command to generate the parquet_types.parquet file\n+\n+SRC_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+HDFS_CMD=${HDFS_CMD:-~/workspace/singlecluster/bin/hdfs}\n+HIVE_CMD=${HIVE_CMD:-~/workspace/singlecluster/bin/hive}\n+HDFS_DIR=${HDFS_DIR:-/tmp/parquet_types/csv}\n+\n+$HDFS_CMD dfs -rm -r -f \"$HDFS_DIR\"\n+$HDFS_CMD dfs -mkdir -p \"$HDFS_DIR\"\n+# Copy source CSV file to HDFS\n+$HDFS_CMD dfs -copyFromLocal \"$SRC_DIR/parquet_types.csv\" \"$HDFS_DIR\"\n+# Run the HQL file\n+$HIVE_CMD -f \"$SRC_DIR/generate_parquet_types.hql\"\n+\n+rm -f \"$SRC_DIR/parquet_types.parquet\"\n+# Copy file to the directory where this script resides\n+$HDFS_CMD dfs -copyToLocal /hive/warehouse/parquet_types/000000_0 \"$SRC_DIR/parquet_types.parquet\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4OTk2Mg=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTk1MA==", "bodyText": "actually, the point of the script is to generate parquet_types.parquet without PXF. We use Hive directly to generate the file (take a look at the .hql files in the PR). The idea is that we assume that Hive is doing the right thing when generating the parquet file, so we don't rely on PXF.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369915950", "createdAt": "2020-01-23T03:18:24Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/test/resources/parquet/generate_parquet_types.bash", "diffHunk": "@@ -0,0 +1,23 @@\n+#!/bin/bash\n+\n+# Requires Hive 2.3+\n+\n+set -euxo pipefail\n+\n+# Run this command to generate the parquet_types.parquet file\n+\n+SRC_DIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n+HDFS_CMD=${HDFS_CMD:-~/workspace/singlecluster/bin/hdfs}\n+HIVE_CMD=${HIVE_CMD:-~/workspace/singlecluster/bin/hive}\n+HDFS_DIR=${HDFS_DIR:-/tmp/parquet_types/csv}\n+\n+$HDFS_CMD dfs -rm -r -f \"$HDFS_DIR\"\n+$HDFS_CMD dfs -mkdir -p \"$HDFS_DIR\"\n+# Copy source CSV file to HDFS\n+$HDFS_CMD dfs -copyFromLocal \"$SRC_DIR/parquet_types.csv\" \"$HDFS_DIR\"\n+# Run the HQL file\n+$HIVE_CMD -f \"$SRC_DIR/generate_parquet_types.hql\"\n+\n+rm -f \"$SRC_DIR/parquet_types.parquet\"\n+# Copy file to the directory where this script resides\n+$HDFS_CMD dfs -copyToLocal /hive/warehouse/parquet_types/000000_0 \"$SRC_DIR/parquet_types.parquet\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg4OTk2Mg=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjIwMDAwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMToxODozMlrOFgwWng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMDo0OToyNlrOFhMLaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5MDk3NA==", "bodyText": "We add support for date and decimal types?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369890974", "createdAt": "2020-01-23T01:18:32Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "diffHunk": "@@ -68,7 +69,11 @@ public Object getValue(Group group, int columnIndex, int repeatIndex, Type type)\n         @Override\n         public DataType getDataType(Type type) {\n             OriginalType originalType = type.getOriginalType();\n-            if (originalType == OriginalType.INT_8 || originalType == OriginalType.INT_16) {\n+            if (originalType == OriginalType.DATE) {\n+                return DataType.DATE;\n+            } else if (originalType == OriginalType.DECIMAL) {\n+                return DataType.NUMERIC;\n+            } else if (originalType == OriginalType.INT_8 || originalType == OriginalType.INT_16) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM0Njg1Nw==", "bodyText": "so date is serialized as an integer by hive, and this supports reading from parquet file generated by hive.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r370346857", "createdAt": "2020-01-23T20:49:26Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "diffHunk": "@@ -68,7 +69,11 @@ public Object getValue(Group group, int columnIndex, int repeatIndex, Type type)\n         @Override\n         public DataType getDataType(Type type) {\n             OriginalType originalType = type.getOriginalType();\n-            if (originalType == OriginalType.INT_8 || originalType == OriginalType.INT_16) {\n+            if (originalType == OriginalType.DATE) {\n+                return DataType.DATE;\n+            } else if (originalType == OriginalType.DECIMAL) {\n+                return DataType.NUMERIC;\n+            } else if (originalType == OriginalType.INT_8 || originalType == OriginalType.INT_16) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5MDk3NA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjIwOTMyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMToyNToxMFrOFgwcgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoxODo1N1rOFgx4lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5MjQ4MA==", "bodyText": "maybe we should refactor this to a function that takes a long:\nprivate BigDecimal bigDecimalFromLong(long val)\nand above (int32) you could cast the int to long.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369892480", "createdAt": "2020-01-23T01:25:10Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "diffHunk": "@@ -94,12 +105,22 @@ public void addValueToJsonArray(Group group, int columnIndex, int repeatIndex, T\n     INT64 {\n         @Override\n         public DataType getDataType(Type type) {\n+            OriginalType originalType = type.getOriginalType();\n+            if (originalType == OriginalType.DECIMAL) {\n+                return DataType.NUMERIC;\n+            }\n             return DataType.BIGINT;\n         }\n \n         @Override\n         public Object getValue(Group group, int columnIndex, int repeatIndex, Type type) {\n-            return group.getLong(columnIndex, repeatIndex);\n+            long value = group.getLong(columnIndex, repeatIndex);\n+            OriginalType originalType = type.getOriginalType();\n+            if (originalType == OriginalType.DECIMAL) {\n+                int scale = type.asPrimitiveType().getDecimalMetadata().getScale();\n+                return new BigDecimal(BigInteger.valueOf(value), scale);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNjA1Mg==", "bodyText": "hmm, good point", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369916052", "createdAt": "2020-01-23T03:18:57Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "diffHunk": "@@ -94,12 +105,22 @@ public void addValueToJsonArray(Group group, int columnIndex, int repeatIndex, T\n     INT64 {\n         @Override\n         public DataType getDataType(Type type) {\n+            OriginalType originalType = type.getOriginalType();\n+            if (originalType == OriginalType.DECIMAL) {\n+                return DataType.NUMERIC;\n+            }\n             return DataType.BIGINT;\n         }\n \n         @Override\n         public Object getValue(Group group, int columnIndex, int repeatIndex, Type type) {\n-            return group.getLong(columnIndex, repeatIndex);\n+            long value = group.getLong(columnIndex, repeatIndex);\n+            OriginalType originalType = type.getOriginalType();\n+            if (originalType == OriginalType.DECIMAL) {\n+                int scale = type.asPrimitiveType().getDecimalMetadata().getScale();\n+                return new BigDecimal(BigInteger.valueOf(value), scale);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5MjQ4MA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjIxOTIwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/SupportedParquetPrimitiveTypePruner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTozMjoxMlrOFgwiwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoyMTowNVrOFgx56A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NDA4MA==", "bodyText": "this seems like a lot of work to get the type for a node. Is it possible that this could be cached on the node itself?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369894080", "createdAt": "2020-01-23T01:32:12Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/SupportedParquetPrimitiveTypePruner.java", "diffHunk": "@@ -0,0 +1,75 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.EnumSet;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Prunes unsupported {@link org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName}s\n+ * from an expression tree.\n+ */\n+public class SupportedParquetPrimitiveTypePruner extends SupportedOperatorPruner {\n+    // INT96 and FIXED_LEN_BYTE_ARRAY cannot be pushed down\n+    // for more details look at\n+    // org.apache.parquet.filter2.dictionarylevel.DictionaryFilter#expandDictionary\n+    // where INT96 and FIXED_LEN_BYTE_ARRAY are not dictionary values\n+    private static final EnumSet<PrimitiveType.PrimitiveTypeName> SUPPORTED_PRIMITIVE_TYPES =\n+            EnumSet.of(\n+                    PrimitiveType.PrimitiveTypeName.INT32,\n+                    PrimitiveType.PrimitiveTypeName.INT64,\n+                    PrimitiveType.PrimitiveTypeName.BOOLEAN,\n+                    PrimitiveType.PrimitiveTypeName.BINARY,\n+                    PrimitiveType.PrimitiveTypeName.FLOAT,\n+                    PrimitiveType.PrimitiveTypeName.DOUBLE);\n+\n+    private final Map<String, Type> fields;\n+    private final List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors  the list of column descriptors for the table\n+     * @param originalFields     a map of field names to types\n+     * @param supportedOperators the EnumSet of supported operators\n+     */\n+    public SupportedParquetPrimitiveTypePruner(List<ColumnDescriptor> columnDescriptors,\n+                                               Map<String, Type> originalFields,\n+                                               EnumSet<Operator> supportedOperators) {\n+        super(supportedOperators);\n+        this.columnDescriptors = columnDescriptors;\n+        this.fields = originalFields;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical() &&\n+                    !SUPPORTED_PRIMITIVE_TYPES.contains(getPrimitiveType(operatorNode)))\n+                return null;\n+        }\n+\n+        return super.visit(node, level);\n+    }\n+\n+    private PrimitiveType.PrimitiveTypeName getPrimitiveType(OperatorNode operatorNode) {\n+        ColumnIndexOperandNode columnIndexOperand = operatorNode.getColumnIndexOperand();\n+        ColumnDescriptor columnDescriptor = columnDescriptors.get(columnIndexOperand.index());\n+        String filterColumnName = columnDescriptor.columnName();\n+        Type type = fields.get(filterColumnName);\n+        return type.asPrimitiveType().getPrimitiveTypeName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNjM5Mg==", "bodyText": "Well, the node is agnostic to parquet types. These nodes are the result of the expression tree that we obtain from greenplum. We look up the types by name in a hashmap. I think hashmaps should be fast enough for this operation", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369916392", "createdAt": "2020-01-23T03:21:05Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/SupportedParquetPrimitiveTypePruner.java", "diffHunk": "@@ -0,0 +1,75 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.ColumnIndexOperandNode;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.greenplum.pxf.api.filter.Operator;\n+import org.greenplum.pxf.api.filter.OperatorNode;\n+import org.greenplum.pxf.api.filter.SupportedOperatorPruner;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+\n+import java.util.EnumSet;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Prunes unsupported {@link org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName}s\n+ * from an expression tree.\n+ */\n+public class SupportedParquetPrimitiveTypePruner extends SupportedOperatorPruner {\n+    // INT96 and FIXED_LEN_BYTE_ARRAY cannot be pushed down\n+    // for more details look at\n+    // org.apache.parquet.filter2.dictionarylevel.DictionaryFilter#expandDictionary\n+    // where INT96 and FIXED_LEN_BYTE_ARRAY are not dictionary values\n+    private static final EnumSet<PrimitiveType.PrimitiveTypeName> SUPPORTED_PRIMITIVE_TYPES =\n+            EnumSet.of(\n+                    PrimitiveType.PrimitiveTypeName.INT32,\n+                    PrimitiveType.PrimitiveTypeName.INT64,\n+                    PrimitiveType.PrimitiveTypeName.BOOLEAN,\n+                    PrimitiveType.PrimitiveTypeName.BINARY,\n+                    PrimitiveType.PrimitiveTypeName.FLOAT,\n+                    PrimitiveType.PrimitiveTypeName.DOUBLE);\n+\n+    private final Map<String, Type> fields;\n+    private final List<ColumnDescriptor> columnDescriptors;\n+\n+    /**\n+     * Constructor\n+     *\n+     * @param columnDescriptors  the list of column descriptors for the table\n+     * @param originalFields     a map of field names to types\n+     * @param supportedOperators the EnumSet of supported operators\n+     */\n+    public SupportedParquetPrimitiveTypePruner(List<ColumnDescriptor> columnDescriptors,\n+                                               Map<String, Type> originalFields,\n+                                               EnumSet<Operator> supportedOperators) {\n+        super(supportedOperators);\n+        this.columnDescriptors = columnDescriptors;\n+        this.fields = originalFields;\n+    }\n+\n+    @Override\n+    public Node visit(Node node, int level) {\n+\n+        if (node instanceof OperatorNode) {\n+\n+            OperatorNode operatorNode = (OperatorNode) node;\n+            Operator operator = operatorNode.getOperator();\n+\n+            if (!operator.isLogical() &&\n+                    !SUPPORTED_PRIMITIVE_TYPES.contains(getPrimitiveType(operatorNode)))\n+                return null;\n+        }\n+\n+        return super.visit(node, level);\n+    }\n+\n+    private PrimitiveType.PrimitiveTypeName getPrimitiveType(OperatorNode operatorNode) {\n+        ColumnIndexOperandNode columnIndexOperand = operatorNode.getColumnIndexOperand();\n+        ColumnDescriptor columnDescriptor = columnDescriptors.get(columnIndexOperand.index());\n+        String filterColumnName = columnDescriptor.columnName();\n+        Type type = fields.get(filterColumnName);\n+        return type.asPrimitiveType().getPrimitiveTypeName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NDA4MA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjIyMTEzOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetBaseTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTozMzo0MVrOFgwkDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTozMzo0MVrOFgwkDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NDQxNA==", "bodyText": "interesting!", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369894414", "createdAt": "2020-01-23T01:33:41Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetBaseTest.java", "diffHunk": "@@ -0,0 +1,83 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class ParquetBaseTest {\n+\n+    protected static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    protected Map<String, Type> originalFieldsMap;\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        columnDescriptors = new ArrayList<>();\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 1, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 2, \"date\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 3, \"float8\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"grade\", DataType.TEXT.getOID(), 4, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 6, \"timestamp\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 7, \"bigint\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 8, \"bytea\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 9, \"int2\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"r\", DataType.REAL.getOID(), 10, \"real\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"vc1\", DataType.VARCHAR.getOID(), 11, \"varchar\", new Integer[]{5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"c1\", DataType.BPCHAR.getOID(), 12, \"char\", new Integer[]{3}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec1\", DataType.NUMERIC.getOID(), 13, \"numeric\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec2\", DataType.NUMERIC.getOID(), 14, \"numeric\", new Integer[]{5, 2}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec3\", DataType.NUMERIC.getOID(), 15, \"numeric\", new Integer[]{13, 5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"num1\", DataType.INTEGER.getOID(), 16, \"int\", null));\n+\n+        MessageType schema = MessageTypeParser.parseMessageType(\"message hive_schema {\\n\" +\n+                \"  optional int32 id;\\n\" +\n+                \"  optional binary name (UTF8);\\n\" +\n+                \"  optional int32 cdate (DATE);\\n\" +\n+                \"  optional double amt;\\n\" +\n+                \"  optional binary grade (UTF8);\\n\" +\n+                \"  optional boolean b;\\n\" +\n+                \"  optional int96 tm;\\n\" +\n+                \"  optional int64 bg;\\n\" +\n+                \"  optional binary bin;\\n\" +\n+                \"  optional int32 sml (INT_16);\\n\" +\n+                \"  optional float r;\\n\" +\n+                \"  optional binary vc1 (UTF8);\\n\" +\n+                \"  optional binary c1 (UTF8);\\n\" +\n+                \"  optional fixed_len_byte_array(16) dec1 (DECIMAL(38,18));\\n\" +\n+                \"  optional fixed_len_byte_array(3) dec2 (DECIMAL(5,2));\\n\" +\n+                \"  optional fixed_len_byte_array(6) dec3 (DECIMAL(13,5));\\n\" +\n+                \"  optional int32 num1;\\n\" +\n+                \"}\");\n+\n+        originalFieldsMap = getOriginalFieldsMap(schema);\n+    }\n+\n+    private Map<String, Type> getOriginalFieldsMap(MessageType originalSchema) {\n+        Map<String, Type> originalFields = new HashMap<>(originalSchema.getFieldCount() * 2);\n+\n+        // We need to add the original name and lower cased name to\n+        // the map to support mixed case where in GPDB the column name\n+        // was created with quotes i.e \"mIxEd CaSe\". When quotes are not\n+        // used to create a table in GPDB, the name of the column will\n+        // always come in lower-case", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjIyMTY3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetBaseTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTozNDoxNFrOFgwkbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMzoyMjoyOFrOFgx6xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NDUxMA==", "bodyText": "no tests?", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369894510", "createdAt": "2020-01-23T01:34:14Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetBaseTest.java", "diffHunk": "@@ -0,0 +1,83 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class ParquetBaseTest {\n+\n+    protected static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    protected Map<String, Type> originalFieldsMap;\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        columnDescriptors = new ArrayList<>();\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 1, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 2, \"date\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 3, \"float8\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"grade\", DataType.TEXT.getOID(), 4, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 6, \"timestamp\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 7, \"bigint\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 8, \"bytea\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 9, \"int2\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"r\", DataType.REAL.getOID(), 10, \"real\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"vc1\", DataType.VARCHAR.getOID(), 11, \"varchar\", new Integer[]{5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"c1\", DataType.BPCHAR.getOID(), 12, \"char\", new Integer[]{3}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec1\", DataType.NUMERIC.getOID(), 13, \"numeric\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec2\", DataType.NUMERIC.getOID(), 14, \"numeric\", new Integer[]{5, 2}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec3\", DataType.NUMERIC.getOID(), 15, \"numeric\", new Integer[]{13, 5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"num1\", DataType.INTEGER.getOID(), 16, \"int\", null));\n+\n+        MessageType schema = MessageTypeParser.parseMessageType(\"message hive_schema {\\n\" +\n+                \"  optional int32 id;\\n\" +\n+                \"  optional binary name (UTF8);\\n\" +\n+                \"  optional int32 cdate (DATE);\\n\" +\n+                \"  optional double amt;\\n\" +\n+                \"  optional binary grade (UTF8);\\n\" +\n+                \"  optional boolean b;\\n\" +\n+                \"  optional int96 tm;\\n\" +\n+                \"  optional int64 bg;\\n\" +\n+                \"  optional binary bin;\\n\" +\n+                \"  optional int32 sml (INT_16);\\n\" +\n+                \"  optional float r;\\n\" +\n+                \"  optional binary vc1 (UTF8);\\n\" +\n+                \"  optional binary c1 (UTF8);\\n\" +\n+                \"  optional fixed_len_byte_array(16) dec1 (DECIMAL(38,18));\\n\" +\n+                \"  optional fixed_len_byte_array(3) dec2 (DECIMAL(5,2));\\n\" +\n+                \"  optional fixed_len_byte_array(6) dec3 (DECIMAL(13,5));\\n\" +\n+                \"  optional int32 num1;\\n\" +\n+                \"}\");\n+\n+        originalFieldsMap = getOriginalFieldsMap(schema);\n+    }\n+\n+    private Map<String, Type> getOriginalFieldsMap(MessageType originalSchema) {\n+        Map<String, Type> originalFields = new HashMap<>(originalSchema.getFieldCount() * 2);\n+\n+        // We need to add the original name and lower cased name to\n+        // the map to support mixed case where in GPDB the column name\n+        // was created with quotes i.e \"mIxEd CaSe\". When quotes are not\n+        // used to create a table in GPDB, the name of the column will\n+        // always come in lower-case\n+        originalSchema.getFields().forEach(t -> {\n+            String columnName = t.getName();\n+            originalFields.put(columnName, t);\n+            originalFields.put(columnName.toLowerCase(), t);\n+        });\n+\n+        return originalFields;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNjYxNA==", "bodyText": "some of the tests in this namespace share the same schema for test ParquetBaseTest. To avoid code duplication, I created a base class where all the other classes that need the schema inherit.", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369916614", "createdAt": "2020-01-23T03:22:28Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetBaseTest.java", "diffHunk": "@@ -0,0 +1,83 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.apache.parquet.schema.Type;\n+import org.greenplum.pxf.api.filter.TreeTraverser;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class ParquetBaseTest {\n+\n+    protected static final TreeTraverser TRAVERSER = new TreeTraverser();\n+\n+    protected Map<String, Type> originalFieldsMap;\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        columnDescriptors = new ArrayList<>();\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 1, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 2, \"date\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 3, \"float8\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"grade\", DataType.TEXT.getOID(), 4, \"text\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 6, \"timestamp\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 7, \"bigint\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 8, \"bytea\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 9, \"int2\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"r\", DataType.REAL.getOID(), 10, \"real\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"vc1\", DataType.VARCHAR.getOID(), 11, \"varchar\", new Integer[]{5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"c1\", DataType.BPCHAR.getOID(), 12, \"char\", new Integer[]{3}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec1\", DataType.NUMERIC.getOID(), 13, \"numeric\", null));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec2\", DataType.NUMERIC.getOID(), 14, \"numeric\", new Integer[]{5, 2}));\n+        columnDescriptors.add(new ColumnDescriptor(\"dec3\", DataType.NUMERIC.getOID(), 15, \"numeric\", new Integer[]{13, 5}));\n+        columnDescriptors.add(new ColumnDescriptor(\"num1\", DataType.INTEGER.getOID(), 16, \"int\", null));\n+\n+        MessageType schema = MessageTypeParser.parseMessageType(\"message hive_schema {\\n\" +\n+                \"  optional int32 id;\\n\" +\n+                \"  optional binary name (UTF8);\\n\" +\n+                \"  optional int32 cdate (DATE);\\n\" +\n+                \"  optional double amt;\\n\" +\n+                \"  optional binary grade (UTF8);\\n\" +\n+                \"  optional boolean b;\\n\" +\n+                \"  optional int96 tm;\\n\" +\n+                \"  optional int64 bg;\\n\" +\n+                \"  optional binary bin;\\n\" +\n+                \"  optional int32 sml (INT_16);\\n\" +\n+                \"  optional float r;\\n\" +\n+                \"  optional binary vc1 (UTF8);\\n\" +\n+                \"  optional binary c1 (UTF8);\\n\" +\n+                \"  optional fixed_len_byte_array(16) dec1 (DECIMAL(38,18));\\n\" +\n+                \"  optional fixed_len_byte_array(3) dec2 (DECIMAL(5,2));\\n\" +\n+                \"  optional fixed_len_byte_array(6) dec3 (DECIMAL(13,5));\\n\" +\n+                \"  optional int32 num1;\\n\" +\n+                \"}\");\n+\n+        originalFieldsMap = getOriginalFieldsMap(schema);\n+    }\n+\n+    private Map<String, Type> getOriginalFieldsMap(MessageType originalSchema) {\n+        Map<String, Type> originalFields = new HashMap<>(originalSchema.getFieldCount() * 2);\n+\n+        // We need to add the original name and lower cased name to\n+        // the map to support mixed case where in GPDB the column name\n+        // was created with quotes i.e \"mIxEd CaSe\". When quotes are not\n+        // used to create a table in GPDB, the name of the column will\n+        // always come in lower-case\n+        originalSchema.getFields().forEach(t -> {\n+            String columnName = t.getName();\n+            originalFields.put(columnName, t);\n+            originalFields.put(columnName.toLowerCase(), t);\n+        });\n+\n+        return originalFields;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NDUxMA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjI0MDQ5OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetFilterPushDownTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTo0NTo0NVrOFgwvRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QxODowNTozNVrOFhHozQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NzI4NA==", "bodyText": "oh my gosh an extra comma", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369897284", "createdAt": "2020-01-23T01:45:45Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetFilterPushDownTest.java", "diffHunk": "@@ -0,0 +1,836 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.Fragment;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetResolver;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ParquetFilterPushDownTest extends ParquetBaseTest {\n+\n+    // From resources/parquet/parquet_types.csv\n+    private static final int[] COL1 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+    private static final String[] COL2 = {\"row1\", \"row2\", \"row3\", \"row4\", \"row5\", \"row6\", \"row7\", \"row8\", \"row9\", \"row10\", \"row11\", \"row12_text_null\", \"row13_int_null\", \"row14_double_null\", \"row15_decimal_null\", \"row16_timestamp_null\", \"row17_real_null\", \"row18_bigint_null\", \"row19_bool_null\", \"row20\", \"row21_smallint_null\", \"row22_date_null\", \"row23_varchar_null\", \"row24_char_null\", \"row25_binary_null\"};\n+    private static final String[] COL3 = {\"2019-12-01\", \"2019-12-02\", \"2019-12-03\", \"2019-12-04\", \"2019-12-05\", \"2019-12-06\", \"2019-12-07\", \"2019-12-08\", \"2019-12-09\", \"2019-12-10\", \"2019-12-11\", \"2019-12-12\", \"2019-12-13\", \"2019-12-14\", \"2019-12-15\", \"2019-12-16\", \"2019-12-17\", \"2019-12-18\", \"2019-12-19\", \"2019-12-20\", \"2019-12-21\", null, \"2019-12-23\", \"2019-12-24\", \"2019-12-25\"};\n+    private static final Double[] COL4 = {1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0, 2100.0, 2200.0, 2300.0, 2400.0, null, 2500.0, 2550.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0,};\n+    private static final String[] COL5 = {\"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"excellent\", \"bad\", \"good\", null, \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\"};\n+    private static final Boolean[] COL6 = {false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, null, false, false, false, false, false, false};\n+    private static final String[] COL7 = {\"2013-07-14T04:00:00Z\", \"2013-07-14T04:00:00Z\", \"2013-07-16T04:00:00Z\", \"2013-07-17T04:00:00Z\", \"2013-07-18T04:00:00Z\", \"2013-07-19T04:00:00Z\", \"2013-07-20T04:00:00Z\", \"2013-07-21T04:00:00Z\", \"2013-07-22T04:00:00Z\", \"2013-07-23T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-25T04:00:00Z\", null, \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\"};\n+    private static final Long[] COL8 = {2147483647L, 2147483648L, 2147483649L, 2147483650L, 2147483651L, 2147483652L, 2147483653L, 2147483654L, 2147483655L, 2147483656L, 2147483657L, 2147483658L, 2147483659L, 2147483660L, 2147483661L, 2147483662L, 2147483663L, null, -1L, -2147483643L, -2147483644L, -2147483645L, -2147483646L, -2147483647L, -2147483648L};\n+    private static final Byte[] COL9 = {0b00110001, 0b00110010, 0b00110011, 0b00110100, 0b00110101, 0b00110110, 0b00110111, 0b00111000, 0b00111001, 0b00110000, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, null};\n+    private static final Short[] COL10 = {-32768, -31500, -31000, -30000, -20000, -10000, -1000, -550, -320, -120, -40, -1, 0, 1, 100, 1000, 10000, 20000, 30000, 31000, null, 32100, 32200, 32500, 32767};\n+    private static final Float[] COL11 = {7.7F, 8.7F, 9.7F, 10.7F, 11.7F, 12.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, null, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F};\n+    private static final String[] COL12 = {\"s_6\", \"s_7\", \"s_8\", \"s_9\", \"s_10\", \"s_11\", \"s_12\", \"s_13\", \"s_14\", \"s_15\", \"s_16\", \"s_16\", \"s_16\", \"s_16\", \"s_17\", \"s_160\", \"s_161\", \"s_162\", \"s_163\", \"s_164\", \"s_165\", \"s_166\", null, \"s_168\", \"s_169\"};\n+    private static final String[] COL13 = {\"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"EUR\", \"UAH\", \"USD\", \"UAH\", \"EUR\", \"USD\", \"UAH\", \"USD\", \"USD\", \"EUR\", \"USD\", \"USD\", \"UAH\", \"USD\", \"EUR\", \"EUR\", null, \"USD\"};\n+    private static final Double[] COL14 = {1.23456, 1.23456, -1.23456, 123456789.1, 1E-12, 1234.889, 0.0001, 45678.00002, 23457.1, 45678.00002, 0.123456789, 0.123456789, 0.123456789, 0.123456789, null, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789};\n+    private static final Double[] COL15 = {0.0, 123.45, -1.45, 0.25, -.25, 999.99, -999.99, 1.0, -1.0, 789.0, -789.0, 0.99, -0.99, 1.99, null, -1.99, 15.99, -15.99, -299.99, 299.99, 555.55, 0.15, 3.89, 3.14, 8.0};\n+    private static final Double[] COL16 = {0.12345, -0.12345, 12345678.90123, -12345678.90123, 99999999.0, -99999999.0, -99999999.99999, 99999999.99999, 0.0, 1.0, -1.0, 0.9, -0.9, 45.0, null, -45.0, 3.14159, -3.14159, 2.71828, -2.71828, 45.99999, -45.99999, 450.45001, 0.00001, -0.00001};\n+    private static final Integer[] COL17 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, null, 11, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11};\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        super.setup();\n+\n+        accessor = new ParquetFileAccessor();\n+        resolver = new ParquetResolver();\n+        context = new RequestContext();\n+\n+        String path = Objects.requireNonNull(getClass().getClassLoader().getResource(\"parquet/parquet_types.parquet\")).getPath();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setRequestType(RequestContext.RequestType.READ_BRIDGE);\n+        context.setDataSource(path);\n+        context.setFragmentMetadata(HdfsUtilities.prepareFragmentMetadata(0, 4196, Fragment.HOSTS));\n+        context.setTupleDescription(columnDescriptors);\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+    }\n+\n+    @Test\n+    public void testNoFilter() throws Exception {\n+        // all rows are expected\n+        helper(COL1);\n+    }\n+\n+    @Test\n+    public void testIdPushDown() throws Exception {\n+\n+        for (int i = 1; i <= 25; i++) {\n+            // id = i\n+            String index = String.valueOf(i);\n+            String filterString = String.format(\"a0c20s%dd%so5\", index.length(), index);\n+            context.setFilterString(filterString);\n+            helper(new int[]{i});\n+        }\n+    }\n+\n+    @Test\n+    public void testIdPushDownWithProjectedColumns() throws Exception {\n+        List<ColumnDescriptor> columnDescriptors = context.getTupleDescription();\n+        columnDescriptors.forEach(d -> d.setProjected(false));\n+        columnDescriptors.get(0).setProjected(true);\n+\n+        for (int i = 1; i <= 25; i++) {\n+            // id = i\n+            String index = String.valueOf(i);\n+            String filterString = String.format(\"a0c20s%dd%so5\", index.length(), index);\n+            context.setFilterString(filterString);\n+            helper(new int[]{i});\n+        }\n+    }\n+\n+    @Test\n+    public void testBooleanPushDown() throws Exception {\n+        int[] expectedRows = {2, 4, 6, 8, 10};\n+        // a5 == true\n+        context.setFilterString(\"a5c16s4dtrueo0\");\n+        helper(expectedRows);\n+\n+        // a5 <> true\n+        expectedRows = new int[]{1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a5c16s4dtrueo6\");\n+        helper(expectedRows);\n+\n+        // a5 == false\n+        expectedRows = new int[]{1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25,};", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDI3MjQ2MQ==", "bodyText": "how do you even see this!", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r370272461", "createdAt": "2020-01-23T18:05:35Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetFilterPushDownTest.java", "diffHunk": "@@ -0,0 +1,836 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.Fragment;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetResolver;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ParquetFilterPushDownTest extends ParquetBaseTest {\n+\n+    // From resources/parquet/parquet_types.csv\n+    private static final int[] COL1 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+    private static final String[] COL2 = {\"row1\", \"row2\", \"row3\", \"row4\", \"row5\", \"row6\", \"row7\", \"row8\", \"row9\", \"row10\", \"row11\", \"row12_text_null\", \"row13_int_null\", \"row14_double_null\", \"row15_decimal_null\", \"row16_timestamp_null\", \"row17_real_null\", \"row18_bigint_null\", \"row19_bool_null\", \"row20\", \"row21_smallint_null\", \"row22_date_null\", \"row23_varchar_null\", \"row24_char_null\", \"row25_binary_null\"};\n+    private static final String[] COL3 = {\"2019-12-01\", \"2019-12-02\", \"2019-12-03\", \"2019-12-04\", \"2019-12-05\", \"2019-12-06\", \"2019-12-07\", \"2019-12-08\", \"2019-12-09\", \"2019-12-10\", \"2019-12-11\", \"2019-12-12\", \"2019-12-13\", \"2019-12-14\", \"2019-12-15\", \"2019-12-16\", \"2019-12-17\", \"2019-12-18\", \"2019-12-19\", \"2019-12-20\", \"2019-12-21\", null, \"2019-12-23\", \"2019-12-24\", \"2019-12-25\"};\n+    private static final Double[] COL4 = {1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0, 2100.0, 2200.0, 2300.0, 2400.0, null, 2500.0, 2550.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0,};\n+    private static final String[] COL5 = {\"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"excellent\", \"bad\", \"good\", null, \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\"};\n+    private static final Boolean[] COL6 = {false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, null, false, false, false, false, false, false};\n+    private static final String[] COL7 = {\"2013-07-14T04:00:00Z\", \"2013-07-14T04:00:00Z\", \"2013-07-16T04:00:00Z\", \"2013-07-17T04:00:00Z\", \"2013-07-18T04:00:00Z\", \"2013-07-19T04:00:00Z\", \"2013-07-20T04:00:00Z\", \"2013-07-21T04:00:00Z\", \"2013-07-22T04:00:00Z\", \"2013-07-23T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-25T04:00:00Z\", null, \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\"};\n+    private static final Long[] COL8 = {2147483647L, 2147483648L, 2147483649L, 2147483650L, 2147483651L, 2147483652L, 2147483653L, 2147483654L, 2147483655L, 2147483656L, 2147483657L, 2147483658L, 2147483659L, 2147483660L, 2147483661L, 2147483662L, 2147483663L, null, -1L, -2147483643L, -2147483644L, -2147483645L, -2147483646L, -2147483647L, -2147483648L};\n+    private static final Byte[] COL9 = {0b00110001, 0b00110010, 0b00110011, 0b00110100, 0b00110101, 0b00110110, 0b00110111, 0b00111000, 0b00111001, 0b00110000, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, null};\n+    private static final Short[] COL10 = {-32768, -31500, -31000, -30000, -20000, -10000, -1000, -550, -320, -120, -40, -1, 0, 1, 100, 1000, 10000, 20000, 30000, 31000, null, 32100, 32200, 32500, 32767};\n+    private static final Float[] COL11 = {7.7F, 8.7F, 9.7F, 10.7F, 11.7F, 12.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, null, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F};\n+    private static final String[] COL12 = {\"s_6\", \"s_7\", \"s_8\", \"s_9\", \"s_10\", \"s_11\", \"s_12\", \"s_13\", \"s_14\", \"s_15\", \"s_16\", \"s_16\", \"s_16\", \"s_16\", \"s_17\", \"s_160\", \"s_161\", \"s_162\", \"s_163\", \"s_164\", \"s_165\", \"s_166\", null, \"s_168\", \"s_169\"};\n+    private static final String[] COL13 = {\"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"EUR\", \"UAH\", \"USD\", \"UAH\", \"EUR\", \"USD\", \"UAH\", \"USD\", \"USD\", \"EUR\", \"USD\", \"USD\", \"UAH\", \"USD\", \"EUR\", \"EUR\", null, \"USD\"};\n+    private static final Double[] COL14 = {1.23456, 1.23456, -1.23456, 123456789.1, 1E-12, 1234.889, 0.0001, 45678.00002, 23457.1, 45678.00002, 0.123456789, 0.123456789, 0.123456789, 0.123456789, null, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789};\n+    private static final Double[] COL15 = {0.0, 123.45, -1.45, 0.25, -.25, 999.99, -999.99, 1.0, -1.0, 789.0, -789.0, 0.99, -0.99, 1.99, null, -1.99, 15.99, -15.99, -299.99, 299.99, 555.55, 0.15, 3.89, 3.14, 8.0};\n+    private static final Double[] COL16 = {0.12345, -0.12345, 12345678.90123, -12345678.90123, 99999999.0, -99999999.0, -99999999.99999, 99999999.99999, 0.0, 1.0, -1.0, 0.9, -0.9, 45.0, null, -45.0, 3.14159, -3.14159, 2.71828, -2.71828, 45.99999, -45.99999, 450.45001, 0.00001, -0.00001};\n+    private static final Integer[] COL17 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, null, 11, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11};\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        super.setup();\n+\n+        accessor = new ParquetFileAccessor();\n+        resolver = new ParquetResolver();\n+        context = new RequestContext();\n+\n+        String path = Objects.requireNonNull(getClass().getClassLoader().getResource(\"parquet/parquet_types.parquet\")).getPath();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setRequestType(RequestContext.RequestType.READ_BRIDGE);\n+        context.setDataSource(path);\n+        context.setFragmentMetadata(HdfsUtilities.prepareFragmentMetadata(0, 4196, Fragment.HOSTS));\n+        context.setTupleDescription(columnDescriptors);\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+    }\n+\n+    @Test\n+    public void testNoFilter() throws Exception {\n+        // all rows are expected\n+        helper(COL1);\n+    }\n+\n+    @Test\n+    public void testIdPushDown() throws Exception {\n+\n+        for (int i = 1; i <= 25; i++) {\n+            // id = i\n+            String index = String.valueOf(i);\n+            String filterString = String.format(\"a0c20s%dd%so5\", index.length(), index);\n+            context.setFilterString(filterString);\n+            helper(new int[]{i});\n+        }\n+    }\n+\n+    @Test\n+    public void testIdPushDownWithProjectedColumns() throws Exception {\n+        List<ColumnDescriptor> columnDescriptors = context.getTupleDescription();\n+        columnDescriptors.forEach(d -> d.setProjected(false));\n+        columnDescriptors.get(0).setProjected(true);\n+\n+        for (int i = 1; i <= 25; i++) {\n+            // id = i\n+            String index = String.valueOf(i);\n+            String filterString = String.format(\"a0c20s%dd%so5\", index.length(), index);\n+            context.setFilterString(filterString);\n+            helper(new int[]{i});\n+        }\n+    }\n+\n+    @Test\n+    public void testBooleanPushDown() throws Exception {\n+        int[] expectedRows = {2, 4, 6, 8, 10};\n+        // a5 == true\n+        context.setFilterString(\"a5c16s4dtrueo0\");\n+        helper(expectedRows);\n+\n+        // a5 <> true\n+        expectedRows = new int[]{1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+        context.setFilterString(\"a5c16s4dtrueo6\");\n+        helper(expectedRows);\n+\n+        // a5 == false\n+        expectedRows = new int[]{1, 3, 5, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25,};", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NzI4NA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjI0NDk2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetFilterPushDownTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTo0Nzo1NVrOFgwxtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMDo0ODowM1rOFhMJWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NzkwOA==", "bodyText": "I like this way of testing. Can we make an alias for COL1 called ALL? helper() could then be named assertRowsReturned(). This line would then read something like assertRowsReturned(ALL) which would be easier to understand than helper(COL1).", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369897908", "createdAt": "2020-01-23T01:47:55Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetFilterPushDownTest.java", "diffHunk": "@@ -0,0 +1,836 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.Fragment;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetResolver;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ParquetFilterPushDownTest extends ParquetBaseTest {\n+\n+    // From resources/parquet/parquet_types.csv\n+    private static final int[] COL1 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+    private static final String[] COL2 = {\"row1\", \"row2\", \"row3\", \"row4\", \"row5\", \"row6\", \"row7\", \"row8\", \"row9\", \"row10\", \"row11\", \"row12_text_null\", \"row13_int_null\", \"row14_double_null\", \"row15_decimal_null\", \"row16_timestamp_null\", \"row17_real_null\", \"row18_bigint_null\", \"row19_bool_null\", \"row20\", \"row21_smallint_null\", \"row22_date_null\", \"row23_varchar_null\", \"row24_char_null\", \"row25_binary_null\"};\n+    private static final String[] COL3 = {\"2019-12-01\", \"2019-12-02\", \"2019-12-03\", \"2019-12-04\", \"2019-12-05\", \"2019-12-06\", \"2019-12-07\", \"2019-12-08\", \"2019-12-09\", \"2019-12-10\", \"2019-12-11\", \"2019-12-12\", \"2019-12-13\", \"2019-12-14\", \"2019-12-15\", \"2019-12-16\", \"2019-12-17\", \"2019-12-18\", \"2019-12-19\", \"2019-12-20\", \"2019-12-21\", null, \"2019-12-23\", \"2019-12-24\", \"2019-12-25\"};\n+    private static final Double[] COL4 = {1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0, 2100.0, 2200.0, 2300.0, 2400.0, null, 2500.0, 2550.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0,};\n+    private static final String[] COL5 = {\"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"excellent\", \"bad\", \"good\", null, \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\"};\n+    private static final Boolean[] COL6 = {false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, null, false, false, false, false, false, false};\n+    private static final String[] COL7 = {\"2013-07-14T04:00:00Z\", \"2013-07-14T04:00:00Z\", \"2013-07-16T04:00:00Z\", \"2013-07-17T04:00:00Z\", \"2013-07-18T04:00:00Z\", \"2013-07-19T04:00:00Z\", \"2013-07-20T04:00:00Z\", \"2013-07-21T04:00:00Z\", \"2013-07-22T04:00:00Z\", \"2013-07-23T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-25T04:00:00Z\", null, \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\"};\n+    private static final Long[] COL8 = {2147483647L, 2147483648L, 2147483649L, 2147483650L, 2147483651L, 2147483652L, 2147483653L, 2147483654L, 2147483655L, 2147483656L, 2147483657L, 2147483658L, 2147483659L, 2147483660L, 2147483661L, 2147483662L, 2147483663L, null, -1L, -2147483643L, -2147483644L, -2147483645L, -2147483646L, -2147483647L, -2147483648L};\n+    private static final Byte[] COL9 = {0b00110001, 0b00110010, 0b00110011, 0b00110100, 0b00110101, 0b00110110, 0b00110111, 0b00111000, 0b00111001, 0b00110000, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, null};\n+    private static final Short[] COL10 = {-32768, -31500, -31000, -30000, -20000, -10000, -1000, -550, -320, -120, -40, -1, 0, 1, 100, 1000, 10000, 20000, 30000, 31000, null, 32100, 32200, 32500, 32767};\n+    private static final Float[] COL11 = {7.7F, 8.7F, 9.7F, 10.7F, 11.7F, 12.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, null, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F};\n+    private static final String[] COL12 = {\"s_6\", \"s_7\", \"s_8\", \"s_9\", \"s_10\", \"s_11\", \"s_12\", \"s_13\", \"s_14\", \"s_15\", \"s_16\", \"s_16\", \"s_16\", \"s_16\", \"s_17\", \"s_160\", \"s_161\", \"s_162\", \"s_163\", \"s_164\", \"s_165\", \"s_166\", null, \"s_168\", \"s_169\"};\n+    private static final String[] COL13 = {\"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"EUR\", \"UAH\", \"USD\", \"UAH\", \"EUR\", \"USD\", \"UAH\", \"USD\", \"USD\", \"EUR\", \"USD\", \"USD\", \"UAH\", \"USD\", \"EUR\", \"EUR\", null, \"USD\"};\n+    private static final Double[] COL14 = {1.23456, 1.23456, -1.23456, 123456789.1, 1E-12, 1234.889, 0.0001, 45678.00002, 23457.1, 45678.00002, 0.123456789, 0.123456789, 0.123456789, 0.123456789, null, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789};\n+    private static final Double[] COL15 = {0.0, 123.45, -1.45, 0.25, -.25, 999.99, -999.99, 1.0, -1.0, 789.0, -789.0, 0.99, -0.99, 1.99, null, -1.99, 15.99, -15.99, -299.99, 299.99, 555.55, 0.15, 3.89, 3.14, 8.0};\n+    private static final Double[] COL16 = {0.12345, -0.12345, 12345678.90123, -12345678.90123, 99999999.0, -99999999.0, -99999999.99999, 99999999.99999, 0.0, 1.0, -1.0, 0.9, -0.9, 45.0, null, -45.0, 3.14159, -3.14159, 2.71828, -2.71828, 45.99999, -45.99999, 450.45001, 0.00001, -0.00001};\n+    private static final Integer[] COL17 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, null, 11, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11};\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        super.setup();\n+\n+        accessor = new ParquetFileAccessor();\n+        resolver = new ParquetResolver();\n+        context = new RequestContext();\n+\n+        String path = Objects.requireNonNull(getClass().getClassLoader().getResource(\"parquet/parquet_types.parquet\")).getPath();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setRequestType(RequestContext.RequestType.READ_BRIDGE);\n+        context.setDataSource(path);\n+        context.setFragmentMetadata(HdfsUtilities.prepareFragmentMetadata(0, 4196, Fragment.HOSTS));\n+        context.setTupleDescription(columnDescriptors);\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+    }\n+\n+    @Test\n+    public void testNoFilter() throws Exception {\n+        // all rows are expected\n+        helper(COL1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM0NjMzMA==", "bodyText": "great suggestion", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r370346330", "createdAt": "2020-01-23T20:48:03Z", "author": {"login": "frankgh"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetFilterPushDownTest.java", "diffHunk": "@@ -0,0 +1,836 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.Fragment;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetFileAccessor;\n+import org.greenplum.pxf.plugins.hdfs.ParquetResolver;\n+import org.greenplum.pxf.plugins.hdfs.utilities.HdfsUtilities;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ParquetFilterPushDownTest extends ParquetBaseTest {\n+\n+    // From resources/parquet/parquet_types.csv\n+    private static final int[] COL1 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25};\n+    private static final String[] COL2 = {\"row1\", \"row2\", \"row3\", \"row4\", \"row5\", \"row6\", \"row7\", \"row8\", \"row9\", \"row10\", \"row11\", \"row12_text_null\", \"row13_int_null\", \"row14_double_null\", \"row15_decimal_null\", \"row16_timestamp_null\", \"row17_real_null\", \"row18_bigint_null\", \"row19_bool_null\", \"row20\", \"row21_smallint_null\", \"row22_date_null\", \"row23_varchar_null\", \"row24_char_null\", \"row25_binary_null\"};\n+    private static final String[] COL3 = {\"2019-12-01\", \"2019-12-02\", \"2019-12-03\", \"2019-12-04\", \"2019-12-05\", \"2019-12-06\", \"2019-12-07\", \"2019-12-08\", \"2019-12-09\", \"2019-12-10\", \"2019-12-11\", \"2019-12-12\", \"2019-12-13\", \"2019-12-14\", \"2019-12-15\", \"2019-12-16\", \"2019-12-17\", \"2019-12-18\", \"2019-12-19\", \"2019-12-20\", \"2019-12-21\", null, \"2019-12-23\", \"2019-12-24\", \"2019-12-25\"};\n+    private static final Double[] COL4 = {1200.0, 1300.0, 1400.0, 1500.0, 1600.0, 1700.0, 1800.0, 1900.0, 2000.0, 2100.0, 2200.0, 2300.0, 2400.0, null, 2500.0, 2550.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0, 2600.0,};\n+    private static final String[] COL5 = {\"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"excellent\", \"bad\", \"good\", null, \"good\", \"excellent\", \"good\", \"bad\", \"good\", \"bad\", \"good\", \"excellent\", \"good\", \"excellent\", \"good\", \"bad\", \"good\"};\n+    private static final Boolean[] COL6 = {false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, null, false, false, false, false, false, false};\n+    private static final String[] COL7 = {\"2013-07-14T04:00:00Z\", \"2013-07-14T04:00:00Z\", \"2013-07-16T04:00:00Z\", \"2013-07-17T04:00:00Z\", \"2013-07-18T04:00:00Z\", \"2013-07-19T04:00:00Z\", \"2013-07-20T04:00:00Z\", \"2013-07-21T04:00:00Z\", \"2013-07-22T04:00:00Z\", \"2013-07-23T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-25T04:00:00Z\", null, \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\", \"2013-07-24T04:00:00Z\"};\n+    private static final Long[] COL8 = {2147483647L, 2147483648L, 2147483649L, 2147483650L, 2147483651L, 2147483652L, 2147483653L, 2147483654L, 2147483655L, 2147483656L, 2147483657L, 2147483658L, 2147483659L, 2147483660L, 2147483661L, 2147483662L, 2147483663L, null, -1L, -2147483643L, -2147483644L, -2147483645L, -2147483646L, -2147483647L, -2147483648L};\n+    private static final Byte[] COL9 = {0b00110001, 0b00110010, 0b00110011, 0b00110100, 0b00110101, 0b00110110, 0b00110111, 0b00111000, 0b00111001, 0b00110000, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, 0b00110001, null};\n+    private static final Short[] COL10 = {-32768, -31500, -31000, -30000, -20000, -10000, -1000, -550, -320, -120, -40, -1, 0, 1, 100, 1000, 10000, 20000, 30000, 31000, null, 32100, 32200, 32500, 32767};\n+    private static final Float[] COL11 = {7.7F, 8.7F, 9.7F, 10.7F, 11.7F, 12.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, null, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F, 7.7F};\n+    private static final String[] COL12 = {\"s_6\", \"s_7\", \"s_8\", \"s_9\", \"s_10\", \"s_11\", \"s_12\", \"s_13\", \"s_14\", \"s_15\", \"s_16\", \"s_16\", \"s_16\", \"s_16\", \"s_17\", \"s_160\", \"s_161\", \"s_162\", \"s_163\", \"s_164\", \"s_165\", \"s_166\", null, \"s_168\", \"s_169\"};\n+    private static final String[] COL13 = {\"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"EUR\", \"UAH\", \"USD\", \"UAH\", \"EUR\", \"USD\", \"UAH\", \"USD\", \"USD\", \"EUR\", \"USD\", \"USD\", \"UAH\", \"USD\", \"EUR\", \"EUR\", null, \"USD\"};\n+    private static final Double[] COL14 = {1.23456, 1.23456, -1.23456, 123456789.1, 1E-12, 1234.889, 0.0001, 45678.00002, 23457.1, 45678.00002, 0.123456789, 0.123456789, 0.123456789, 0.123456789, null, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789, 0.123456789};\n+    private static final Double[] COL15 = {0.0, 123.45, -1.45, 0.25, -.25, 999.99, -999.99, 1.0, -1.0, 789.0, -789.0, 0.99, -0.99, 1.99, null, -1.99, 15.99, -15.99, -299.99, 299.99, 555.55, 0.15, 3.89, 3.14, 8.0};\n+    private static final Double[] COL16 = {0.12345, -0.12345, 12345678.90123, -12345678.90123, 99999999.0, -99999999.0, -99999999.99999, 99999999.99999, 0.0, 1.0, -1.0, 0.9, -0.9, 45.0, null, -45.0, 3.14159, -3.14159, 2.71828, -2.71828, 45.99999, -45.99999, 450.45001, 0.00001, -0.00001};\n+    private static final Integer[] COL17 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, null, 11, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11};\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+\n+    @Before\n+    public void setup() throws Exception {\n+        super.setup();\n+\n+        accessor = new ParquetFileAccessor();\n+        resolver = new ParquetResolver();\n+        context = new RequestContext();\n+\n+        String path = Objects.requireNonNull(getClass().getClassLoader().getResource(\"parquet/parquet_types.parquet\")).getPath();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setRequestType(RequestContext.RequestType.READ_BRIDGE);\n+        context.setDataSource(path);\n+        context.setFragmentMetadata(HdfsUtilities.prepareFragmentMetadata(0, 4196, Fragment.HOSTS));\n+        context.setTupleDescription(columnDescriptors);\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+    }\n+\n+    @Test\n+    public void testNoFilter() throws Exception {\n+        // all rows are expected\n+        helper(COL1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5NzkwOA=="}, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NjI1MjMyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilderTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTo1MjoxNFrOFgw1zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QwMTo1MjoxNFrOFgw1zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTg5ODk1OQ==", "bodyText": "In this case you might rename helper to filterBuilderFromFilterString()", "url": "https://github.com/greenplum-db/pxf/pull/286#discussion_r369898959", "createdAt": "2020-01-23T01:52:14Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetRecordFilterBuilderTest.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package org.greenplum.pxf.plugins.hdfs.parquet;\n+\n+import org.greenplum.pxf.api.filter.FilterParser;\n+import org.greenplum.pxf.api.filter.Node;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+public class ParquetRecordFilterBuilderTest extends ParquetBaseTest {\n+\n+    @Rule\n+    public ExpectedException thrown = ExpectedException.none();\n+\n+    @Test\n+    public void testUnsupportedOperationError() throws Exception {\n+        thrown.expect(UnsupportedOperationException.class);\n+        thrown.expectMessage(\"not supported IN\");\n+\n+        // a16 in (11, 12)\n+        helper(\"a16m1007s2d11s2d12o10\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cce8f1bdfa32a465bc68b9825b88d81f7fd0683"}, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3560, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}