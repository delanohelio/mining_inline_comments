{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUwMjc5NDIw", "number": 412, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzozMzoyOVrOEPO0Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzozNDoyMVrOEPO1Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NDA2ODIzOnYy", "diffSide": "RIGHT", "path": "docs/content/hdfs_avro.html.md.erb", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzozMzoyOVrOGy1Tmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzo1MDoyOFrOGy17Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1NTM1NQ==", "bodyText": "Does it really pad to n+1? I thought it would be n.", "url": "https://github.com/greenplum-db/pxf/pull/412#discussion_r455955355", "createdAt": "2020-07-16T17:33:29Z", "author": {"login": "oliverralbertini"}, "path": "docs/content/hdfs_avro.html.md.erb", "diffHunk": "@@ -75,14 +75,18 @@ PXF uses the following data type mapping when writing Avro data:\n | boolean | boolean |\n | bytea | bytes |\n | double | double |\n+| char<sup>1</sup> | string |\n | enum | string |\n | int |  int |\n | real | float |\n-| smallint<sup>1</sup> | int |\n+| smallint<sup>2</sup> | int |\n | text | string |\n+| varchar | string |\n | array ([]), enum, record | string |\n+| date, time, timestamp, timestamptz | string |\n \n-</br><sup>1</sup>&nbsp;PXF converts Greenplum <code>smallint</code> types to <code>int</code> before it writes the Avro data. Be sure to read the field into an <code>int</code>.\n+</br><sup>1</sup>&nbsp;PXF right-pads <code>char[<i>n</i>]</code> types to length <code><i>n</i> + 1</code>, if required, with white space.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7c46559f4140d10450a88f7b0afd8b2cf523fc8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2NTQ4Mw==", "bodyText": "i saw on https://gpdb.docs.pivotal.io/6-9/ref_guide/data_types.html that the length of a char array is size of array plus one byte, assumed that would carry over.  i changed this to n.", "url": "https://github.com/greenplum-db/pxf/pull/412#discussion_r455965483", "createdAt": "2020-07-16T17:50:28Z", "author": {"login": "lisakowen"}, "path": "docs/content/hdfs_avro.html.md.erb", "diffHunk": "@@ -75,14 +75,18 @@ PXF uses the following data type mapping when writing Avro data:\n | boolean | boolean |\n | bytea | bytes |\n | double | double |\n+| char<sup>1</sup> | string |\n | enum | string |\n | int |  int |\n | real | float |\n-| smallint<sup>1</sup> | int |\n+| smallint<sup>2</sup> | int |\n | text | string |\n+| varchar | string |\n | array ([]), enum, record | string |\n+| date, time, timestamp, timestamptz | string |\n \n-</br><sup>1</sup>&nbsp;PXF converts Greenplum <code>smallint</code> types to <code>int</code> before it writes the Avro data. Be sure to read the field into an <code>int</code>.\n+</br><sup>1</sup>&nbsp;PXF right-pads <code>char[<i>n</i>]</code> types to length <code><i>n</i> + 1</code>, if required, with white space.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1NTM1NQ=="}, "originalCommit": {"oid": "c7c46559f4140d10450a88f7b0afd8b2cf523fc8"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NDA3MTI2OnYy", "diffSide": "RIGHT", "path": "docs/content/hdfs_avro.html.md.erb", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzozNDoyMlrOGy1VgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNzozNjowMFrOGy1ZMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1NTg0MQ==", "bodyText": "xz also would be impacted by CODEC_LEVEL", "url": "https://github.com/greenplum-db/pxf/pull/412#discussion_r455955841", "createdAt": "2020-07-16T17:34:22Z", "author": {"login": "oliverralbertini"}, "path": "docs/content/hdfs_avro.html.md.erb", "diffHunk": "@@ -150,7 +154,7 @@ The PXF `hdfs:avro` profile supports encoding- and compression-related write opt\n \n | Write Option  | Value Description |\n |-------|-------------------------------------|\n-| COMPRESSION_CODEC    | The compression codec alias. Supported compression codecs for writing Avro data include: `snappy`, `deflate`, and `uncompressed` . If this option is not provided, PXF compresses the data using `deflate` compression. |\n+| COMPRESSION_CODEC    | The compression codec alias. Supported compression codecs for writing Avro data include: `bzip2`, `xz`, `snappy`, `deflate`, and `uncompressed` . If this option is not provided, PXF compresses the data using `deflate` compression. |\n | CODEC_LEVEL    | The compression level (applicable to the `deflate` codec only). This level controls the trade-off between speed and compression. Valid values are 1 (fastest) to 9 (most compressed). The default compression level when using the `deflate` codec is 6. |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7c46559f4140d10450a88f7b0afd8b2cf523fc8"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1Njc4NA==", "bodyText": "see here for details: \n  \n    \n      pxf/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/AvroFileAccessor.java\n    \n    \n         Line 162\n      in\n      e18ee9e\n    \n    \n    \n    \n\n        \n          \n           writer.setCodec(CodecFactory.xzCodec(codecCompressionLevel));", "url": "https://github.com/greenplum-db/pxf/pull/412#discussion_r455956784", "createdAt": "2020-07-16T17:36:00Z", "author": {"login": "oliverralbertini"}, "path": "docs/content/hdfs_avro.html.md.erb", "diffHunk": "@@ -150,7 +154,7 @@ The PXF `hdfs:avro` profile supports encoding- and compression-related write opt\n \n | Write Option  | Value Description |\n |-------|-------------------------------------|\n-| COMPRESSION_CODEC    | The compression codec alias. Supported compression codecs for writing Avro data include: `snappy`, `deflate`, and `uncompressed` . If this option is not provided, PXF compresses the data using `deflate` compression. |\n+| COMPRESSION_CODEC    | The compression codec alias. Supported compression codecs for writing Avro data include: `bzip2`, `xz`, `snappy`, `deflate`, and `uncompressed` . If this option is not provided, PXF compresses the data using `deflate` compression. |\n | CODEC_LEVEL    | The compression level (applicable to the `deflate` codec only). This level controls the trade-off between speed and compression. Valid values are 1 (fastest) to 9 (most compressed). The default compression level when using the `deflate` codec is 6. |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1NTg0MQ=="}, "originalCommit": {"oid": "c7c46559f4140d10450a88f7b0afd8b2cf523fc8"}, "originalPosition": 36}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3413, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}