{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxODQ0MTc4", "number": 492, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDo1Mjo0M1rOE5q49g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxOTo0MDowMlrOE6MEfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTA2OTk4OnYy", "diffSide": "RIGHT", "path": "docs/content/access_hdfs.html.md.erb", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDo1Mjo0M1rOH0Q54A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDo1Nzo0OVrOH0RQAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2NDk2MA==", "bodyText": "product question: should we rename these as hive, hive:orc, hive:rc with vectorized being a user option ? @frankgh ?", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r524564960", "createdAt": "2020-11-16T20:52:43Z", "author": {"login": "denalex"}, "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -103,21 +103,21 @@ The PXF Hadoop connectors provide built-in profiles to support the following dat\n \n The PXF Hadoop connectors expose the following profiles to read, and in many cases write, these supported data formats:\n \n-| Data Source | Data Format | Profile Name(s) | Deprecated Profile Name |\n-|-----|------|---------|------------|\n-| HDFS | delimited single line [text](hdfs_text.html#profile_text) | hdfs:text | HdfsTextSimple |\n-| HDFS | delimited [text with quoted linefeeds](hdfs_text.html#profile_textmulti) | hdfs:text:multi | HdfsTextMulti |\n-| HDFS | [Avro](hdfs_avro.html) | hdfs:avro | Avro |\n-| HDFS | [JSON](hdfs_json.html) | hdfs:json | Json |\n-| HDFS | [Parquet](hdfs_parquet.html) | hdfs:parquet | Parquet |\n-| HDFS | AvroSequenceFile | hdfs:AvroSequenceFile | n/a |\n-| HDFS | [SequenceFile](hdfs_seqfile.html) | hdfs:SequenceFile | SequenceWritable |\n-| [Hive](hive_pxf.html) | stored as TextFile | Hive, [HiveText](hive_pxf.html#hive_text) | n/a |\n-| [Hive](hive_pxf.html) | stored as SequenceFile | Hive | n/a |\n-| [Hive](hive_pxf.html) | stored as RCFile | Hive, [HiveRC](hive_pxf.html#hive_hiverc) | n/a |\n-| [Hive](hive_pxf.html) | stored as ORC | Hive, [HiveORC](hive_pxf.html#hive_orc), HiveVectorizedORC | n/a |\n-| [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n-| [HBase](hbase_pxf.html) | Any | HBase | n/a |\n+| Data Source | Data Format | Profile Name(s) |\n+|-------------|------|---------|\n+| HDFS | delimited single line [text](hdfs_text.html#profile_text) | hdfs:text |\n+| HDFS | delimited [text with quoted linefeeds](hdfs_text.html#profile_textmulti) |\n+| HDFS | [Avro](hdfs_avro.html) | hdfs:avro |\n+| HDFS | [JSON](hdfs_json.html) | hdfs:json |\n+| HDFS | [Parquet](hdfs_parquet.html) | hdfs:parquet |\n+| HDFS | AvroSequenceFile | hdfs:AvroSequenceFile |\n+| HDFS | [SequenceFile](hdfs_seqfile.html) | hdfs:SequenceFile |\n+| [Hive](hive_pxf.html) | stored as TextFile | Hive, [HiveText](hive_pxf.html#hive_text) |\n+| [Hive](hive_pxf.html) | stored as SequenceFile | Hive |\n+| [Hive](hive_pxf.html) | stored as RCFile | Hive, [HiveRC](hive_pxf.html#hive_hiverc) |\n+| [Hive](hive_pxf.html) | stored as ORC | Hive, [HiveORC](hive_pxf.html#hive_orc), HiveVectorizedORC |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f036fe24a9a35c7eccdb179563bc3544a6884ae2"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU3MDYyNQ==", "bodyText": "that makes sense, we wanted to do that at some point, so it makes sense to do it now", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r524570625", "createdAt": "2020-11-16T20:57:49Z", "author": {"login": "frankgh"}, "path": "docs/content/access_hdfs.html.md.erb", "diffHunk": "@@ -103,21 +103,21 @@ The PXF Hadoop connectors provide built-in profiles to support the following dat\n \n The PXF Hadoop connectors expose the following profiles to read, and in many cases write, these supported data formats:\n \n-| Data Source | Data Format | Profile Name(s) | Deprecated Profile Name |\n-|-----|------|---------|------------|\n-| HDFS | delimited single line [text](hdfs_text.html#profile_text) | hdfs:text | HdfsTextSimple |\n-| HDFS | delimited [text with quoted linefeeds](hdfs_text.html#profile_textmulti) | hdfs:text:multi | HdfsTextMulti |\n-| HDFS | [Avro](hdfs_avro.html) | hdfs:avro | Avro |\n-| HDFS | [JSON](hdfs_json.html) | hdfs:json | Json |\n-| HDFS | [Parquet](hdfs_parquet.html) | hdfs:parquet | Parquet |\n-| HDFS | AvroSequenceFile | hdfs:AvroSequenceFile | n/a |\n-| HDFS | [SequenceFile](hdfs_seqfile.html) | hdfs:SequenceFile | SequenceWritable |\n-| [Hive](hive_pxf.html) | stored as TextFile | Hive, [HiveText](hive_pxf.html#hive_text) | n/a |\n-| [Hive](hive_pxf.html) | stored as SequenceFile | Hive | n/a |\n-| [Hive](hive_pxf.html) | stored as RCFile | Hive, [HiveRC](hive_pxf.html#hive_hiverc) | n/a |\n-| [Hive](hive_pxf.html) | stored as ORC | Hive, [HiveORC](hive_pxf.html#hive_orc), HiveVectorizedORC | n/a |\n-| [Hive](hive_pxf.html) | stored as Parquet | Hive | n/a |\n-| [HBase](hbase_pxf.html) | Any | HBase | n/a |\n+| Data Source | Data Format | Profile Name(s) |\n+|-------------|------|---------|\n+| HDFS | delimited single line [text](hdfs_text.html#profile_text) | hdfs:text |\n+| HDFS | delimited [text with quoted linefeeds](hdfs_text.html#profile_textmulti) |\n+| HDFS | [Avro](hdfs_avro.html) | hdfs:avro |\n+| HDFS | [JSON](hdfs_json.html) | hdfs:json |\n+| HDFS | [Parquet](hdfs_parquet.html) | hdfs:parquet |\n+| HDFS | AvroSequenceFile | hdfs:AvroSequenceFile |\n+| HDFS | [SequenceFile](hdfs_seqfile.html) | hdfs:SequenceFile |\n+| [Hive](hive_pxf.html) | stored as TextFile | Hive, [HiveText](hive_pxf.html#hive_text) |\n+| [Hive](hive_pxf.html) | stored as SequenceFile | Hive |\n+| [Hive](hive_pxf.html) | stored as RCFile | Hive, [HiveRC](hive_pxf.html#hive_hiverc) |\n+| [Hive](hive_pxf.html) | stored as ORC | Hive, [HiveORC](hive_pxf.html#hive_orc), HiveVectorizedORC |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2NDk2MA=="}, "originalCommit": {"oid": "f036fe24a9a35c7eccdb179563bc3544a6884ae2"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NDQ5MzQ5OnYy", "diffSide": "RIGHT", "path": "docs/content/upgrade_5_to_6.html.md.erb", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxOTozODoxOFrOH1GOWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxOTozODoxOFrOH1GOWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQzODU1Mg==", "bodyText": "are -> were\n?  Maybe, to work for later releases.", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r525438552", "createdAt": "2020-11-17T19:38:18Z", "author": {"login": "dyozie"}, "path": "docs/content/upgrade_5_to_6.html.md.erb", "diffHunk": "@@ -106,7 +106,16 @@ After you install the new version of PXF, perform the following procedure:\n \n 1. **If you are upgrading to PXF version 6.0**:\n \n-    1. step 1\n+    1. Ensure that you no longer reference previously deprecated features that are removed in PXF 6.0:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f036fe24a9a35c7eccdb179563bc3544a6884ae2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NDUwNjIzOnYy", "diffSide": "RIGHT", "path": "docs/content/upgrade_5_to_6.html.md.erb", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxOTo0MDowMlrOH1GW1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxOTo0MDowMlrOH1GW1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQ0MDcyNg==", "bodyText": "Not sure why the all-caps property names aren't in code format here.  But if that's the convention used throughout it's fine.", "url": "https://github.com/greenplum-db/pxf/pull/492#discussion_r525440726", "createdAt": "2020-11-17T19:40:02Z", "author": {"login": "dyozie"}, "path": "docs/content/upgrade_5_to_6.html.md.erb", "diffHunk": "@@ -106,7 +106,16 @@ After you install the new version of PXF, perform the following procedure:\n \n 1. **If you are upgrading to PXF version 6.0**:\n \n-    1. step 1\n+    1. Ensure that you no longer reference previously deprecated features that are removed in PXF 6.0:\n+\n+        | Deprecated Feature | Use Instead |\n+        | -------------------|-------------|\n+        | Hadoop profile names | hdfs:\\<profile> as noted [here](access_hdfs.html#hadoop_connectors) |\n+        | `jdbc.user.impersonation` property | `pxf.service.user.impersonation` property in the [jdbc&#8209;site.xml](jdbc_cfg.html#jdbcimpers) server configuration file |\n+        | PXF_KEYTAB configuration property | `pxf.service.kerberos.keytab` property in the [pxf&#8209;site.xml](cfg_server.html#pxf-site) server configuration file |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f036fe24a9a35c7eccdb179563bc3544a6884ae2"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3539, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}