{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3OTkzODYy", "number": 368, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozNDowNVrOECr1Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo1MjoxMFrOECsT9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjUwNzU4OnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/MultipleHiveFragmentsPerFileFragmenter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozNDowNVrOGfS1kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowMjo0OFrOGfT9ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2NzY2NQ==", "bodyText": "when will this be uncommented ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435467665", "createdAt": "2020-06-04T18:34:05Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/MultipleHiveFragmentsPerFileFragmenter.java", "diffHunk": "@@ -1,122 +1,122 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n-import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n-import org.apache.hadoop.mapred.FileInputFormat;\n-import org.apache.hadoop.mapred.FileSplit;\n-import org.apache.hadoop.mapred.InputFormat;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.greenplum.pxf.api.model.BaseFragmenter;\n-import org.greenplum.pxf.api.model.Fragment;\n-import org.greenplum.pxf.api.model.Metadata;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveClientWrapper;\n-import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n-import org.greenplum.pxf.plugins.hive.HiveUserData;\n-\n-import java.io.ByteArrayOutputStream;\n-import java.io.ObjectOutputStream;\n-import java.util.List;\n-import java.util.Properties;\n-\n-\n-/**\n- * Fragmenter which splits one file into multiple fragments. Helps to simulate a\n- * case of big files.\n- * <p>\n- * inputData has to have following parameters:\n- * TEST-FRAGMENTS-NUM - defines how many fragments will be returned for current file\n- */\n-public class MultipleHiveFragmentsPerFileFragmenter extends BaseFragmenter {\n-    private static final Log LOG = LogFactory.getLog(MultipleHiveFragmentsPerFileFragmenter.class);\n-\n-    private static final long SPLIT_SIZE = 1024;\n-    private JobConf jobConf;\n-    private IMetaStoreClient client;\n-    private HiveClientWrapper hiveClientWrapper;\n-\n-    public MultipleHiveFragmentsPerFileFragmenter() {\n-        hiveClientWrapper = HiveClientWrapper.getInstance();\n-    }\n-\n-    @Override\n-    public void initialize(RequestContext context) {\n-        super.initialize(context);\n-        jobConf = new JobConf(configuration, MultipleHiveFragmentsPerFileFragmenter.class);\n-        client = hiveClientWrapper.initHiveClient(context, configuration);\n-    }\n-\n-    @Override\n-    public List<Fragment> getFragments() throws Exception {\n-        String localhostname = java.net.InetAddress.getLocalHost().getHostName();\n-        String[] localHosts = new String[]{localhostname, localhostname};\n-\n-        // TODO whitelist property\n-        int fragmentsNum = Integer.parseInt(context.getOption(\"TEST-FRAGMENTS-NUM\"));\n-        Metadata.Item tblDesc = hiveClientWrapper.extractTableFromName(context.getDataSource());\n-        Table tbl = hiveClientWrapper.getHiveTable(client, tblDesc);\n-        Properties properties = getSchema(tbl);\n-\n-        for (int i = 0; i < fragmentsNum; i++) {\n-\n-            String userData = \"inputFormatName\" + HiveUserData.HIVE_UD_DELIM\n-                    + tbl.getSd().getSerdeInfo().getSerializationLib()\n-                    + HiveUserData.HIVE_UD_DELIM + \"propertiesString\"\n-                    + HiveUserData.HIVE_UD_DELIM + HiveDataFragmenter.HIVE_NO_PART_TBL\n-                    + HiveUserData.HIVE_UD_DELIM + \"filterInFragmenter\"\n-                    + HiveUserData.HIVE_UD_DELIM + \"delimiter\"\n-                    + HiveUserData.HIVE_UD_DELIM + properties.getProperty(\"columns.types\");\n-\n-            ByteArrayOutputStream bas = new ByteArrayOutputStream();\n-            ObjectOutputStream os = new ObjectOutputStream(bas);\n-            os.writeLong(i * SPLIT_SIZE); // start\n-            os.writeLong(SPLIT_SIZE); // length\n-            os.writeObject(localHosts); // hosts\n-            os.close();\n-\n-            String filePath = getFilePath(tbl);\n-\n-            fragments.add(new Fragment(filePath, localHosts, bas.toByteArray(), userData.getBytes()));\n-        }\n-\n-        return fragments;\n-    }\n-\n-\n-    private static Properties getSchema(Table table) {\n-        return MetaStoreUtils.getSchema(table.getSd(), table.getSd(),\n-                table.getParameters(), table.getDbName(), table.getTableName(),\n-                table.getPartitionKeys());\n-    }\n-\n-    private String getFilePath(Table tbl) throws Exception {\n-\n-        StorageDescriptor descTable = tbl.getSd();\n-\n-        InputFormat<?, ?> fformat = HiveDataFragmenter.makeInputFormat(descTable.getInputFormat(), jobConf);\n-\n-        FileInputFormat.setInputPaths(jobConf, new Path(descTable.getLocation()));\n-\n-        InputSplit[] splits;\n-        try {\n-            splits = fformat.getSplits(jobConf, 1);\n-        } catch (org.apache.hadoop.mapred.InvalidInputException e) {\n-            LOG.debug(\"getSplits failed on \" + e.getMessage());\n-            throw new RuntimeException(\"Unable to get file path for table.\");\n-        }\n-\n-        for (InputSplit split : splits) {\n-            FileSplit fsp = (FileSplit) split;\n-            String[] hosts = fsp.getLocations();\n-            String filepath = fsp.getPath().toString();\n-            return filepath;\n-        }\n-        throw new RuntimeException(\"Unable to get file path for table.\");\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.apache.hadoop.fs.Path;\n+//import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+//import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n+//import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+//import org.apache.hadoop.hive.metastore.api.Table;\n+//import org.apache.hadoop.mapred.FileInputFormat;\n+//import org.apache.hadoop.mapred.FileSplit;\n+//import org.apache.hadoop.mapred.InputFormat;\n+//import org.apache.hadoop.mapred.InputSplit;\n+//import org.apache.hadoop.mapred.JobConf;\n+//import org.greenplum.pxf.api.model.BaseFragmenter;\n+//import org.greenplum.pxf.api.model.Fragment;\n+//import org.greenplum.pxf.api.model.Metadata;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveClientWrapper;\n+//import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n+//import org.greenplum.pxf.plugins.hive.HiveUserData;\n+//\n+//import java.io.ByteArrayOutputStream;\n+//import java.io.ObjectOutputStream;\n+//import java.util.List;\n+//import java.util.Properties;\n+//\n+//\n+///**\n+// * Fragmenter which splits one file into multiple fragments. Helps to simulate a\n+// * case of big files.\n+// * <p>\n+// * inputData has to have following parameters:\n+// * TEST-FRAGMENTS-NUM - defines how many fragments will be returned for current file\n+// */\n+//public class MultipleHiveFragmentsPerFileFragmenter extends BaseFragmenter {\n+//    private static final Log LOG = LogFactory.getLog(MultipleHiveFragmentsPerFileFragmenter.class);\n+//\n+//    private static final long SPLIT_SIZE = 1024;\n+//    private JobConf jobConf;\n+//    private IMetaStoreClient client;\n+//    private HiveClientWrapper hiveClientWrapper;\n+//\n+//    public MultipleHiveFragmentsPerFileFragmenter() {\n+//        hiveClientWrapper = HiveClientWrapper.getInstance();\n+//    }\n+//\n+//    @Override\n+//    public void initialize(RequestContext context) {\n+//        super.initialize(context);\n+//        jobConf = new JobConf(configuration, MultipleHiveFragmentsPerFileFragmenter.class);\n+//        client = hiveClientWrapper.initHiveClient(context, configuration);\n+//    }\n+//\n+//    @Override\n+//    public List<Fragment> getFragments() throws Exception {\n+//        String localhostname = java.net.InetAddress.getLocalHost().getHostName();\n+//        String[] localHosts = new String[]{localhostname, localhostname};\n+//\n+//        // TODO whitelist property\n+//        int fragmentsNum = Integer.parseInt(context.getOption(\"TEST-FRAGMENTS-NUM\"));\n+//        Metadata.Item tblDesc = hiveClientWrapper.extractTableFromName(context.getDataSource());\n+//        Table tbl = hiveClientWrapper.getHiveTable(client, tblDesc);\n+//        Properties properties = getSchema(tbl);\n+//\n+//        for (int i = 0; i < fragmentsNum; i++) {\n+//\n+//            String userData = \"inputFormatName\" + HiveUserData.HIVE_UD_DELIM\n+//                    + tbl.getSd().getSerdeInfo().getSerializationLib()\n+//                    + HiveUserData.HIVE_UD_DELIM + \"propertiesString\"\n+//                    + HiveUserData.HIVE_UD_DELIM + HiveDataFragmenter.HIVE_NO_PART_TBL\n+//                    + HiveUserData.HIVE_UD_DELIM + \"filterInFragmenter\"\n+//                    + HiveUserData.HIVE_UD_DELIM + \"delimiter\"\n+//                    + HiveUserData.HIVE_UD_DELIM + properties.getProperty(\"columns.types\");\n+//\n+//            ByteArrayOutputStream bas = new ByteArrayOutputStream();\n+//            ObjectOutputStream os = new ObjectOutputStream(bas);\n+//            os.writeLong(i * SPLIT_SIZE); // start\n+//            os.writeLong(SPLIT_SIZE); // length\n+//            os.writeObject(localHosts); // hosts\n+//            os.close();\n+//\n+//            String filePath = getFilePath(tbl);\n+//\n+//            fragments.add(new Fragment(filePath, localHosts, bas.toByteArray(), userData.getBytes()));\n+//        }\n+//\n+//        return fragments;\n+//    }\n+//\n+//\n+//    private static Properties getSchema(Table table) {\n+//        return MetaStoreUtils.getSchema(table.getSd(), table.getSd(),\n+//                table.getParameters(), table.getDbName(), table.getTableName(),\n+//                table.getPartitionKeys());\n+//    }\n+//\n+//    private String getFilePath(Table tbl) throws Exception {\n+//\n+//        StorageDescriptor descTable = tbl.getSd();\n+//\n+//        InputFormat<?, ?> fformat = HiveDataFragmenter.makeInputFormat(descTable.getInputFormat(), jobConf);\n+//\n+//        FileInputFormat.setInputPaths(jobConf, new Path(descTable.getLocation()));\n+//\n+//        InputSplit[] splits;\n+//        try {\n+//            splits = fformat.getSplits(jobConf, 1);\n+//        } catch (org.apache.hadoop.mapred.InvalidInputException e) {\n+//            LOG.debug(\"getSplits failed on \" + e.getMessage());\n+//            throw new RuntimeException(\"Unable to get file path for table.\");\n+//        }\n+//\n+//        for (InputSplit split : splits) {\n+//            FileSplit fsp = (FileSplit) split;\n+//            String[] hosts = fsp.getLocations();\n+//            String filepath = fsp.getPath().toString();\n+//            return filepath;\n+//        }\n+//        throw new RuntimeException(\"Unable to get file path for table.\");\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NjExNQ==", "bodyText": "In the PR message I state that we intentionally comment out the automation tests for Hive, which should be restored with the inclusion of the pxf-hive profile. . So with the pxf-hive profile, these classes have to be uncommented", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435486115", "createdAt": "2020-06-04T19:02:48Z", "author": {"login": "frankgh"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/MultipleHiveFragmentsPerFileFragmenter.java", "diffHunk": "@@ -1,122 +1,122 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n-import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n-import org.apache.hadoop.mapred.FileInputFormat;\n-import org.apache.hadoop.mapred.FileSplit;\n-import org.apache.hadoop.mapred.InputFormat;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.greenplum.pxf.api.model.BaseFragmenter;\n-import org.greenplum.pxf.api.model.Fragment;\n-import org.greenplum.pxf.api.model.Metadata;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveClientWrapper;\n-import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n-import org.greenplum.pxf.plugins.hive.HiveUserData;\n-\n-import java.io.ByteArrayOutputStream;\n-import java.io.ObjectOutputStream;\n-import java.util.List;\n-import java.util.Properties;\n-\n-\n-/**\n- * Fragmenter which splits one file into multiple fragments. Helps to simulate a\n- * case of big files.\n- * <p>\n- * inputData has to have following parameters:\n- * TEST-FRAGMENTS-NUM - defines how many fragments will be returned for current file\n- */\n-public class MultipleHiveFragmentsPerFileFragmenter extends BaseFragmenter {\n-    private static final Log LOG = LogFactory.getLog(MultipleHiveFragmentsPerFileFragmenter.class);\n-\n-    private static final long SPLIT_SIZE = 1024;\n-    private JobConf jobConf;\n-    private IMetaStoreClient client;\n-    private HiveClientWrapper hiveClientWrapper;\n-\n-    public MultipleHiveFragmentsPerFileFragmenter() {\n-        hiveClientWrapper = HiveClientWrapper.getInstance();\n-    }\n-\n-    @Override\n-    public void initialize(RequestContext context) {\n-        super.initialize(context);\n-        jobConf = new JobConf(configuration, MultipleHiveFragmentsPerFileFragmenter.class);\n-        client = hiveClientWrapper.initHiveClient(context, configuration);\n-    }\n-\n-    @Override\n-    public List<Fragment> getFragments() throws Exception {\n-        String localhostname = java.net.InetAddress.getLocalHost().getHostName();\n-        String[] localHosts = new String[]{localhostname, localhostname};\n-\n-        // TODO whitelist property\n-        int fragmentsNum = Integer.parseInt(context.getOption(\"TEST-FRAGMENTS-NUM\"));\n-        Metadata.Item tblDesc = hiveClientWrapper.extractTableFromName(context.getDataSource());\n-        Table tbl = hiveClientWrapper.getHiveTable(client, tblDesc);\n-        Properties properties = getSchema(tbl);\n-\n-        for (int i = 0; i < fragmentsNum; i++) {\n-\n-            String userData = \"inputFormatName\" + HiveUserData.HIVE_UD_DELIM\n-                    + tbl.getSd().getSerdeInfo().getSerializationLib()\n-                    + HiveUserData.HIVE_UD_DELIM + \"propertiesString\"\n-                    + HiveUserData.HIVE_UD_DELIM + HiveDataFragmenter.HIVE_NO_PART_TBL\n-                    + HiveUserData.HIVE_UD_DELIM + \"filterInFragmenter\"\n-                    + HiveUserData.HIVE_UD_DELIM + \"delimiter\"\n-                    + HiveUserData.HIVE_UD_DELIM + properties.getProperty(\"columns.types\");\n-\n-            ByteArrayOutputStream bas = new ByteArrayOutputStream();\n-            ObjectOutputStream os = new ObjectOutputStream(bas);\n-            os.writeLong(i * SPLIT_SIZE); // start\n-            os.writeLong(SPLIT_SIZE); // length\n-            os.writeObject(localHosts); // hosts\n-            os.close();\n-\n-            String filePath = getFilePath(tbl);\n-\n-            fragments.add(new Fragment(filePath, localHosts, bas.toByteArray(), userData.getBytes()));\n-        }\n-\n-        return fragments;\n-    }\n-\n-\n-    private static Properties getSchema(Table table) {\n-        return MetaStoreUtils.getSchema(table.getSd(), table.getSd(),\n-                table.getParameters(), table.getDbName(), table.getTableName(),\n-                table.getPartitionKeys());\n-    }\n-\n-    private String getFilePath(Table tbl) throws Exception {\n-\n-        StorageDescriptor descTable = tbl.getSd();\n-\n-        InputFormat<?, ?> fformat = HiveDataFragmenter.makeInputFormat(descTable.getInputFormat(), jobConf);\n-\n-        FileInputFormat.setInputPaths(jobConf, new Path(descTable.getLocation()));\n-\n-        InputSplit[] splits;\n-        try {\n-            splits = fformat.getSplits(jobConf, 1);\n-        } catch (org.apache.hadoop.mapred.InvalidInputException e) {\n-            LOG.debug(\"getSplits failed on \" + e.getMessage());\n-            throw new RuntimeException(\"Unable to get file path for table.\");\n-        }\n-\n-        for (InputSplit split : splits) {\n-            FileSplit fsp = (FileSplit) split;\n-            String[] hosts = fsp.getLocations();\n-            String filepath = fsp.getPath().toString();\n-            return filepath;\n-        }\n-        throw new RuntimeException(\"Unable to get file path for table.\");\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.apache.hadoop.fs.Path;\n+//import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+//import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n+//import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+//import org.apache.hadoop.hive.metastore.api.Table;\n+//import org.apache.hadoop.mapred.FileInputFormat;\n+//import org.apache.hadoop.mapred.FileSplit;\n+//import org.apache.hadoop.mapred.InputFormat;\n+//import org.apache.hadoop.mapred.InputSplit;\n+//import org.apache.hadoop.mapred.JobConf;\n+//import org.greenplum.pxf.api.model.BaseFragmenter;\n+//import org.greenplum.pxf.api.model.Fragment;\n+//import org.greenplum.pxf.api.model.Metadata;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveClientWrapper;\n+//import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n+//import org.greenplum.pxf.plugins.hive.HiveUserData;\n+//\n+//import java.io.ByteArrayOutputStream;\n+//import java.io.ObjectOutputStream;\n+//import java.util.List;\n+//import java.util.Properties;\n+//\n+//\n+///**\n+// * Fragmenter which splits one file into multiple fragments. Helps to simulate a\n+// * case of big files.\n+// * <p>\n+// * inputData has to have following parameters:\n+// * TEST-FRAGMENTS-NUM - defines how many fragments will be returned for current file\n+// */\n+//public class MultipleHiveFragmentsPerFileFragmenter extends BaseFragmenter {\n+//    private static final Log LOG = LogFactory.getLog(MultipleHiveFragmentsPerFileFragmenter.class);\n+//\n+//    private static final long SPLIT_SIZE = 1024;\n+//    private JobConf jobConf;\n+//    private IMetaStoreClient client;\n+//    private HiveClientWrapper hiveClientWrapper;\n+//\n+//    public MultipleHiveFragmentsPerFileFragmenter() {\n+//        hiveClientWrapper = HiveClientWrapper.getInstance();\n+//    }\n+//\n+//    @Override\n+//    public void initialize(RequestContext context) {\n+//        super.initialize(context);\n+//        jobConf = new JobConf(configuration, MultipleHiveFragmentsPerFileFragmenter.class);\n+//        client = hiveClientWrapper.initHiveClient(context, configuration);\n+//    }\n+//\n+//    @Override\n+//    public List<Fragment> getFragments() throws Exception {\n+//        String localhostname = java.net.InetAddress.getLocalHost().getHostName();\n+//        String[] localHosts = new String[]{localhostname, localhostname};\n+//\n+//        // TODO whitelist property\n+//        int fragmentsNum = Integer.parseInt(context.getOption(\"TEST-FRAGMENTS-NUM\"));\n+//        Metadata.Item tblDesc = hiveClientWrapper.extractTableFromName(context.getDataSource());\n+//        Table tbl = hiveClientWrapper.getHiveTable(client, tblDesc);\n+//        Properties properties = getSchema(tbl);\n+//\n+//        for (int i = 0; i < fragmentsNum; i++) {\n+//\n+//            String userData = \"inputFormatName\" + HiveUserData.HIVE_UD_DELIM\n+//                    + tbl.getSd().getSerdeInfo().getSerializationLib()\n+//                    + HiveUserData.HIVE_UD_DELIM + \"propertiesString\"\n+//                    + HiveUserData.HIVE_UD_DELIM + HiveDataFragmenter.HIVE_NO_PART_TBL\n+//                    + HiveUserData.HIVE_UD_DELIM + \"filterInFragmenter\"\n+//                    + HiveUserData.HIVE_UD_DELIM + \"delimiter\"\n+//                    + HiveUserData.HIVE_UD_DELIM + properties.getProperty(\"columns.types\");\n+//\n+//            ByteArrayOutputStream bas = new ByteArrayOutputStream();\n+//            ObjectOutputStream os = new ObjectOutputStream(bas);\n+//            os.writeLong(i * SPLIT_SIZE); // start\n+//            os.writeLong(SPLIT_SIZE); // length\n+//            os.writeObject(localHosts); // hosts\n+//            os.close();\n+//\n+//            String filePath = getFilePath(tbl);\n+//\n+//            fragments.add(new Fragment(filePath, localHosts, bas.toByteArray(), userData.getBytes()));\n+//        }\n+//\n+//        return fragments;\n+//    }\n+//\n+//\n+//    private static Properties getSchema(Table table) {\n+//        return MetaStoreUtils.getSchema(table.getSd(), table.getSd(),\n+//                table.getParameters(), table.getDbName(), table.getTableName(),\n+//                table.getPartitionKeys());\n+//    }\n+//\n+//    private String getFilePath(Table tbl) throws Exception {\n+//\n+//        StorageDescriptor descTable = tbl.getSd();\n+//\n+//        InputFormat<?, ?> fformat = HiveDataFragmenter.makeInputFormat(descTable.getInputFormat(), jobConf);\n+//\n+//        FileInputFormat.setInputPaths(jobConf, new Path(descTable.getLocation()));\n+//\n+//        InputSplit[] splits;\n+//        try {\n+//            splits = fformat.getSplits(jobConf, 1);\n+//        } catch (org.apache.hadoop.mapred.InvalidInputException e) {\n+//            LOG.debug(\"getSplits failed on \" + e.getMessage());\n+//            throw new RuntimeException(\"Unable to get file path for table.\");\n+//        }\n+//\n+//        for (InputSplit split : splits) {\n+//            FileSplit fsp = (FileSplit) split;\n+//            String[] hosts = fsp.getLocations();\n+//            String filepath = fsp.getPath().toString();\n+//            return filepath;\n+//        }\n+//        throw new RuntimeException(\"Unable to get file path for table.\");\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2NzY2NQ=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 244}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjUwODMzOnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveInputFormatFragmenterWithFilter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozNDoxNVrOGfS2Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToyMDoyNVrOGfUhZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2Nzc4Ng==", "bodyText": "when will this be uncommented ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435467786", "createdAt": "2020-06-04T18:34:15Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveInputFormatFragmenterWithFilter.java", "diffHunk": "@@ -1,35 +1,35 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveInputFormatFragmenter;\n-\n-public class HiveInputFormatFragmenterWithFilter extends HiveInputFormatFragmenter {\n-\n-    private static final Log LOG = LogFactory.getLog(HiveInputFormatFragmenterWithFilter.class);\n-\n-    @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n-        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n-    }\n-\n-    /*\n-     *  Ignores filter from gpdb, use user defined filter\n-     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n-     */\n-    private void addFilters() {\n-\n-        // TODO: whitelist the option\n-        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n-        LOG.debug(\"user defined filter: \" + filterStr);\n-        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n-            return;\n-\n-            context.setFilterString(filterStr);\n-            LOG.debug(\"User defined filter: \" + context.getFilterString());\n-\n-            LOG.debug(\"User defined filter: \" + context.hasFilter());\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveInputFormatFragmenter;\n+//\n+//public class HiveInputFormatFragmenterWithFilter extends HiveInputFormatFragmenter {\n+//\n+//    private static final Log LOG = LogFactory.getLog(HiveInputFormatFragmenterWithFilter.class);\n+//\n+//    @Override\n+//    public void initialize(RequestContext requestContext) {\n+//        super.initialize(requestContext);\n+//        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n+//    }\n+//\n+//    /*\n+//     *  Ignores filter from gpdb, use user defined filter\n+//     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n+//     */\n+//    private void addFilters() {\n+//\n+//        // TODO: whitelist the option\n+//        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n+//        LOG.debug(\"user defined filter: \" + filterStr);\n+//        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n+//            return;\n+//\n+//            context.setFilterString(filterStr);\n+//            LOG.debug(\"User defined filter: \" + context.getFilterString());\n+//\n+//            LOG.debug(\"User defined filter: \" + context.hasFilter());\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NTI3MA==", "bodyText": "it will be uncommented with pxf-hive", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435495270", "createdAt": "2020-06-04T19:20:25Z", "author": {"login": "frankgh"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveInputFormatFragmenterWithFilter.java", "diffHunk": "@@ -1,35 +1,35 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveInputFormatFragmenter;\n-\n-public class HiveInputFormatFragmenterWithFilter extends HiveInputFormatFragmenter {\n-\n-    private static final Log LOG = LogFactory.getLog(HiveInputFormatFragmenterWithFilter.class);\n-\n-    @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n-        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n-    }\n-\n-    /*\n-     *  Ignores filter from gpdb, use user defined filter\n-     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n-     */\n-    private void addFilters() {\n-\n-        // TODO: whitelist the option\n-        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n-        LOG.debug(\"user defined filter: \" + filterStr);\n-        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n-            return;\n-\n-            context.setFilterString(filterStr);\n-            LOG.debug(\"User defined filter: \" + context.getFilterString());\n-\n-            LOG.debug(\"User defined filter: \" + context.hasFilter());\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveInputFormatFragmenter;\n+//\n+//public class HiveInputFormatFragmenterWithFilter extends HiveInputFormatFragmenter {\n+//\n+//    private static final Log LOG = LogFactory.getLog(HiveInputFormatFragmenterWithFilter.class);\n+//\n+//    @Override\n+//    public void initialize(RequestContext requestContext) {\n+//        super.initialize(requestContext);\n+//        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n+//    }\n+//\n+//    /*\n+//     *  Ignores filter from gpdb, use user defined filter\n+//     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n+//     */\n+//    private void addFilters() {\n+//\n+//        // TODO: whitelist the option\n+//        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n+//        LOG.debug(\"user defined filter: \" + filterStr);\n+//        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n+//            return;\n+//\n+//            context.setFilterString(filterStr);\n+//            LOG.debug(\"User defined filter: \" + context.getFilterString());\n+//\n+//            LOG.debug(\"User defined filter: \" + context.hasFilter());\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2Nzc4Ng=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjUwODg4OnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveDataFragmenterWithFilter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozNDoyMVrOGfS2Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToyMDoxMlrOGfUhDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2Nzg3MQ==", "bodyText": "when will this be uncommented ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435467871", "createdAt": "2020-06-04T18:34:21Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveDataFragmenterWithFilter.java", "diffHunk": "@@ -1,35 +1,35 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n-\n-public class HiveDataFragmenterWithFilter extends HiveDataFragmenter {\n-\n-    private static final Log LOG = LogFactory.getLog(HiveDataFragmenterWithFilter.class);\n-\n-    @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n-        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n-    }\n-\n-    /*\n-     *  Ignores filter from gpdb, use user defined filter\n-     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n-     */\n-    private void addFilters() {\n-\n-        //TODO whitelist the option\n-        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n-        LOG.debug(\"user defined filter: \" + filterStr);\n-        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n-            return;\n-\n-        context.setFilterString(filterStr);\n-        LOG.debug(\"User defined filter: \" + context.getFilterString());\n-\n-        LOG.debug(\"User defined filter: \" + context.hasFilter());\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n+//\n+//public class HiveDataFragmenterWithFilter extends HiveDataFragmenter {\n+//\n+//    private static final Log LOG = LogFactory.getLog(HiveDataFragmenterWithFilter.class);\n+//\n+//    @Override\n+//    public void initialize(RequestContext requestContext) {\n+//        super.initialize(requestContext);\n+//        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n+//    }\n+//\n+//    /*\n+//     *  Ignores filter from gpdb, use user defined filter\n+//     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n+//     */\n+//    private void addFilters() {\n+//\n+//        //TODO whitelist the option\n+//        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n+//        LOG.debug(\"user defined filter: \" + filterStr);\n+//        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n+//            return;\n+//\n+//        context.setFilterString(filterStr);\n+//        LOG.debug(\"User defined filter: \" + context.getFilterString());\n+//\n+//        LOG.debug(\"User defined filter: \" + context.hasFilter());\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NTE4MA==", "bodyText": "it will be uncommented with pxf-hive", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435495180", "createdAt": "2020-06-04T19:20:12Z", "author": {"login": "frankgh"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/HiveDataFragmenterWithFilter.java", "diffHunk": "@@ -1,35 +1,35 @@\n-package org.greenplum.pxf.automation.testplugin;\n-\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.greenplum.pxf.api.model.RequestContext;\n-import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n-\n-public class HiveDataFragmenterWithFilter extends HiveDataFragmenter {\n-\n-    private static final Log LOG = LogFactory.getLog(HiveDataFragmenterWithFilter.class);\n-\n-    @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n-        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n-    }\n-\n-    /*\n-     *  Ignores filter from gpdb, use user defined filter\n-     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n-     */\n-    private void addFilters() {\n-\n-        //TODO whitelist the option\n-        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n-        LOG.debug(\"user defined filter: \" + filterStr);\n-        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n-            return;\n-\n-        context.setFilterString(filterStr);\n-        LOG.debug(\"User defined filter: \" + context.getFilterString());\n-\n-        LOG.debug(\"User defined filter: \" + context.hasFilter());\n-    }\n-}\n+//package org.greenplum.pxf.automation.testplugin;\n+//\n+//import org.apache.commons.logging.Log;\n+//import org.apache.commons.logging.LogFactory;\n+//import org.greenplum.pxf.api.model.RequestContext;\n+//import org.greenplum.pxf.plugins.hive.HiveDataFragmenter;\n+//\n+//public class HiveDataFragmenterWithFilter extends HiveDataFragmenter {\n+//\n+//    private static final Log LOG = LogFactory.getLog(HiveDataFragmenterWithFilter.class);\n+//\n+//    @Override\n+//    public void initialize(RequestContext requestContext) {\n+//        super.initialize(requestContext);\n+//        addFilters();  // Set the test hive filter (overwrite gpdb filter)\n+//    }\n+//\n+//    /*\n+//     *  Ignores filter from gpdb, use user defined filter\n+//     *  Set the protected filterString by reflection (only for regression, dont want to modify the original code)\n+//     */\n+//    private void addFilters() {\n+//\n+//        //TODO whitelist the option\n+//        String filterStr = context.getOption(\"TEST-HIVE-FILTER\");\n+//        LOG.debug(\"user defined filter: \" + filterStr);\n+//        if ((filterStr == null) || filterStr.isEmpty() || \"null\".equals(filterStr))\n+//            return;\n+//\n+//        context.setFilterString(filterStr);\n+//        LOG.debug(\"User defined filter: \" + context.getFilterString());\n+//\n+//        LOG.debug(\"User defined filter: \" + context.hasFilter());\n+//    }\n+//}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2Nzg3MQ=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjUxMzU1OnYy", "diffSide": "RIGHT", "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/FilterVerifyFragmentMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozNToxN1rOGfS5XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowMzowMVrOGfT-Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2ODYzNg==", "bodyText": "this does not need @JsonCreator because it has a single argument constructor and there's only 1 property ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435468636", "createdAt": "2020-06-04T18:35:17Z", "author": {"login": "denalex"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/FilterVerifyFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package org.greenplum.pxf.automation.testplugin;\n+\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+public class FilterVerifyFragmentMetadata implements FragmentMetadata {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NjI0Ng==", "bodyText": "correct", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435486246", "createdAt": "2020-06-04T19:03:01Z", "author": {"login": "frankgh"}, "path": "automation/src/main/java/org/greenplum/pxf/automation/testplugin/FilterVerifyFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package org.greenplum.pxf.automation.testplugin;\n+\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+public class FilterVerifyFragmentMetadata implements FragmentMetadata {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ2ODYzNg=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjUzNTQ3OnYy", "diffSide": "RIGHT", "path": "server/pxf-hbase/build.gradle", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODozOTo1NlrOGfTHBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToxMToxNVrOGfUO5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3MjEzMw==", "bodyText": "wonder why we need the whole netty and the whole zookeeper (where only client jars would suffice) ? also is yammer still needed with Boot ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435472133", "createdAt": "2020-06-04T18:39:56Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/build.gradle", "diffHunk": "@@ -0,0 +1,38 @@\n+apply plugin: \"java-library\"\n+\n+jar {\n+    enabled = true\n+}\n+\n+dependencies {\n+    /*******************************\n+     * API Dependencies\n+     *******************************/\n+\n+    /*******************************\n+     * Implementation Dependencies\n+     *******************************/\n+\n+    implementation(project(':pxf-api'))\n+    implementation(\"org.apache.hbase:hbase-client:${hbaseVersion}\")\n+    implementation(\"org.apache.hbase:hbase-common:${hbaseVersion}\")\n+    implementation(\"org.apache.hbase:hbase-protocol:${hbaseVersion}\")\n+    implementation(\"org.apache.htrace:htrace-core:3.1.0-incubating\")\n+    implementation(\"io.netty:netty-all:4.0.23.Final\")\n+    implementation(\"org.apache.zookeeper:zookeeper:3.4.6\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5MDUzMg==", "bodyText": "not sure, I can comment these out and see if it runs correctly", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435490532", "createdAt": "2020-06-04T19:11:15Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/build.gradle", "diffHunk": "@@ -0,0 +1,38 @@\n+apply plugin: \"java-library\"\n+\n+jar {\n+    enabled = true\n+}\n+\n+dependencies {\n+    /*******************************\n+     * API Dependencies\n+     *******************************/\n+\n+    /*******************************\n+     * Implementation Dependencies\n+     *******************************/\n+\n+    implementation(project(':pxf-api'))\n+    implementation(\"org.apache.hbase:hbase-client:${hbaseVersion}\")\n+    implementation(\"org.apache.hbase:hbase-common:${hbaseVersion}\")\n+    implementation(\"org.apache.hbase:hbase-protocol:${hbaseVersion}\")\n+    implementation(\"org.apache.htrace:htrace-core:3.1.0-incubating\")\n+    implementation(\"io.netty:netty-all:4.0.23.Final\")\n+    implementation(\"org.apache.zookeeper:zookeeper:3.4.6\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3MjEzMw=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjU1MDUxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hbase/src/main/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadata.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo0Mjo1MFrOGfTQIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTo0Nzo0OVrOGfVXjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NDQ2NQ==", "bodyText": "wonder how serialized byte[] looks like in JSON ? Is it a base64 encoded string ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435474465", "createdAt": "2020-06-04T18:42:50Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/main/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import lombok.Getter;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.codehaus.jackson.annotate.JsonCreator;\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+import java.util.Map;\n+\n+public class HBaseFragmentMetadata implements FragmentMetadata {\n+\n+    @Getter\n+    private final byte[] startKey;\n+\n+    @Getter\n+    private final byte[] endKey;\n+\n+    @Getter\n+    private final Map<String, byte[]> columnMapping;\n+\n+    public HBaseFragmentMetadata(HRegionInfo region, Map<String, byte[]> columnMapping) {\n+        this(region.getStartKey(), region.getEndKey(), columnMapping);\n+    }\n+\n+    @JsonCreator\n+    public HBaseFragmentMetadata(\n+            @JsonProperty(\"startKey\") byte[] startKey,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5MDIyNw==", "bodyText": "i will add some tests to see what the output looks like", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435490227", "createdAt": "2020-06-04T19:10:37Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/src/main/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import lombok.Getter;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.codehaus.jackson.annotate.JsonCreator;\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+import java.util.Map;\n+\n+public class HBaseFragmentMetadata implements FragmentMetadata {\n+\n+    @Getter\n+    private final byte[] startKey;\n+\n+    @Getter\n+    private final byte[] endKey;\n+\n+    @Getter\n+    private final Map<String, byte[]> columnMapping;\n+\n+    public HBaseFragmentMetadata(HRegionInfo region, Map<String, byte[]> columnMapping) {\n+        this(region.getStartKey(), region.getEndKey(), columnMapping);\n+    }\n+\n+    @JsonCreator\n+    public HBaseFragmentMetadata(\n+            @JsonProperty(\"startKey\") byte[] startKey,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NDQ2NQ=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTUwOTEzNA==", "bodyText": "just checked (and added a test), it is a base64 encoded string", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435509134", "createdAt": "2020-06-04T19:47:49Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/src/main/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadata.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import lombok.Getter;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.codehaus.jackson.annotate.JsonCreator;\n+import org.greenplum.pxf.api.utilities.FragmentMetadata;\n+\n+import java.util.Map;\n+\n+public class HBaseFragmentMetadata implements FragmentMetadata {\n+\n+    @Getter\n+    private final byte[] startKey;\n+\n+    @Getter\n+    private final byte[] endKey;\n+\n+    @Getter\n+    private final Map<String, byte[]> columnMapping;\n+\n+    public HBaseFragmentMetadata(HRegionInfo region, Map<String, byte[]> columnMapping) {\n+        this(region.getStartKey(), region.getEndKey(), columnMapping);\n+    }\n+\n+    @JsonCreator\n+    public HBaseFragmentMetadata(\n+            @JsonProperty(\"startKey\") byte[] startKey,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NDQ2NQ=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjU2ODM0OnYy", "diffSide": "RIGHT", "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo0Njo0NVrOGfTbdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToxMDowNlrOGfUMnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NzM2Nw==", "bodyText": "the more I see this, the more it reminds me of https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/beans/factory/InitializingBean.html -- wonder if we should take out explicit initialization in plugins and have them implement afterPropertiesSet() instead and have it being managed by Spring and not the Bridge. The tests will still have to do it, though, like here. Just food for thought.", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435477367", "createdAt": "2020-06-04T18:46:45Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "diffHunk": "@@ -73,47 +67,44 @@ public void tearDown() throws Exception {\n         accessor = null;\n     }\n \n-\t/*\n-\t * Test construction of HBaseAccessor.\n-\t * Actually no need for this as it is tested in all other tests\n-\t * constructing HBaseAccessor but it serves as a simple example\n-\t * of mocking\n-\t *\n-\t * HBaseAccessor is created and then HBaseTupleDescriptioncreation\n-\t * is verified\n-\t */\n+    /*\n+     * Test construction of HBaseAccessor.\n+     * Actually no need for this as it is tested in all other tests\n+     * constructing HBaseAccessor but it serves as a simple example\n+     * of mocking\n+     *\n+     * HBaseAccessor is created and then HBaseTupleDescriptioncreation\n+     * is verified\n+     */\n     @Test\n-    public void construction() throws Exception {\n+    public void construction() {\n         prepareConstruction();\n         HBaseAccessor accessor = new HBaseAccessor();\n-        accessor.initialize(context);\n-        PowerMockito.verifyNew(HBaseTupleDescription.class).withArguments(context);\n+        accessor.setRequestContext(context);\n+        accessor.initialize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4OTk1MQ==", "bodyText": "interesting. I did not know about it.", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435489951", "createdAt": "2020-06-04T19:10:06Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "diffHunk": "@@ -73,47 +67,44 @@ public void tearDown() throws Exception {\n         accessor = null;\n     }\n \n-\t/*\n-\t * Test construction of HBaseAccessor.\n-\t * Actually no need for this as it is tested in all other tests\n-\t * constructing HBaseAccessor but it serves as a simple example\n-\t * of mocking\n-\t *\n-\t * HBaseAccessor is created and then HBaseTupleDescriptioncreation\n-\t * is verified\n-\t */\n+    /*\n+     * Test construction of HBaseAccessor.\n+     * Actually no need for this as it is tested in all other tests\n+     * constructing HBaseAccessor but it serves as a simple example\n+     * of mocking\n+     *\n+     * HBaseAccessor is created and then HBaseTupleDescriptioncreation\n+     * is verified\n+     */\n     @Test\n-    public void construction() throws Exception {\n+    public void construction() {\n         prepareConstruction();\n         HBaseAccessor accessor = new HBaseAccessor();\n-        accessor.initialize(context);\n-        PowerMockito.verifyNew(HBaseTupleDescription.class).withArguments(context);\n+        accessor.setRequestContext(context);\n+        accessor.initialize();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3NzM2Nw=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjU3MDc1OnYy", "diffSide": "RIGHT", "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo0NzozNFrOGfTdFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo0NzozNFrOGfTdFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3Nzc4Mg==", "bodyText": "remove ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435477782", "createdAt": "2020-06-04T18:47:34Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseAccessorTest.java", "diffHunk": "@@ -156,11 +142,10 @@ private void prepareTableOpen() throws Exception {\n      * Helper for test setup.\n      * Sets zero columns (not realistic) and no filter\n      */\n-    private void prepareEmptyScanner() throws Exception {\n+    private void prepareEmptyScanner() {\n         scanDetails = mock(Scan.class);\n-        PowerMockito.whenNew(Scan.class).withNoArguments().thenReturn(scanDetails);\n \n-        when(tupleDescription.columns()).thenReturn(0);\n+        // when(tupleDescription.columns()).thenReturn(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjU3NDA5OnYy", "diffSide": "RIGHT", "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFilterBuilderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo0ODozNVrOGfTfLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowODoyMVrOGfUJMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3ODMxNg==", "bodyText": "did behavior switch from returning null to throwing NPE ? Is it Ok ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435478316", "createdAt": "2020-06-04T18:48:35Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFilterBuilderTest.java", "diffHunk": "@@ -94,12 +90,11 @@ public void parseNotExpressionIgnored() throws Exception {\n \n     @Test\n     public void parseNotOpCodeInConstant() throws Exception {\n-        thrown.expect(NullPointerException.class);\n \n         String filter = \"a1c25s2dl2o1a1c20s1d2o2l0\";\n         // Testing that we get past the parsing stage\n         // Very crude but it avoids instantiating all the necessary dependencies\n-        assertNull(helper(filter, null));\n+        assertThrows(NullPointerException.class, () -> helper(filter, null));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4OTA3NA==", "bodyText": "this one threw me off too, if you look closely, this class expects a NPE in line 97 of the previous version of this class.", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435489074", "createdAt": "2020-06-04T19:08:21Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFilterBuilderTest.java", "diffHunk": "@@ -94,12 +90,11 @@ public void parseNotExpressionIgnored() throws Exception {\n \n     @Test\n     public void parseNotOpCodeInConstant() throws Exception {\n-        thrown.expect(NullPointerException.class);\n \n         String filter = \"a1c25s2dl2o1a1c20s1d2o2l0\";\n         // Testing that we get past the parsing stage\n         // Very crude but it avoids instantiating all the necessary dependencies\n-        assertNull(helper(filter, null));\n+        assertThrows(NullPointerException.class, () -> helper(filter, null));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3ODMxNg=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjU4MTcxOnYy", "diffSide": "RIGHT", "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadataTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo1MDo1MFrOGfTkFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToyMjo0OFrOGfUmMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3OTU3NA==", "bodyText": "maybe also have a few tests to serialize / deserialize the POJO with ObjectMapper to make sure @JsonCreator and others are configured correctly ?", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435479574", "createdAt": "2020-06-04T18:50:50Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadataTest.java", "diffHunk": "@@ -0,0 +1,52 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertSame;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class HBaseFragmentMetadataTest {\n+\n+    @Test\n+    public void testHRegionInfoConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HRegionInfo hRegionInfo = mock(HRegionInfo.class);\n+        when(hRegionInfo.getStartKey()).thenReturn(startKey);\n+        when(hRegionInfo.getEndKey()).thenReturn(endKey);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(hRegionInfo, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+\n+    @Test\n+    public void testConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(startKey, endKey, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5MjMzNw==", "bodyText": "we have tests for json serialization / deserialization in FragmentMetadataSerDeTest", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435492337", "createdAt": "2020-06-04T19:14:45Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadataTest.java", "diffHunk": "@@ -0,0 +1,52 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertSame;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class HBaseFragmentMetadataTest {\n+\n+    @Test\n+    public void testHRegionInfoConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HRegionInfo hRegionInfo = mock(HRegionInfo.class);\n+        when(hRegionInfo.getStartKey()).thenReturn(startKey);\n+        when(hRegionInfo.getEndKey()).thenReturn(endKey);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(hRegionInfo, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+\n+    @Test\n+    public void testConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(startKey, endKey, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3OTU3NA=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NDgwMQ==", "bodyText": "yes, but SerDe is specific to the POJO, let's say if the names of properties on @JsonCreator are mis-types or mixed up, this specific POJO will have problems deserializing, while others will be ok. The test will essentially ensure that the annotation is configured correctly", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435494801", "createdAt": "2020-06-04T19:19:28Z", "author": {"login": "denalex"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadataTest.java", "diffHunk": "@@ -0,0 +1,52 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertSame;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class HBaseFragmentMetadataTest {\n+\n+    @Test\n+    public void testHRegionInfoConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HRegionInfo hRegionInfo = mock(HRegionInfo.class);\n+        when(hRegionInfo.getStartKey()).thenReturn(startKey);\n+        when(hRegionInfo.getEndKey()).thenReturn(endKey);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(hRegionInfo, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+\n+    @Test\n+    public void testConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(startKey, endKey, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3OTU3NA=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NjQ5Ng==", "bodyText": "true, I am not too keen on @JsonCreator. I much prefer clean POJOs with no annotations, but I will add tests to serialize, deserialize this class specifically.", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435496496", "createdAt": "2020-06-04T19:22:48Z", "author": {"login": "frankgh"}, "path": "server/pxf-hbase/src/test/java/org/greenplum/pxf/plugins/hbase/HBaseFragmentMetadataTest.java", "diffHunk": "@@ -0,0 +1,52 @@\n+package org.greenplum.pxf.plugins.hbase;\n+\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertSame;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class HBaseFragmentMetadataTest {\n+\n+    @Test\n+    public void testHRegionInfoConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HRegionInfo hRegionInfo = mock(HRegionInfo.class);\n+        when(hRegionInfo.getStartKey()).thenReturn(startKey);\n+        when(hRegionInfo.getEndKey()).thenReturn(endKey);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(hRegionInfo, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+\n+    @Test\n+    public void testConstructor() {\n+        final byte[] startKey = new byte[0];\n+        final byte[] endKey = new byte[0];\n+        final byte[] fooValue = new byte[0];\n+        Map<String, byte[]> columnMapping = new HashMap<>();\n+        columnMapping.put(\"foo\", fooValue);\n+\n+        HBaseFragmentMetadata metadata = new HBaseFragmentMetadata(startKey, endKey, columnMapping);\n+        assertNotNull(metadata);\n+        assertSame(startKey, metadata.getStartKey());\n+        assertSame(endKey, metadata.getEndKey());\n+        assertSame(columnMapping, metadata.getColumnMapping());\n+        assertSame(fooValue, metadata.getColumnMapping().get(\"foo\"));\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3OTU3NA=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjU4NjEzOnYy", "diffSide": "RIGHT", "path": "server/pxf-jdbc/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo1MjoxMFrOGfTnBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowNDo0NVrOGfUBoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4MDMyNA==", "bodyText": "wonder why the change", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435480324", "createdAt": "2020-06-04T18:52:10Z", "author": {"login": "denalex"}, "path": "server/pxf-jdbc/build.gradle", "diffHunk": "@@ -9,12 +9,11 @@ dependencies {\n      * API Dependencies\n      *******************************/\n \n-    api(project(':pxf-api'))\n-\n     /*******************************\n      * Implementation Dependencies\n      *******************************/\n \n+    implementation(project(':pxf-api'))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NzEzNw==", "bodyText": "api is not an api dependency for pxf-jdbc", "url": "https://github.com/greenplum-db/pxf/pull/368#discussion_r435487137", "createdAt": "2020-06-04T19:04:45Z", "author": {"login": "frankgh"}, "path": "server/pxf-jdbc/build.gradle", "diffHunk": "@@ -9,12 +9,11 @@ dependencies {\n      * API Dependencies\n      *******************************/\n \n-    api(project(':pxf-api'))\n-\n     /*******************************\n      * Implementation Dependencies\n      *******************************/\n \n+    implementation(project(':pxf-api'))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4MDMyNA=="}, "originalCommit": {"oid": "16c64bc3c56ed2f0a4aa4e81eaf13667d9a47fb9"}, "originalPosition": 10}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3668, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}