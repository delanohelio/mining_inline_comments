{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk4MTc5ODIx", "number": 333, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxOToyMDozMlrODxWu8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyNzozM1rOD3eGMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDc5MjgxOnYy", "diffSide": "RIGHT", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxOToyMDozMlrOGEx8Cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQwMDozMTo0OVrOGFlqmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NTY3NQ==", "bodyText": "can this be uncommented now ? If not, it's be better to mark it as @Ignore if it fails temporarily or just remove the test if the behavior changed.", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r407665675", "createdAt": "2020-04-13T19:20:32Z", "author": {"login": "denalex"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java", "diffHunk": "@@ -244,7 +331,7 @@ public void severalPartitionsDefaultSerde() throws Exception {\n      *\n      * @throws Exception if test fails to run\n      */\n-    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })\n+//    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODUxMzE3OA==", "bodyText": "Removing this test as we now support querying a subset of Hive columns in Greenplum", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408513178", "createdAt": "2020-04-15T00:31:49Z", "author": {"login": "frankgh"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java", "diffHunk": "@@ -244,7 +331,7 @@ public void severalPartitionsDefaultSerde() throws Exception {\n      *\n      * @throws Exception if test fails to run\n      */\n-    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })\n+//    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NTY3NQ=="}, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 219}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMDgwMTk5OnYy", "diffSide": "RIGHT", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxOToyMzowOVrOGEyBmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQwMDozMjowMlrOGFlq2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NzA5OQ==", "bodyText": "better to remove the test if the behavior changed, @Ignore is usually used for temporarily failing tests that are supposed to work.", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r407667099", "createdAt": "2020-04-13T19:23:09Z", "author": {"login": "denalex"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java", "diffHunk": "@@ -517,7 +577,8 @@ public void incorrectProfile() throws Exception {\n      *\n      * @throws Exception if test fails to run\n      */\n-    @Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+    //@Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+    @Ignore(\"we now support column count mismatch for Hive\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODUxMzI0Mw==", "bodyText": "Removing this test as this is now supported", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408513243", "createdAt": "2020-04-15T00:32:02Z", "author": {"login": "frankgh"}, "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java", "diffHunk": "@@ -517,7 +577,8 @@ public void incorrectProfile() throws Exception {\n      *\n      * @throws Exception if test fails to run\n      */\n-    @Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+    //@Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+    @Ignore(\"we now support column count mismatch for Hive\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NzA5OQ=="}, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNTQxNzIyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxOTo0NToyNlrOGFeOKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjoyNTozOFrOGF3Qug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5MTIwOQ==", "bodyText": "can we do lower case first and then always compare lower-cased strings ?", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408391209", "createdAt": "2020-04-14T19:45:26Z", "author": {"login": "denalex"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));\n+                traverseTuple(structFields.get(i), fields.get(i).getFieldObjectInspector(), complexRecord, true);\n+                addOneFieldToRecord(structRecord, DataType.TEXT, HdfsUtilities.toString(complexRecord, mapkeyDelim));\n                 complexRecord.clear();\n             }\n+        } else {\n+            Map<String, Integer> columnNameToStructIndexMap =\n+                    IntStream.range(0, fields.size())\n+                            .boxed()\n+                            .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n+\n+            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n+            for (int j = 0; j < tupleDescription.size(); j++) {\n+                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+                Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n+                        columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n+                Integer structIndex = hiveIndexes.get(j);\n+\n+                if ((partitionField = getPartitionField(columnDescriptor.columnName())) != null) {\n+                    // Skip partitioned columns\n+                    complexRecord.add(partitionField);\n+                } else if (i == null || structIndex >= structFields.size()) {\n+                    // This is a column not present in the file, but defined in greenplum.\n+                    LOG.warn(\"Column {} is not present in the source file, but it is defined in the table\", columnDescriptor.columnName());\n+                    addOneFieldToRecord(complexRecord, columnDescriptor.getDataType(), null);\n+                } else if (!columnDescriptor.isProjected()) {\n+                    // Non-projected fields will be sent as null values.\n+                    // This case is invoked only in the top level of fields and\n+                    // not when interpreting fields of type struct.\n+                    traverseTuple(null, fields.get(i).getFieldObjectInspector(), complexRecord, false);\n+                } else {\n+                    traverseTuple(structFields.get(structIndex), fields.get(i).getFieldObjectInspector(), complexRecord, false);\n+                }\n+            }\n         }\n+\n         return toFlatten ? structRecord : complexRecord;\n     }\n \n+    private OneField getPartitionField(String columnName) {\n+        OneField oneField = partitionColumnNames.get(columnName);\n+        if (oneField == null) {\n+            oneField = partitionColumnNames.get(columnName.toLowerCase());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgwMTQ2Ng==", "bodyText": "This was mostly to support a very edge case. But I am reading that the Hive metastore stores the table schema in all lowercase. So it does not apply. I will fix it", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408801466", "createdAt": "2020-04-15T12:25:38Z", "author": {"login": "frankgh"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));\n+                traverseTuple(structFields.get(i), fields.get(i).getFieldObjectInspector(), complexRecord, true);\n+                addOneFieldToRecord(structRecord, DataType.TEXT, HdfsUtilities.toString(complexRecord, mapkeyDelim));\n                 complexRecord.clear();\n             }\n+        } else {\n+            Map<String, Integer> columnNameToStructIndexMap =\n+                    IntStream.range(0, fields.size())\n+                            .boxed()\n+                            .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n+\n+            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n+            for (int j = 0; j < tupleDescription.size(); j++) {\n+                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+                Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n+                        columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n+                Integer structIndex = hiveIndexes.get(j);\n+\n+                if ((partitionField = getPartitionField(columnDescriptor.columnName())) != null) {\n+                    // Skip partitioned columns\n+                    complexRecord.add(partitionField);\n+                } else if (i == null || structIndex >= structFields.size()) {\n+                    // This is a column not present in the file, but defined in greenplum.\n+                    LOG.warn(\"Column {} is not present in the source file, but it is defined in the table\", columnDescriptor.columnName());\n+                    addOneFieldToRecord(complexRecord, columnDescriptor.getDataType(), null);\n+                } else if (!columnDescriptor.isProjected()) {\n+                    // Non-projected fields will be sent as null values.\n+                    // This case is invoked only in the top level of fields and\n+                    // not when interpreting fields of type struct.\n+                    traverseTuple(null, fields.get(i).getFieldObjectInspector(), complexRecord, false);\n+                } else {\n+                    traverseTuple(structFields.get(structIndex), fields.get(i).getFieldObjectInspector(), complexRecord, false);\n+                }\n+            }\n         }\n+\n         return toFlatten ? structRecord : complexRecord;\n     }\n \n+    private OneField getPartitionField(String columnName) {\n+        OneField oneField = partitionColumnNames.get(columnName);\n+        if (oneField == null) {\n+            oneField = partitionColumnNames.get(columnName.toLowerCase());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5MTIwOQ=="}, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNTQyODAzOnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxOTo0ODo1M1rOGFeVFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjoyMzozMVrOGF3L7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5Mjk4Mw==", "bodyText": "are we sure we do not need column mapping inside struct here ?", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408392983", "createdAt": "2020-04-14T19:48:53Z", "author": {"login": "denalex"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgwMDIzOA==", "bodyText": "Yes, flattening does not happen for tuples \n  \n    \n      pxf/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n    \n    \n         Line 131\n      in\n      4e363c4\n    \n    \n    \n    \n\n        \n          \n           return traverseStruct(tuple, soi, false); \n        \n    \n  \n\n but rather on nested structs \n  \n    \n      pxf/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n    \n    \n         Line 413\n      in\n      4e363c4\n    \n    \n    \n    \n\n        \n          \n           List<OneField> structRecord = traverseStruct(obj, \n        \n    \n  \n\n . The flattening happens on columns of type collection for example here: \n  \n    \n      pxf/automation/tincrepo/main/pxf/features/hive/hive_collection_types/expected/query01.ans\n    \n    \n         Line 6\n      in\n      4e363c4\n    \n    \n    \n    \n\n        \n          \n           giraffe |  2 | [\"Nige\",\"Kenya\",\"Congo\"]     | {\"height\":4.0,\"wheight\":100.0} | {\"street\":\"agripas\",\"city\":\"aviv\",\"state\":\"israel\",\"zip\":56303}  | [[\"abc\"]] \n        \n    \n  \n\n Take a look at column t2, t3, t4, and t5, where t2 is a ARRAY, t3 a MAP<STRING, FLOAT>, t4 a STRUCT<street:STRING, city:STRING, state:STRING, zip:INT> (this column specifically will call the traverseStruct for it's value with a flatten = true value), t5 a UNIONTYPE<STRING, INT, ARRAY, ARRAY", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408800238", "createdAt": "2020-04-15T12:23:31Z", "author": {"login": "frankgh"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5Mjk4Mw=="}, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 193}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNTQ1OTY2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxOTo1Nzo1OVrOGFeo8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjoyNjoyMVrOGF3STA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5ODA2Ng==", "bodyText": "why are we mapping GP column twice - once via column Name and once via hiveuserdata index mapping ?", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408398066", "createdAt": "2020-04-14T19:57:59Z", "author": {"login": "denalex"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));\n+                traverseTuple(structFields.get(i), fields.get(i).getFieldObjectInspector(), complexRecord, true);\n+                addOneFieldToRecord(structRecord, DataType.TEXT, HdfsUtilities.toString(complexRecord, mapkeyDelim));\n                 complexRecord.clear();\n             }\n+        } else {\n+            Map<String, Integer> columnNameToStructIndexMap =\n+                    IntStream.range(0, fields.size())\n+                            .boxed()\n+                            .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n+\n+            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n+            for (int j = 0; j < tupleDescription.size(); j++) {\n+                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+                Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n+                        columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n+                Integer structIndex = hiveIndexes.get(j);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgwMTg2OA==", "bodyText": "good question, I will add a note here, in case we need both", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408801868", "createdAt": "2020-04-15T12:26:21Z", "author": {"login": "frankgh"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));\n+                traverseTuple(structFields.get(i), fields.get(i).getFieldObjectInspector(), complexRecord, true);\n+                addOneFieldToRecord(structRecord, DataType.TEXT, HdfsUtilities.toString(complexRecord, mapkeyDelim));\n                 complexRecord.clear();\n             }\n+        } else {\n+            Map<String, Integer> columnNameToStructIndexMap =\n+                    IntStream.range(0, fields.size())\n+                            .boxed()\n+                            .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n+\n+            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n+            for (int j = 0; j < tupleDescription.size(); j++) {\n+                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+                Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n+                        columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n+                Integer structIndex = hiveIndexes.get(j);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5ODA2Ng=="}, "originalCommit": {"oid": "4e363c4ef2bd2a488895586ca6cab8736069f162"}, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDgxMTU1OnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveColumnarSerdeResolver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMzoxOTozOVrOGNub9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMzoxOTozOVrOGNub9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0NTQ5Mw==", "bodyText": "I wonder why include the entire array? Can we just include the 1 up to last elements?", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417045493", "createdAt": "2020-04-29T03:19:39Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveColumnarSerdeResolver.java", "diffHunk": "@@ -65,29 +75,51 @@\n     private static final Log LOG = LogFactory.getLog(HiveColumnarSerdeResolver.class);\n     private boolean firstColumn;\n     private StringBuilder builder;\n-    private StringBuilder parts;\n     private String serdeType;\n-\n+    private String allColumnNames;\n+    private String allColumnTypes;\n+    private Map<String, String[]> partitionColumnNames;\n+    \n     /* read the data supplied by the fragmenter: inputformat name, serde name, partition keys */\n     @Override\n-    void parseUserData(RequestContext input) throws Exception {\n+    void parseUserData(RequestContext input) {\n         HiveUserData hiveUserData = HiveUtilities.parseHiveUserData(input);\n \n+        partitionColumnNames = new HashMap<>();\n         serdeType = hiveUserData.getSerdeClassName();\n-        parts = new StringBuilder();\n         partitionKeys = hiveUserData.getPartitionKeys();\n+        hiveIndexes = hiveUserData.getHiveIndexes();\n+        allColumnNames = hiveUserData.getAllColumnNames();\n+        allColumnTypes = hiveUserData.getAllColumnTypes();\n         parseDelimiterChar(input);\n     }\n \n     @Override\n     void initPartitionFields() {\n         if (context.getOutputFormat() == OutputFormat.TEXT) {\n-            initTextPartitionFields(parts);\n+            initTextPartitionFields(builder);\n         } else {\n             super.initPartitionFields();\n         }\n     }\n \n+    /*\n+     * The partition fields are initialized one time based on userData provided\n+     * by the fragmenter.\n+     */\n+    @Override\n+    void initTextPartitionFields(StringBuilder parts) {\n+        if (partitionKeys.equals(HiveDataFragmenter.HIVE_NO_PART_TBL)) {\n+            return;\n+        }\n+\n+        String[] partitionLevels = partitionKeys.split(HiveDataFragmenter.HIVE_PARTITIONS_DELIM);\n+        for (String partLevel : partitionLevels) {\n+            String[] levelKey = partLevel.split(HiveDataFragmenter.HIVE_1_PART_DELIM);\n+            partitionColumnNames.put(StringUtils.lowerCase(levelKey[0]), levelKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDgzMjEwOnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveDataFragmenter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMzozMzoyM1rOGNunfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMzozMzoyM1rOGNunfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0ODQ0Ng==", "bodyText": "This looks very similar to the code in HiveColumnarSerdeResolver above.", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417048446", "createdAt": "2020-04-29T03:33:23Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveDataFragmenter.java", "diffHunk": "@@ -166,7 +172,23 @@ private void fetchTableMetaData(Metadata.Item tblDesc) throws Exception {\n         hiveClientWrapper.getSchema(tbl, metadata);\n         boolean hasComplexTypes = hiveClientWrapper.hasComplexTypes(metadata);\n \n-        verifySchema(tbl);\n+        // Keep a list of indices from the Hive schema columns that we need to\n+        // retrieve\n+        List<Integer> hiveIndexes = verifySchema(tbl);\n+        List<FieldSchema> fieldSchemaList = tbl.getSd().getCols();\n+\n+        // Get the column names and column types\n+        StringBuilder allColumnNames = new StringBuilder();\n+        StringBuilder allColumnTypes = new StringBuilder();\n+        String delim = \",\";\n+        for (FieldSchema fieldSchema : fieldSchemaList) {\n+            if (allColumnNames.length() > 0) {\n+                allColumnNames.append(delim);\n+                allColumnTypes.append(delim);\n+            }\n+            allColumnNames.append(fieldSchema.getName());\n+            allColumnTypes.append(fieldSchema.getType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDkwMzk2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveDataFragmenter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyMTozM1rOGNvPvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyMTozM1rOGNvPvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA1ODc0OQ==", "bodyText": "I understand the need to have this metadata on each fragment, but it would be nice to not have to have so much duplicate data around.", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417058749", "createdAt": "2020-04-29T04:21:33Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveDataFragmenter.java", "diffHunk": "@@ -324,8 +404,15 @@ private void fetchMetaData(HiveTablePartition tablePartition, boolean hasComplex\n             String filepath = fsp.getPath().toString();\n \n             byte[] locationInfo = HdfsUtilities.prepareFragmentMetadata(fsp);\n+            byte[] userData = hiveClientWrapper.makeUserData(\n+                    fragmenterForProfile,\n+                    tablePartition,\n+                    filterInFragmenter,\n+                    hiveIndexes,\n+                    allColumnNames,\n+                    allColumnTypes);\n             Fragment fragment = new Fragment(filepath, hosts, locationInfo,\n-                    hiveClientWrapper.makeUserData(fragmenterForProfile, tablePartition, filterInFragmenter), profile);\n+                    userData, profile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDkwODQyOnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveInputFormatFragmenter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyNDozM1rOGNvSRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyNDozM1rOGNvSRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA1OTM5Nw==", "bodyText": "What's the difference between this code and the one in the parent class? Does it just ignore partitioned columns? I don't have much context on the Hive input format.", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417059397", "createdAt": "2020-04-29T04:24:33Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveInputFormatFragmenter.java", "diffHunk": "@@ -59,43 +65,57 @@\n         ORC_FILE_INPUT_FORMAT\n     }\n \n-    /*\n-     * Checks that hive fields and partitions match the GPDB schema. Throws an\n-     * exception if: - the number of fields (+ partitions) do not match the GPDB\n-     * table definition. - the hive fields types do not match the GPDB fields.\n+    /**\n+     * Checks that hive fields and partitions match the Greenplum schema.\n+     * Throws an exception if:\n+     * - A Greenplum column does not match any columns or partitions on the\n+     * Hive table definition\n+     * - The hive fields types do not match the Greenplum fields.\n+     * Then return a list of indexes corresponding to the matching columns in\n+     * Greenplum, ordered by the Greenplum schema order. It excludes any\n+     * partition column\n+     *\n+     * @param tbl the hive table\n+     * @return a list of indexes\n      */\n     @Override\n-    void verifySchema(Table tbl) throws Exception {\n+    List<Integer> verifySchema(Table tbl) {\n \n-        int columnsSize = context.getColumns();\n-        int hiveColumnsSize = tbl.getSd().getColsSize();\n-        int hivePartitionsSize = tbl.getPartitionKeysSize();\n+        List<Integer> indexes = new ArrayList<>();\n+        List<FieldSchema> hiveColumns = tbl.getSd().getCols();\n+        List<FieldSchema> hivePartitions = tbl.getPartitionKeys();\n \n-        LOG.debug(\"Hive table: {} fields, {} partitions. GPDB table: {} fields.\",\n-                hiveColumnsSize, hivePartitionsSize, columnsSize);\n+        Map<String, FieldSchema> columnNameToFieldSchema =\n+                Stream.concat(hiveColumns.stream(), hivePartitions.stream())\n+                        .collect(Collectors.toMap(FieldSchema::getName, fieldSchema -> fieldSchema));\n \n-        // check schema size\n-        if (columnsSize != (hiveColumnsSize + hivePartitionsSize)) {\n-            throw new IllegalArgumentException(\n-                    String.format(\"Hive table schema (%d fields, %d partitions) doesn't match PXF table (%d fields)\",\n-                            hiveColumnsSize, hivePartitionsSize, columnsSize));\n-        }\n+        Map<String, Integer> columnNameToColsIndexMap =\n+                IntStream.range(0, hiveColumns.size())\n+                        .boxed()\n+                        .collect(Collectors.toMap(i -> hiveColumns.get(i).getName(), i -> i));\n \n-        int index = 0;\n-        // check hive fields\n-        List<FieldSchema> hiveColumns = tbl.getSd().getCols();\n-        for (FieldSchema hiveCol : hiveColumns) {\n-            ColumnDescriptor colDesc = context.getColumn(index++);\n-            DataType colType = colDesc.getDataType();\n-            HiveUtilities.validateTypeCompatible(colType, colDesc.columnTypeModifiers(), hiveCol.getType(), colDesc.columnName());\n-        }\n-        // check partition fields\n-        List<FieldSchema> hivePartitions = tbl.getPartitionKeys();\n-        for (FieldSchema hivePart : hivePartitions) {\n-            ColumnDescriptor colDesc = context.getColumn(index++);\n-            DataType colType = colDesc.getDataType();\n-            HiveUtilities.validateTypeCompatible(colType, colDesc.columnTypeModifiers(), hivePart.getType(), colDesc.columnName());\n-        }\n+        FieldSchema fieldSchema;\n+        for (ColumnDescriptor cd : context.getTupleDescription()) {\n+            if ((fieldSchema = columnNameToFieldSchema.get(cd.columnName())) == null &&\n+                    (fieldSchema = columnNameToFieldSchema.get(cd.columnName().toLowerCase())) == null) {\n+                throw new IllegalArgumentException(\n+                        String.format(\"Column '%s' does not exist in the Hive schema or Hive Partition. \" +\n+                                        \"Ensure the column or partition exists and check the name spelling and case\",\n+                                cd.columnName()));\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDkxMzc2OnYy", "diffSide": "RIGHT", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveORCAccessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyNzozM1rOGNvVCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNDoyNzozM1rOGNvVCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA2MDEwNQ==", "bodyText": "good catch!", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417060105", "createdAt": "2020-04-29T04:27:33Z", "author": {"login": "oliverralbertini"}, "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveORCAccessor.java", "diffHunk": "@@ -116,16 +116,18 @@ public boolean openForRead() throws Exception {\n     }\n \n     /**\n-     * Adds the table tuple description to JobConf ojbect\n+     * Adds the table tuple description to JobConf object", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3615, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}