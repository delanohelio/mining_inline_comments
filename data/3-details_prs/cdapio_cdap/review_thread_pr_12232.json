{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI1Mzc3OTEx", "number": 12232, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyMDozM1rOEB_KQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzo0ODo1OVrOEB_aZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTE4ODQ4OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyMDozM1rOGeKtjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyMDozM1rOGeKtjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTk2Nw==", "bodyText": "Feel this should generate more meaningful error messages like in the comment to tell user what is the expected behavior to fix the problem", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434285967", "createdAt": "2020-06-03T03:20:33Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java", "diffHunk": "@@ -48,8 +48,15 @@\n   private final Map<String, List<String>> joinKeys;\n   private final Map<String, List<JoinField>> stageFields;\n   private Schema keySchema;\n-\n-  public JoinerBridge(BatchAutoJoiner autoJoiner, JoinDefinition joinDefinition) {\n+  private Schema outputSchema;\n+\n+  public JoinerBridge(String stageName, BatchAutoJoiner autoJoiner, JoinDefinition joinDefinition) {\n+    // if this is not an inner join and the output schema is not set,\n+    // we have no way of determining what the output schema should be and need to error out.\n+    if (joinDefinition.getOutputSchema() == null &&\n+      joinDefinition.getStages().stream().anyMatch(s -> !s.isRequired())) {\n+      throw new IllegalArgumentException(\"An output schema could not be generated for joiner stage \" + stageName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTE5NDAwOnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyNDo0OVrOGeKxKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyNDo0OVrOGeKxKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Njg4OQ==", "bodyText": "nit - unused import", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434286889", "createdAt": "2020-06-03T03:24:49Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java", "diffHunk": "@@ -20,6 +20,7 @@\n import io.cdap.cdap.etl.spark.SparkCollection;\n \n import java.util.List;\n+import javax.annotation.Nullable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTE5NDE3OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyNDo1OFrOGeKxSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyNDo1OFrOGeKxSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjkyMQ==", "bodyText": "nit - unused import", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434286921", "createdAt": "2020-06-03T03:24:58Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java", "diffHunk": "@@ -20,6 +20,7 @@\n import io.cdap.cdap.etl.api.join.JoinField;\n \n import java.util.List;\n+import javax.annotation.Nullable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTE5Njg0OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyNjo0NVrOGeKy6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzoyNjo0NVrOGeKy6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzMzNg==", "bodyText": "Can be replaced with outputSchema", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434287336", "createdAt": "2020-06-03T03:26:45Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTIwMzA5OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzozMDo0MlrOGeK2nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzozMDo0MlrOGeK2nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODI4NQ==", "bodyText": "It is better to put this logic under \n  \n    \n      cdap/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n    \n    \n         Line 403\n      in\n      fdf4d5c\n    \n    \n    \n    \n\n        \n          \n           Schema leftSchema = left.getSchema(); \n        \n    \n  \n\n, so that it is easier to know they are related", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434288285", "createdAt": "2020-06-03T03:30:42Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTIwNTA2OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzozMjowOFrOGeK34A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzowMjo0N1rOGelQ-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODYwOA==", "bodyText": "Just curious - is there any benefit to use Table instead of Map<String, Map<String, JoinField>>?", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434288608", "createdAt": "2020-06-03T03:32:08Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,\n+                                     joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+    }\n+\n     // JoinRequest contains the left side of the join, plus 1 or more other stages to join to.\n-    JoinRequest joinRequest = new JoinRequest(left.getStageName(), stageKeys.get(left.getStageName()), leftSchema,\n+    JoinRequest joinRequest = new JoinRequest(leftName, leftKey, leftSchema,\n                                               left.isRequired(), onKeys.isNullSafe(),\n                                               joinDefinition.getSelectedFields(),\n                                               joinDefinition.getOutputSchema(), toJoin);\n     return leftCollection.join(joinRequest);\n   }\n \n+  /**\n+   * Derive the key schema based on the provided output schema and the final list of selected fields.\n+   * This is not possible if the key is not present in the output schema, in which case the user will need to\n+   * add it to the output schema. However, this is an unlikely scenario as most joins will want to\n+   * preserve the key.\n+   */\n+  @VisibleForTesting\n+  static List<Schema> deriveKeySchema(String joinerStageName, Map<String, List<String>> keys,\n+                                      JoinDefinition joinDefinition) {\n+    int numKeyFields = keys.values().iterator().next().size();\n+\n+    // (stage, field) -> JoinField\n+    Table<String, String, JoinField> fieldTable = HashBasedTable.create();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMTAxNw==", "bodyText": "it's just convenience, can do a single put instead of computeIfAbsent follow by a put, and can do a single get instead of two gets", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434721017", "createdAt": "2020-06-03T17:02:47Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,\n+                                     joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+    }\n+\n     // JoinRequest contains the left side of the join, plus 1 or more other stages to join to.\n-    JoinRequest joinRequest = new JoinRequest(left.getStageName(), stageKeys.get(left.getStageName()), leftSchema,\n+    JoinRequest joinRequest = new JoinRequest(leftName, leftKey, leftSchema,\n                                               left.isRequired(), onKeys.isNullSafe(),\n                                               joinDefinition.getSelectedFields(),\n                                               joinDefinition.getOutputSchema(), toJoin);\n     return leftCollection.join(joinRequest);\n   }\n \n+  /**\n+   * Derive the key schema based on the provided output schema and the final list of selected fields.\n+   * This is not possible if the key is not present in the output schema, in which case the user will need to\n+   * add it to the output schema. However, this is an unlikely scenario as most joins will want to\n+   * preserve the key.\n+   */\n+  @VisibleForTesting\n+  static List<Schema> deriveKeySchema(String joinerStageName, Map<String, List<String>> keys,\n+                                      JoinDefinition joinDefinition) {\n+    int numKeyFields = keys.values().iterator().next().size();\n+\n+    // (stage, field) -> JoinField\n+    Table<String, String, JoinField> fieldTable = HashBasedTable.create();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODYwOA=="}, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTIxNDA1OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzozODozMlrOGeK9ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzowMzo0OVrOGelTSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MDAyNg==", "bodyText": "should we log something here as a TRACE or DEBUG message to let the user know we mark it as nullable, since there is possibility that it is not nullable", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434290026", "createdAt": "2020-06-03T03:38:32Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,\n+                                     joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+    }\n+\n     // JoinRequest contains the left side of the join, plus 1 or more other stages to join to.\n-    JoinRequest joinRequest = new JoinRequest(left.getStageName(), stageKeys.get(left.getStageName()), leftSchema,\n+    JoinRequest joinRequest = new JoinRequest(leftName, leftKey, leftSchema,\n                                               left.isRequired(), onKeys.isNullSafe(),\n                                               joinDefinition.getSelectedFields(),\n                                               joinDefinition.getOutputSchema(), toJoin);\n     return leftCollection.join(joinRequest);\n   }\n \n+  /**\n+   * Derive the key schema based on the provided output schema and the final list of selected fields.\n+   * This is not possible if the key is not present in the output schema, in which case the user will need to\n+   * add it to the output schema. However, this is an unlikely scenario as most joins will want to\n+   * preserve the key.\n+   */\n+  @VisibleForTesting\n+  static List<Schema> deriveKeySchema(String joinerStageName, Map<String, List<String>> keys,\n+                                      JoinDefinition joinDefinition) {\n+    int numKeyFields = keys.values().iterator().next().size();\n+\n+    // (stage, field) -> JoinField\n+    Table<String, String, JoinField> fieldTable = HashBasedTable.create();\n+    for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+      fieldTable.put(joinField.getStageName(), joinField.getFieldName(), joinField);\n+    }\n+\n+    /*\n+        Suppose the join keys are:\n+\n+        A, (x, y)\n+        B, (x, z))\n+\n+        and selected fields are:\n+\n+        A.x as id, A.y as name, B.z as item\n+     */\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    List<Schema> keySchema = new ArrayList<>(numKeyFields);\n+    for (int i = 0; i < numKeyFields; i++) {\n+      keySchema.add(null);\n+    }\n+    for (Map.Entry<String, List<String>> keyEntry : keys.entrySet()) {\n+      String keyStage = keyEntry.getKey();\n+      int keyFieldNum = 0;\n+      for (String keyField : keyEntry.getValue()) {\n+        // If key field is A.x, this will fetch (A.x as id)\n+        // the JoinField might not exist. For example, B.x will not find anything in the output\n+        JoinField selectedKeyField = fieldTable.get(keyStage, keyField);\n+        if (selectedKeyField == null) {\n+          continue;\n+        }\n+\n+        // if the key field is A.x, JoinField is (A.x as id), and outputField is the 'id' field in the output schema\n+        String outputFieldName = selectedKeyField.getAlias() == null ?\n+          selectedKeyField.getFieldName() : selectedKeyField.getAlias();\n+        Schema.Field outputField = outputSchema.getField(outputFieldName);\n+\n+        if (outputField == null) {\n+          // this is an invalid join definition\n+          throw new IllegalArgumentException(\n+            String.format(\"Joiner stage '%s' provided an invalid definition. \" +\n+                            \"The output schema does not contain a field for selected field '%s'.'%s'%s\",\n+                          joinerStageName, keyStage, selectedKeyField.getFieldName(),\n+                          selectedKeyField.getAlias() == null ? \"\" : \"as \" + selectedKeyField.getAlias()));\n+        }\n+\n+        // make the schema nullable because one stage might have it as non-nullable\n+        // while another stage has it as nullable.\n+        // for example, when joining on A.id = B.id,\n+        // A.id might not be nullable, but B.id could be.\n+        Schema keyFieldSchema = outputField.getSchema();\n+        if (!keyFieldSchema.isNullable()) {\n+          keyFieldSchema = Schema.nullableOf(keyFieldSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMTYwOQ==", "bodyText": "this won't matter in the end because this is just intermediate data, the output will still use whatever the user provided. I can add a comment about that.", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434721609", "createdAt": "2020-06-03T17:03:49Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,\n+                                     joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+    }\n+\n     // JoinRequest contains the left side of the join, plus 1 or more other stages to join to.\n-    JoinRequest joinRequest = new JoinRequest(left.getStageName(), stageKeys.get(left.getStageName()), leftSchema,\n+    JoinRequest joinRequest = new JoinRequest(leftName, leftKey, leftSchema,\n                                               left.isRequired(), onKeys.isNullSafe(),\n                                               joinDefinition.getSelectedFields(),\n                                               joinDefinition.getOutputSchema(), toJoin);\n     return leftCollection.join(joinRequest);\n   }\n \n+  /**\n+   * Derive the key schema based on the provided output schema and the final list of selected fields.\n+   * This is not possible if the key is not present in the output schema, in which case the user will need to\n+   * add it to the output schema. However, this is an unlikely scenario as most joins will want to\n+   * preserve the key.\n+   */\n+  @VisibleForTesting\n+  static List<Schema> deriveKeySchema(String joinerStageName, Map<String, List<String>> keys,\n+                                      JoinDefinition joinDefinition) {\n+    int numKeyFields = keys.values().iterator().next().size();\n+\n+    // (stage, field) -> JoinField\n+    Table<String, String, JoinField> fieldTable = HashBasedTable.create();\n+    for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+      fieldTable.put(joinField.getStageName(), joinField.getFieldName(), joinField);\n+    }\n+\n+    /*\n+        Suppose the join keys are:\n+\n+        A, (x, y)\n+        B, (x, z))\n+\n+        and selected fields are:\n+\n+        A.x as id, A.y as name, B.z as item\n+     */\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    List<Schema> keySchema = new ArrayList<>(numKeyFields);\n+    for (int i = 0; i < numKeyFields; i++) {\n+      keySchema.add(null);\n+    }\n+    for (Map.Entry<String, List<String>> keyEntry : keys.entrySet()) {\n+      String keyStage = keyEntry.getKey();\n+      int keyFieldNum = 0;\n+      for (String keyField : keyEntry.getValue()) {\n+        // If key field is A.x, this will fetch (A.x as id)\n+        // the JoinField might not exist. For example, B.x will not find anything in the output\n+        JoinField selectedKeyField = fieldTable.get(keyStage, keyField);\n+        if (selectedKeyField == null) {\n+          continue;\n+        }\n+\n+        // if the key field is A.x, JoinField is (A.x as id), and outputField is the 'id' field in the output schema\n+        String outputFieldName = selectedKeyField.getAlias() == null ?\n+          selectedKeyField.getFieldName() : selectedKeyField.getAlias();\n+        Schema.Field outputField = outputSchema.getField(outputFieldName);\n+\n+        if (outputField == null) {\n+          // this is an invalid join definition\n+          throw new IllegalArgumentException(\n+            String.format(\"Joiner stage '%s' provided an invalid definition. \" +\n+                            \"The output schema does not contain a field for selected field '%s'.'%s'%s\",\n+                          joinerStageName, keyStage, selectedKeyField.getFieldName(),\n+                          selectedKeyField.getAlias() == null ? \"\" : \"as \" + selectedKeyField.getAlias()));\n+        }\n+\n+        // make the schema nullable because one stage might have it as non-nullable\n+        // while another stage has it as nullable.\n+        // for example, when joining on A.id = B.id,\n+        // A.id might not be nullable, but B.id could be.\n+        Schema keyFieldSchema = outputField.getSchema();\n+        if (!keyFieldSchema.isNullable()) {\n+          keyFieldSchema = Schema.nullableOf(keyFieldSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MDAyNg=="}, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNTIyOTgyOnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzo0ODo1OVrOGeLGiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzowNDo0NVrOGelVZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MjM2Mw==", "bodyText": "notice there is no trim for the schema in this case, is there a reason for that?", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434292363", "createdAt": "2020-06-03T03:48:59Z", "author": {"login": "yaojiefeng"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMjE1MQ==", "bodyText": "can add a comment, basically the derived schema is guaranteed to be trimmed because it is generated from the output schema and the join key, which means it will never have extra fields", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434722151", "createdAt": "2020-06-03T17:04:45Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MjM2Mw=="}, "originalCommit": {"oid": "fdf4d5c6fc991e44b35de436c17d067a72318a1f"}, "originalPosition": 111}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2701, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}