{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc4NzczMTY5", "number": 11880, "title": "CDAP-13643 add streaming pipeline field lineage", "bodyText": "JIRA: https://issues.cask.co/browse/CDAP-13643\nbuild: https://builds.cask.co/browse/CDAP-DUT7108\nAdded the ability for streaming pipeline to emit field lineage. Currently we record lineage before anything runs and we force people to emit lineage in prepareRun instead of getStream because due to checkpointing, some part of code might not get run. For example, I have seen getStream() not getting called during the second run in the unit test. And thus no lineage is getting emitted by the source.\nThis means before any data is written, the dataset lineage for streaming source is recorded as well as all the field lineage. And there are two drawbacks:\n\nDataset lineage for the sink is recorded automatically using external dataset, which means it will only be recorded if there is actually data written to it. So the dataset lineage will have an incomplete graph(showing just the streaming source) if the pipeline is just starting or it fails to process any data.\nField lineage will be complete. But since dataset lineage is not, clicking into field lineage page will result in a confusing complete graph.\n\nLooking to see if I can move the code of recording lineage in the sink runnables. This we can make sure the lineage is only written if the data is written to the sink. But since there are already so many files changed, looking to merge this one first", "createdAt": "2020-02-24T00:43:37Z", "url": "https://github.com/cdapio/cdap/pull/11880", "merged": true, "mergeCommit": {"oid": "f18be7d23ff9e02b9669c5585185edfd28092549"}, "closed": true, "closedAt": "2020-02-25T06:22:09Z", "author": {"login": "yaojiefeng"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcHhisjgFqTM2MzU2NDU0Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcHrsnSABqjMwNjgwNzM2NzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzNTY0NTQ2", "url": "https://github.com/cdapio/cdap/pull/11880#pullrequestreview-363564546", "createdAt": "2020-02-24T17:23:16Z", "commit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxNzoyMzoxNlrOFtpHbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxODowMjowMVrOFtqU1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwMzg4Nw==", "bodyText": "flushRecord() -> flushLineage()", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383403887", "createdAt": "2020-02-24T17:23:16Z", "author": {"login": "albertshau"}, "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.\n+   */\n+  void flushRecord();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNDAwOQ==", "bodyText": "what happens if they are not complete? Is an exception thrown?", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383404009", "createdAt": "2020-02-24T17:23:28Z", "author": {"login": "albertshau"}, "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNDc1MA==", "bodyText": "this involves IO so it seems like it should throw some type of exception. Depending on how this is being used, programs may want to fail or they may just want to log a warning and continue on.\nIf this is too much refactoring for the time being, please open a jira.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383404750", "createdAt": "2020-02-24T17:24:53Z", "author": {"login": "albertshau"}, "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.\n+   */\n+  void flushRecord();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwMzg4Nw=="}, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNzYxMA==", "bodyText": "this and record() should be synchronized, or the operations set changed to a concurrent set.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383407610", "createdAt": "2020-02-24T17:30:12Z", "author": {"login": "albertshau"}, "path": "cdap-app-fabric/src/main/java/io/cdap/cdap/internal/app/runtime/AbstractContext.java", "diffHunk": "@@ -873,4 +877,11 @@ public NamespacedEntityId getComponentId() {\n   public void record(Collection<? extends Operation> operations) {\n     fieldLineageOperations.addAll(operations);\n   }\n+\n+  @Override\n+  public void flushRecord() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwODQyOQ==", "bodyText": "It would be good to add some docs around when operations are automatically flushed by programs vs when you would need to manually flush them.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383408429", "createdAt": "2020-02-24T17:31:41Z", "author": {"login": "albertshau"}, "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwODk1OA==", "bodyText": "unused?", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383408958", "createdAt": "2020-02-24T17:32:41Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-data-pipeline/src/main/java/io/cdap/cdap/datapipeline/SmartWorkflow.java", "diffHunk": "@@ -78,6 +78,7 @@\n import io.cdap.cdap.etl.common.PipelineRuntime;\n import io.cdap.cdap.etl.common.TrackedIterator;\n import io.cdap.cdap.etl.common.plugin.PipelinePluginContext;\n+import io.cdap.cdap.etl.lineage.FieldLineageProcessor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxNjIzOA==", "bodyText": "this doesn't look like it should be a static util method, as it's only used in one place.\nAlso, the pluginContext, pipelineRuntime, macro evaluator, etc can be re-used.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383416238", "createdAt": "2020-02-24T17:47:08Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/main/java/io/cdap/cdap/datastreams/SparkFieldLineageRecorder.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.datastreams;\n+\n+import io.cdap.cdap.api.lineage.field.Operation;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.lineage.field.FieldOperation;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelinePhase;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.plugin.PipelinePluginContext;\n+import io.cdap.cdap.etl.lineage.FieldLineageProcessor;\n+import io.cdap.cdap.etl.proto.v2.spec.PipelineSpec;\n+import io.cdap.cdap.etl.spark.SparkPipelineRuntime;\n+import io.cdap.cdap.etl.spark.streaming.SparkStreamingPreparer;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Field lineage recorder for streaming pipeline\n+ */\n+public final class SparkFieldLineageRecorder {\n+  private SparkFieldLineageRecorder() {\n+  }\n+\n+  public static void recordFieldLineage(JavaSparkExecutionContext sec, PipelinePhase pipelinePhase,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxNzM3OA==", "bodyText": "this doesn't seem like a debug, but a warning.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383417378", "createdAt": "2020-02-24T17:49:29Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/main/java/io/cdap/cdap/datastreams/SparkStreamingPipelineDriver.java", "diffHunk": "@@ -180,37 +180,41 @@ public void run(DatasetContext context) throws Exception {\n \n   }\n \n-  private JavaStreamingContext run(final DataStreamsPipelineSpec pipelineSpec,\n-                                   final PipelinePhase pipelinePhase,\n-                                   final JavaSparkExecutionContext sec,\n-                                   @Nullable final String checkpointDir,\n-                                   @Nullable final JavaSparkContext context) throws Exception {\n-    Function0<JavaStreamingContext> contextFunction = new Function0<JavaStreamingContext>() {\n-      @Override\n-      public JavaStreamingContext call() throws Exception {\n-        JavaSparkContext javaSparkContext = context == null ? new JavaSparkContext() : context;\n-        JavaStreamingContext jssc = new JavaStreamingContext(\n-          javaSparkContext, Durations.milliseconds(pipelineSpec.getBatchIntervalMillis()));\n-        SparkStreamingPipelineRunner runner = new SparkStreamingPipelineRunner(sec, jssc, pipelineSpec,\n-                                                                               pipelineSpec.isCheckpointsDisabled());\n-        PipelinePluginContext pluginContext = new PipelinePluginContext(sec.getPluginContext(), sec.getMetrics(),\n-                                                                        pipelineSpec.isStageLoggingEnabled(),\n-                                                                        pipelineSpec.isProcessTimingEnabled());\n-        // TODO: figure out how to get partitions to use for aggregators and joiners.\n-        // Seems like they should be set at configure time instead of runtime? but that requires an API change.\n-        try {\n-          runner.runPipeline(pipelinePhase, StreamingSource.PLUGIN_TYPE,\n-                             sec, new HashMap<String, Integer>(), pluginContext,\n-                             new HashMap<String, StageStatisticsCollector>());\n-        } catch (Exception e) {\n-          throw new RuntimeException(e);\n-        }\n-        if (checkpointDir != null) {\n-          jssc.checkpoint(checkpointDir);\n-          jssc.sparkContext().hadoopConfiguration().set(\"fs.defaultFS\", checkpointDir);\n-        }\n-        return jssc;\n+  private JavaStreamingContext run(DataStreamsPipelineSpec pipelineSpec,\n+                                   PipelinePhase pipelinePhase,\n+                                   JavaSparkExecutionContext sec,\n+                                   @Nullable String checkpointDir,\n+                                   @Nullable JavaSparkContext context) throws Exception {\n+    try {\n+      SparkFieldLineageRecorder.recordFieldLineage(sec, pipelinePhase, pipelineSpec);\n+    } catch (Exception e) {\n+      LOG.debug(\"Failed to emit field lineage operations for streaming pipeline\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQyMTYzNw==", "bodyText": "don't add it to core, spark things should be kept in their own modules. We don't want spark stuff to sneak into mapreduce.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383421637", "createdAt": "2020-02-24T17:57:59Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/pom.xml", "diffHunk": "@@ -36,6 +36,11 @@\n       <artifactId>cdap-etl-api</artifactId>\n       <version>${project.version}</version>\n     </dependency>\n+    <dependency>\n+      <groupId>io.cdap.cdap</groupId>\n+      <artifactId>cdap-etl-api-spark</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQyMzcwMg==", "bodyText": "shouldn't add this method, we don't want spark and mapreduce dependencies to mix. The PipelinePhasePreparer has a catch-all create() method that can be used.", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383423702", "createdAt": "2020-02-24T18:02:01Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-batch/src/main/java/io/cdap/cdap/etl/batch/mapreduce/MapReducePreparer.java", "diffHunk": "@@ -133,6 +133,12 @@ protected SubmitterPlugin createSource(BatchConfigurable<BatchSourceContext> bat\n     });\n   }\n \n+  @Nullable\n+  @Override\n+  protected SubmitterPlugin createStreamingSource(StreamingSource<?> streamingSource, StageSpec stageSpec) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70cff0073470002c7b8b0069c6df2676b51ca3dd"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzNzc2NTE5", "url": "https://github.com/cdapio/cdap/pull/11880#pullrequestreview-363776519", "createdAt": "2020-02-24T23:33:07Z", "commit": {"oid": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQyMzozMzowOFrOFtzr7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQyMzozODowNVrOFtzx_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU3NzA3MQ==", "bodyText": "names, or there", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383577071", "createdAt": "2020-02-24T23:33:08Z", "author": {"login": "albertshau"}, "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -36,7 +36,16 @@\n   /**\n    * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n    * the recorded operations. The existing operations should be complete, they should have at least one\n-   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}. If the operations are\n+   * not complete, {@link IllegalArgumentException} will be thrown and existing records will be preserved.\n+   *\n+   * For batch programs(workflow, batch spark, mapreduce), the method is called automatically at the end of the\n+   * successful run.\n+   * For realtime programs(worker, service, spark streaming), the method has to be manually called to write the\n+   * lineage since realtime program will not stop automatically.\n+   *\n+   * @throws IllegalArgumentException if the validation of the field operations fails, this can happen when there is\n+   * no read or write operations, operations have same names, there is a cycle in the operations.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU3ODUxOA==", "bodyText": "shouldn't this still be nullable?", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383578518", "createdAt": "2020-02-24T23:37:48Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java", "diffHunk": "@@ -132,15 +128,9 @@ public PipelinePhasePreparer(PluginContext pluginContext, Metrics metrics, Macro\n   protected abstract SubmitterPlugin create(PipelinePluginInstantiator pluginInstantiator, StageSpec stageSpec)\n     throws InstantiationException;\n \n-  // for streaming pipeline, batch source cannot be created\n-  @Nullable", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU3ODYyMw==", "bodyText": "extra space after =", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383578623", "createdAt": "2020-02-24T23:38:05Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/SparkStreamingPreparer.java", "diffHunk": "@@ -57,8 +58,10 @@ protected SubmitterPlugin createSource(BatchConfigurable<BatchSourceContext> bat\n   }\n \n   @Override\n-  protected SubmitterPlugin createStreamingSource(StreamingSource<?> streamingSource, StageSpec stageSpec) {\n+  protected SubmitterPlugin createStreamingSource(PipelinePluginInstantiator pluginInstantiator,\n+                                                  StageSpec stageSpec) throws InstantiationException {\n     String stageName = stageSpec.getName();\n+    StreamingSource<?> streamingSource =  pluginInstantiator.newPluginInstance(stageName, macroEvaluator);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c"}, "originalPosition": 16}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c", "author": {"user": {"login": "yaojiefeng", "name": null}}, "url": "https://github.com/cdapio/cdap/commit/8eb6b29c3db9a84306c72cf880a1dbd31fddd30c", "committedDate": "2020-02-24T23:23:22Z", "message": "address comments"}, "afterCommit": {"oid": "b8c4f57aae53ce7fe32a1e784d149ca35ff72160", "author": {"user": {"login": "yaojiefeng", "name": null}}, "url": "https://github.com/cdapio/cdap/commit/b8c4f57aae53ce7fe32a1e784d149ca35ff72160", "committedDate": "2020-02-24T23:45:57Z", "message": "CDAP-13643 add streaming pipeline field lineage"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzODY0NDgy", "url": "https://github.com/cdapio/cdap/pull/11880#pullrequestreview-363864482", "createdAt": "2020-02-25T04:31:23Z", "commit": {"oid": "d3f18ccdb3ae3309cc4c781799138d63b35e665f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3fce9227ec18131fc601b4f20d9246bf12e26c83", "author": {"user": {"login": "yaojiefeng", "name": null}}, "url": "https://github.com/cdapio/cdap/commit/3fce9227ec18131fc601b4f20d9246bf12e26c83", "committedDate": "2020-02-25T05:57:54Z", "message": "CDAP-13643 add streaming pipeline field lineage"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d3f18ccdb3ae3309cc4c781799138d63b35e665f", "author": {"user": {"login": "yaojiefeng", "name": null}}, "url": "https://github.com/cdapio/cdap/commit/d3f18ccdb3ae3309cc4c781799138d63b35e665f", "committedDate": "2020-02-25T03:47:40Z", "message": "fix broken tests"}, "afterCommit": {"oid": "3fce9227ec18131fc601b4f20d9246bf12e26c83", "author": {"user": {"login": "yaojiefeng", "name": null}}, "url": "https://github.com/cdapio/cdap/commit/3fce9227ec18131fc601b4f20d9246bf12e26c83", "committedDate": "2020-02-25T05:57:54Z", "message": "CDAP-13643 add streaming pipeline field lineage"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1281, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}