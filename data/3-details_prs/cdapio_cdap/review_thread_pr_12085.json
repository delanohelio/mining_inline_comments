{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NjM3OTg5", "number": 12085, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNjo0NjoxNFrODz0pfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMDo1MToxMVrODz6vrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NjY2NTU2OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNjo0NjoxNFrOGId4HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxNzoyNTozMlrOGIfd6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTUzMTI5Mw==", "bodyText": "instead of reading data here, another possibility is to create one combined split that contains all the original splits, and have the record reader iterate through the splits later.\nSeems like it would be better, since the data is only read once, and protects against weird behavior in certain types of input formats (for example, if there's a pubsub source where the message is acked in the reader).", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411531293", "createdAt": "2020-04-20T16:46:14Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingInputFormat.java", "diffHunk": "@@ -34,25 +44,100 @@\n  * @param <V> type of value to read\n  */\n public class LimitingInputFormat<K, V> extends InputFormat<K, V> implements Configurable {\n-  public static final String DELEGATE_CLASS_NAME = \"io.cdap.pipeline.preview.input.classname\";\n-  public static final String MAX_RECORDS = \"io.cdap.pipeline.preview.max.records\";\n+\n+  static final String DELEGATE_CLASS_NAME = \"io.cdap.pipeline.preview.input.classname\";\n+  static final String MAX_RECORDS = \"io.cdap.pipeline.preview.max.records\";\n \n   private InputFormat<K, V> delegateFormat;\n   private Configuration conf;\n \n   @Override\n   public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException {\n     Configuration conf = context.getConfiguration();\n-    return createDelegate(conf).getSplits(context);\n+    int maxRecords = conf.getInt(MAX_RECORDS, 100);\n+\n+    List<InputSplit> splits = createDelegate(conf).getSplits(context);\n+    if (splits.size() <= 1) {\n+      int limit = maxRecords;\n+      return splits.stream().map(s -> new LimitingInputSplit(conf, s, limit)).collect(Collectors.toList());\n+    }\n+\n+    // If there are more than one splits, try to read records from each split to determine what's the actual split\n+    // limit per split.\n+    Map<InputSplit, Integer> recordLimits = new IdentityHashMap<>();\n+    TaskID taskId = new TaskID(context.getJobID(), TaskType.JOB_SETUP, 0);\n+    TaskAttemptContext taskContext = new TaskAttemptContextImpl(conf, new TaskAttemptID(taskId, 0));\n+    List<InputSplit> activeSplits = new LinkedList<>(splits);\n+    while (maxRecords > 0 && !activeSplits.isEmpty()) {\n+      int recordPerSplit = Math.max(1, maxRecords / activeSplits.size());\n+\n+      Iterator<InputSplit> iterator = activeSplits.iterator();\n+      while (iterator.hasNext()) {\n+        InputSplit split = iterator.next();\n+\n+        // We close the record reader in each iteration to avoid keeping all of them open to preserve memory\n+        // in case there are a lot of partitions.\n+        try (RecordReader<K, V> reader = createRecordReader(split, taskContext)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab1d4b9accce76b09b7e4c7061536028dff7b62e"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU1NzM1NQ==", "bodyText": "hum.. we could and move this logic to the new reader.", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411557355", "createdAt": "2020-04-20T17:25:32Z", "author": {"login": "chtyim"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingInputFormat.java", "diffHunk": "@@ -34,25 +44,100 @@\n  * @param <V> type of value to read\n  */\n public class LimitingInputFormat<K, V> extends InputFormat<K, V> implements Configurable {\n-  public static final String DELEGATE_CLASS_NAME = \"io.cdap.pipeline.preview.input.classname\";\n-  public static final String MAX_RECORDS = \"io.cdap.pipeline.preview.max.records\";\n+\n+  static final String DELEGATE_CLASS_NAME = \"io.cdap.pipeline.preview.input.classname\";\n+  static final String MAX_RECORDS = \"io.cdap.pipeline.preview.max.records\";\n \n   private InputFormat<K, V> delegateFormat;\n   private Configuration conf;\n \n   @Override\n   public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException {\n     Configuration conf = context.getConfiguration();\n-    return createDelegate(conf).getSplits(context);\n+    int maxRecords = conf.getInt(MAX_RECORDS, 100);\n+\n+    List<InputSplit> splits = createDelegate(conf).getSplits(context);\n+    if (splits.size() <= 1) {\n+      int limit = maxRecords;\n+      return splits.stream().map(s -> new LimitingInputSplit(conf, s, limit)).collect(Collectors.toList());\n+    }\n+\n+    // If there are more than one splits, try to read records from each split to determine what's the actual split\n+    // limit per split.\n+    Map<InputSplit, Integer> recordLimits = new IdentityHashMap<>();\n+    TaskID taskId = new TaskID(context.getJobID(), TaskType.JOB_SETUP, 0);\n+    TaskAttemptContext taskContext = new TaskAttemptContextImpl(conf, new TaskAttemptID(taskId, 0));\n+    List<InputSplit> activeSplits = new LinkedList<>(splits);\n+    while (maxRecords > 0 && !activeSplits.isEmpty()) {\n+      int recordPerSplit = Math.max(1, maxRecords / activeSplits.size());\n+\n+      Iterator<InputSplit> iterator = activeSplits.iterator();\n+      while (iterator.hasNext()) {\n+        InputSplit split = iterator.next();\n+\n+        // We close the record reader in each iteration to avoid keeping all of them open to preserve memory\n+        // in case there are a lot of partitions.\n+        try (RecordReader<K, V> reader = createRecordReader(split, taskContext)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTUzMTI5Mw=="}, "originalCommit": {"oid": "ab1d4b9accce76b09b7e4c7061536028dff7b62e"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NzYzODgyOnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingInputSplit.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMDo0NDo1MVrOGIm7DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMDo0Nzo0N1rOGInBpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY3OTUwMA==", "bodyText": "since InputSplit is writable, can this just delegate to the split? Or do we need the classname and to use Serializer for some other reason?", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411679500", "createdAt": "2020-04-20T20:44:51Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingInputSplit.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.batch.preview;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.serializer.Deserializer;\n+import org.apache.hadoop.io.serializer.SerializationFactory;\n+import org.apache.hadoop.io.serializer.Serializer;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * An {@link InputSplit} that delegates to another {@link InputSplit} and also carries record limit information.\n+ */\n+public class LimitingInputSplit extends InputSplit implements Writable, Configurable {\n+\n+  private List<InputSplit> inputSplits;\n+  private int recordLimit;\n+  private Configuration conf;\n+  private long length;\n+  private String[] locations;\n+\n+  @SuppressWarnings(\"unused\")\n+  public LimitingInputSplit() {\n+    // no-op, for deserialization\n+  }\n+\n+  LimitingInputSplit(Configuration conf, List<InputSplit> inputSplits, int recordLimit) throws IOException {\n+    this.conf = conf;\n+    this.inputSplits = inputSplits;\n+    this.recordLimit = recordLimit;\n+    initialize();\n+  }\n+\n+  private void initialize() throws IOException {\n+    try {\n+      long length = 0L;\n+      Set<String> locations = new LinkedHashSet<>();\n+      for (InputSplit split : inputSplits) {\n+        length += split.getLength();\n+        String[] splitLocations = split.getLocations();\n+        if (splitLocations != null) {\n+          locations.addAll(Arrays.asList(splitLocations));\n+        }\n+      }\n+      this.length = length;\n+      this.locations = locations.toArray(new String[0]);\n+    } catch (InterruptedException e) {\n+      throw new IOException(\"Interrupted during initialization\", e);\n+    }\n+  }\n+\n+  List<InputSplit> getInputSplits() {\n+    return inputSplits;\n+  }\n+\n+  int getRecordLimit() {\n+    return recordLimit;\n+  }\n+\n+  @Override\n+  public long getLength() {\n+    return length;\n+  }\n+\n+  @Override\n+  public String[] getLocations() {\n+    return locations;\n+  }\n+\n+  @Override\n+  public void write(DataOutput out) throws IOException {\n+    out.writeInt(recordLimit);\n+    out.writeInt(inputSplits.size());\n+    for (InputSplit split : inputSplits) {\n+      Text.writeString(out, split.getClass().getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7963d5fc6c44bfb75192e661e2babdd75ba533a4"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY4MTE4OA==", "bodyText": "nevermind, was looking at mapred.InputSplit. Don't know why they didn't make it part of the InputSplit contract, seems like it would always be needed...", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411681188", "createdAt": "2020-04-20T20:47:47Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingInputSplit.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.batch.preview;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.serializer.Deserializer;\n+import org.apache.hadoop.io.serializer.SerializationFactory;\n+import org.apache.hadoop.io.serializer.Serializer;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * An {@link InputSplit} that delegates to another {@link InputSplit} and also carries record limit information.\n+ */\n+public class LimitingInputSplit extends InputSplit implements Writable, Configurable {\n+\n+  private List<InputSplit> inputSplits;\n+  private int recordLimit;\n+  private Configuration conf;\n+  private long length;\n+  private String[] locations;\n+\n+  @SuppressWarnings(\"unused\")\n+  public LimitingInputSplit() {\n+    // no-op, for deserialization\n+  }\n+\n+  LimitingInputSplit(Configuration conf, List<InputSplit> inputSplits, int recordLimit) throws IOException {\n+    this.conf = conf;\n+    this.inputSplits = inputSplits;\n+    this.recordLimit = recordLimit;\n+    initialize();\n+  }\n+\n+  private void initialize() throws IOException {\n+    try {\n+      long length = 0L;\n+      Set<String> locations = new LinkedHashSet<>();\n+      for (InputSplit split : inputSplits) {\n+        length += split.getLength();\n+        String[] splitLocations = split.getLocations();\n+        if (splitLocations != null) {\n+          locations.addAll(Arrays.asList(splitLocations));\n+        }\n+      }\n+      this.length = length;\n+      this.locations = locations.toArray(new String[0]);\n+    } catch (InterruptedException e) {\n+      throw new IOException(\"Interrupted during initialization\", e);\n+    }\n+  }\n+\n+  List<InputSplit> getInputSplits() {\n+    return inputSplits;\n+  }\n+\n+  int getRecordLimit() {\n+    return recordLimit;\n+  }\n+\n+  @Override\n+  public long getLength() {\n+    return length;\n+  }\n+\n+  @Override\n+  public String[] getLocations() {\n+    return locations;\n+  }\n+\n+  @Override\n+  public void write(DataOutput out) throws IOException {\n+    out.writeInt(recordLimit);\n+    out.writeInt(inputSplits.size());\n+    for (InputSplit split : inputSplits) {\n+      Text.writeString(out, split.getClass().getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY3OTUwMA=="}, "originalCommit": {"oid": "7963d5fc6c44bfb75192e661e2babdd75ba533a4"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NzY2NDQ0OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingRecordReader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMDo1MToxMVrOGInJ3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMjo1Njo1MlrOGIrAgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY4MzI5NA==", "bodyText": "Though the current behavior is a limit per split, don't we actually want the limit to be applied overall?", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411683294", "createdAt": "2020-04-20T20:51:11Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingRecordReader.java", "diffHunk": "@@ -29,47 +31,77 @@\n  * @param <V> type of value to read\n  */\n public class LimitingRecordReader<K, V> extends RecordReader<K, V> {\n-  private final RecordReader<K, V> delegate;\n-  private final int maxToRead;\n+\n+  private final InputFormat<K, V> delegateFormat;\n+  private TaskAttemptContext context;\n+  private LimitingInputSplit inputSplit;\n+  private int perSplitLimit;\n   private int numRead;\n+  private Iterator<InputSplit> splitIterator;\n+  private RecordReader<K, V> currentReader;\n \n-  public LimitingRecordReader(RecordReader<K, V> delegate, int maxToRead) {\n-    this.delegate = delegate;\n-    this.maxToRead = maxToRead;\n+  LimitingRecordReader(InputFormat<K, V> delegateFormat) {\n+    this.delegateFormat = delegateFormat;\n   }\n \n   @Override\n   public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {\n-    delegate.initialize(split, context);\n+    if (!(split instanceof LimitingInputSplit)) {\n+      throw new IOException(\"Expected input split class \" + LimitingInputSplit.class.getName()\n+                              + \", but got \" + split.getClass().getName());\n+    }\n+    this.context = context;\n+    this.inputSplit = (LimitingInputSplit) split;\n+    this.splitIterator = inputSplit.getInputSplits().iterator();\n+\n+    // Round up the per split limit so that each reader at most is opened once\n+    int numberOfSplits = inputSplit.getInputSplits().size();\n+    int perSplitLimit = (inputSplit.getRecordLimit() + numberOfSplits - 1) / numberOfSplits;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7963d5fc6c44bfb75192e661e2babdd75ba533a4"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0MTkwNg==", "bodyText": "We calculate limit per split from the overall limit. And in the nextKeyValue() method, the overall limit is enforced there.", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411741906", "createdAt": "2020-04-20T22:46:03Z", "author": {"login": "chtyim"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingRecordReader.java", "diffHunk": "@@ -29,47 +31,77 @@\n  * @param <V> type of value to read\n  */\n public class LimitingRecordReader<K, V> extends RecordReader<K, V> {\n-  private final RecordReader<K, V> delegate;\n-  private final int maxToRead;\n+\n+  private final InputFormat<K, V> delegateFormat;\n+  private TaskAttemptContext context;\n+  private LimitingInputSplit inputSplit;\n+  private int perSplitLimit;\n   private int numRead;\n+  private Iterator<InputSplit> splitIterator;\n+  private RecordReader<K, V> currentReader;\n \n-  public LimitingRecordReader(RecordReader<K, V> delegate, int maxToRead) {\n-    this.delegate = delegate;\n-    this.maxToRead = maxToRead;\n+  LimitingRecordReader(InputFormat<K, V> delegateFormat) {\n+    this.delegateFormat = delegateFormat;\n   }\n \n   @Override\n   public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {\n-    delegate.initialize(split, context);\n+    if (!(split instanceof LimitingInputSplit)) {\n+      throw new IOException(\"Expected input split class \" + LimitingInputSplit.class.getName()\n+                              + \", but got \" + split.getClass().getName());\n+    }\n+    this.context = context;\n+    this.inputSplit = (LimitingInputSplit) split;\n+    this.splitIterator = inputSplit.getInputSplits().iterator();\n+\n+    // Round up the per split limit so that each reader at most is opened once\n+    int numberOfSplits = inputSplit.getInputSplits().size();\n+    int perSplitLimit = (inputSplit.getRecordLimit() + numberOfSplits - 1) / numberOfSplits;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY4MzI5NA=="}, "originalCommit": {"oid": "7963d5fc6c44bfb75192e661e2babdd75ba533a4"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0NjQzNQ==", "bodyText": "Also there is always only one LimitingInputSplit. That's why the read count can be tracked locally.", "url": "https://github.com/cdapio/cdap/pull/12085#discussion_r411746435", "createdAt": "2020-04-20T22:56:52Z", "author": {"login": "chtyim"}, "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/batch/preview/LimitingRecordReader.java", "diffHunk": "@@ -29,47 +31,77 @@\n  * @param <V> type of value to read\n  */\n public class LimitingRecordReader<K, V> extends RecordReader<K, V> {\n-  private final RecordReader<K, V> delegate;\n-  private final int maxToRead;\n+\n+  private final InputFormat<K, V> delegateFormat;\n+  private TaskAttemptContext context;\n+  private LimitingInputSplit inputSplit;\n+  private int perSplitLimit;\n   private int numRead;\n+  private Iterator<InputSplit> splitIterator;\n+  private RecordReader<K, V> currentReader;\n \n-  public LimitingRecordReader(RecordReader<K, V> delegate, int maxToRead) {\n-    this.delegate = delegate;\n-    this.maxToRead = maxToRead;\n+  LimitingRecordReader(InputFormat<K, V> delegateFormat) {\n+    this.delegateFormat = delegateFormat;\n   }\n \n   @Override\n   public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {\n-    delegate.initialize(split, context);\n+    if (!(split instanceof LimitingInputSplit)) {\n+      throw new IOException(\"Expected input split class \" + LimitingInputSplit.class.getName()\n+                              + \", but got \" + split.getClass().getName());\n+    }\n+    this.context = context;\n+    this.inputSplit = (LimitingInputSplit) split;\n+    this.splitIterator = inputSplit.getInputSplits().iterator();\n+\n+    // Round up the per split limit so that each reader at most is opened once\n+    int numberOfSplits = inputSplit.getInputSplits().size();\n+    int perSplitLimit = (inputSplit.getRecordLimit() + numberOfSplits - 1) / numberOfSplits;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY4MzI5NA=="}, "originalCommit": {"oid": "7963d5fc6c44bfb75192e661e2babdd75ba533a4"}, "originalPosition": 49}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2761, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}