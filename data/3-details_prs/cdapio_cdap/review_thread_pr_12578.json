{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MTU4ODY4", "number": 12578, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowMDoxMlrOEV_bdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjo1NDoxNlrOEWBLBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk0Nzc0OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/test/java/io/cdap/cdap/datastreams/DataStreamsTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowMDoxMlrOG9EGJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwMzoyNjoyMlrOG9LNxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4MzQyOA==", "bodyText": "// sink2 should have anything with a non-null name or non-null email\n\nthis comment should be removed since the code was removed", "url": "https://github.com/cdapio/cdap/pull/12578#discussion_r466683428", "createdAt": "2020-08-06T21:00:12Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/test/java/io/cdap/cdap/datastreams/DataStreamsTest.java", "diffHunk": "@@ -1186,33 +1187,29 @@ public void testSplitterTransform() throws Exception {\n     ApplicationManager appManager = deployApplication(appId, appRequest);\n \n     // run pipeline\n+    Map<String, String> args = Collections.singletonMap(io.cdap.cdap.etl.common.Constants.CONSOLIDATE_STAGES, \"true\");\n     SparkManager sparkManager = appManager.getSparkManager(DataStreamsSparkLauncher.NAME);\n-    sparkManager.start();\n-    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);\n+    sparkManager.startAndWaitForRun(args, ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);\n \n     // check output\n     // sink1 should only have records where both name and email are null (user0)\n-    DataSetManager<Table> sink1Manager = getDataset(sink1Name);\n     Set<StructuredRecord> expected1 = ImmutableSet.of(user0);\n     Tasks.waitFor(\n       true,\n       () -> {\n-        sink1Manager.flush();\n-        Set<StructuredRecord> outputRecords = new HashSet<>(MockSink.readOutput(sink1Manager));\n+        Set<StructuredRecord> outputRecords = new HashSet<>(MockExternalSink.readOutput(output1, schema));\n         return expected1.equals(outputRecords);\n       },\n       4,\n       TimeUnit.MINUTES);\n \n     // sink2 should have anything with a non-null name or non-null email", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f1dbd39194166d870af75ac17f0e1ea87933b05"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMDA3MQ==", "bodyText": "this comment is still true, it's more talking about what's in the expected set", "url": "https://github.com/cdapio/cdap/pull/12578#discussion_r466800071", "createdAt": "2020-08-07T03:26:22Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/test/java/io/cdap/cdap/datastreams/DataStreamsTest.java", "diffHunk": "@@ -1186,33 +1187,29 @@ public void testSplitterTransform() throws Exception {\n     ApplicationManager appManager = deployApplication(appId, appRequest);\n \n     // run pipeline\n+    Map<String, String> args = Collections.singletonMap(io.cdap.cdap.etl.common.Constants.CONSOLIDATE_STAGES, \"true\");\n     SparkManager sparkManager = appManager.getSparkManager(DataStreamsSparkLauncher.NAME);\n-    sparkManager.start();\n-    sparkManager.waitForRun(ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);\n+    sparkManager.startAndWaitForRun(args, ProgramRunStatus.RUNNING, 10, TimeUnit.SECONDS);\n \n     // check output\n     // sink1 should only have records where both name and email are null (user0)\n-    DataSetManager<Table> sink1Manager = getDataset(sink1Name);\n     Set<StructuredRecord> expected1 = ImmutableSet.of(user0);\n     Tasks.waitFor(\n       true,\n       () -> {\n-        sink1Manager.flush();\n-        Set<StructuredRecord> outputRecords = new HashSet<>(MockSink.readOutput(sink1Manager));\n+        Set<StructuredRecord> outputRecords = new HashSet<>(MockExternalSink.readOutput(output1, schema));\n         return expected1.equals(outputRecords);\n       },\n       4,\n       TimeUnit.MINUTES);\n \n     // sink2 should have anything with a non-null name or non-null email", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4MzQyOA=="}, "originalCommit": {"oid": "0f1dbd39194166d870af75ac17f0e1ea87933b05"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNTIzMzMyOnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/function/StreamingMultiSinkFunction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjo1NDoxNlrOG9G0Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMzo1OTo0N1rOG9IBLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyODAwNg==", "bodyText": "It seems like traversalOrder is only used to affect sinks, is there a way to apply the filtering here instead having it in run method of the TxRunable in the prepareRun and onRunFinish methods? It would help with readability. Can we do sinkNames.contains(stageName) instead of this check?", "url": "https://github.com/cdapio/cdap/pull/12578#discussion_r466728006", "createdAt": "2020-08-06T22:54:16Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/function/StreamingMultiSinkFunction.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Copyright \u00a9 2016 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.streaming.function;\n+\n+import io.cdap.cdap.api.TxRunnable;\n+import io.cdap.cdap.api.data.DatasetContext;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.plugin.PluginContext;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.StageSubmitterContext;\n+import io.cdap.cdap.etl.api.SubmitterLifecycle;\n+import io.cdap.cdap.etl.api.batch.BatchSink;\n+import io.cdap.cdap.etl.api.batch.BatchSinkContext;\n+import io.cdap.cdap.etl.api.lineage.AccessType;\n+import io.cdap.cdap.etl.common.BasicArguments;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.ExternalDatasets;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.RecordInfo;\n+import io.cdap.cdap.etl.common.StageStatisticsCollector;\n+import io.cdap.cdap.etl.proto.v2.spec.StageSpec;\n+import io.cdap.cdap.etl.spark.Compat;\n+import io.cdap.cdap.etl.spark.SparkPipelineRuntime;\n+import io.cdap.cdap.etl.spark.SparkSubmitterContext;\n+import io.cdap.cdap.etl.spark.batch.SparkBatchSinkContext;\n+import io.cdap.cdap.etl.spark.batch.SparkBatchSinkFactory;\n+import io.cdap.cdap.etl.spark.function.MultiSinkFunction;\n+import io.cdap.cdap.etl.spark.plugin.SparkPipelinePluginContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.streaming.Time;\n+import org.apache.tephra.TransactionFailureException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Function used to write a batch of data to a batch sink for use with a JavaDStream.\n+ * note: not using foreachRDD(VoidFunction2) method, because spark 1.3 doesn't have VoidFunction2.\n+ */\n+public class StreamingMultiSinkFunction implements Function2<JavaRDD<RecordInfo<Object>>, Time, Void> {\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMultiSinkFunction.class);\n+  private final JavaSparkExecutionContext sec;\n+  private final PhaseSpec phaseSpec;\n+  private final Set<String> group;\n+  private final Set<String> sinkNames;\n+  private final Map<String, StageStatisticsCollector> collectors;\n+\n+  public StreamingMultiSinkFunction(JavaSparkExecutionContext sec, PhaseSpec phaseSpec,\n+                                    Set<String> group, Set<String> sinkNames,\n+                                    Map<String, StageStatisticsCollector> collectors) {\n+    this.sec = sec;\n+    this.phaseSpec = phaseSpec;\n+    this.group = group;\n+    this.sinkNames = sinkNames;\n+    this.collectors = collectors;\n+  }\n+\n+  @Override\n+  public Void call(JavaRDD<RecordInfo<Object>> data, Time batchTime) throws Exception {\n+    long logicalStartTime = batchTime.milliseconds();\n+    MacroEvaluator evaluator = new DefaultMacroEvaluator(new BasicArguments(sec),\n+                                                         logicalStartTime,\n+                                                         sec.getSecureStore(),\n+                                                         sec.getNamespace());\n+    PluginContext pluginContext = new SparkPipelinePluginContext(sec.getPluginContext(), sec.getMetrics(),\n+                                                                 phaseSpec.isStageLoggingEnabled(),\n+                                                                 phaseSpec.isProcessTimingEnabled());\n+    SparkBatchSinkFactory sinkFactory = new SparkBatchSinkFactory();\n+    PipelineRuntime pipelineRuntime = new SparkPipelineRuntime(sec, logicalStartTime);\n+\n+    Map<String, SubmitterLifecycle<?>> stages = createStages(evaluator);\n+\n+    // call prepareRun() on all the stages in the group\n+    // need to call it in an order that guarantees that inputs are called before outputs\n+    // this is because plugins can call getArguments().set() in the prepareRun() method,\n+    // which downstream stages should be able to read\n+    List<String> traversalOrder = new ArrayList(group.size());\n+    for (String stageName : phaseSpec.getPhase().getDag().getTopologicalOrder()) {\n+      if (group.contains(stageName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f1dbd39194166d870af75ac17f0e1ea87933b05"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NzY5NA==", "bodyText": "it affects transforms in the group as well. We need to call prepareRun() on things in order to make sure that if a transform before a sink sets an argument, the sink will be able to read it.", "url": "https://github.com/cdapio/cdap/pull/12578#discussion_r466747694", "createdAt": "2020-08-06T23:59:47Z", "author": {"login": "albertshau"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/function/StreamingMultiSinkFunction.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Copyright \u00a9 2016 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.streaming.function;\n+\n+import io.cdap.cdap.api.TxRunnable;\n+import io.cdap.cdap.api.data.DatasetContext;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.plugin.PluginContext;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.StageSubmitterContext;\n+import io.cdap.cdap.etl.api.SubmitterLifecycle;\n+import io.cdap.cdap.etl.api.batch.BatchSink;\n+import io.cdap.cdap.etl.api.batch.BatchSinkContext;\n+import io.cdap.cdap.etl.api.lineage.AccessType;\n+import io.cdap.cdap.etl.common.BasicArguments;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.ExternalDatasets;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.RecordInfo;\n+import io.cdap.cdap.etl.common.StageStatisticsCollector;\n+import io.cdap.cdap.etl.proto.v2.spec.StageSpec;\n+import io.cdap.cdap.etl.spark.Compat;\n+import io.cdap.cdap.etl.spark.SparkPipelineRuntime;\n+import io.cdap.cdap.etl.spark.SparkSubmitterContext;\n+import io.cdap.cdap.etl.spark.batch.SparkBatchSinkContext;\n+import io.cdap.cdap.etl.spark.batch.SparkBatchSinkFactory;\n+import io.cdap.cdap.etl.spark.function.MultiSinkFunction;\n+import io.cdap.cdap.etl.spark.plugin.SparkPipelinePluginContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.streaming.Time;\n+import org.apache.tephra.TransactionFailureException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Function used to write a batch of data to a batch sink for use with a JavaDStream.\n+ * note: not using foreachRDD(VoidFunction2) method, because spark 1.3 doesn't have VoidFunction2.\n+ */\n+public class StreamingMultiSinkFunction implements Function2<JavaRDD<RecordInfo<Object>>, Time, Void> {\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMultiSinkFunction.class);\n+  private final JavaSparkExecutionContext sec;\n+  private final PhaseSpec phaseSpec;\n+  private final Set<String> group;\n+  private final Set<String> sinkNames;\n+  private final Map<String, StageStatisticsCollector> collectors;\n+\n+  public StreamingMultiSinkFunction(JavaSparkExecutionContext sec, PhaseSpec phaseSpec,\n+                                    Set<String> group, Set<String> sinkNames,\n+                                    Map<String, StageStatisticsCollector> collectors) {\n+    this.sec = sec;\n+    this.phaseSpec = phaseSpec;\n+    this.group = group;\n+    this.sinkNames = sinkNames;\n+    this.collectors = collectors;\n+  }\n+\n+  @Override\n+  public Void call(JavaRDD<RecordInfo<Object>> data, Time batchTime) throws Exception {\n+    long logicalStartTime = batchTime.milliseconds();\n+    MacroEvaluator evaluator = new DefaultMacroEvaluator(new BasicArguments(sec),\n+                                                         logicalStartTime,\n+                                                         sec.getSecureStore(),\n+                                                         sec.getNamespace());\n+    PluginContext pluginContext = new SparkPipelinePluginContext(sec.getPluginContext(), sec.getMetrics(),\n+                                                                 phaseSpec.isStageLoggingEnabled(),\n+                                                                 phaseSpec.isProcessTimingEnabled());\n+    SparkBatchSinkFactory sinkFactory = new SparkBatchSinkFactory();\n+    PipelineRuntime pipelineRuntime = new SparkPipelineRuntime(sec, logicalStartTime);\n+\n+    Map<String, SubmitterLifecycle<?>> stages = createStages(evaluator);\n+\n+    // call prepareRun() on all the stages in the group\n+    // need to call it in an order that guarantees that inputs are called before outputs\n+    // this is because plugins can call getArguments().set() in the prepareRun() method,\n+    // which downstream stages should be able to read\n+    List<String> traversalOrder = new ArrayList(group.size());\n+    for (String stageName : phaseSpec.getPhase().getDag().getTopologicalOrder()) {\n+      if (group.contains(stageName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyODAwNg=="}, "originalCommit": {"oid": "0f1dbd39194166d870af75ac17f0e1ea87933b05"}, "originalPosition": 100}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3163, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}