{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYyNDQ5NjQy", "number": 12558, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDo1ODo1MVrOEVKnaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMzowMDoxNlrOEVMmkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNjI5NDgzOnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDo1ODo1MVrOG7xSNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDo1ODo1MVrOG7xSNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMyNjY0NQ==", "bodyText": "isGroup is never used", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465326645", "createdAt": "2020-08-04T20:58:51Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -107,28 +118,52 @@\n     SparkPairCollection<Object, List<JoinElement<Object>>> joinedInputs,\n     StageStatisticsCollector collector) throws Exception;\n \n-  public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n+  public void runPipeline(PhaseSpec phaseSpec, String sourcePluginType,\n                           JavaSparkExecutionContext sec,\n                           Map<String, Integer> stagePartitions,\n                           PluginContext pluginContext,\n-                          Map<String, StageStatisticsCollector> collectors) throws Exception {\n-\n+                          Map<String, StageStatisticsCollector> collectors,\n+                          Set<String> uncombinableSinks,\n+                          boolean consolidateStages) throws Exception {\n+    PipelinePhase pipelinePhase = phaseSpec.getPhase();\n+    BasicArguments arguments = new BasicArguments(sec);\n     MacroEvaluator macroEvaluator =\n-      new DefaultMacroEvaluator(new BasicArguments(sec),\n-                                sec.getLogicalStartTime(), sec,\n-                                sec.getNamespace());\n+      new DefaultMacroEvaluator(arguments, sec.getLogicalStartTime(), sec, sec.getNamespace());\n     Map<String, EmittedRecords> emittedRecords = new HashMap<>();\n \n     // should never happen, but removes warning\n     if (pipelinePhase.getDag() == null) {\n       throw new IllegalStateException(\"Pipeline phase has no connections.\");\n     }\n \n+    Set<String> uncombinableStages = new HashSet<>(uncombinableSinks);\n+    for (String uncombinableType : UNCOMBINABLE_PLUGIN_TYPES) {\n+      pipelinePhase.getStagesOfType(uncombinableType).stream()\n+        .map(StageSpec::getName)\n+        .forEach(s -> uncombinableStages.add(s));\n+    }\n+\n+    CombinerDag groupedDag = new CombinerDag(pipelinePhase.getDag(), uncombinableStages);\n+    Map<String, Set<String>> groups = consolidateStages ? groupedDag.groupNodes() : Collections.emptyMap();\n+    if (!groups.isEmpty()) {\n+      LOG.debug(\"Stage consolidation is on.\");\n+      int groupNum = 1;\n+      for (Set<String> group : groups.values()) {\n+        LOG.debug(\"Group{}: {}\", groupNum, group);\n+        groupNum++;\n+      }\n+    }\n+\n     Collection<Runnable> sinkRunnables = new ArrayList<>();\n-    for (String stageName : pipelinePhase.getDag().getTopologicalOrder()) {\n+    for (String stageName : groupedDag.getTopologicalOrder()) {\n+      boolean isGroup = groups.containsKey(stageName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5de8fd11d4bde8bc7d42149666495eaec485b1ef"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNjQ1NTE3OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/SparkBatchSinkFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTo1MzozMlrOG7yz-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTo1MzozMlrOG7yz-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MTY3NA==", "bodyText": "This can be for (String outputName : sinkOutputNames)", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465351674", "createdAt": "2020-08-04T21:53:32Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/SparkBatchSinkFactory.java", "diffHunk": "@@ -76,18 +98,65 @@ private void addOutput(String stageName, String datasetName, String alias, Map<S\n     addStageOutput(stageName, alias);\n   }\n \n+  /**\n+   * Writes a combined RDD using multiple OutputFormatProviders.\n+   * Returns the set of output names that were written, which still require dataset lineage to be recorded.\n+   */\n+  public <K, V> Set<String> writeCombinedRDD(JavaPairRDD<String, KeyValue<K, V>> combinedRDD,\n+                                             JavaSparkExecutionContext sec, Set<String> sinkNames) {\n+    Map<String, OutputFormatProvider> outputs = new HashMap<>();\n+    Set<String> lineageNames = new HashSet<>();\n+    for (String sinkName : sinkNames) {\n+      Set<String> sinkOutputNames = sinkOutputs.get(sinkName);\n+      if (sinkOutputNames == null || sinkOutputNames.isEmpty()) {\n+        // should never happen if validation happened correctly at pipeline configure time\n+        throw new IllegalStateException(sinkName + \" has no outputs. \" +\n+                                          \"Please check that the sink calls addOutput at some point.\");\n+      }\n+\n+      for (String outputName : sinkOutputs.get(sinkName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5de8fd11d4bde8bc7d42149666495eaec485b1ef"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNjQ2MTA3OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/function/MultiSinkFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTo1NTozNlrOG7y3Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTo1NTozNlrOG7y3Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MjU0Mw==", "bodyText": "unfinished comment?", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465352543", "createdAt": "2020-08-04T21:55:36Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/function/MultiSinkFunction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.function;\n+\n+import com.google.common.collect.Sets;\n+import io.cdap.cdap.api.dataset.lib.KeyValue;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.preview.DataTracer;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.ErrorTransform;\n+import io.cdap.cdap.etl.batch.PipelinePluginInstantiator;\n+import io.cdap.cdap.etl.batch.connector.SingleConnectorFactory;\n+import io.cdap.cdap.etl.common.BasicArguments;\n+import io.cdap.cdap.etl.common.Constants;\n+import io.cdap.cdap.etl.common.DefaultEmitter;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelinePhase;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.RecordInfo;\n+import io.cdap.cdap.etl.common.RecordType;\n+import io.cdap.cdap.etl.common.StageStatisticsCollector;\n+import io.cdap.cdap.etl.exec.PipeTransformExecutor;\n+import io.cdap.cdap.etl.proto.v2.spec.StageSpec;\n+import io.cdap.cdap.etl.spark.SparkTransformExecutorFactory;\n+import scala.Tuple2;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Executes transforms leading into a grouped sink.\n+ * With a pipeline like:\n+ *\n+ *      |--> t1 --> k1\n+ * s1 --|\n+ *      |--> k2\n+ *           ^\n+ *     s2 ---|\n+ *\n+ * the group will include stages t1, k1, and k2:\n+ *\n+ *  s1 --|\n+ *       |--> group1\n+ *  s2 --|\n+ *\n+ * note that although s1 and s2 are both inputs to the group, the records for s2 should only be sent to\n+ * k2 and not to t1. Thus, every input record must be tagged with the stage that it came from, so that they can\n+ * be sent to the right underlying stages. This is why the input is a Tuple2. The key is the name of the stage\n+ * that emitted it and the value is the actual record.\n+ *\n+ * This function is meant to be executed right before saving the Spark collection using a Multi OutputFormat that\n+ * delegates to underlying output formats. This is", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5de8fd11d4bde8bc7d42149666495eaec485b1ef"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNjQ5NzI2OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/function/MultiSinkFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMjowOTo0NlrOG7zNag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMjowOTo0NlrOG7zNag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1ODE4Ng==", "bodyText": "Should this say \"(t1 and k2 in the example above).\"? Since k1 has t1 as an input which means it shouldnt be a source given the definition provided", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465358186", "createdAt": "2020-08-04T22:09:46Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/function/MultiSinkFunction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.function;\n+\n+import com.google.common.collect.Sets;\n+import io.cdap.cdap.api.dataset.lib.KeyValue;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.preview.DataTracer;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.ErrorTransform;\n+import io.cdap.cdap.etl.batch.PipelinePluginInstantiator;\n+import io.cdap.cdap.etl.batch.connector.SingleConnectorFactory;\n+import io.cdap.cdap.etl.common.BasicArguments;\n+import io.cdap.cdap.etl.common.Constants;\n+import io.cdap.cdap.etl.common.DefaultEmitter;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelinePhase;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.RecordInfo;\n+import io.cdap.cdap.etl.common.RecordType;\n+import io.cdap.cdap.etl.common.StageStatisticsCollector;\n+import io.cdap.cdap.etl.exec.PipeTransformExecutor;\n+import io.cdap.cdap.etl.proto.v2.spec.StageSpec;\n+import io.cdap.cdap.etl.spark.SparkTransformExecutorFactory;\n+import scala.Tuple2;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Executes transforms leading into a grouped sink.\n+ * With a pipeline like:\n+ *\n+ *      |--> t1 --> k1\n+ * s1 --|\n+ *      |--> k2\n+ *           ^\n+ *     s2 ---|\n+ *\n+ * the group will include stages t1, k1, and k2:\n+ *\n+ *  s1 --|\n+ *       |--> group1\n+ *  s2 --|\n+ *\n+ * note that although s1 and s2 are both inputs to the group, the records for s2 should only be sent to\n+ * k2 and not to t1. Thus, every input record must be tagged with the stage that it came from, so that they can\n+ * be sent to the right underlying stages. This is why the input is a Tuple2. The key is the name of the stage\n+ * that emitted it and the value is the actual record.\n+ *\n+ * This function is meant to be executed right before saving the Spark collection using a Multi OutputFormat that\n+ * delegates to underlying output formats. This is\n+ */\n+public class MultiSinkFunction implements PairFlatMapFunc<RecordInfo<Object>, String, KeyValue<Object, Object>> {\n+  private final PipelineRuntime pipelineRuntime;\n+  private final PhaseSpec phaseSpec;\n+  private final Set<String> group;\n+  private final Map<String, DataTracer> dataTracers;\n+  private final Map<String, StageStatisticsCollector> collectors;\n+  private transient DefaultEmitter<Tuple2<String, KeyValue<Object, Object>>> emitter;\n+  private transient SparkTransformExecutorFactory executorFactory;\n+  private transient Map<InputInfo, Set<String>> inputConnections;\n+  private transient Map<String, PipeTransformExecutor<Object>> branchExecutors;\n+\n+  public MultiSinkFunction(JavaSparkExecutionContext sec, PhaseSpec phaseSpec, Set<String> group,\n+                           Map<String, StageStatisticsCollector> collectors) {\n+    this.pipelineRuntime = new PipelineRuntime(\n+      sec.getNamespace(), sec.getApplicationSpecification().getName(), sec.getLogicalStartTime(),\n+      new BasicArguments(sec), sec.getMetrics(), sec.getPluginContext(), sec.getServiceDiscoverer(),\n+      sec.getSecureStore(), null, null);\n+    // create a copy because BatchPhaseSpec contains things that are not serializable while PhaseSpec does not\n+    this.phaseSpec = new PhaseSpec(phaseSpec.getPhaseName(), phaseSpec.getPhase(), phaseSpec.getConnectorDatasets(),\n+                                   phaseSpec.isStageLoggingEnabled(), phaseSpec.isProcessTimingEnabled());\n+    this.group = group;\n+    this.collectors = collectors;\n+    this.dataTracers = new HashMap<>();\n+    for (String stage : group) {\n+      dataTracers.put(stage, sec.getDataTracer(stage));\n+    }\n+  }\n+\n+  @Override\n+  public Iterable<Tuple2<String, KeyValue<Object, Object>>> call(RecordInfo<Object> input) throws Exception {\n+    if (branchExecutors == null) {\n+      // branch executors must be created lazily here instead of passed into the constructor to ensure that\n+      // they are not serialized in the function. This ensures that macros are evaluated each run instead of just for\n+      // the first run and then serialized.\n+      initializeBranchExecutors();\n+    }\n+\n+    /*\n+       Input records are a union of RecordInfo<Object> from all possible inputs to the group.\n+       For example, suppose the pipeline looks like:\n+\n+                              |port A --> agg --> k1\n+            s1 --> splitter --|\n+                      |       |port B --> k2\n+                      |\n+                      |--> error collector --> k3\n+\n+       In this scenario, k2, error collector, and k3 are grouped together.\n+       However, the input will contain all records output by the splitter.\n+       More specifically, it contains outputs for portA, outputs for portB, and errors.\n+       Records from portB need to be sent to the k2 branch, errors need to be sent to the error collector branch,\n+       and portA records need to be dropped.\n+     */\n+    InputInfo inputInfo = new InputInfo(input.getFromStage(), input.getType(), input.getFromPort());\n+    Object record = input.getValue();\n+    emitter.reset();\n+\n+    /*\n+        inputConnections contains a map from input source to the branch that should receive it.\n+        With the example pipeline above, it will look like:\n+          { stageName: splitter, port: B, type: output } -> [k2]\n+          { stageName: splitter, type: error } -> [error collector]\n+     */\n+    Set<String> groupSources = inputConnections.getOrDefault(inputInfo, Collections.emptySet());\n+    for (String groupSource : groupSources) {\n+      branchExecutors.get(groupSource).runOneIteration(record);\n+    }\n+\n+    return emitter.getEntries();\n+  }\n+\n+  private void initializeBranchExecutors() {\n+    emitter = new DefaultEmitter<>();\n+    PipelinePluginInstantiator pluginInstantiator =\n+      new PipelinePluginInstantiator(pipelineRuntime.getPluginContext(), pipelineRuntime.getMetrics(),\n+                                     phaseSpec, new SingleConnectorFactory());\n+    MacroEvaluator macroEvaluator = new DefaultMacroEvaluator(\n+      pipelineRuntime.getArguments(), pipelineRuntime.getLogicalStartTime(), pipelineRuntime.getSecureStore(),\n+      pipelineRuntime.getNamespace());\n+    executorFactory = new SparkTransformExecutorFactory(pluginInstantiator, macroEvaluator, null,\n+                                                        collectors, dataTracers, pipelineRuntime, emitter);\n+\n+    /*\n+       If the dag is:\n+\n+            |--> t1 --> k1\n+       s1 --|\n+            |--> k2\n+                 ^\n+           s2 ---|\n+\n+       the group is t1, k1, and k2.\n+     */\n+    PipelinePhase pipelinePhase = phaseSpec.getPhase();\n+    branchExecutors = new HashMap<>();\n+    inputConnections = new HashMap<>();\n+    for (String groupSource : group) {\n+      // start by finding the \"sources\" of the group (t1 and k1 in the example above).\n+      // group \"sources\" are stages in the group that don't have an input from another stage in the group.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5de8fd11d4bde8bc7d42149666495eaec485b1ef"}, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNjYyMDM0OnYy", "diffSide": "RIGHT", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/DStreamCollection.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMzowMDoxNlrOG70VdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMzowMDoxNlrOG70VdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM3NjYyOQ==", "bodyText": "Should we create a JIRA to track this and link it in this TODO?", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465376629", "createdAt": "2020-08-04T23:00:16Z", "author": {"login": "MEseifan"}, "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/DStreamCollection.java", "diffHunk": "@@ -168,6 +176,13 @@ public void run() {\n     };\n   }\n \n+  @Override\n+  public Runnable createMultiStoreTask(Set<String> sinkNames,\n+                                       PairFlatMapFunction<T, String, KeyValue<Object, Object>> multiSinkFunction) {\n+    // TODO: implement", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5de8fd11d4bde8bc7d42149666495eaec485b1ef"}, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3148, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}