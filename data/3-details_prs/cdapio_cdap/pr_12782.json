{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkxNTcxNzE0", "number": 12782, "title": "(CDAP-17287) Collects Spark execution event logs", "bodyText": "This PR has three commits:\n\nMake runtime system services shutdown after the SparkExecutionContext stopped. Currently they stop concurrently. It is not a problem before this PR since we are not doing any important work at the context stop. However, this PR introduces uploading the event logs at the context stop, which needs to be completed before the system services shutdown and process termination.\nJust a refactoring of the SparkRuntimeEnv class to simplify the logic.\nIntroducing the Spark event logging and upload of logs through the runtime service. Based on the cConf settings, the event logging collection will be on/off regardless of the Spark job spark.eventLog.* configurations.", "createdAt": "2020-09-23T07:20:54Z", "url": "https://github.com/cdapio/cdap/pull/12782", "merged": true, "mergeCommit": {"oid": "3e8fd78330e17e3d907dd8222bd04748584b60b5"}, "closed": true, "closedAt": "2020-09-24T03:27:41Z", "author": {"login": "chtyim"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLyrWGAFqTQ5NTAyMTg3Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdL1EhzgBqjM4MDA3MzkwMDA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MDIxODcy", "url": "https://github.com/cdapio/cdap/pull/12782#pullrequestreview-495021872", "createdAt": "2020-09-23T20:26:51Z", "commit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMDoyNjo1MVrOHW_sog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMDozMjoxN1rOHW_35g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NDMzOA==", "bodyText": "Maybe we can use javax.ws.rs.core.MediaType.APPLICATION_OCTET_STREAM?", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493874338", "createdAt": "2020-09-23T20:26:51Z", "author": {"login": "dli357"}, "path": "cdap-app-fabric/src/main/java/io/cdap/cdap/internal/app/runtime/monitor/RuntimeHandler.java", "diffHunk": "@@ -135,14 +153,77 @@ public BodyConsumer writeMessages(HttpRequest request, HttpResponder responder,\n     });\n   }\n \n+  /**\n+   * Handles call for Spark event logs upload.\n+   */\n+  @Path(\"/spark-event-logs/{id}\")\n+  @POST\n+  public BodyConsumer writeSparkEventLogs(HttpRequest request, HttpResponder responder,\n+                                          @PathParam(\"namespace\") String namespace,\n+                                          @PathParam(\"app\") String app,\n+                                          @PathParam(\"version\") String version,\n+                                          @PathParam(\"program-type\") String programType,\n+                                          @PathParam(\"program\") String program,\n+                                          @PathParam(\"run\") String run,\n+                                          @PathParam(\"id\") String id) throws Exception {\n+    if (!eventLogsEnabled) {\n+      throw new UnsupportedOperationException(\"Spark event logs collection is not enabled\");\n+    }\n+\n+    if (!\"application/octet-stream\".equals(request.headers().get(HttpHeaderNames.CONTENT_TYPE))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NDQ1NQ==", "bodyText": "nit: \"refer back\" instead of \"rely back\"", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493874455", "createdAt": "2020-09-23T20:27:04Z", "author": {"login": "dli357"}, "path": "cdap-spark-core-base/src/main/java/io/cdap/cdap/app/runtime/spark/SparkRuntimeUtils.java", "diffHunk": "@@ -232,10 +258,68 @@ private void handleCompleted(boolean failure) {\n         if (failure && driverService instanceof SparkDriverService) {\n           ((SparkDriverService) driverService).stopWithoutComplete();\n         } else {\n-          driverService.stopAndWait();\n+          driverService.stop();\n         }\n+\n+        // Wait for the spark execution context to stop to make sure everything related\n+        // to the spark execution is shutdown properly before returning.\n+        // When this method returns, the driver's main thread returns, which will have system services shutdown\n+        // and process terminated. If system services stopped concurrently with the execution context, we may have some\n+        // logs and tasks missing.\n+        Uninterruptibles.awaitUninterruptibly(secStopLatch, 30, TimeUnit.SECONDS);\n+      }\n+    };\n+  }\n+\n+  /**\n+   * Adds a {@link EventLoggingListener} to the Spark context.\n+   *\n+   * @param runtimeContext the {@link SparkRuntimeContext} for connecting to CDAP services\n+   * @return A {@link Closeable} which should be called when the Spark application completed\n+   */\n+  public static Closeable addEventLoggingListener(SparkRuntimeContext runtimeContext) throws IOException {\n+    // If upload event logs is not enabled, just return a dummy closeable\n+    if (!runtimeContext.getCConfiguration().getBoolean(Constants.AppFabric.SPARK_EVENT_LOGS_ENABLED)) {\n+      return () -> { };\n+    }\n+\n+    SparkConf sparkConf = new SparkConf();\n+    sparkConf.set(\"spark.eventLog.enabled\", Boolean.toString(true));\n+    sparkConf.set(\"spark.eventLog.compress\", Boolean.toString(true));\n+\n+    ProgramRunId programRunId = runtimeContext.getProgramRunId();\n+\n+    File eventLogDir = Files.createTempDirectory(\"spark-events\").toFile();\n+    LOG.debug(\"Spark events log directory for {} is {}\", programRunId, eventLogDir);\n+\n+    // EvevntLogginginListener is Scala package private[spark] class.\n+    // However, since JVM bytecode doesn't really have this type of cross package private support, the class\n+    // is actually public class. This is why we can access it in Java code.\n+    EventLoggingListener listener = new EventLoggingListener(Long.toString(System.currentTimeMillis()), Option.empty(),\n+                                                             eventLogDir.toURI(), sparkConf) {\n+      @Override\n+      public void onApplicationStart(SparkListenerApplicationStart event) {\n+        // Rewrite the application start event with identifiable names based on the program run id such that\n+        // it can rely back to the program run.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NDQ3OQ==", "bodyText": "Looks like NullPointerException will be thrown if the log directory is empty. Can we expect there to always be exactly one log file?", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493874479", "createdAt": "2020-09-23T20:27:07Z", "author": {"login": "dli357"}, "path": "cdap-spark-core-base/src/main/java/io/cdap/cdap/app/runtime/spark/SparkRuntimeUtils.java", "diffHunk": "@@ -262,4 +346,54 @@ protected void doStop() {\n       }\n     };\n   }\n+\n+  /**\n+   * Uploads the spark event logs through the runtime service.\n+   */\n+  private static void uploadEventLogs(File eventLogDir, SparkRuntimeContext runtimeContext) throws IOException {\n+\n+    ProgramRunId programRunId = runtimeContext.getProgramRunId();\n+\n+    // Find the event file to upload. There should only be one for the current application.\n+    File eventFile = Optional.ofNullable(eventLogDir.listFiles())\n+      .map(Arrays::stream)\n+      .flatMap(Stream::findFirst)\n+      .orElse(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NDUwOQ==", "bodyText": "Maybe we can use javax.ws.rs.core.MediaType.APPLICATION_OCTET_STREAM?", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493874509", "createdAt": "2020-09-23T20:27:11Z", "author": {"login": "dli357"}, "path": "cdap-spark-core-base/src/main/java/io/cdap/cdap/app/runtime/spark/SparkRuntimeUtils.java", "diffHunk": "@@ -262,4 +346,54 @@ protected void doStop() {\n       }\n     };\n   }\n+\n+  /**\n+   * Uploads the spark event logs through the runtime service.\n+   */\n+  private static void uploadEventLogs(File eventLogDir, SparkRuntimeContext runtimeContext) throws IOException {\n+\n+    ProgramRunId programRunId = runtimeContext.getProgramRunId();\n+\n+    // Find the event file to upload. There should only be one for the current application.\n+    File eventFile = Optional.ofNullable(eventLogDir.listFiles())\n+      .map(Arrays::stream)\n+      .flatMap(Stream::findFirst)\n+      .orElse(null);\n+    LOG.debug(\"Uploading event file {} for program run {}\", eventFile, programRunId);\n+\n+    RemoteClient remoteClient = new RemoteClient(runtimeContext.getDiscoveryServiceClient(), Constants.Service.RUNTIME,\n+                                                 new DefaultHttpRequestConfig(false),\n+                                                 Constants.Gateway.INTERNAL_API_VERSION_3 + \"/runtime/namespaces/\");\n+    String path = String.format(\"%s/apps/%s/versions/%s/%s/%s/runs/%s/spark-event-logs/%s\",\n+                                programRunId.getNamespace(),\n+                                programRunId.getApplication(),\n+                                programRunId.getVersion(),\n+                                programRunId.getType().getCategoryName(),\n+                                programRunId.getProgram(),\n+                                programRunId.getRun(),\n+                                eventFile.getName());\n+\n+    Retries.runWithRetries(() -> {\n+      // Stream out the messages\n+      HttpURLConnection urlConn = remoteClient.openConnection(HttpMethod.POST, path);\n+      try {\n+        urlConn.setChunkedStreamingMode(CHUNK_SIZE);\n+        urlConn.setRequestProperty(HttpHeaders.CONTENT_TYPE, \"application/octet-stream\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NDU1MA==", "bodyText": "nit: EventLoggingListener", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493874550", "createdAt": "2020-09-23T20:27:15Z", "author": {"login": "dli357"}, "path": "cdap-spark-core-base/src/main/java/io/cdap/cdap/app/runtime/spark/SparkRuntimeUtils.java", "diffHunk": "@@ -232,10 +258,68 @@ private void handleCompleted(boolean failure) {\n         if (failure && driverService instanceof SparkDriverService) {\n           ((SparkDriverService) driverService).stopWithoutComplete();\n         } else {\n-          driverService.stopAndWait();\n+          driverService.stop();\n         }\n+\n+        // Wait for the spark execution context to stop to make sure everything related\n+        // to the spark execution is shutdown properly before returning.\n+        // When this method returns, the driver's main thread returns, which will have system services shutdown\n+        // and process terminated. If system services stopped concurrently with the execution context, we may have some\n+        // logs and tasks missing.\n+        Uninterruptibles.awaitUninterruptibly(secStopLatch, 30, TimeUnit.SECONDS);\n+      }\n+    };\n+  }\n+\n+  /**\n+   * Adds a {@link EventLoggingListener} to the Spark context.\n+   *\n+   * @param runtimeContext the {@link SparkRuntimeContext} for connecting to CDAP services\n+   * @return A {@link Closeable} which should be called when the Spark application completed\n+   */\n+  public static Closeable addEventLoggingListener(SparkRuntimeContext runtimeContext) throws IOException {\n+    // If upload event logs is not enabled, just return a dummy closeable\n+    if (!runtimeContext.getCConfiguration().getBoolean(Constants.AppFabric.SPARK_EVENT_LOGS_ENABLED)) {\n+      return () -> { };\n+    }\n+\n+    SparkConf sparkConf = new SparkConf();\n+    sparkConf.set(\"spark.eventLog.enabled\", Boolean.toString(true));\n+    sparkConf.set(\"spark.eventLog.compress\", Boolean.toString(true));\n+\n+    ProgramRunId programRunId = runtimeContext.getProgramRunId();\n+\n+    File eventLogDir = Files.createTempDirectory(\"spark-events\").toFile();\n+    LOG.debug(\"Spark events log directory for {} is {}\", programRunId, eventLogDir);\n+\n+    // EvevntLogginginListener is Scala package private[spark] class.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NzIyMg==", "bodyText": "Should we log a warning if the wait times out? That way if logs are missing we can easily tell that is was due to the timeout or some other reason.", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493877222", "createdAt": "2020-09-23T20:32:17Z", "author": {"login": "dli357"}, "path": "cdap-spark-core-base/src/main/java/io/cdap/cdap/app/runtime/spark/SparkRuntimeUtils.java", "diffHunk": "@@ -232,10 +258,68 @@ private void handleCompleted(boolean failure) {\n         if (failure && driverService instanceof SparkDriverService) {\n           ((SparkDriverService) driverService).stopWithoutComplete();\n         } else {\n-          driverService.stopAndWait();\n+          driverService.stop();\n         }\n+\n+        // Wait for the spark execution context to stop to make sure everything related\n+        // to the spark execution is shutdown properly before returning.\n+        // When this method returns, the driver's main thread returns, which will have system services shutdown\n+        // and process terminated. If system services stopped concurrently with the execution context, we may have some\n+        // logs and tasks missing.\n+        Uninterruptibles.awaitUninterruptibly(secStopLatch, 30, TimeUnit.SECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57e89749873dcd4d106a7b9e51f5259ddfbefb52"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MDY1Njkx", "url": "https://github.com/cdapio/cdap/pull/12782#pullrequestreview-495065691", "createdAt": "2020-09-23T21:35:20Z", "commit": {"oid": "5c65437c6416a142acb171443babf21f591e0c16"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozNToyMFrOHXBz6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozNToyMFrOHXBz6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwODk3MQ==", "bodyText": "nit: EventLoggingListener instead of EvevntLoggingListener", "url": "https://github.com/cdapio/cdap/pull/12782#discussion_r493908971", "createdAt": "2020-09-23T21:35:20Z", "author": {"login": "dli357"}, "path": "cdap-spark-core-base/src/main/java/io/cdap/cdap/app/runtime/spark/SparkRuntimeUtils.java", "diffHunk": "@@ -292,15 +295,15 @@ public static Closeable addEventLoggingListener(SparkRuntimeContext runtimeConte\n     File eventLogDir = Files.createTempDirectory(\"spark-events\").toFile();\n     LOG.debug(\"Spark events log directory for {} is {}\", programRunId, eventLogDir);\n \n-    // EvevntLogginginListener is Scala package private[spark] class.\n+    // EvevntLoggingListener is Scala package private[spark] class.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5c65437c6416a142acb171443babf21f591e0c16"}, "originalPosition": 24}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5c65437c6416a142acb171443babf21f591e0c16", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/5c65437c6416a142acb171443babf21f591e0c16", "committedDate": "2020-09-23T21:13:09Z", "message": "Address comments"}, "afterCommit": {"oid": "0277c884f22733fef51fa556010aa83c015ed7a4", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/0277c884f22733fef51fa556010aa83c015ed7a4", "committedDate": "2020-09-23T22:06:12Z", "message": "(CDAP-17287) Collection of Spark execution event logs\n\n- Introducing the Spark event logging and upload of logs through the runtime service.\n  Based on the cConf settings, the event logging collection will be on/off regardless\n  of the Spark job spark.eventLog.* configurations.\n- Refactoring of the SparkRuntimeEnv class to simplify the logic by using CompletableFuture.\n- Make runtime system services shutdown after the SparkExecutionContext stopped.\n  Currently they stop concurrently. It is not a problem before this PR since we\n  are not doing any important work at the context stop.\n  However, this PR introduces uploading the event logs at the context stop,\n  which needs to be completed before the system services shutdown and process termination."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0277c884f22733fef51fa556010aa83c015ed7a4", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/0277c884f22733fef51fa556010aa83c015ed7a4", "committedDate": "2020-09-23T22:06:12Z", "message": "(CDAP-17287) Collection of Spark execution event logs\n\n- Introducing the Spark event logging and upload of logs through the runtime service.\n  Based on the cConf settings, the event logging collection will be on/off regardless\n  of the Spark job spark.eventLog.* configurations.\n- Refactoring of the SparkRuntimeEnv class to simplify the logic by using CompletableFuture.\n- Make runtime system services shutdown after the SparkExecutionContext stopped.\n  Currently they stop concurrently. It is not a problem before this PR since we\n  are not doing any important work at the context stop.\n  However, this PR introduces uploading the event logs at the context stop,\n  which needs to be completed before the system services shutdown and process termination."}, "afterCommit": {"oid": "83144b472f95a44f94b5f9a675c994a7563e8042", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/83144b472f95a44f94b5f9a675c994a7563e8042", "committedDate": "2020-09-23T22:33:52Z", "message": "(CDAP-17287) Collection of Spark execution event logs\n\n- Introducing the Spark event logging and upload of logs through the runtime service.\n  Based on the cConf settings, the event logging collection will be on/off regardless\n  of the Spark job spark.eventLog.* configurations.\n- Refactoring of the SparkRuntimeEnv class to simplify the logic by using CompletableFuture.\n- Make runtime system services shutdown after the SparkExecutionContext stopped.\n  Currently they stop concurrently. It is not a problem before this PR since we\n  are not doing any important work at the context stop.\n  However, this PR introduces uploading the event logs at the context stop,\n  which needs to be completed before the system services shutdown and process termination."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "63aa9a75cc72dbe0cc11b61f7a15fc0c12274427", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/63aa9a75cc72dbe0cc11b61f7a15fc0c12274427", "committedDate": "2020-09-23T23:20:09Z", "message": "(CDAP-17287) Collection of Spark execution event logs\n\n- Introducing the Spark event logging and upload of logs through the runtime service.\n  Based on the cConf settings, the event logging collection will be on/off regardless\n  of the Spark job spark.eventLog.* configurations.\n- Refactoring of the SparkRuntimeEnv class to simplify the logic by using CompletableFuture.\n- Make runtime system services shutdown after the SparkExecutionContext stopped.\n  Currently they stop concurrently. It is not a problem before this PR since we\n  are not doing any important work at the context stop.\n  However, this PR introduces uploading the event logs at the context stop,\n  which needs to be completed before the system services shutdown and process termination."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "83144b472f95a44f94b5f9a675c994a7563e8042", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/83144b472f95a44f94b5f9a675c994a7563e8042", "committedDate": "2020-09-23T22:33:52Z", "message": "(CDAP-17287) Collection of Spark execution event logs\n\n- Introducing the Spark event logging and upload of logs through the runtime service.\n  Based on the cConf settings, the event logging collection will be on/off regardless\n  of the Spark job spark.eventLog.* configurations.\n- Refactoring of the SparkRuntimeEnv class to simplify the logic by using CompletableFuture.\n- Make runtime system services shutdown after the SparkExecutionContext stopped.\n  Currently they stop concurrently. It is not a problem before this PR since we\n  are not doing any important work at the context stop.\n  However, this PR introduces uploading the event logs at the context stop,\n  which needs to be completed before the system services shutdown and process termination."}, "afterCommit": {"oid": "63aa9a75cc72dbe0cc11b61f7a15fc0c12274427", "author": {"user": {"login": "chtyim", "name": "Terence Yim"}}, "url": "https://github.com/cdapio/cdap/commit/63aa9a75cc72dbe0cc11b61f7a15fc0c12274427", "committedDate": "2020-09-23T23:20:09Z", "message": "(CDAP-17287) Collection of Spark execution event logs\n\n- Introducing the Spark event logging and upload of logs through the runtime service.\n  Based on the cConf settings, the event logging collection will be on/off regardless\n  of the Spark job spark.eventLog.* configurations.\n- Refactoring of the SparkRuntimeEnv class to simplify the logic by using CompletableFuture.\n- Make runtime system services shutdown after the SparkExecutionContext stopped.\n  Currently they stop concurrently. It is not a problem before this PR since we\n  are not doing any important work at the context stop.\n  However, this PR introduces uploading the event logs at the context stop,\n  which needs to be completed before the system services shutdown and process termination."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1661, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}