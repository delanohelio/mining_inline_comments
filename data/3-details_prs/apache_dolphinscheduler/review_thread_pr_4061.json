{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE5NzIyMzAw", "number": 4061, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDoyNzowNFrOE5YmEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDozMTo1NlrOE5YylQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NjA3MjQ4OnYy", "diffSide": "RIGHT", "path": "dolphinscheduler-server/src/main/java/org/apache/dolphinscheduler/server/utils/SparkArgsUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDoyNzowNFrOHzzvQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDoyNzowNFrOHzzvQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA4NzEwNw==", "bodyText": "magic number", "url": "https://github.com/apache/dolphinscheduler/pull/4061#discussion_r524087107", "createdAt": "2020-11-16T10:27:04Z", "author": {"login": "lenboo"}, "path": "dolphinscheduler-server/src/main/java/org/apache/dolphinscheduler/server/utils/SparkArgsUtils.java", "diffHunk": "@@ -43,11 +42,11 @@\n         String deployMode = \"cluster\";\n \n         args.add(Constants.MASTER);\n-        if(StringUtils.isNotEmpty(param.getDeployMode())){\n+        if (StringUtils.isNotEmpty(param.getDeployMode())) {\n             deployMode = param.getDeployMode();\n \n         }\n-        if(!\"local\".equals(deployMode)){\n+        if (!\"local\".equals(deployMode)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "749e0ae559532f8813492b2b2ce36cbc86ee3577"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NjEwNDUzOnYy", "diffSide": "RIGHT", "path": "dolphinscheduler-server/src/main/java/org/apache/dolphinscheduler/server/worker/task/spark/SparkTask.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDozMTo1NlrOHz0EKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxMDozMTo1NlrOHz0EKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA5MjQ1OQ==", "bodyText": "i think it's better to return void or throw ex if null.", "url": "https://github.com/apache/dolphinscheduler/pull/4061#discussion_r524092459", "createdAt": "2020-11-16T10:31:56Z", "author": {"login": "lenboo"}, "path": "dolphinscheduler-server/src/main/java/org/apache/dolphinscheduler/server/worker/task/spark/SparkTask.java", "diffHunk": "@@ -22,133 +23,132 @@\n import org.apache.dolphinscheduler.common.process.ResourceInfo;\n import org.apache.dolphinscheduler.common.task.AbstractParameters;\n import org.apache.dolphinscheduler.common.task.spark.SparkParameters;\n-import org.apache.dolphinscheduler.common.utils.*;\n+import org.apache.dolphinscheduler.common.utils.JSONUtils;\n import org.apache.dolphinscheduler.common.utils.ParameterUtils;\n-import org.apache.dolphinscheduler.common.utils.StringUtils;\n-import org.apache.dolphinscheduler.server.entity.TaskExecutionContext;\n import org.apache.dolphinscheduler.dao.entity.Resource;\n+import org.apache.dolphinscheduler.server.entity.TaskExecutionContext;\n import org.apache.dolphinscheduler.server.utils.ParamUtils;\n import org.apache.dolphinscheduler.server.utils.SparkArgsUtils;\n import org.apache.dolphinscheduler.server.worker.task.AbstractYarnTask;\n-import org.slf4j.Logger;\n \n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n \n+import org.slf4j.Logger;\n+\n /**\n  * spark task\n  */\n public class SparkTask extends AbstractYarnTask {\n \n-  /**\n-   * spark1 command\n-   */\n-  private static final String SPARK1_COMMAND = \"${SPARK_HOME1}/bin/spark-submit\";\n+    /**\n+     * spark1 command\n+     */\n+    private static final String SPARK1_COMMAND = \"${SPARK_HOME1}/bin/spark-submit\";\n+\n+    /**\n+     * spark2 command\n+     */\n+    private static final String SPARK2_COMMAND = \"${SPARK_HOME2}/bin/spark-submit\";\n+\n+    /**\n+     * spark parameters\n+     */\n+    private SparkParameters sparkParameters;\n+\n+    /**\n+     * taskExecutionContext\n+     */\n+    private final TaskExecutionContext sparkTaskExecutionContext;\n+\n+    public SparkTask(TaskExecutionContext taskExecutionContext, Logger logger) {\n+        super(taskExecutionContext, logger);\n+        this.sparkTaskExecutionContext = taskExecutionContext;\n+    }\n \n-  /**\n-   * spark2 command\n-   */\n-  private static final String SPARK2_COMMAND = \"${SPARK_HOME2}/bin/spark-submit\";\n+    @Override\n+    public void init() {\n \n-  /**\n-   *  spark parameters\n-   */\n-  private SparkParameters sparkParameters;\n+        logger.info(\"spark task params {}\", sparkTaskExecutionContext.getTaskParams());\n \n-  /**\n-   * taskExecutionContext\n-   */\n-  private TaskExecutionContext taskExecutionContext;\n+        sparkParameters = JSONUtils.parseObject(sparkTaskExecutionContext.getTaskParams(), SparkParameters.class);\n \n-  public SparkTask(TaskExecutionContext taskExecutionContext, Logger logger) {\n-    super(taskExecutionContext, logger);\n-    this.taskExecutionContext = taskExecutionContext;\n-  }\n+        if (null == sparkParameters) {\n+            logger.error(\"Spark params is null\");\n+            return;\n+        }\n \n-  @Override\n-  public void init() {\n+        if (!sparkParameters.checkParameters()) {\n+            throw new RuntimeException(\"spark task params is not valid\");\n+        }\n+        sparkParameters.setQueue(sparkTaskExecutionContext.getQueue());\n+        setMainJarName();\n+    }\n \n-    logger.info(\"spark task params {}\", taskExecutionContext.getTaskParams());\n+    /**\n+     * create command\n+     *\n+     * @return command\n+     */\n+    @Override\n+    protected String buildCommand() {\n+        List<String> args = new ArrayList<>();\n \n-    sparkParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), SparkParameters.class);\n+        //spark version\n+        String sparkCommand = SPARK2_COMMAND;\n \n-    if (!sparkParameters.checkParameters()) {\n-      throw new RuntimeException(\"spark task params is not valid\");\n-    }\n-    sparkParameters.setQueue(taskExecutionContext.getQueue());\n+        if (SparkVersion.SPARK1.name().equals(sparkParameters.getSparkVersion())) {\n+            sparkCommand = SPARK1_COMMAND;\n+        }\n \n-    setMainJarName();\n+        args.add(sparkCommand);\n \n-    if (StringUtils.isNotEmpty(sparkParameters.getMainArgs())) {\n-      String args = sparkParameters.getMainArgs();\n+        // other parameters\n+        args.addAll(SparkArgsUtils.buildArgs(sparkParameters));\n \n-      // replace placeholder\n-      Map<String, Property> paramsMap = ParamUtils.convert(ParamUtils.getUserDefParamsMap(taskExecutionContext.getDefinedParams()),\n-              taskExecutionContext.getDefinedParams(),\n-              sparkParameters.getLocalParametersMap(),\n-              CommandType.of(taskExecutionContext.getCmdTypeIfComplement()),\n-              taskExecutionContext.getScheduleTime());\n+        // replace placeholder\n+        Map<String, Property> paramsMap = ParamUtils.convert(ParamUtils.getUserDefParamsMap(sparkTaskExecutionContext.getDefinedParams()),\n+            sparkTaskExecutionContext.getDefinedParams(),\n+            sparkParameters.getLocalParametersMap(),\n+            CommandType.of(sparkTaskExecutionContext.getCmdTypeIfComplement()),\n+            sparkTaskExecutionContext.getScheduleTime());\n \n-      if (paramsMap != null ){\n-        args = ParameterUtils.convertParameterPlaceholders(args, ParamUtils.convert(paramsMap));\n-      }\n-      sparkParameters.setMainArgs(args);\n-    }\n-  }\n+        String command = null;\n \n-  /**\n-   * create command\n-   * @return command\n-   */\n-  @Override\n-  protected String buildCommand() {\n-    List<String> args = new ArrayList<>();\n+        if (null != paramsMap) {\n+            command = ParameterUtils.convertParameterPlaceholders(String.join(\" \", args), ParamUtils.convert(paramsMap));\n+        }\n \n-    //spark version\n-    String sparkCommand = SPARK2_COMMAND;\n+        logger.info(\"spark task command: {}\", command);\n \n-    if (SparkVersion.SPARK1.name().equals(sparkParameters.getSparkVersion())) {\n-      sparkCommand = SPARK1_COMMAND;\n+        return command;\n     }\n \n-    args.add(sparkCommand);\n-\n-    // other parameters\n-    args.addAll(SparkArgsUtils.buildArgs(sparkParameters));\n-\n-    String command = ParameterUtils\n-            .convertParameterPlaceholders(String.join(\" \", args), taskExecutionContext.getDefinedParams());\n-\n-    logger.info(\"spark task command : {}\", command);\n-\n-    return command;\n-  }\n-\n-  @Override\n-  protected void setMainJarName() {\n-    // main jar\n-    ResourceInfo mainJar = sparkParameters.getMainJar();\n-    if (mainJar != null) {\n-      int resourceId = mainJar.getId();\n-      String resourceName;\n-      if (resourceId == 0) {\n-        resourceName = mainJar.getRes();\n-      } else {\n-        Resource resource = processService.getResourceById(sparkParameters.getMainJar().getId());\n-        if (resource == null) {\n-          logger.error(\"resource id: {} not exist\", resourceId);\n-          throw new RuntimeException(String.format(\"resource id: %d not exist\", resourceId));\n+    @Override\n+    protected void setMainJarName() {\n+        // main jar\n+        ResourceInfo mainJar = sparkParameters.getMainJar();\n+        if (mainJar != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "749e0ae559532f8813492b2b2ce36cbc86ee3577"}, "originalPosition": 203}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3289, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}