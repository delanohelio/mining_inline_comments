{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0ODIzNDk1", "number": 793, "title": "FacetHandler ingest code + FacetedQueryLogic Configuration", "bodyText": "", "createdAt": "2020-03-27T15:35:03Z", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793", "merged": true, "mergeCommit": {"oid": "9b7e202af3a35368a3df2dfeadce9d3b6243cd77"}, "closed": true, "closedAt": "2020-07-07T13:34:58Z", "author": {"login": "drewfarris"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcZfU-CgBqjMyNTE4MTkxMjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcyl4UhAFqTQ0Mzg5MTAwMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1b18c3a0201acf8cc406216414b18b512b165ae4", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/1b18c3a0201acf8cc406216414b18b512b165ae4", "committedDate": "2020-04-13T01:19:01Z", "message": "WIP facet code"}, "afterCommit": {"oid": "6f118cc22f8bf3fb66aca461a48f60222b3c8ac9", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/6f118cc22f8bf3fb66aca461a48f60222b3c8ac9", "committedDate": "2020-04-20T13:43:48Z", "message": "WIP facet code"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "273c9ebd488be666288fbc3a39d4bda4b430b51f", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/273c9ebd488be666288fbc3a39d4bda4b430b51f", "committedDate": "2020-04-24T03:21:00Z", "message": "WIP Facet Handler\n\n* Unit testing continued"}, "afterCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/b8a54b54c4ca4167d9c3b693ef9682c7210c916e", "committedDate": "2020-04-24T17:14:52Z", "message": "Faceted Ingest & Search for Datawave\n\n* FacetHandler generates facets for data at ingest time.\n  * Adds 3 tables: datawave.facets, datawave.facetMetadata, datawave.facetHashes.\n  * Adds dependency on com.clearspring.analytics:stream for summarization and cardinality estimation.\n  * Ingest configurations that add the FacetHandler for tvmaze/myjson data\n* Adds FacetedQuery query logic and FacetedQueryPlanner"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwMjQ4NTg5", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-400248589", "createdAt": "2020-04-24T20:26:23Z", "commit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoyNjoyM1rOGLn_Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoyNjoyM1rOGLn_Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg0MjY4Mw==", "bodyText": "Bad import? Probably.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r414842683", "createdAt": "2020-04-24T20:26:23Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import jj2000.j2k.util.FacilityManager;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNDU4MTA1", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-401458105", "createdAt": "2020-04-28T02:38:13Z", "commit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMjozODoxM1rOGM_7rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMjozODoxM1rOGM_7rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4MzU2Nw==", "bodyText": "This error message seems like it was copied from elsewhere in the test & should probably say that there was an error in processing", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r416283567", "createdAt": "2020-04-28T02:38:13Z", "author": {"login": "billoley"}, "path": "warehouse/ingest-core/src/test/java/datawave/ingest/mapreduce/handler/facet/FacetHandlerTest.java", "diffHunk": "@@ -0,0 +1,595 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.LinkedListMultimap;\n+import com.google.common.collect.Multimap;\n+import datawave.data.hash.UID;\n+import datawave.data.type.LcNoDiacriticsType;\n+import datawave.ingest.config.RawRecordContainerImpl;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.NormalizedFieldAndValue;\n+import datawave.ingest.data.config.ingest.BaseIngestHelper;\n+import datawave.ingest.data.config.ingest.ContentBaseIngestHelper;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestContentBaseIngestHelper;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestContentIndexingColumnBasedHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestEventRecordReader;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.AbstractContextWriter;\n+import datawave.ingest.test.StandaloneStatusReporter;\n+import datawave.ingest.test.StandaloneTaskAttemptContext;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntOpenHashMap;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.accumulo.core.util.Pair;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.log4j.Logger;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.junit.Assert.*;\n+\n+public class FacetHandlerTest {\n+    \n+    private static final String TEST_TYPE = \"test\";\n+    private static final UID TEST_UID = UID.builder().newId();\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandlerTest.class);\n+    public static final String DATAWAVE_FACETS = \"datawave.facets\";\n+    public static final String DATAWAVE_FACET_METADATA = \"datawave.facetMetadata\";\n+    public static final String DATAWAVE_FACET_HASHES = \"datawave.facetHashes\";\n+    \n+    private TaskAttemptContext ctx = null;\n+    \n+    private RawRecordContainer event = EasyMock.createMock(RawRecordContainer.class);\n+    private ContentBaseIngestHelper helper;\n+    private ColumnVisibility colVis;\n+    \n+    @Before\n+    public void setUp() throws Exception {\n+        Configuration conf = new Configuration();\n+        conf.addResource(\"config/all-config.xml\");\n+        conf.addResource(\"config/facet-config.xml\");\n+        \n+        ctx = new TaskAttemptContextImpl(conf, new org.apache.hadoop.mapred.TaskAttemptID());\n+        ctx.getConfiguration().setInt(ContentIndexingColumnBasedHandler.NUM_SHARDS, 1);\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_TNAME, \"shard\");\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_GIDX_TNAME, \"shardIndex\");\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_GRIDX_TNAME, \"shardIndex\");\n+        \n+        ctx.getConfiguration().set(\"data.name\", TEST_TYPE);\n+        ctx.getConfiguration().set(\"test.data.auth.id.mode\", \"NEVER\");\n+        ctx.getConfiguration().set(\"test\" + BaseIngestHelper.DEFAULT_TYPE, LcNoDiacriticsType.class.getName());\n+        \n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.HANDLER_CLASSES, TestContentIndexingColumnBasedHandler.class.getName());\n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.RAW_READER, TestEventRecordReader.class.getName());\n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.INGEST_HELPER, TestContentBaseIngestHelper.class.getName());\n+        ctx.getConfiguration().set(TypeRegistry.EXCLUDED_HANDLER_CLASSES, \"FAKE_HANDLER_CLASS\"); // it will die if this field is not faked\n+        \n+        colVis = new ColumnVisibility(\"\");\n+        helper = new TestContentBaseIngestHelper();\n+        \n+        TypeRegistry.reset();\n+        TypeRegistry.getInstance(ctx.getConfiguration());\n+        \n+        setupMocks();\n+        \n+        configureFacets(ctx.getConfiguration());\n+        \n+        // log.setLevel(Level.DEBUG);\n+    }\n+    \n+    private void setupMocks() {\n+        try {\n+            long timestamp = Instant.parse(\"2020-04-01T08:00:00.0z\").toEpochMilli();\n+            EasyMock.expect(event.getVisibility()).andReturn(colVis).anyTimes();\n+            EasyMock.expect(event.getDataType()).andReturn(TypeRegistry.getType(TEST_TYPE)).anyTimes();\n+            EasyMock.expect(event.getId()).andReturn(TEST_UID).anyTimes();\n+            EasyMock.expect(event.getDate()).andReturn(timestamp).anyTimes();\n+            EasyMock.replay(event);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+    \n+    public void configureFacets(Configuration conf) {\n+        conf.set(\"test.facet.category.name.manufacturer\", \"MAKE/STYLE,MODEL,COLOR\");\n+        conf.set(\"test.facet.category.name.style\", \"STYLE/MANUFACTURER,MODEL,COLOR\");\n+        conf.set(\"test.facet.category.name.color\", \"COLOR/MANUFACTURER,MODEL,STYLE\");\n+    }\n+    \n+    @Test\n+    public void testSingleEvent() {\n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        helper.setup(ctx.getConfiguration());\n+        \n+        Multimap<String,NormalizedContentInterface> fields = TestData.getDataItem(1);\n+        setupTaskAttemptContext();\n+        processEvent(event, fields, handler);\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        Set<String> expectedFacets = Stream.of(TestData.getExpectedFacetData(1)).collect(Collectors.toSet());\n+        Set<String> expectedFacetMetadata = Stream.of(TestData.getExpectedFacetMetadataData(1)).collect(Collectors.toSet());\n+        \n+        evaluateSingleEventResults(keysByTable, expectedFacets, expectedFacetMetadata);\n+    }\n+    \n+    @Test\n+    public void testMultipleEvents() {\n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        helper.setup(ctx.getConfiguration());\n+        \n+        Collection<Multimap<String,NormalizedContentInterface>> items = TestData.getDataItems();\n+        int size = items.size();\n+        \n+        setupTaskAttemptContext();\n+        items.forEach(f -> processEvent(event, f, handler));\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        Object2IntMap<String> expectedFacetKeyCounts = TestData.getExpectedFacetCounts();\n+        evaluateMultipleEventResults(keysByTable, size, expectedFacetKeyCounts);\n+    }\n+    \n+    @Test\n+    public void testMultiValueHashing() {\n+        \n+        ctx.getConfiguration().set(\"test.facet.hash.threshold\", \"2\");\n+        \n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        Collection<Multimap<String,NormalizedContentInterface>> items = TestData.generateMultivaluedColorData();\n+        \n+        setupTaskAttemptContext();\n+        items.forEach(f -> processEvent(event, f, handler));\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        evaluateMultiValueResults(keysByTable, TestData.getExpectedFacetHashes());\n+    }\n+    \n+    StandaloneTaskAttemptContext<Text,RawRecordContainerImpl,BulkIngestKey,Value> sctx;\n+    CachingContextWriter contextWriter;\n+    \n+    private void setupTaskAttemptContext() {\n+        sctx = new StandaloneTaskAttemptContext<>(ctx.getConfiguration(), new StandaloneStatusReporter());\n+        contextWriter = new CachingContextWriter();\n+        try {\n+            contextWriter.setup(sctx.getConfiguration(), false);\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Error setting up context writer\", e);\n+        }\n+    }\n+    \n+    /**\n+     * Process a single event using the handler\n+     *\n+     * @param event\n+     *            the event we're processing\n+     * @param eventFields\n+     *            the fields from the event to process\n+     * @param handler\n+     *            the handler to do the procesing\n+     */\n+    private void processEvent(RawRecordContainer event, Multimap<String,NormalizedContentInterface> eventFields,\n+                    ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler) {\n+        \n+        assertNotNull(\"Event was null\", event);\n+        \n+        try {\n+            handler.process(null, event, eventFields, sctx, contextWriter);\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Error setting up context writer\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 206}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyNTI4ODMy", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-402528832", "createdAt": "2020-04-29T10:07:31Z", "commit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDowNzozMVrOGN4HzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDoxMTowMFrOGN4Owg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzIwNDE3Mg==", "bodyText": "Remove commented code", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417204172", "createdAt": "2020-04-29T10:07:31Z", "author": {"login": "apmoriarty"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzIwNTk1NA==", "bodyText": "Extra TODO comment?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417205954", "createdAt": "2020-04-29T10:11:00Z", "author": {"login": "apmoriarty"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 324}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzMTU0Mzcw", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-403154370", "createdAt": "2020-04-30T01:55:04Z", "commit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMTo1NTowNFrOGOXGlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMTo1NTowNFrOGOXGlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxMTc2Ng==", "bodyText": "Are you avoiding using the sparse setting for HLL++ because of some of the issues with size in certain cases when serializing it to disk?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417711766", "createdAt": "2020-04-30T01:55:04Z", "author": {"login": "cawaring"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */\n+    @Override\n+    public FacetValue estimate(RawRecordContainer input) {\n+        // precision value: 10, sparse set disabled.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 327}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzMTU4ODgx", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-403158881", "createdAt": "2020-04-30T02:12:05Z", "commit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxMjowNVrOGOXWgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxMjowNVrOGOXWgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxNTg0Mg==", "bodyText": "I might not be reading this correctly, but using this constructor we're creating a CMS with depth of 10 and width of 1.  We should probable make these parameters configurable, likewise for hLL++.  That way one could set the parameters based on the error and probability they want to support.  Could add some bounds checking to make it reasonable.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417715842", "createdAt": "2020-04-30T02:12:05Z", "author": {"login": "cawaring"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */\n+    @Override\n+    public FacetValue estimate(RawRecordContainer input) {\n+        // precision value: 10, sparse set disabled.\n+        HyperLogLogPlus card = new HyperLogLogPlus(10);\n+        card.offer(input.getId().toString());\n+        \n+        return new FacetValue(card, new CountMinSketch(10, 1, 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 331}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzMTU5NjU4", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-403159658", "createdAt": "2020-04-30T02:14:46Z", "commit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxNDo0NlrOGOXZMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxNDo0NlrOGOXZMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxNjUyOA==", "bodyText": "We may want to include some configurations for hll++ and cms here.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417716528", "createdAt": "2020-04-30T02:14:46Z", "author": {"login": "cawaring"}, "path": "warehouse/ingest-core/src/test/resources/config/facet-config.xml", "diffHunk": "@@ -0,0 +1,28 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n+<?xml-stylesheet type=\"test/xsl\" href=\"configuration.xsl\"?>\n+<configuration>\n+    <property>\n+        <name>facet.table.name</name>\n+        <value>datawave.facets</value>\n+    </property>\n+    <property>\n+        <name>facet.table.loader.priority</name>\n+        <value>40</value>\n+    </property>\n+    <property>\n+        <name>facet.metadata.table.name</name>\n+        <value>datawave.facetMetadata</value>\n+    </property>\n+    <property>\n+        <name>facet.metadata.table.loader.priority</name>\n+        <value>40</value>\n+    </property>\n+    <property>\n+        <name>facet.hash.table.name</name>\n+        <value>datawave.facetHashes</value>\n+    </property>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 23}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6fef60ed33de01f55d2a31161ddcd3f5c5b63e03", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/6fef60ed33de01f55d2a31161ddcd3f5c5b63e03", "committedDate": "2020-05-08T18:02:54Z", "message": "WIP: Added field information to facet values"}, "afterCommit": {"oid": "0d4eb3643cb0765d950b951423ef844311ed6545", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/0d4eb3643cb0765d950b951423ef844311ed6545", "committedDate": "2020-05-20T17:15:20Z", "message": "WIP: Added field information to facet values"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0d4eb3643cb0765d950b951423ef844311ed6545", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/0d4eb3643cb0765d950b951423ef844311ed6545", "committedDate": "2020-05-20T17:15:20Z", "message": "WIP: Added field information to facet values"}, "afterCommit": {"oid": "022f1dd43047ddaf81420edeb2aa353e1f4db8f0", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/022f1dd43047ddaf81420edeb2aa353e1f4db8f0", "committedDate": "2020-05-22T16:51:55Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "022f1dd43047ddaf81420edeb2aa353e1f4db8f0", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/022f1dd43047ddaf81420edeb2aa353e1f4db8f0", "committedDate": "2020-05-22T16:51:55Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups."}, "afterCommit": {"oid": "ffa1d7286ecd47ebb49975972be7a4cfaefa3176", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/ffa1d7286ecd47ebb49975972be7a4cfaefa3176", "committedDate": "2020-05-22T17:04:53Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups.\n* Changes in response to comments from code review."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ffa1d7286ecd47ebb49975972be7a4cfaefa3176", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/ffa1d7286ecd47ebb49975972be7a4cfaefa3176", "committedDate": "2020-05-22T17:04:53Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups.\n* Changes in response to comments from code review."}, "afterCommit": {"oid": "d9b45f03921afe6592200dc114864415ac74f1a8", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/d9b45f03921afe6592200dc114864415ac74f1a8", "committedDate": "2020-05-22T18:34:39Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups.\n* Changes in response to comments from code review."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d9b45f03921afe6592200dc114864415ac74f1a8", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/d9b45f03921afe6592200dc114864415ac74f1a8", "committedDate": "2020-05-22T18:34:39Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups.\n* Changes in response to comments from code review."}, "afterCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/1cdd73da4fb6ced05cf44c38db95c8075a101288", "committedDate": "2020-06-01T18:19:09Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* FacetHandler generates facets for data at ingest time.\n  * Adds 3 tables: datawave.facets, datawave.facetMetadata, datawave.facetHashes.\n  * Adds dependency on com.clearspring.analytics:stream for summarization and cardinality estimation.\n  * Ingest configurations that add the FacetHandler for tvmaze/myjson data\n* Adds FacetedQuery query logic and FacetedQueryPlanner\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups.\n* Changes in response to comments from code review."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNzY1Njcy", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-431765672", "createdAt": "2020-06-16T18:09:23Z", "commit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODowOToyM1rOGknRuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODo0NTo0NVrOGkoqwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NTQzMg==", "bodyText": "Just a nit, but '/' suggests a path of some sort which is not the case here.  This is more like a definition so perhaps colon, equals or some sort of bracket may be better.  Again, just a nit.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441045432", "createdAt": "2020-06-16T18:09:23Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0Nzk0Mg==", "bodyText": "use ShardIdFactory.getDateString(shardId)", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441047942", "createdAt": "2020-06-16T18:13:54Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2NzUzMA==", "bodyText": "Shouldn't this check be moved outside of this for loop but inside the containing loop (i.e move to line 247)?  Alternatively the eventFields could be prefiltered to not include any \"reduced\" members.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441067530", "createdAt": "2020-06-16T18:44:28Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2ODIyNA==", "bodyText": "yup, missing that", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441068224", "createdAt": "2020-06-16T18:45:45Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import org.apache.accumulo.core.data.Value;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public class FacetValue extends Value {\n+    private HyperLogLogPlus cardinalityEstimate;\n+    private CountMinSketch frequencyEstimate;\n+    protected final AtomicBoolean written = new AtomicBoolean(false);\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate, CountMinSketch frequencyEstimate) {\n+        this.cardinalityEstimate = cardinalityEstimate;\n+        this.frequencyEstimate = frequencyEstimate;\n+    }\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate) {\n+        this(cardinalityEstimate, null);\n+    }\n+    \n+    protected FacetValue() {\n+        \n+    }\n+    \n+    public static FacetValue buildFrom(final DataInput in) throws IOException {\n+        FacetValue value = new FacetValue();\n+        value.readFields(in);\n+        return value;\n+    }\n+    \n+    public byte[] get() {\n+        try {\n+            ensureWritten();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        return super.get();\n+    }\n+    \n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+        int cardSize = in.readInt();\n+        byte[] cardinality = new byte[cardSize];\n+        in.readFully(cardinality);\n+        \n+        // TODO: frequencyEstimate", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 53}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "602e25d1827ee80d643192d360b3326272cd8e95", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/602e25d1827ee80d643192d360b3326272cd8e95", "committedDate": "2020-06-23T17:31:24Z", "message": "Facethandler updates from code review"}, "afterCommit": {"oid": "caa910be93de142a0a6b90219878b298279edc91", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/caa910be93de142a0a6b90219878b298279edc91", "committedDate": "2020-06-23T17:33:11Z", "message": "Facethandler updates from code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "caa910be93de142a0a6b90219878b298279edc91", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/caa910be93de142a0a6b90219878b298279edc91", "committedDate": "2020-06-23T17:33:11Z", "message": "Facethandler updates from code review"}, "afterCommit": {"oid": "7870357b413527098add92103d099f6aebe6689d", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/7870357b413527098add92103d099f6aebe6689d", "committedDate": "2020-06-24T13:49:08Z", "message": "Facethandler updates from code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebfe3e0ba52dc569f5abb856b43f253132752e40", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/ebfe3e0ba52dc569f5abb856b43f253132752e40", "committedDate": "2020-06-24T15:49:44Z", "message": "Faceted Ingest & Search for Datawave\n\n* FacetHandler generates facets for data at ingest time.\n  * Adds 3 tables: datawave.facets, datawave.facetMetadata, datawave.facetHashes.\n  * Adds dependency on com.clearspring.analytics:stream for summarization and cardinality estimation.\n  * Ingest configurations that add the FacetHandler for tvmaze/myjson data\n* Adds FacetedQuery query logic and FacetedQueryPlanner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b210981d659a3cb56fe0ec67006f0c411105be2c", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/b210981d659a3cb56fe0ec67006f0c411105be2c", "committedDate": "2020-06-24T15:49:44Z", "message": "Table configuration plumbing for the query side"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5f05edda01bfffdb93c6f24c6661f8cc9e9eb14", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/e5f05edda01bfffdb93c6f24c6661f8cc9e9eb14", "committedDate": "2020-06-24T15:49:44Z", "message": "Facet query logic improvements, test and configuration updates (#793)\n\n* Override hardcoded references to facet metadata in `FacetedQueryPlanner`, `FacetCheck`, `FacetQueryPlanVisitor`.\n* Facet Table test related code added to `AccumuloSetupHelper`.\n* Removed `FacetedQueryPlanner` instantiation from `QueryLogicFactory.xml` is is not needed.\n* Added build properties for facet tables in `QueryLogicFactory.xml`.\n* `FacetedQueryLogicTest` now validates results, may be more work needed for the dynamic computation case.\n* Added field name to facet values so they can be distinguished when we're faceting multiple fields.\n* `DynamicFacetIterator` rebuilds fixed by having cardinality summaries include a max key.\n* Improvements to table value handing in `PrintUtility` test utility.\n* Fixes for clarity in `RebuildingScannerTestHelper`.\n* Misc. warning cleanups.\n* Changes in response to comments from code review."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6a09d2b8b8ecaf35e549b136b666927d9001738", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/c6a09d2b8b8ecaf35e549b136b666927d9001738", "committedDate": "2020-06-24T15:49:44Z", "message": "Facethandler updates from code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7870357b413527098add92103d099f6aebe6689d", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/7870357b413527098add92103d099f6aebe6689d", "committedDate": "2020-06-24T13:49:08Z", "message": "Facethandler updates from code review"}, "afterCommit": {"oid": "c6a09d2b8b8ecaf35e549b136b666927d9001738", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/c6a09d2b8b8ecaf35e549b136b666927d9001738", "committedDate": "2020-06-24T15:49:44Z", "message": "Facethandler updates from code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ef26f8d8853b93e011a3305afe4ed43cfba7f09a", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/ef26f8d8853b93e011a3305afe4ed43cfba7f09a", "committedDate": "2020-06-24T20:26:34Z", "message": "Update telemetry configuration to identify root cause of intermittent unit test failure"}, "afterCommit": {"oid": "611df878faeedadfccad3908c2913d1b9cd17e19", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/611df878faeedadfccad3908c2913d1b9cd17e19", "committedDate": "2020-06-25T14:13:07Z", "message": "Update telemetry configuration to identify root cause of intermittent unit test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9a1523286931076fa118be469709d0d2be6ae2b7", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/9a1523286931076fa118be469709d0d2be6ae2b7", "committedDate": "2020-06-26T17:27:00Z", "message": "Basic test for some approximate algorithms"}, "afterCommit": {"oid": "7474e7a09809dd5a4b185631c5edc805bb1763ca", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/7474e7a09809dd5a4b185631c5edc805bb1763ca", "committedDate": "2020-06-28T17:53:40Z", "message": "Fix for intermittent test failures + Probabilistic data structure test.\n\n- Cleaned up MergedReadAhead to avoid concurrency issues.\n- Added simple ApproximateAlgorithmsTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b755485746ce93df0ef893d2b7a39ecf4e75049", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/6b755485746ce93df0ef893d2b7a39ecf4e75049", "committedDate": "2020-06-30T18:31:52Z", "message": "Fix for intermittent test failures + Probabilistic data structure test.\n\n- Cleaned up MergedReadAhead to avoid concurrency issues.\n- Added simple ApproximateAlgorithmsTest"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7474e7a09809dd5a4b185631c5edc805bb1763ca", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/7474e7a09809dd5a4b185631c5edc805bb1763ca", "committedDate": "2020-06-28T17:53:40Z", "message": "Fix for intermittent test failures + Probabilistic data structure test.\n\n- Cleaned up MergedReadAhead to avoid concurrency issues.\n- Added simple ApproximateAlgorithmsTest"}, "afterCommit": {"oid": "6b755485746ce93df0ef893d2b7a39ecf4e75049", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/6b755485746ce93df0ef893d2b7a39ecf4e75049", "committedDate": "2020-06-30T18:31:52Z", "message": "Fix for intermittent test failures + Probabilistic data structure test.\n\n- Cleaned up MergedReadAhead to avoid concurrency issues.\n- Added simple ApproximateAlgorithmsTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93d2e839c4b907069eb22dd116afeb44c1cb4deb", "author": {"user": {"login": "drewfarris", "name": "Drew Farris"}}, "url": "https://github.com/NationalSecurityAgency/datawave/commit/93d2e839c4b907069eb22dd116afeb44c1cb4deb", "committedDate": "2020-07-01T14:29:40Z", "message": "Resolved concurrency issue in MergedReadAhead"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzODkxMDAx", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#pullrequestreview-443891001", "createdAt": "2020-07-07T13:30:18Z", "commit": {"oid": "93d2e839c4b907069eb22dd116afeb44c1cb4deb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1183, "cost": 1, "resetAt": "2021-11-01T15:33:45Z"}}}