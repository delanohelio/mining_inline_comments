{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0ODIzNDk1", "number": 793, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoyNjoyM1rOD18Wtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODo0NTo0NVrOEGELpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODg5OTc1OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoyNjoyM1rOGLn_Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMDoyNjoyM1rOGLn_Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg0MjY4Mw==", "bodyText": "Bad import? Probably.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r414842683", "createdAt": "2020-04-24T20:26:23Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "diffHunk": "@@ -0,0 +1,86 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import jj2000.j2k.util.FacilityManager;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTg1NDczOnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/test/java/datawave/ingest/mapreduce/handler/facet/FacetHandlerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMjozODoxM1rOGM_7rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNjo1OToxOFrOGZd5mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4MzU2Nw==", "bodyText": "This error message seems like it was copied from elsewhere in the test & should probably say that there was an error in processing", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r416283567", "createdAt": "2020-04-28T02:38:13Z", "author": {"login": "billoley"}, "path": "warehouse/ingest-core/src/test/java/datawave/ingest/mapreduce/handler/facet/FacetHandlerTest.java", "diffHunk": "@@ -0,0 +1,595 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.LinkedListMultimap;\n+import com.google.common.collect.Multimap;\n+import datawave.data.hash.UID;\n+import datawave.data.type.LcNoDiacriticsType;\n+import datawave.ingest.config.RawRecordContainerImpl;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.NormalizedFieldAndValue;\n+import datawave.ingest.data.config.ingest.BaseIngestHelper;\n+import datawave.ingest.data.config.ingest.ContentBaseIngestHelper;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestContentBaseIngestHelper;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestContentIndexingColumnBasedHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestEventRecordReader;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.AbstractContextWriter;\n+import datawave.ingest.test.StandaloneStatusReporter;\n+import datawave.ingest.test.StandaloneTaskAttemptContext;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntOpenHashMap;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.accumulo.core.util.Pair;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.log4j.Logger;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.junit.Assert.*;\n+\n+public class FacetHandlerTest {\n+    \n+    private static final String TEST_TYPE = \"test\";\n+    private static final UID TEST_UID = UID.builder().newId();\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandlerTest.class);\n+    public static final String DATAWAVE_FACETS = \"datawave.facets\";\n+    public static final String DATAWAVE_FACET_METADATA = \"datawave.facetMetadata\";\n+    public static final String DATAWAVE_FACET_HASHES = \"datawave.facetHashes\";\n+    \n+    private TaskAttemptContext ctx = null;\n+    \n+    private RawRecordContainer event = EasyMock.createMock(RawRecordContainer.class);\n+    private ContentBaseIngestHelper helper;\n+    private ColumnVisibility colVis;\n+    \n+    @Before\n+    public void setUp() throws Exception {\n+        Configuration conf = new Configuration();\n+        conf.addResource(\"config/all-config.xml\");\n+        conf.addResource(\"config/facet-config.xml\");\n+        \n+        ctx = new TaskAttemptContextImpl(conf, new org.apache.hadoop.mapred.TaskAttemptID());\n+        ctx.getConfiguration().setInt(ContentIndexingColumnBasedHandler.NUM_SHARDS, 1);\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_TNAME, \"shard\");\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_GIDX_TNAME, \"shardIndex\");\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_GRIDX_TNAME, \"shardIndex\");\n+        \n+        ctx.getConfiguration().set(\"data.name\", TEST_TYPE);\n+        ctx.getConfiguration().set(\"test.data.auth.id.mode\", \"NEVER\");\n+        ctx.getConfiguration().set(\"test\" + BaseIngestHelper.DEFAULT_TYPE, LcNoDiacriticsType.class.getName());\n+        \n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.HANDLER_CLASSES, TestContentIndexingColumnBasedHandler.class.getName());\n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.RAW_READER, TestEventRecordReader.class.getName());\n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.INGEST_HELPER, TestContentBaseIngestHelper.class.getName());\n+        ctx.getConfiguration().set(TypeRegistry.EXCLUDED_HANDLER_CLASSES, \"FAKE_HANDLER_CLASS\"); // it will die if this field is not faked\n+        \n+        colVis = new ColumnVisibility(\"\");\n+        helper = new TestContentBaseIngestHelper();\n+        \n+        TypeRegistry.reset();\n+        TypeRegistry.getInstance(ctx.getConfiguration());\n+        \n+        setupMocks();\n+        \n+        configureFacets(ctx.getConfiguration());\n+        \n+        // log.setLevel(Level.DEBUG);\n+    }\n+    \n+    private void setupMocks() {\n+        try {\n+            long timestamp = Instant.parse(\"2020-04-01T08:00:00.0z\").toEpochMilli();\n+            EasyMock.expect(event.getVisibility()).andReturn(colVis).anyTimes();\n+            EasyMock.expect(event.getDataType()).andReturn(TypeRegistry.getType(TEST_TYPE)).anyTimes();\n+            EasyMock.expect(event.getId()).andReturn(TEST_UID).anyTimes();\n+            EasyMock.expect(event.getDate()).andReturn(timestamp).anyTimes();\n+            EasyMock.replay(event);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+    \n+    public void configureFacets(Configuration conf) {\n+        conf.set(\"test.facet.category.name.manufacturer\", \"MAKE/STYLE,MODEL,COLOR\");\n+        conf.set(\"test.facet.category.name.style\", \"STYLE/MANUFACTURER,MODEL,COLOR\");\n+        conf.set(\"test.facet.category.name.color\", \"COLOR/MANUFACTURER,MODEL,STYLE\");\n+    }\n+    \n+    @Test\n+    public void testSingleEvent() {\n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        helper.setup(ctx.getConfiguration());\n+        \n+        Multimap<String,NormalizedContentInterface> fields = TestData.getDataItem(1);\n+        setupTaskAttemptContext();\n+        processEvent(event, fields, handler);\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        Set<String> expectedFacets = Stream.of(TestData.getExpectedFacetData(1)).collect(Collectors.toSet());\n+        Set<String> expectedFacetMetadata = Stream.of(TestData.getExpectedFacetMetadataData(1)).collect(Collectors.toSet());\n+        \n+        evaluateSingleEventResults(keysByTable, expectedFacets, expectedFacetMetadata);\n+    }\n+    \n+    @Test\n+    public void testMultipleEvents() {\n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        helper.setup(ctx.getConfiguration());\n+        \n+        Collection<Multimap<String,NormalizedContentInterface>> items = TestData.getDataItems();\n+        int size = items.size();\n+        \n+        setupTaskAttemptContext();\n+        items.forEach(f -> processEvent(event, f, handler));\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        Object2IntMap<String> expectedFacetKeyCounts = TestData.getExpectedFacetCounts();\n+        evaluateMultipleEventResults(keysByTable, size, expectedFacetKeyCounts);\n+    }\n+    \n+    @Test\n+    public void testMultiValueHashing() {\n+        \n+        ctx.getConfiguration().set(\"test.facet.hash.threshold\", \"2\");\n+        \n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        Collection<Multimap<String,NormalizedContentInterface>> items = TestData.generateMultivaluedColorData();\n+        \n+        setupTaskAttemptContext();\n+        items.forEach(f -> processEvent(event, f, handler));\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        evaluateMultiValueResults(keysByTable, TestData.getExpectedFacetHashes());\n+    }\n+    \n+    StandaloneTaskAttemptContext<Text,RawRecordContainerImpl,BulkIngestKey,Value> sctx;\n+    CachingContextWriter contextWriter;\n+    \n+    private void setupTaskAttemptContext() {\n+        sctx = new StandaloneTaskAttemptContext<>(ctx.getConfiguration(), new StandaloneStatusReporter());\n+        contextWriter = new CachingContextWriter();\n+        try {\n+            contextWriter.setup(sctx.getConfiguration(), false);\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Error setting up context writer\", e);\n+        }\n+    }\n+    \n+    /**\n+     * Process a single event using the handler\n+     *\n+     * @param event\n+     *            the event we're processing\n+     * @param eventFields\n+     *            the fields from the event to process\n+     * @param handler\n+     *            the handler to do the procesing\n+     */\n+    private void processEvent(RawRecordContainer event, Multimap<String,NormalizedContentInterface> eventFields,\n+                    ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler) {\n+        \n+        assertNotNull(\"Event was null\", event);\n+        \n+        try {\n+            handler.process(null, event, eventFields, sctx, contextWriter);\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Error setting up context writer\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1NzQ2NA==", "bodyText": "Resolved", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r429357464", "createdAt": "2020-05-22T16:59:18Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/test/java/datawave/ingest/mapreduce/handler/facet/FacetHandlerTest.java", "diffHunk": "@@ -0,0 +1,595 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.LinkedListMultimap;\n+import com.google.common.collect.Multimap;\n+import datawave.data.hash.UID;\n+import datawave.data.type.LcNoDiacriticsType;\n+import datawave.ingest.config.RawRecordContainerImpl;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.NormalizedFieldAndValue;\n+import datawave.ingest.data.config.ingest.BaseIngestHelper;\n+import datawave.ingest.data.config.ingest.ContentBaseIngestHelper;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestContentBaseIngestHelper;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestContentIndexingColumnBasedHandler;\n+import datawave.ingest.mapreduce.handler.tokenize.ContentIndexingColumnBasedHandlerTest.TestEventRecordReader;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.AbstractContextWriter;\n+import datawave.ingest.test.StandaloneStatusReporter;\n+import datawave.ingest.test.StandaloneTaskAttemptContext;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntOpenHashMap;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.accumulo.core.util.Pair;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.log4j.Logger;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.junit.Assert.*;\n+\n+public class FacetHandlerTest {\n+    \n+    private static final String TEST_TYPE = \"test\";\n+    private static final UID TEST_UID = UID.builder().newId();\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandlerTest.class);\n+    public static final String DATAWAVE_FACETS = \"datawave.facets\";\n+    public static final String DATAWAVE_FACET_METADATA = \"datawave.facetMetadata\";\n+    public static final String DATAWAVE_FACET_HASHES = \"datawave.facetHashes\";\n+    \n+    private TaskAttemptContext ctx = null;\n+    \n+    private RawRecordContainer event = EasyMock.createMock(RawRecordContainer.class);\n+    private ContentBaseIngestHelper helper;\n+    private ColumnVisibility colVis;\n+    \n+    @Before\n+    public void setUp() throws Exception {\n+        Configuration conf = new Configuration();\n+        conf.addResource(\"config/all-config.xml\");\n+        conf.addResource(\"config/facet-config.xml\");\n+        \n+        ctx = new TaskAttemptContextImpl(conf, new org.apache.hadoop.mapred.TaskAttemptID());\n+        ctx.getConfiguration().setInt(ContentIndexingColumnBasedHandler.NUM_SHARDS, 1);\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_TNAME, \"shard\");\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_GIDX_TNAME, \"shardIndex\");\n+        ctx.getConfiguration().set(ContentIndexingColumnBasedHandler.SHARD_GRIDX_TNAME, \"shardIndex\");\n+        \n+        ctx.getConfiguration().set(\"data.name\", TEST_TYPE);\n+        ctx.getConfiguration().set(\"test.data.auth.id.mode\", \"NEVER\");\n+        ctx.getConfiguration().set(\"test\" + BaseIngestHelper.DEFAULT_TYPE, LcNoDiacriticsType.class.getName());\n+        \n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.HANDLER_CLASSES, TestContentIndexingColumnBasedHandler.class.getName());\n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.RAW_READER, TestEventRecordReader.class.getName());\n+        ctx.getConfiguration().set(\"test\" + TypeRegistry.INGEST_HELPER, TestContentBaseIngestHelper.class.getName());\n+        ctx.getConfiguration().set(TypeRegistry.EXCLUDED_HANDLER_CLASSES, \"FAKE_HANDLER_CLASS\"); // it will die if this field is not faked\n+        \n+        colVis = new ColumnVisibility(\"\");\n+        helper = new TestContentBaseIngestHelper();\n+        \n+        TypeRegistry.reset();\n+        TypeRegistry.getInstance(ctx.getConfiguration());\n+        \n+        setupMocks();\n+        \n+        configureFacets(ctx.getConfiguration());\n+        \n+        // log.setLevel(Level.DEBUG);\n+    }\n+    \n+    private void setupMocks() {\n+        try {\n+            long timestamp = Instant.parse(\"2020-04-01T08:00:00.0z\").toEpochMilli();\n+            EasyMock.expect(event.getVisibility()).andReturn(colVis).anyTimes();\n+            EasyMock.expect(event.getDataType()).andReturn(TypeRegistry.getType(TEST_TYPE)).anyTimes();\n+            EasyMock.expect(event.getId()).andReturn(TEST_UID).anyTimes();\n+            EasyMock.expect(event.getDate()).andReturn(timestamp).anyTimes();\n+            EasyMock.replay(event);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+    \n+    public void configureFacets(Configuration conf) {\n+        conf.set(\"test.facet.category.name.manufacturer\", \"MAKE/STYLE,MODEL,COLOR\");\n+        conf.set(\"test.facet.category.name.style\", \"STYLE/MANUFACTURER,MODEL,COLOR\");\n+        conf.set(\"test.facet.category.name.color\", \"COLOR/MANUFACTURER,MODEL,STYLE\");\n+    }\n+    \n+    @Test\n+    public void testSingleEvent() {\n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        helper.setup(ctx.getConfiguration());\n+        \n+        Multimap<String,NormalizedContentInterface> fields = TestData.getDataItem(1);\n+        setupTaskAttemptContext();\n+        processEvent(event, fields, handler);\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        Set<String> expectedFacets = Stream.of(TestData.getExpectedFacetData(1)).collect(Collectors.toSet());\n+        Set<String> expectedFacetMetadata = Stream.of(TestData.getExpectedFacetMetadataData(1)).collect(Collectors.toSet());\n+        \n+        evaluateSingleEventResults(keysByTable, expectedFacets, expectedFacetMetadata);\n+    }\n+    \n+    @Test\n+    public void testMultipleEvents() {\n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        helper.setup(ctx.getConfiguration());\n+        \n+        Collection<Multimap<String,NormalizedContentInterface>> items = TestData.getDataItems();\n+        int size = items.size();\n+        \n+        setupTaskAttemptContext();\n+        items.forEach(f -> processEvent(event, f, handler));\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        Object2IntMap<String> expectedFacetKeyCounts = TestData.getExpectedFacetCounts();\n+        evaluateMultipleEventResults(keysByTable, size, expectedFacetKeyCounts);\n+    }\n+    \n+    @Test\n+    public void testMultiValueHashing() {\n+        \n+        ctx.getConfiguration().set(\"test.facet.hash.threshold\", \"2\");\n+        \n+        ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler = new FacetHandler<>();\n+        handler.setup(ctx);\n+        \n+        Collection<Multimap<String,NormalizedContentInterface>> items = TestData.generateMultivaluedColorData();\n+        \n+        setupTaskAttemptContext();\n+        items.forEach(f -> processEvent(event, f, handler));\n+        Multimap<String,FacetResult> keysByTable = collectResults();\n+        \n+        evaluateMultiValueResults(keysByTable, TestData.getExpectedFacetHashes());\n+    }\n+    \n+    StandaloneTaskAttemptContext<Text,RawRecordContainerImpl,BulkIngestKey,Value> sctx;\n+    CachingContextWriter contextWriter;\n+    \n+    private void setupTaskAttemptContext() {\n+        sctx = new StandaloneTaskAttemptContext<>(ctx.getConfiguration(), new StandaloneStatusReporter());\n+        contextWriter = new CachingContextWriter();\n+        try {\n+            contextWriter.setup(sctx.getConfiguration(), false);\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Error setting up context writer\", e);\n+        }\n+    }\n+    \n+    /**\n+     * Process a single event using the handler\n+     *\n+     * @param event\n+     *            the event we're processing\n+     * @param eventFields\n+     *            the fields from the event to process\n+     * @param handler\n+     *            the handler to do the procesing\n+     */\n+    private void processEvent(RawRecordContainer event, Multimap<String,NormalizedContentInterface> eventFields,\n+                    ExtendedDataTypeHandler<Text,BulkIngestKey,Value> handler) {\n+        \n+        assertNotNull(\"Event was null\", event);\n+        \n+        try {\n+            handler.process(null, event, eventFields, sctx, contextWriter);\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Error setting up context writer\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4MzU2Nw=="}, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTg1NTAxOnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDowNzozMVrOGN4HzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDowNzozMVrOGN4HzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzIwNDE3Mg==", "bodyText": "Remove commented code", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417204172", "createdAt": "2020-04-29T10:07:31Z", "author": {"login": "apmoriarty"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTg2NjQ0OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDoxMTowMFrOGN4Owg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxMDoxMTowMFrOGN4Owg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzIwNTk1NA==", "bodyText": "Extra TODO comment?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417205954", "createdAt": "2020-04-29T10:11:00Z", "author": {"login": "apmoriarty"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 324}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5OTAzMDcwOnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMTo1NTowNFrOGOXGlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNzowMjowOVrOGZd-Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxMTc2Ng==", "bodyText": "Are you avoiding using the sparse setting for HLL++ because of some of the issues with size in certain cases when serializing it to disk?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417711766", "createdAt": "2020-04-30T01:55:04Z", "author": {"login": "cawaring"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */\n+    @Override\n+    public FacetValue estimate(RawRecordContainer input) {\n+        // precision value: 10, sparse set disabled.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 327}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1ODYwNg==", "bodyText": "I need to take a close look at this - the sparse setting seems a little wasteful, I'm not certain when the sparse setting becomes more efficient than the alternative (?dense?) setting.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r429358606", "createdAt": "2020-05-22T17:02:09Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */\n+    @Override\n+    public FacetValue estimate(RawRecordContainer input) {\n+        // precision value: 10, sparse set disabled.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxMTc2Ng=="}, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 327}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5OTA1ODA4OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxMjowNVrOGOXWgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNzowNDoxN1rOGZeBfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxNTg0Mg==", "bodyText": "I might not be reading this correctly, but using this constructor we're creating a CMS with depth of 10 and width of 1.  We should probable make these parameters configurable, likewise for hLL++.  That way one could set the parameters based on the error and probability they want to support.  Could add some bounds checking to make it reasonable.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417715842", "createdAt": "2020-04-30T02:12:05Z", "author": {"login": "cawaring"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */\n+    @Override\n+    public FacetValue estimate(RawRecordContainer input) {\n+        // precision value: 10, sparse set disabled.\n+        HyperLogLogPlus card = new HyperLogLogPlus(10);\n+        card.offer(input.getId().toString());\n+        \n+        return new FacetValue(card, new CountMinSketch(10, 1, 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 331}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTQ4Ng==", "bodyText": "Good point. I'll work on the plumbing for this. I'm wondering if we might want to do this on a per-field basis or along some other dimension than \"It works this way for everything\". I'm also wondering if these parameters get encoded in the serialized object so that we can track this for individual estimates.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r429359486", "createdAt": "2020-05-22T17:04:17Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,366 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    // protected Multimap<String,NormalizedContentInterface> fields = HashMultimap.create();\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))\n+                        continue;\n+                    \n+                    for (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n+                        if (HashTableFunction.isReduced(facetTypes)) {\n+                            myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n+                        }\n+                        \n+                        Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), facetTypes.getIndexedFieldValue(), event.getDataType());\n+                        Key result = new Key(row, myCf, dateColumnQualifier, cv, event.getDate());\n+                        results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+                        \n+                        countWritten++;\n+                    }\n+                    \n+                    myCf = generatedCf;\n+                }\n+                \n+                Text row = createFieldValuePair(pivotTypes.getIndexedFieldValue(), pivotTypes.getIndexedFieldValue(), event.getDataType());\n+                Key result = new Key(row, reflexiveCf, dateColumnQualifier, cv, event.getDate());\n+                results.put(new BulkIngestKey(facetTableName, result), sharedValue);\n+            }\n+        }\n+        \n+        for (Map.Entry<String,String> pivot : pivotMap.entries()) {\n+            Key result = new Key(new Text(pivot.getKey() + NULL + pivot.getValue()), PV);\n+            results.put(new BulkIngestKey(facetMetadataTableName, result), EMPTY_VALUE);\n+            countWritten++;\n+        }\n+        contextWriter.write(results, context);\n+        return countWritten;\n+    }\n+    \n+    private Multimap<String,NormalizedContentInterface> hashEventFields(Multimap<String,NormalizedContentInterface> fields,\n+                    HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func) {\n+        Multimap<String,NormalizedContentInterface> eventFields = HashMultimap.create();\n+        for (Map.Entry<String,Collection<NormalizedContentInterface>> entry : fields.asMap().entrySet()) {\n+            Collection<NormalizedContentInterface> coll = func.apply(entry.getValue());\n+            if (coll != null && !coll.isEmpty()) {\n+                eventFields.putAll(entry.getKey(), coll);\n+            }\n+        }\n+        return eventFields;\n+    }\n+    \n+    /**\n+     * Create the column qualifier that includes pivotFieldValue, facetFieldValue and datatype.\n+     * \n+     * @param pivotFieldValue\n+     * @param facetFieldValue\n+     * @param dataType\n+     * @return\n+     */\n+    protected Text createFieldValuePair(String pivotFieldValue, String facetFieldValue, Type dataType) {\n+        return new Text(pivotFieldValue + NULL + facetFieldValue + NULL + dataType.typeName());\n+    }\n+    \n+    /**\n+     * Create the column family consisting of pivotFieldName and facetFieldName\n+     * \n+     * @param pivotFieldName\n+     * @param facetFieldName\n+     * @return\n+     */\n+    protected Text createColumnFamily(String pivotFieldName, String facetFieldName) {\n+        return new Text(pivotFieldName + NULL + facetFieldName);\n+    }\n+    \n+    /** TODO */\n+    @Override\n+    public FacetValue estimate(RawRecordContainer input) {\n+        // precision value: 10, sparse set disabled.\n+        HyperLogLogPlus card = new HyperLogLogPlus(10);\n+        card.offer(input.getId().toString());\n+        \n+        return new FacetValue(card, new CountMinSketch(10, 1, 1));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxNTg0Mg=="}, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 331}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5OTA2Mjc2OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/test/resources/config/facet-config.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxNDo0NlrOGOXZMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMjoxNDo0NlrOGOXZMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzcxNjUyOA==", "bodyText": "We may want to include some configurations for hll++ and cms here.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r417716528", "createdAt": "2020-04-30T02:14:46Z", "author": {"login": "cawaring"}, "path": "warehouse/ingest-core/src/test/resources/config/facet-config.xml", "diffHunk": "@@ -0,0 +1,28 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n+<?xml-stylesheet type=\"test/xsl\" href=\"configuration.xsl\"?>\n+<configuration>\n+    <property>\n+        <name>facet.table.name</name>\n+        <value>datawave.facets</value>\n+    </property>\n+    <property>\n+        <name>facet.table.loader.priority</name>\n+        <value>40</value>\n+    </property>\n+    <property>\n+        <name>facet.metadata.table.name</name>\n+        <value>datawave.facetMetadata</value>\n+    </property>\n+    <property>\n+        <name>facet.metadata.table.loader.priority</name>\n+        <value>40</value>\n+    </property>\n+    <property>\n+        <name>facet.hash.table.name</name>\n+        <value>datawave.facetHashes</value>\n+    </property>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8a54b54c4ca4167d9c3b693ef9682c7210c916e"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzgxNTI5OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODowOToyM1rOGknRuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzozODoyMVrOGnzuXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NTQzMg==", "bodyText": "Just a nit, but '/' suggests a path of some sort which is not the case here.  This is more like a definition so perhaps colon, equals or some sort of bracket may be better.  Again, just a nit.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441045432", "createdAt": "2020-06-16T18:09:23Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU4NzcyMw==", "bodyText": "@ivakegg Thanks, I understand where you're coming from. It's a nit, but it's worth doing the work up-front to get it right.\nIn use, we currently have configuration property values like:\nNETWORK_NAME/GENRES,EMBEDDED_CAST_PERSON_GENDER,RATING_AVERAGE .\nThinking about other delimiters that make sense in this context, ; may be the best bet from a semantic perspective:\nNETWORK_NAME;GENRES,EMBEDDED_CAST_PERSON_GENDER,RATING_AVERAGE\n(I considered '|', '#', and ';', although '|' might be better visually it is also semantically overloaded)\nWhat do you think? The changes I need to make to address this are distributed across code, tests and configuration, so I want to make sure I get something we're satisfied with the first time.\nI won't get started about embedding structured data in a property value in that it forces you do to special parsing on the value itself - considering alternatives I think it's the lesser of evils in this particular case.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441587723", "createdAt": "2020-06-17T14:26:47Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NTQzMg=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDE4ODU2OA==", "bodyText": "Your point about embedding structured data in a value is a very good one.  I have been a huge contributor to this evil practice unfortunately but we all have to learn somehow.  If we are not going to use a separate property, then I vote for a semicolon I guess.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444188568", "createdAt": "2020-06-23T12:36:12Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NTQzMg=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM5NTEwMQ==", "bodyText": "Done. This is now a semicolon.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444395101", "createdAt": "2020-06-23T17:38:21Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NTQzMg=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzgzMDQ3OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMzo1NFrOGknbhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzozODo0NFrOGnzvRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0Nzk0Mg==", "bodyText": "use ShardIdFactory.getDateString(shardId)", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441047942", "createdAt": "2020-06-16T18:13:54Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDE5MDM1Mw==", "bodyText": "final String shardId = ShardIdFactory.getDateString(shardId);", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444190353", "createdAt": "2020-06-23T12:39:06Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0Nzk0Mg=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM5NTMzNQ==", "bodyText": "Done. Thanks for the pointer to how to do this correctly.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444395335", "createdAt": "2020-06-23T17:38:44Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0Nzk0Mg=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 219}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Nzk1MDIzOnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODo0NDoyOFrOGkooCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzozOToxN1rOGnzwjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2NzUzMA==", "bodyText": "Shouldn't this check be moved outside of this for loop but inside the containing loop (i.e move to line 247)?  Alternatively the eventFields could be prefiltered to not include any \"reduced\" members.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441067530", "createdAt": "2020-06-16T18:44:28Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU5ODA2NQ==", "bodyText": "Yes HashTableFunction.isReduced(pivotTypes) can be moved out.\nFor what it's worth, we do use the reduced members in eventFields for the inner loop that iterates over facetFieldName members when generating facets, so I can't remove all reduced members from eventFields (see snippet below). I suppose I could create separate collections of pivot and facet fields, but is that any better?\nfor (NormalizedContentInterface facetTypes : eventFields.get(facetFieldName)) {\n  if (HashTableFunction.isReduced(facetTypes)) {\n     myCf.append(HashTableFunction.FIELD_APPEND_BYTES, 0, HashTableFunction.FIELD_APPEND_BYTES.length);\n  }\n [...]\n}", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441598065", "createdAt": "2020-06-17T14:40:30Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2NzUzMA=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDE4OTIxOA==", "bodyText": "your choice.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444189218", "createdAt": "2020-06-23T12:37:17Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2NzUzMA=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM5NTY2Mg==", "bodyText": "Done. I just moved HashTableFunction.isReduced(pivotTypes) outside of the loop it was in now.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444395662", "createdAt": "2020-06-23T17:39:17Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -0,0 +1,363 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multimap;\n+import datawave.ingest.data.RawRecordContainer;\n+import datawave.ingest.data.Type;\n+import datawave.ingest.data.TypeRegistry;\n+import datawave.ingest.data.config.ConfigurationHelper;\n+import datawave.ingest.data.config.DataTypeHelper.Properties;\n+import datawave.ingest.data.config.NormalizedContentInterface;\n+import datawave.ingest.data.config.ingest.IngestHelperInterface;\n+import datawave.ingest.mapreduce.handler.ExtendedDataTypeHandler;\n+import datawave.ingest.mapreduce.handler.shard.ShardIdFactory;\n+import datawave.ingest.mapreduce.job.BulkIngestKey;\n+import datawave.ingest.mapreduce.job.writer.ContextWriter;\n+import datawave.ingest.metadata.RawRecordMetadata;\n+import datawave.marking.MarkingFunctions;\n+import datawave.util.StringUtils;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class FacetHandler<KEYIN,KEYOUT,VALUEOUT> implements ExtendedDataTypeHandler<KEYIN,KEYOUT,VALUEOUT>, FacetedEstimator<RawRecordContainer> {\n+    \n+    private static final Logger log = Logger.getLogger(FacetHandler.class);\n+    \n+    /* Global configuration properties */\n+    \n+    public static final String FACET_TABLE_NAME = \"facet.table.name\";\n+    public static final String FACET_LPRIORITY = \"facet.table.loader.priority\";\n+    \n+    public static final String FACET_METADATA_TABLE_NAME = \"facet.metadata.table.name\";\n+    public static final String FACET_METADATA_LPRIORITY = \"facet.metadata.table.loader.priority\";\n+    \n+    public static final String FACET_HASH_TABLE_NAME = \"facet.hash.table.name\";\n+    public static final String FACET_HASH_TABLE_LPRIORITY = \"facet.hash.table.loader.priority\";\n+    \n+    /* Per-datatype configuration properties */\n+    \n+    public static final String FACET_HASH_THRESHOLD = \".facet.hash.threshold\";\n+    \n+    public static final String FACET_CATEGORY_DELIMITER = \".facet.category.delimiter\";\n+    public static final String FACET_FIELD_PREDICATE_CLASS = \".facet.field.predicate.class\";\n+    \n+    public static final String FACET_CATEGORY_PREFIX_REGEX = \"\\\\.facet\\\\.category\\\\.name\\\\..*\";\n+    \n+    private static final Text PV = new Text(\"pv\");\n+    private static final String NULL = \"\\0\";\n+    private static final Value EMPTY_VALUE = new Value(new byte[] {});\n+    \n+    /* Global configuration fields */\n+    \n+    protected Text facetTableName;\n+    protected Text facetMetadataTableName;\n+    protected Text facetHashTableName;\n+    \n+    /* Per-datatype configuration fields */\n+    \n+    protected int facetHashThreshold;\n+    protected String categoryDelimiter = \"/\";\n+    \n+    /* Instance variables */\n+    \n+    protected MarkingFunctions markingFunctions;\n+    protected ShardIdFactory shardIdFactory;\n+    protected TaskAttemptContext taskAttemptContext;\n+    \n+    protected Predicate<String> fieldFilter = null;\n+    protected Multimap<String,String> pivotMap;\n+    \n+    @Override\n+    public void setup(TaskAttemptContext context) {\n+        markingFunctions = MarkingFunctions.Factory.createMarkingFunctions();\n+        \n+        taskAttemptContext = context;\n+        \n+        Configuration conf = context.getConfiguration();\n+        \n+        final String t = ConfigurationHelper.isNull(conf, Properties.DATA_NAME, String.class);\n+        TypeRegistry.getInstance(conf);\n+        Type type = TypeRegistry.getType(t);\n+        \n+        categoryDelimiter = conf.get(type.typeName() + FACET_CATEGORY_DELIMITER, categoryDelimiter);\n+        \n+        Map<String,String> categories = conf.getValByRegex(type.typeName() + FACET_CATEGORY_PREFIX_REGEX);\n+        \n+        pivotMap = HashMultimap.create();\n+        \n+        if (null != categories) {\n+            for (Map.Entry<String,String> category : categories.entrySet()) {\n+                final String fields = category.getValue();\n+                Preconditions.checkNotNull(fields);\n+                final String[] fieldArray = StringUtils.split(fields, categoryDelimiter.charAt(0));\n+                Preconditions.checkArgument(fieldArray.length == 2);\n+                final String pivot = fieldArray[0];\n+                final String[] facets = StringUtils.split(fieldArray[1], ',');\n+                pivotMap.putAll(pivot, ImmutableList.copyOf(facets));\n+            }\n+        } else {\n+            throw new IllegalStateException(\"Categories must be specified\");\n+        }\n+        \n+        String predClazzStr = conf.get(FACET_FIELD_PREDICATE_CLASS);\n+        if (null != predClazzStr) {\n+            try {\n+                // Will throw RuntimeException if class can't be coerced into Predicate<String>\n+                @SuppressWarnings(\"unchecked\")\n+                Class<Predicate<String>> projClazz = (Class<Predicate<String>>) Class.forName(predClazzStr).asSubclass(Predicate.class);\n+                fieldFilter = projClazz.newInstance();\n+            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+        \n+        shardIdFactory = new ShardIdFactory(conf);\n+        facetTableName = new Text(ConfigurationHelper.isNull(conf, FACET_TABLE_NAME, String.class));\n+        facetMetadataTableName = new Text(conf.get(FACET_METADATA_TABLE_NAME, facetTableName.toString() + \"Metadata\"));\n+        facetHashTableName = new Text(conf.get(FACET_HASH_TABLE_NAME, facetTableName.toString() + \"Hash\"));\n+        facetHashThreshold = conf.getInt(type.typeName() + FACET_HASH_THRESHOLD, 20);\n+    }\n+    \n+    @Override\n+    public String[] getTableNames(Configuration conf) {\n+        final List<String> tableNames = new ArrayList<>();\n+        \n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            tableNames.add(tableName);\n+        \n+        return tableNames.toArray(new String[0]);\n+    }\n+    \n+    @Override\n+    public int[] getTableLoaderPriorities(Configuration conf) {\n+        int[] priorities = new int[2];\n+        int index = 0;\n+        String tableName = conf.get(FACET_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_METADATA_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_METADATA_LPRIORITY, 40);\n+        \n+        tableName = conf.get(FACET_HASH_TABLE_NAME, null);\n+        if (null != tableName)\n+            priorities[index++] = conf.getInt(FACET_HASH_TABLE_LPRIORITY, 40);\n+        \n+        if (index != priorities.length) {\n+            return Arrays.copyOf(priorities, index);\n+        } else {\n+            return priorities;\n+        }\n+    }\n+    \n+    @Override\n+    public Multimap<BulkIngestKey,Value> processBulk(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    StatusReporter reporter) {\n+        throw new UnsupportedOperationException(\"processBulk is not supported, please use process\");\n+    }\n+    \n+    @Override\n+    public IngestHelperInterface getHelper(Type datatype) {\n+        return datatype.getIngestHelper(this.taskAttemptContext.getConfiguration());\n+    }\n+    \n+    @Override\n+    public void close(TaskAttemptContext context) {\n+        /* no-op */\n+    }\n+    \n+    @Override\n+    public RawRecordMetadata getMetadata() {\n+        return null;\n+    }\n+    \n+    protected byte[] flatten(ColumnVisibility vis) {\n+        return markingFunctions == null ? vis.flatten() : markingFunctions.flatten(vis);\n+    }\n+    \n+    @Override\n+    public long process(KEYIN key, RawRecordContainer event, Multimap<String,NormalizedContentInterface> fields,\n+                    TaskInputOutputContext<KEYIN,? extends RawRecordContainer,KEYOUT,VALUEOUT> context, ContextWriter<KEYOUT,VALUEOUT> contextWriter)\n+                    throws IOException, InterruptedException {\n+        \n+        final String shardId = shardIdFactory.getShardId(event).substring(0, 8);\n+        Text dateColumnQualifier = new Text(shardId); // TODO: Makes an assumption about the structure of the shardId.\n+        \n+        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        \n+        Text cv = new Text(flatten(event.getVisibility()));\n+        \n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n+                        event.getDate());\n+        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        \n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n+        if (fieldFilter != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        }\n+        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        List<Set<String>> keySetList = Lists.newArrayList();\n+        keySetList.add(keySet);\n+        keySetList.add(keySet);\n+        \n+        long countWritten = 0;\n+        \n+        Value sharedValue = new Value(cardinality.getBytes());\n+        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        \n+        for (String pivotFieldName : pivotMap.keySet()) {\n+            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n+                for (String facetFieldName : pivotMap.get(pivotFieldName)) {\n+                    if (pivotFieldName.equals(facetFieldName))\n+                        continue;\n+                    \n+                    Text generatedCf = createColumnFamily(pivotFieldName, facetFieldName);\n+                    Text myCf = generatedCf;\n+                    \n+                    if (HashTableFunction.isReduced(pivotTypes))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2NzUzMA=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 254}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Nzk1NDMwOnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODo0NTo0NVrOGkoqwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMjozNzo0OVrOGnnLog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2ODIyNA==", "bodyText": "yup, missing that", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441068224", "createdAt": "2020-06-16T18:45:45Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import org.apache.accumulo.core.data.Value;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public class FacetValue extends Value {\n+    private HyperLogLogPlus cardinalityEstimate;\n+    private CountMinSketch frequencyEstimate;\n+    protected final AtomicBoolean written = new AtomicBoolean(false);\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate, CountMinSketch frequencyEstimate) {\n+        this.cardinalityEstimate = cardinalityEstimate;\n+        this.frequencyEstimate = frequencyEstimate;\n+    }\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate) {\n+        this(cardinalityEstimate, null);\n+    }\n+    \n+    protected FacetValue() {\n+        \n+    }\n+    \n+    public static FacetValue buildFrom(final DataInput in) throws IOException {\n+        FacetValue value = new FacetValue();\n+        value.readFields(in);\n+        return value;\n+    }\n+    \n+    public byte[] get() {\n+        try {\n+            ensureWritten();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        return super.get();\n+    }\n+    \n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+        int cardSize = in.readInt();\n+        byte[] cardinality = new byte[cardSize];\n+        in.readFully(cardinality);\n+        \n+        // TODO: frequencyEstimate", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYwNDg2Ng==", "bodyText": "Strangely, FacetValue is never used in this code.  FacetHandler.estimate(..)  is the only place it's created. Instead of the FacetValue a Simpler Value is used for the facets.\nBottom line: I need to do more digging to uncover the original intent here, but I'm hoping that might not prevent merging the current implementation", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r441604866", "createdAt": "2020-06-17T14:49:19Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import org.apache.accumulo.core.data.Value;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public class FacetValue extends Value {\n+    private HyperLogLogPlus cardinalityEstimate;\n+    private CountMinSketch frequencyEstimate;\n+    protected final AtomicBoolean written = new AtomicBoolean(false);\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate, CountMinSketch frequencyEstimate) {\n+        this.cardinalityEstimate = cardinalityEstimate;\n+        this.frequencyEstimate = frequencyEstimate;\n+    }\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate) {\n+        this(cardinalityEstimate, null);\n+    }\n+    \n+    protected FacetValue() {\n+        \n+    }\n+    \n+    public static FacetValue buildFrom(final DataInput in) throws IOException {\n+        FacetValue value = new FacetValue();\n+        value.readFields(in);\n+        return value;\n+    }\n+    \n+    public byte[] get() {\n+        try {\n+            ensureWritten();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        return super.get();\n+    }\n+    \n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+        int cardSize = in.readInt();\n+        byte[] cardinality = new byte[cardSize];\n+        in.readFully(cardinality);\n+        \n+        // TODO: frequencyEstimate", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2ODIyNA=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDE4OTYwMg==", "bodyText": "ok", "url": "https://github.com/NationalSecurityAgency/datawave/pull/793#discussion_r444189602", "createdAt": "2020-06-23T12:37:49Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetValue.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package datawave.ingest.mapreduce.handler.facet;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus;\n+import com.clearspring.analytics.stream.frequency.CountMinSketch;\n+import org.apache.accumulo.core.data.Value;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public class FacetValue extends Value {\n+    private HyperLogLogPlus cardinalityEstimate;\n+    private CountMinSketch frequencyEstimate;\n+    protected final AtomicBoolean written = new AtomicBoolean(false);\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate, CountMinSketch frequencyEstimate) {\n+        this.cardinalityEstimate = cardinalityEstimate;\n+        this.frequencyEstimate = frequencyEstimate;\n+    }\n+    \n+    public FacetValue(HyperLogLogPlus cardinalityEstimate) {\n+        this(cardinalityEstimate, null);\n+    }\n+    \n+    protected FacetValue() {\n+        \n+    }\n+    \n+    public static FacetValue buildFrom(final DataInput in) throws IOException {\n+        FacetValue value = new FacetValue();\n+        value.readFields(in);\n+        return value;\n+    }\n+    \n+    public byte[] get() {\n+        try {\n+            ensureWritten();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+        return super.get();\n+    }\n+    \n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+        int cardSize = in.readInt();\n+        byte[] cardinality = new byte[cardSize];\n+        in.readFully(cardinality);\n+        \n+        // TODO: frequencyEstimate", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2ODIyNA=="}, "originalCommit": {"oid": "1cdd73da4fb6ced05cf44c38db95c8075a101288"}, "originalPosition": 53}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4525, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}