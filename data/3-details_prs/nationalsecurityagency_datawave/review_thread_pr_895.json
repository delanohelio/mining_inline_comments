{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4MDk1NzYw", "number": 895, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjozNzo0NFrOEeYcuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNzoyMTo0NVrOElDdSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMjkzMzA1OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjozNzo0NFrOHKAJPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNzoxNDo1NVrOHUVc-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI1MDE3Mw==", "bodyText": "should this be looking at the Type to decide if this field is tokenized?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r480250173", "createdAt": "2020-08-31T16:37:44Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -332,7 +485,6 @@ public FacetValue estimate(RawRecordContainer input) {\n     }\n     \n     /** A predicate used to ignore values that are generated via tokenization */\n-    // TODO: make configurable\n     public static class TokenPredicate implements Predicate<String> {\n         @Override\n         public boolean test(String input) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 442}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTExNDU0OA==", "bodyText": "Yes. Not quite ready to tackle this here, yet. This bit is largely unchanged from the inherited codebase.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r485114548", "createdAt": "2020-09-08T18:26:05Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -332,7 +485,6 @@ public FacetValue estimate(RawRecordContainer input) {\n     }\n     \n     /** A predicate used to ignore values that are generated via tokenization */\n-    // TODO: make configurable\n     public static class TokenPredicate implements Predicate<String> {\n         @Override\n         public boolean test(String input) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI1MDE3Mw=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 442}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA4NTA0OQ==", "bodyText": "Going to punt on this until the next round of work here.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r491085049", "createdAt": "2020-09-18T17:14:55Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -332,7 +485,6 @@ public FacetValue estimate(RawRecordContainer input) {\n     }\n     \n     /** A predicate used to ignore values that are generated via tokenization */\n-    // TODO: make configurable\n     public static class TokenPredicate implements Predicate<String> {\n         @Override\n         public boolean test(String input) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI1MDE3Mw=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 442}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMjk1MDQ0OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjo0Mjo1MFrOHKAT5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNzoxNDoyMFrOHUVb2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI1MjkwMA==", "bodyText": "should we filter before doing any of the work?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r480252900", "createdAt": "2020-08-31T16:42:50Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTExNTI5Mg==", "bodyText": "Yes, I should move the filtering bit to show up prior to the HashTablefunction bit, specifically the call to hashEventFields.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r485115292", "createdAt": "2020-09-08T18:27:38Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI1MjkwMA=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA4NDc2Mg==", "bodyText": "This has been addressed in the latest version", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r491084762", "createdAt": "2020-09-18T17:14:20Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI1MjkwMA=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMzAzMzQ2OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNzowNToyN1rOHKBHTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxODoyODo0NFrOHOpHyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NjA2Mg==", "bodyText": "Is creating this right? Down below we create the pivotType/facetType. Why create pivotType/pivotType here?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r480266062", "createdAt": "2020-08-31T17:05:27Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        final Value sharedValue = new Value(cardinality.getBytes());\n+        \n+        final Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n         \n         for (String pivotFieldName : pivotMap.keySet()) {\n-            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            if (!pivotFieldSet.contains(pivotFieldName))\n+                continue;\n+            \n+            final Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            \n             for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n                 if (HashTableFunction.isReduced(pivotTypes))\n                     continue;\n                 \n+                // Generate the pivot entry.\n+                // @formatter:off\n+                final BulkIngestKey pivotIngestKey = generateFacetIngestKey(\n+                        pivotTypes.getIndexedFieldValue(),\n+                        pivotTypes.getIndexedFieldValue(),\n+                        event.getDataType(),\n+                        reflexiveCf,\n+                        dateColumnQualifier,\n+                        cv,\n+                        event.getDate());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2NjE4MA==", "bodyText": "I am guessing for cardinality computation?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r481166180", "createdAt": "2020-09-01T14:09:53Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        final Value sharedValue = new Value(cardinality.getBytes());\n+        \n+        final Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n         \n         for (String pivotFieldName : pivotMap.keySet()) {\n-            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            if (!pivotFieldSet.contains(pivotFieldName))\n+                continue;\n+            \n+            final Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            \n             for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n                 if (HashTableFunction.isReduced(pivotTypes))\n                     continue;\n                 \n+                // Generate the pivot entry.\n+                // @formatter:off\n+                final BulkIngestKey pivotIngestKey = generateFacetIngestKey(\n+                        pivotTypes.getIndexedFieldValue(),\n+                        pivotTypes.getIndexedFieldValue(),\n+                        event.getDataType(),\n+                        reflexiveCf,\n+                        dateColumnQualifier,\n+                        cv,\n+                        event.getDate());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NjA2Mg=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTExNTg1MQ==", "bodyText": "yes, this is for the cardinality calculation for the pivot field itself. We retain document counts for the pivot in addition to each of the facets related to that pivot.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r485115851", "createdAt": "2020-09-08T18:28:44Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());\n+        final Value sharedValue = new Value(cardinality.getBytes());\n+        \n+        final Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n         \n         for (String pivotFieldName : pivotMap.keySet()) {\n-            Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            if (!pivotFieldSet.contains(pivotFieldName))\n+                continue;\n+            \n+            final Text reflexiveCf = createColumnFamily(pivotFieldName, pivotFieldName);\n+            \n             for (NormalizedContentInterface pivotTypes : eventFields.get(pivotFieldName)) {\n                 if (HashTableFunction.isReduced(pivotTypes))\n                     continue;\n                 \n+                // Generate the pivot entry.\n+                // @formatter:off\n+                final BulkIngestKey pivotIngestKey = generateFacetIngestKey(\n+                        pivotTypes.getIndexedFieldValue(),\n+                        pivotTypes.getIndexedFieldValue(),\n+                        event.getDataType(),\n+                        reflexiveCf,\n+                        dateColumnQualifier,\n+                        cv,\n+                        event.getDate());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NjA2Mg=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMzA0NjYwOnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNzowOToyMVrOHKBPOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNzoxMzo1NFrOHUVbBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2ODA4OQ==", "bodyText": "what happens if we see this event id multiple times?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r480268089", "createdAt": "2020-08-31T17:09:21Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDMxODc2NQ==", "bodyText": "specifically does this throw off counts later if we see the same event_id multiple times?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r480318765", "createdAt": "2020-08-31T18:44:44Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2ODA4OQ=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE2NTM3OQ==", "bodyText": "We guarentee (with high probability) that a document has a unique record id which is a combination of the shardid, datatype, and uid.  I think we need to add shardid and datatype to this offer.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r481165379", "createdAt": "2020-09-01T14:08:45Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2ODA4OQ=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTEyMDMzOA==", "bodyText": "If we see the same event id multiple times, it does not increase the count (cardinality) for that facet. You can see a good example of that in ApproximateAlgorithmsTest for example.\nGood catch @ivakegg - I will look more closely at what event.getId().toString() is returning to ensure it is sufficiently unique. As you point out, I suspect it isn't.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r485120338", "createdAt": "2020-09-08T18:37:03Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2ODA4OQ=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA4NDU1MQ==", "bodyText": "Ivan's comment has been addressed in the latest version (and yes, he identified a bug - hence the unit test updates)", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r491084551", "createdAt": "2020-09-18T17:13:54Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,72 +200,149 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n+        // @formatter:on\n         \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(keySet);\n+        Set<String> facetFieldSet = new HashSet<>(keySet);\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;\n+        if (log.isDebugEnabled()) {\n+            StringBuilder b = new StringBuilder();\n+            for (NormalizedContentInterface f : eventFields.get(\"EVENT_ID\")) {\n+                b.append(f.getEventFieldValue());\n+            }\n+            eventId = b.toString();\n+        }\n+        \n+        // compute the cardinality based on the uid, this becomes the value shared\n+        // across each facet row generated.\n+        final HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n+        cardinality.offer(event.getId().toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2ODA4OQ=="}, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMzM1OTU3OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/table/config/FacetTableConfigHelper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxODo0MToyOFrOHKEOZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxODo0MToyOFrOHKEOZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDMxNzAyOA==", "bodyText": "implement TODOs?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r480317028", "createdAt": "2020-08-31T18:41:28Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/table/config/FacetTableConfigHelper.java", "diffHunk": "@@ -73,7 +75,13 @@ public void configure(TableOperations tops) throws AccumuloException, AccumuloSe\n     }\n     \n     protected void configureFacetTable(TableOperations tops) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {\n-        // TODO:\n+        // Add the facet cardinality aggregator\n+        for (IteratorUtil.IteratorScope scope : IteratorUtil.IteratorScope.values()) {\n+            String stem = String.format(\"%s%s.%s\", Property.TABLE_ITERATOR_PREFIX, scope.name(), \"UIDAggregator\");\n+            setPropertyIfNecessary(tableName, stem, \"19,datawave.iterators.TotalAggregatingIterator\", tops, log);\n+            stem += \".opt.\";\n+            setPropertyIfNecessary(tableName, stem + \"*\", \"datawave.ingest.table.aggregator.CardinalityAggregator\", tops, log);\n+        }\n     }\n     \n     protected void configureFacetMetadataTable(TableOperations tops) throws AccumuloException, AccumuloSecurityException, TableNotFoundException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52aea70e4e722b603b4b6c348f6e646adc926389"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3Mjg5NDE4OnYy", "diffSide": "RIGHT", "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNzoyMTo0NVrOHUVp3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNDoxNjoxM1rOHYFjUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA4ODM0OQ==", "bodyText": "do we still need this? should the use of id below (shard/datatype/uid) replace this?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r491088349", "createdAt": "2020-09-18T17:21:45Z", "author": {"login": "FineAndDandy"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,118 +200,297 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n-        \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> fieldKeyStream = fields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            fieldKeyStream = fieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> filteredFieldSet = fieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(filteredFieldSet);\n+        Set<String> facetFieldSet = new HashSet<>(filteredFieldSet);\n+        \n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = filterAndHashEventFields(fields, filteredFieldSet, func);\n+        // @formatter:on\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21fa4d6be21727dc672b3756bd23005455e1080a"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNTM5OA==", "bodyText": "It appears that eventId is needed to find the document if we failed to ingest.  May still be needed, but perhaps as trace and not debug?", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r495015398", "createdAt": "2020-09-25T14:11:10Z", "author": {"login": "ivakegg"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,118 +200,297 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n-        \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> fieldKeyStream = fields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            fieldKeyStream = fieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> filteredFieldSet = fieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(filteredFieldSet);\n+        Set<String> facetFieldSet = new HashSet<>(filteredFieldSet);\n+        \n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = filterAndHashEventFields(fields, filteredFieldSet, func);\n+        // @formatter:on\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA4ODM0OQ=="}, "originalCommit": {"oid": "21fa4d6be21727dc672b3756bd23005455e1080a"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxODgzNA==", "bodyText": "In this particular case, I needed the identifier that was on the original piece of content that was ingested in order to troubleshoot the problem, not the uid that was being generated because I could not tie that back to the original piece of data e.g via the shard table.", "url": "https://github.com/NationalSecurityAgency/datawave/pull/895#discussion_r495018834", "createdAt": "2020-09-25T14:16:13Z", "author": {"login": "drewfarris"}, "path": "warehouse/ingest-core/src/main/java/datawave/ingest/mapreduce/handler/facet/FacetHandler.java", "diffHunk": "@@ -221,118 +200,297 @@ public long process(KEYIN key, RawRecordContainer event, Multimap<String,Normali\n         final String shardDate = ShardIdFactory.getDateString(shardId);\n         Text dateColumnQualifier = new Text(shardDate);\n         \n-        HyperLogLogPlus cardinality = new HyperLogLogPlus(10);\n-        cardinality.offer(event.getId().toString());\n-        \n         Text cv = new Text(flatten(event.getVisibility()));\n         \n-        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(contextWriter, context, facetHashTableName, facetHashThreshold,\n-                        event.getDate());\n-        Multimap<String,NormalizedContentInterface> eventFields = hashEventFields(fields, func);\n-        \n-        Stream<String> eventFieldKeyStream = eventFields.keySet().stream().filter(new TokenPredicate());\n-        if (fieldFilter != null) {\n-            eventFieldKeyStream = eventFieldKeyStream.filter(fieldFilter);\n+        // filter out event fields that are generated as the result of tokenization.\n+        Stream<String> fieldKeyStream = fields.keySet().stream().filter(fieldSelectionPredicate);\n+        if (fieldFilterPredicate != null) {\n+            fieldKeyStream = fieldKeyStream.filter(fieldFilterPredicate);\n         }\n-        Set<String> keySet = eventFieldKeyStream.collect(Collectors.toSet());\n-        List<Set<String>> keySetList = Lists.newArrayList();\n-        keySetList.add(keySet);\n-        keySetList.add(keySet);\n+        final Set<String> filteredFieldSet = fieldKeyStream.collect(Collectors.toSet());\n+        Set<String> pivotFieldSet = new HashSet<>(filteredFieldSet);\n+        Set<String> facetFieldSet = new HashSet<>(filteredFieldSet);\n+        \n+        // fields with a large number of values are hashed. See HashTableFunction for details\n+        // @formatter:off\n+        final HashTableFunction<KEYIN,KEYOUT,VALUEOUT> func = new HashTableFunction<>(\n+                contextWriter, context, facetHashTableName, facetHashThreshold, event.getDate());\n+        final Multimap<String,NormalizedContentInterface> eventFields = filterAndHashEventFields(fields, filteredFieldSet, func);\n+        // @formatter:on\n         \n         long countWritten = 0;\n         \n-        Value sharedValue = new Value(cardinality.getBytes());\n-        Multimap<BulkIngestKey,Value> results = ArrayListMultimap.create();\n+        // the event id offered to the cardinality is a uid based on the 'EVENT_ID',\n+        // so it's helpful to have that around for debugging when logging about the\n+        // facet keys that are created.\n+        String eventId = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA4ODM0OQ=="}, "originalCommit": {"oid": "21fa4d6be21727dc672b3756bd23005455e1080a"}, "originalPosition": 167}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4411, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}