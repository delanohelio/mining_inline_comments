{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0NzE1NTAy", "number": 14989, "title": "Batch presto spark rows", "bodyText": "Producing tiny rows to the shuffle has significant performance implications, as the shuffle algorithm has non zero cost of appending a row. By grouping multiple rows for a single partition together into a single row batch increases shuffle efficiency and allows to achieve higher shuffle throughput.\n== NO RELEASE NOTE ==", "createdAt": "2020-08-07T16:54:18Z", "url": "https://github.com/prestodb/presto/pull/14989", "merged": true, "mergeCommit": {"oid": "c8d9198860b46f6da72be2605ac2ac642c422184"}, "closed": true, "closedAt": "2020-08-14T20:17:25Z", "author": {"login": "arhimondr"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc9unsTgBqjM2NDEzMjAyMzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-aDASAFqTQ2NjQ3NzUwOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f7fc1d57523ca1bda20f079b4a0d766e07eb327e", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/f7fc1d57523ca1bda20f079b4a0d766e07eb327e", "committedDate": "2020-08-07T16:53:39Z", "message": "Implement batching of tiny rows for Presto on Spark"}, "afterCommit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/99a3724921617370bdc90147bf7429ac7520f16a", "committedDate": "2020-08-11T03:54:13Z", "message": "Implement batching of tiny rows for Presto on Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MzMwMTY0", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-465330164", "createdAt": "2020-08-11T18:39:35Z", "commit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/99a3724921617370bdc90147bf7429ac7520f16a", "committedDate": "2020-08-11T03:54:13Z", "message": "Implement batching of tiny rows for Presto on Spark"}, "afterCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/5efc3982ea46e269f0d24f480691ce3d754935a5", "committedDate": "2020-08-11T20:44:26Z", "message": "Implement batching of tiny rows for Presto on Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MzA4MDMx", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-465308031", "createdAt": "2020-08-11T18:07:36Z", "commit": {"oid": "fd40afa0853b9c593e63e54b4cccf5cb3644e450"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODowNzozN1rOG_DXlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODowNzozN1rOG_DXlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODY2MQ==", "bodyText": "nit. totalSizeInBytes", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468768661", "createdAt": "2020-08-11T18:07:37Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd40afa0853b9c593e63e54b4cccf5cb3644e450"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NDM0MDgz", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-465434083", "createdAt": "2020-08-11T21:15:02Z", "commit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMToxNTowMlrOG_JfYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMToxNTowMlrOG_JfYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2ODk2MA==", "bodyText": "Put comment separately in the end or use a new line?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468868960", "createdAt": "2020-08-11T21:15:02Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 266}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NDcyOTUw", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-465472950", "createdAt": "2020-08-11T22:31:58Z", "commit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMjozMTo1OFrOG_LdmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMjozMTo1OFrOG_LdmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMTI3Mg==", "bodyText": "peekRow(partition) != NIL", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468901272", "createdAt": "2020-08-11T22:31:58Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];\n+            fill(nextRow, NIL);\n+            int[] rowIndex = new int[rowCount];\n+            fill(rowIndex, NIL);\n+\n+            for (int row = rowCount - 1; row >= 0; row--) {\n+                int partition = partitions[row];\n+                int partitionIndex = getPartitionIndex(partition, nextRow);\n+                int currentPointer = nextRow[partitionIndex];\n+                nextRow[partitionIndex] = row;\n+                rowIndex[row] = currentPointer;\n+            }\n+\n+            return new RowIndex(nextRow, rowIndex);\n+        }\n+\n+        private RowIndex(int[] nextRow, int[] rowIndex)\n+        {\n+            this.nextRow = requireNonNull(nextRow, \"nextRow is null\");\n+            this.rowIndex = requireNonNull(rowIndex, \"rowIndex is null\");\n+        }\n+\n+        public boolean hasNextRow(int partition)\n+        {\n+            return nextRow[getPartitionIndex(partition, nextRow)] != NIL;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 290}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MzQxOTA3", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-465341907", "createdAt": "2020-08-11T18:53:53Z", "commit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a"}, "state": "APPROVED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODo1Mzo1NFrOG_FEQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzoxNDo0MlrOG_MWGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg==", "bodyText": "when would this happen? essentially it's an empty row batch?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468796482", "createdAt": "2020-08-11T18:53:54Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,6 +220,21 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg==", "bodyText": "Add some comment like \"Array-simulated linked-list to find the row offset for each partition\" assume i read the intention of this class right :)", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468903642", "createdAt": "2020-08-11T22:38:15Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA==", "bodyText": "hmm? what's this for? :)", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911068", "createdAt": "2020-08-11T23:00:02Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -204,8 +333,9 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             this.totalSize = totalSize;\n \n             this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            this.rowData.order(LITTLE_ENDIAN);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ==", "bodyText": "is this while try to skip empty partition? -- in that case can it be an if statement instead ? :)", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911401", "createdAt": "2020-08-11T23:01:02Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDQ1NQ==", "bodyText": "nit: new line.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468914455", "createdAt": "2020-08-11T23:10:23Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java", "diffHunk": "@@ -170,7 +179,7 @@ public OperatorFactory duplicate()\n                     partitionChannels,\n                     partitionConstants,\n                     replicateNullsAndAny,\n-                    nullChannel);\n+                    nullChannel, targetAverageRowSizeInBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg==", "bodyText": "lol", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468915736", "createdAt": "2020-08-11T23:14:42Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java", "diffHunk": "@@ -76,32 +78,76 @@ public Page getNextPage()\n             while (currentIteratorIndex < shuffleInputs.size()) {\n                 PrestoSparkShuffleInput input = shuffleInputs.get(currentIteratorIndex);\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> iterator = input.getIterator();\n-                long processedBytes = 0;\n+                long currentIteratorProcessedBytes = 0;\n+                long currentIteratorProcessedRows = 0;\n+                long currentIteratorProcessedEntries = 0;\n                 long start = System.currentTimeMillis();\n                 while (iterator.hasNext() && output.size() <= TARGET_SIZE && rowCount <= MAX_ROWS_PER_PAGE) {\n+                    currentIteratorProcessedEntries++;\n                     PrestoSparkMutableRow row = iterator.next()._2;\n                     if (row.getBuffer() != null) {\n                         ByteBuffer buffer = row.getBuffer();\n-                        output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                        processedBytes += buffer.remaining();\n+                        verify(buffer.remaining() >= 1, \"row must contain at least a single byte\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NTM4NjAw", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-465538600", "createdAt": "2020-08-12T01:50:53Z", "commit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMTo1MDo1NFrOG_PALg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzowMjo0MVrOG_QHgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA==", "bodyText": "This is a nice implementation but a little difficult to understand, rowIndex[] is next next row I would prefer to rename nextRow to currentRow and rowIndex to nextRow. If you convert this implementation to an array of stacks, it could be simplified a lot. Are you worried about creating a lot objects when partitionCount is more than a few thousands.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468959278", "createdAt": "2020-08-12T01:50:54Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2NDUzOA==", "bodyText": "Entries is a little difficult to guess what it means without reading the code. Have you consider to use totalProcessedRowBatches, at least for logging.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468964538", "createdAt": "2020-08-12T02:11:20Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -819,33 +819,44 @@ private void processShuffleStats()\n         private void logShuffleStatsSummary(ShuffleStatsKey key, List<PrestoSparkShuffleStats> statsList)\n         {\n             long totalProcessedRows = 0;\n+            long totalProcessedEntries = 0;\n             long totalProcessedBytes = 0;\n             long totalElapsedWallTimeMills = 0;\n             for (PrestoSparkShuffleStats stats : statsList) {\n                 totalProcessedRows += stats.getProcessedRows();\n+                totalProcessedEntries += stats.getProcessedEntries();\n                 totalProcessedBytes += stats.getProcessedBytes();\n                 totalElapsedWallTimeMills += stats.getElapsedWallTimeMills();\n             }\n             long totalElapsedWallTimeSeconds = totalElapsedWallTimeMills / 1000;\n             long rowsPerSecond = totalProcessedRows;\n+            long entriesPerSecond = totalProcessedEntries;\n             long bytesPerSecond = totalProcessedBytes;\n             if (totalElapsedWallTimeSeconds > 0) {\n                 rowsPerSecond = totalProcessedRows / totalElapsedWallTimeSeconds;\n+                entriesPerSecond = totalProcessedEntries / totalElapsedWallTimeSeconds;\n                 bytesPerSecond = totalProcessedBytes / totalElapsedWallTimeSeconds;\n             }\n             long averageRowSize = 0;\n             if (totalProcessedRows > 0) {\n                 averageRowSize = totalProcessedBytes / totalProcessedRows;\n             }\n+            long averageEntrySize = 0;\n+            if (totalProcessedEntries > 0) {\n+                averageEntrySize = totalProcessedBytes / totalProcessedEntries;\n+            }\n             log.info(\n-                    \"Fragment: %s, Operation: %s, Rows: %s, Size: %s, Avg Row Size: %s, Time: %s, %srows/s, %s/s\",\n+                    \"Fragment: %s, Operation: %s, Rows: %s, Entries: %s, Size: %s, Avg Row Size: %s, Avg Entry Size: %s, Time: %s, %s rows/s, %s entries/s, %s/s\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA==", "bodyText": "Ideally each partition is expected to receive at least one row before doing a shuffle to make a shuffle meaningful ?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468968220", "createdAt": "2020-08-12T02:25:59Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -77,14 +89,38 @@ public int getPositionCount()\n         return rowCount;\n     }\n \n-    public static PrestoSparkRowBatchBuilder builder(int partitionCount)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetAverageRowSizeInBytes)\n     {\n-        return new PrestoSparkRowBatchBuilder(partitionCount, DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+        checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+        int targetSizeInBytes = partitionCount * targetAverageRowSizeInBytes;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MzM5NA==", "bodyText": "I think one partition may receive multiple row batches (entries), thus requires a while loop.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468973394", "createdAt": "2020-08-12T02:46:33Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA==", "bodyText": "Constructor parameter names for PrestoSparkRowBatch is not updated.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468975634", "createdAt": "2020-08-12T02:55:17Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    output.writeByte(MULTI_ROW_ENTRY_MARKER);\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSize;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;\n+                        verify(rowSize >= 1, \"rowSize is expected to be greater than or equal to zero: %s\", rowSize);\n+\n+                        // skip the marker\n+                        rowOffset++;\n+                        rowSize--;\n+\n+                        if (currentEntryRowCount > 0 && (currentEntrySize + rowSize > maxEntrySizeInBytes || currentEntryRowCount + 1 > maxRowsPerEntry)) {\n+                            break;\n+                        }\n+\n+                        output.writeBytes(data, rowOffset, rowSize);\n+                        currentEntrySize += rowSize;\n+                        currentEntryRowCount++;\n+\n+                        rowIndex.nextRow(partition);\n+                    }\n+\n+                    // entry is done\n+                    output.getUnderlyingSlice().setShort(currentEntryOffset + 1, currentEntryRowCount);\n+                    entriesCount++;\n+                }\n+            }\n+\n+            return new PrestoSparkRowBatch(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ==", "bodyText": "Have you considered to not distinguish SINGLE and MULTIPLE.  For SINGLE_ROW, one byte (marker) is changed to short (rowCount),  for MULTIPLE row, three bytes (marker and rowCount) becomes two bytes (rowCount).", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468976199", "createdAt": "2020-08-12T02:57:34Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ==", "bodyText": "entryData, entryOffsets, entryPartitions", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468977539", "createdAt": "2020-08-12T03:02:41Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;\n     private final long retainedSizeInBytes;\n \n-    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "707081b6641904d0c71fcf0ff65d90c3ecff6f59"}, "originalPosition": 10}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/5efc3982ea46e269f0d24f480691ce3d754935a5", "committedDate": "2020-08-11T20:44:26Z", "message": "Implement batching of tiny rows for Presto on Spark"}, "afterCommit": {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/e77a7081b87d3e8bd67a469b317a2a18db676a81", "committedDate": "2020-08-12T16:50:27Z", "message": "Implement batching of tiny rows for Presto on Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MzA1NDg5", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-466305489", "createdAt": "2020-08-12T21:57:41Z", "commit": {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Nzo0MVrOG_0ReQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Nzo0MVrOG_0ReQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTkxMw==", "bodyText": "Out of dated comment", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469569913", "createdAt": "2020-08-12T21:57:41Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,13 +217,91 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {\n+                return createDirectRowBatch();\n+            }\n+\n+            int averageRowSize = totalSizeInBytes / rowCount;\n+            if (averageRowSize < targetAverageRowSizeInBytes) {\n+                return createGroupedRowBatch();\n+            }\n+\n+            return createDirectRowBatch();\n+        }\n+\n+        private PrestoSparkRowBatch createDirectRowBatch()\n+        {\n             return new PrestoSparkRowBatch(\n                     partitionCount,\n                     rowCount,\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSize);\n+                    totalSizeInBytes);\n+        }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSizeInBytes * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSizeInBytes / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSizeInBytes;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81"}, "originalPosition": 216}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "committedDate": "2020-08-13T02:17:11Z", "message": "Use offset instead of size in PrestoSparkRowBatch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/0d8ba5cc581f395de9ab2301e627ede08c239ca1", "committedDate": "2020-08-13T02:18:51Z", "message": "Implement batching of tiny rows for Presto on Spark"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/e77a7081b87d3e8bd67a469b317a2a18db676a81", "committedDate": "2020-08-12T16:50:27Z", "message": "Implement batching of tiny rows for Presto on Spark"}, "afterCommit": {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/0d8ba5cc581f395de9ab2301e627ede08c239ca1", "committedDate": "2020-08-13T02:18:51Z", "message": "Implement batching of tiny rows for Presto on Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2NDc3NTA4", "url": "https://github.com/prestodb/presto/pull/14989#pullrequestreview-466477508", "createdAt": "2020-08-13T06:30:12Z", "commit": {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1121, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}