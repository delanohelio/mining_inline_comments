{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0NzE1NTAy", "number": 14989, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODowNzozN1rOEXUuXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Nzo0MVrOEX0EqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyODkyMjUzOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODowNzozN1rOG_DXlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODowNzozN1rOG_DXlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODY2MQ==", "bodyText": "nit. totalSizeInBytes", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468768661", "createdAt": "2020-08-11T18:07:37Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fd40afa0853b9c593e63e54b4cccf5cb3644e450"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTA5NzY2OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODo1Mzo1NFrOG_FEQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNjoyNzozN1rOG_pH-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg==", "bodyText": "when would this happen? essentially it's an empty row batch?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468796482", "createdAt": "2020-08-11T18:53:54Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,6 +220,21 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NzI1Nw==", "bodyText": "In theory it should never happen. We create the builder lazily. It should never be created if theres no rows to be appended. However I added it here for completeness, as the interface in theory allows you to create a zero rows batch.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469387257", "createdAt": "2020-08-12T16:27:37Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,6 +220,21 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg=="}, "originalCommit": {"oid": "99a3724921617370bdc90147bf7429ac7520f16a"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTU1NjY4OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMToxNTowMlrOG_JfYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMToxNTowMlrOG_JfYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2ODk2MA==", "bodyText": "Put comment separately in the end or use a new line?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468868960", "createdAt": "2020-08-11T21:15:02Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 266}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTc2NjYxOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMjozMTo1OFrOG_LdmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMjozMTo1OFrOG_LdmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMTI3Mg==", "bodyText": "peekRow(partition) != NIL", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468901272", "createdAt": "2020-08-11T22:31:58Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];\n+            fill(nextRow, NIL);\n+            int[] rowIndex = new int[rowCount];\n+            fill(rowIndex, NIL);\n+\n+            for (int row = rowCount - 1; row >= 0; row--) {\n+                int partition = partitions[row];\n+                int partitionIndex = getPartitionIndex(partition, nextRow);\n+                int currentPointer = nextRow[partitionIndex];\n+                nextRow[partitionIndex] = row;\n+                rowIndex[row] = currentPointer;\n+            }\n+\n+            return new RowIndex(nextRow, rowIndex);\n+        }\n+\n+        private RowIndex(int[] nextRow, int[] rowIndex)\n+        {\n+            this.nextRow = requireNonNull(nextRow, \"nextRow is null\");\n+            this.rowIndex = requireNonNull(rowIndex, \"rowIndex is null\");\n+        }\n+\n+        public boolean hasNextRow(int partition)\n+        {\n+            return nextRow[getPartitionIndex(partition, nextRow)] != NIL;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 290}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTc4MjM3OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMjozODoxNVrOG_Lm2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNjo0MjoyNlrOG_pp-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg==", "bodyText": "Add some comment like \"Array-simulated linked-list to find the row offset for each partition\" assume i read the intention of this class right :)", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468903642", "createdAt": "2020-08-11T22:38:15Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5NTk2Mg==", "bodyText": "Added more descriptive comment", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469395962", "createdAt": "2020-08-12T16:42:26Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTgzMTQ4OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzowMDowMlrOG_MD3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTozNDoxN1rOG_m0zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA==", "bodyText": "hmm? what's this for? :)", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911068", "createdAt": "2020-08-11T23:00:02Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -204,8 +333,9 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             this.totalSize = totalSize;\n \n             this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            this.rowData.order(LITTLE_ENDIAN);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM0OTU4Mw==", "bodyText": "To make sure we are writing short in LITTLE_ENDIAN byte order, as the default byte order is BIG_ENDIAN that is different that the machine byte order.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469349583", "createdAt": "2020-08-12T15:34:17Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -204,8 +333,9 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             this.totalSize = totalSize;\n \n             this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            this.rowData.order(LITTLE_ENDIAN);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 226}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTgzMzY4OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzowMTowMlrOG_MFKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0NjozM1rOG_P3Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ==", "bodyText": "is this while try to skip empty partition? -- in that case can it be an if statement instead ? :)", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911401", "createdAt": "2020-08-11T23:01:02Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MzM5NA==", "bodyText": "I think one partition may receive multiple row batches (entries), thus requires a while loop.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468973394", "createdAt": "2020-08-12T02:46:33Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTg1MzYwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzoxMDoyM1rOG_MRFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzoxMDoyM1rOG_MRFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDQ1NQ==", "bodyText": "nit: new line.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468914455", "createdAt": "2020-08-11T23:10:23Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java", "diffHunk": "@@ -170,7 +179,7 @@ public OperatorFactory duplicate()\n                     partitionChannels,\n                     partitionConstants,\n                     replicateNullsAndAny,\n-                    nullChannel);\n+                    nullChannel, targetAverageRowSizeInBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTg2MTgwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMzoxNDo0MlrOG_MWGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTozNTowMVrOG_m2yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg==", "bodyText": "lol", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468915736", "createdAt": "2020-08-11T23:14:42Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java", "diffHunk": "@@ -76,32 +78,76 @@ public Page getNextPage()\n             while (currentIteratorIndex < shuffleInputs.size()) {\n                 PrestoSparkShuffleInput input = shuffleInputs.get(currentIteratorIndex);\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> iterator = input.getIterator();\n-                long processedBytes = 0;\n+                long currentIteratorProcessedBytes = 0;\n+                long currentIteratorProcessedRows = 0;\n+                long currentIteratorProcessedEntries = 0;\n                 long start = System.currentTimeMillis();\n                 while (iterator.hasNext() && output.size() <= TARGET_SIZE && rowCount <= MAX_ROWS_PER_PAGE) {\n+                    currentIteratorProcessedEntries++;\n                     PrestoSparkMutableRow row = iterator.next()._2;\n                     if (row.getBuffer() != null) {\n                         ByteBuffer buffer = row.getBuffer();\n-                        output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                        processedBytes += buffer.remaining();\n+                        verify(buffer.remaining() >= 1, \"row must contain at least a single byte\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MDA5MQ==", "bodyText": "Now even a zero columns row will contain at least a single byte, that is a marker.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469350091", "createdAt": "2020-08-12T15:35:01Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java", "diffHunk": "@@ -76,32 +78,76 @@ public Page getNextPage()\n             while (currentIteratorIndex < shuffleInputs.size()) {\n                 PrestoSparkShuffleInput input = shuffleInputs.get(currentIteratorIndex);\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> iterator = input.getIterator();\n-                long processedBytes = 0;\n+                long currentIteratorProcessedBytes = 0;\n+                long currentIteratorProcessedRows = 0;\n+                long currentIteratorProcessedEntries = 0;\n                 long start = System.currentTimeMillis();\n                 while (iterator.hasNext() && output.size() <= TARGET_SIZE && rowCount <= MAX_ROWS_PER_PAGE) {\n+                    currentIteratorProcessedEntries++;\n                     PrestoSparkMutableRow row = iterator.next()._2;\n                     if (row.getBuffer() != null) {\n                         ByteBuffer buffer = row.getBuffer();\n-                        output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                        processedBytes += buffer.remaining();\n+                        verify(buffer.remaining() >= 1, \"row must contain at least a single byte\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDE1NDEzOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMTo1MDo1NFrOG_PALg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNjozMzo1MlrOG_pWhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA==", "bodyText": "This is a nice implementation but a little difficult to understand, rowIndex[] is next next row I would prefer to rename nextRow to currentRow and rowIndex to nextRow. If you convert this implementation to an array of stacks, it could be simplified a lot. Are you worried about creating a lot objects when partitionCount is more than a few thousands.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468959278", "createdAt": "2020-08-12T01:50:54Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5MDk4Mw==", "bodyText": "The main reason for implementing RowIndex was to avoid allocating extra objects along the way.\nLet me add a comment explaining how it works at the top of the class.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469390983", "createdAt": "2020-08-12T16:33:52Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDE4OTYxOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjoxMToyMFrOG_PUug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjoxMToyMFrOG_PUug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2NDUzOA==", "bodyText": "Entries is a little difficult to guess what it means without reading the code. Have you consider to use totalProcessedRowBatches, at least for logging.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468964538", "createdAt": "2020-08-12T02:11:20Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -819,33 +819,44 @@ private void processShuffleStats()\n         private void logShuffleStatsSummary(ShuffleStatsKey key, List<PrestoSparkShuffleStats> statsList)\n         {\n             long totalProcessedRows = 0;\n+            long totalProcessedEntries = 0;\n             long totalProcessedBytes = 0;\n             long totalElapsedWallTimeMills = 0;\n             for (PrestoSparkShuffleStats stats : statsList) {\n                 totalProcessedRows += stats.getProcessedRows();\n+                totalProcessedEntries += stats.getProcessedEntries();\n                 totalProcessedBytes += stats.getProcessedBytes();\n                 totalElapsedWallTimeMills += stats.getElapsedWallTimeMills();\n             }\n             long totalElapsedWallTimeSeconds = totalElapsedWallTimeMills / 1000;\n             long rowsPerSecond = totalProcessedRows;\n+            long entriesPerSecond = totalProcessedEntries;\n             long bytesPerSecond = totalProcessedBytes;\n             if (totalElapsedWallTimeSeconds > 0) {\n                 rowsPerSecond = totalProcessedRows / totalElapsedWallTimeSeconds;\n+                entriesPerSecond = totalProcessedEntries / totalElapsedWallTimeSeconds;\n                 bytesPerSecond = totalProcessedBytes / totalElapsedWallTimeSeconds;\n             }\n             long averageRowSize = 0;\n             if (totalProcessedRows > 0) {\n                 averageRowSize = totalProcessedBytes / totalProcessedRows;\n             }\n+            long averageEntrySize = 0;\n+            if (totalProcessedEntries > 0) {\n+                averageEntrySize = totalProcessedBytes / totalProcessedEntries;\n+            }\n             log.info(\n-                    \"Fragment: %s, Operation: %s, Rows: %s, Size: %s, Avg Row Size: %s, Time: %s, %srows/s, %s/s\",\n+                    \"Fragment: %s, Operation: %s, Rows: %s, Entries: %s, Size: %s, Avg Row Size: %s, Avg Entry Size: %s, Time: %s, %s rows/s, %s entries/s, %s/s\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDIxNDE1OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjoyNTo1OVrOG_PjHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTozNzoyMlrOG_m9FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA==", "bodyText": "Ideally each partition is expected to receive at least one row before doing a shuffle to make a shuffle meaningful ?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468968220", "createdAt": "2020-08-12T02:25:59Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -77,14 +89,38 @@ public int getPositionCount()\n         return rowCount;\n     }\n \n-    public static PrestoSparkRowBatchBuilder builder(int partitionCount)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetAverageRowSizeInBytes)\n     {\n-        return new PrestoSparkRowBatchBuilder(partitionCount, DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+        checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+        int targetSizeInBytes = partitionCount * targetAverageRowSizeInBytes;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MTcwMQ==", "bodyText": "I'm trying to figure out an average here. It is fine when one partitions receives more than the other.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469351701", "createdAt": "2020-08-12T15:37:22Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -77,14 +89,38 @@ public int getPositionCount()\n         return rowCount;\n     }\n \n-    public static PrestoSparkRowBatchBuilder builder(int partitionCount)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetAverageRowSizeInBytes)\n     {\n-        return new PrestoSparkRowBatchBuilder(partitionCount, DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+        checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+        int targetSizeInBytes = partitionCount * targetAverageRowSizeInBytes;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDI2Mzk1OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo1NToxN1rOG_QAEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNjo0Njo1OVrOG_p0wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA==", "bodyText": "Constructor parameter names for PrestoSparkRowBatch is not updated.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468975634", "createdAt": "2020-08-12T02:55:17Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    output.writeByte(MULTI_ROW_ENTRY_MARKER);\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSize;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;\n+                        verify(rowSize >= 1, \"rowSize is expected to be greater than or equal to zero: %s\", rowSize);\n+\n+                        // skip the marker\n+                        rowOffset++;\n+                        rowSize--;\n+\n+                        if (currentEntryRowCount > 0 && (currentEntrySize + rowSize > maxEntrySizeInBytes || currentEntryRowCount + 1 > maxRowsPerEntry)) {\n+                            break;\n+                        }\n+\n+                        output.writeBytes(data, rowOffset, rowSize);\n+                        currentEntrySize += rowSize;\n+                        currentEntryRowCount++;\n+\n+                        rowIndex.nextRow(partition);\n+                    }\n+\n+                    // entry is done\n+                    output.getUnderlyingSlice().setShort(currentEntryOffset + 1, currentEntryRowCount);\n+                    entriesCount++;\n+                }\n+            }\n+\n+            return new PrestoSparkRowBatch(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5ODcyMQ==", "bodyText": "Yeah, I deliberately decided not to do a more extensive rename, as this code should be pretty transitional", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469398721", "createdAt": "2020-08-12T16:46:59Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    output.writeByte(MULTI_ROW_ENTRY_MARKER);\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSize;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;\n+                        verify(rowSize >= 1, \"rowSize is expected to be greater than or equal to zero: %s\", rowSize);\n+\n+                        // skip the marker\n+                        rowOffset++;\n+                        rowSize--;\n+\n+                        if (currentEntryRowCount > 0 && (currentEntrySize + rowSize > maxEntrySizeInBytes || currentEntryRowCount + 1 > maxRowsPerEntry)) {\n+                            break;\n+                        }\n+\n+                        output.writeBytes(data, rowOffset, rowSize);\n+                        currentEntrySize += rowSize;\n+                        currentEntryRowCount++;\n+\n+                        rowIndex.nextRow(partition);\n+                    }\n+\n+                    // entry is done\n+                    output.getUnderlyingSlice().setShort(currentEntryOffset + 1, currentEntryRowCount);\n+                    entriesCount++;\n+                }\n+            }\n+\n+            return new PrestoSparkRowBatch(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDI2Nzc3OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo1NzozNFrOG_QCRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNDo0NToxM1rOHAO7Lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ==", "bodyText": "Have you considered to not distinguish SINGLE and MULTIPLE.  For SINGLE_ROW, one byte (marker) is changed to short (rowCount),  for MULTIPLE row, three bytes (marker and rowCount) becomes two bytes (rowCount).", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468976199", "createdAt": "2020-08-12T02:57:34Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MzI0Ng==", "bodyText": "That is a very good point. Let me refactor it.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469353246", "createdAt": "2020-08-12T15:39:31Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2MTM5Ng==", "bodyText": "Actually now I realize that we might not even need short to store number of rows.\nWe can use unsigned byte instead that gives us a good 0 - 255 range and allows us to store 255 rows per batch. The theoretical minimum row size is 2 bytes (1 byte for null marker and 1 byte for data). With 255 rows per batch we can get to moderately decent 510 bytes. It is still less that expected 1kb, but with a single short column row we are already getting very close to it (3 * 255 = 765 bytes). And with a single integer column rows we are even above the threshold (5 * 255 = 1275)\nOn the other end one byte of overhead per row (to store short) is also nothing terrible. We can do that as well.\nWhat do you think?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469361396", "createdAt": "2020-08-12T15:47:45Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3NjE4Ng==", "bodyText": "Actually let me proceed with short, as 1 vs 2 bytes won't make a big difference with the target entry size ~1kb", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469376186", "createdAt": "2020-08-12T16:09:49Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcyNDUxNg==", "bodyText": "as 1 vs 2 bytes won't make a big difference with the target entry size ~1kb\n\nYeah, it's also valuable to make encoding simple :).  Just make sure I understand correctly -- for now a single row is considered as a \"row batch\" with rowCount = 1?", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469724516", "createdAt": "2020-08-13T06:28:06Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAwNjU3NQ==", "bodyText": "Yeah", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r470006575", "createdAt": "2020-08-13T14:45:13Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, "originalCommit": {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMDI3NjY0OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMzowMjo0MVrOG_QHgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTo1MDoyMFrOG_np2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ==", "bodyText": "entryData, entryOffsets, entryPartitions", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468977539", "createdAt": "2020-08-12T03:02:41Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;\n     private final long retainedSizeInBytes;\n \n-    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "707081b6641904d0c71fcf0ff65d90c3ecff6f59"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2MzE2MQ==", "bodyText": "I deliberately didn't want to do the extensive renames, as this might potentially go to a very large scope. Since this code is transitional I would like to keep it the way it is, as then when we revert this patch we would have to rename everything back.", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469363161", "createdAt": "2020-08-12T15:50:20Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;\n     private final long retainedSizeInBytes;\n \n-    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ=="}, "originalCommit": {"oid": "707081b6641904d0c71fcf0ff65d90c3ecff6f59"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA1ODY0OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Nzo0MVrOG_0ReQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Nzo0MVrOG_0ReQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTkxMw==", "bodyText": "Out of dated comment", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469569913", "createdAt": "2020-08-12T21:57:41Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,13 +217,91 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {\n+                return createDirectRowBatch();\n+            }\n+\n+            int averageRowSize = totalSizeInBytes / rowCount;\n+            if (averageRowSize < targetAverageRowSizeInBytes) {\n+                return createGroupedRowBatch();\n+            }\n+\n+            return createDirectRowBatch();\n+        }\n+\n+        private PrestoSparkRowBatch createDirectRowBatch()\n+        {\n             return new PrestoSparkRowBatch(\n                     partitionCount,\n                     rowCount,\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSize);\n+                    totalSizeInBytes);\n+        }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSizeInBytes * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSizeInBytes / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSizeInBytes;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81"}, "originalPosition": 216}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2275, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}