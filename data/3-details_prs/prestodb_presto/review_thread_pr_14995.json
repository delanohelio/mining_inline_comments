{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY1MTIxODg1", "number": 14995, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMzozNjowMVrOEX1eIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMzozNjoyMlrOEX1eYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDI4NzY5OnYy", "diffSide": "RIGHT", "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMzozNjowMVrOG_2aJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwNjoxMDowN1rOG_9TIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNDkwMg==", "bodyText": "s/DruidIngestInputSourceLocal/DruidIngestLocalInput/g", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469604902", "createdAt": "2020-08-12T23:36:01Z", "author": {"login": "zhenxiao"}, "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcxNzc5Mw==", "bodyText": "done", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469717793", "createdAt": "2020-08-13T06:10:07Z", "author": {"login": "beinan"}, "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNDkwMg=="}, "originalCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "originalPosition": 278}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDI4ODMyOnYy", "diffSide": "RIGHT", "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMzozNjoyMlrOG_2ajw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwNjoxMDoxNlrOG_9TTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNTAwNw==", "bodyText": "s/DruidIngestInputSourceHDFS/DruidIngestHDFSInput/g", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469605007", "createdAt": "2020-08-12T23:36:22Z", "author": {"login": "zhenxiao"}, "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal\n+            implements DruidIngestInputSource\n+    {\n+        private final String type;\n+        private final String baseDir;\n+        private final String filter;\n+\n+        public DruidIngestInputSourceLocal(String type, String baseDir, String filter)\n+        {\n+            this.type = type;\n+            this.baseDir = baseDir;\n+            this.filter = filter;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"baseDir\")\n+        public String getBaseDir()\n+        {\n+            return baseDir;\n+        }\n+\n+        @JsonProperty(\"filter\")\n+        public String getFilter()\n+        {\n+            return filter;\n+        }\n+    }\n+\n+    public static class DruidIngestInputSourceHDFS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "originalPosition": 311}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcxNzgzNw==", "bodyText": "done", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469717837", "createdAt": "2020-08-13T06:10:16Z", "author": {"login": "beinan"}, "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal\n+            implements DruidIngestInputSource\n+    {\n+        private final String type;\n+        private final String baseDir;\n+        private final String filter;\n+\n+        public DruidIngestInputSourceLocal(String type, String baseDir, String filter)\n+        {\n+            this.type = type;\n+            this.baseDir = baseDir;\n+            this.filter = filter;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"baseDir\")\n+        public String getBaseDir()\n+        {\n+            return baseDir;\n+        }\n+\n+        @JsonProperty(\"filter\")\n+        public String getFilter()\n+        {\n+            return filter;\n+        }\n+    }\n+\n+    public static class DruidIngestInputSourceHDFS", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNTAwNw=="}, "originalCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "originalPosition": 311}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2279, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}