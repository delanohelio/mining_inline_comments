{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE2ODA0MzI0", "number": 14515, "title": "Run AbstractTestQueries suite with Presto on Spark", "bodyText": "== NO RELEASE NOTE ==", "createdAt": "2020-05-12T15:33:02Z", "url": "https://github.com/prestodb/presto/pull/14515", "merged": true, "mergeCommit": {"oid": "e906ea6582570d134634397b978d8a4839acfa6e"}, "closed": true, "closedAt": "2020-05-22T14:02:28Z", "author": {"login": "arhimondr"}, "timelineItems": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcgmQFOgBqjMzMjgxMzc4NTQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjs60ZAFqTQxNjY5NTUxMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9c17ab076d4d98f3f7c8c80543a7f3dd42a7b1a1", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/9c17ab076d4d98f3f7c8c80543a7f3dd42a7b1a1", "committedDate": "2020-05-12T15:30:38Z", "message": "Collect input rdds in parallel"}, "afterCommit": {"oid": "6a06981469a8a3b97894269dcc3b7d764b28281f", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/6a06981469a8a3b97894269dcc3b7d764b28281f", "committedDate": "2020-05-12T15:45:20Z", "message": "Collect input rdds in parallel"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "005e47ea12ec3aaa1121624f9dd1f052ab2e75fb", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/005e47ea12ec3aaa1121624f9dd1f052ab2e75fb", "committedDate": "2020-05-12T16:04:37Z", "message": "Do not close testing SparkContext that is still in use"}, "afterCommit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/d856452d6c2caa1c65a6df21dd732ae36d40b297", "committedDate": "2020-05-13T22:29:27Z", "message": "Do not close testing SparkContext that is still in use"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExNDY5NDcx", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-411469471", "createdAt": "2020-05-14T05:13:41Z", "commit": {"oid": "e6295347080e36a5ce956be7009876a30c08636b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNToxMzo0MVrOGVMTNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNToxMzo0MVrOGVMTNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3NDgwNw==", "bodyText": "When the fragment doesn't have any child, it cannot have broadcast input as well right?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r424874807", "createdAt": "2020-05-14T05:13:41Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -225,7 +227,15 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n         SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-        if (rddInputs.size() == 1) {\n+        if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n+                    .mapPartitionsToPair(createTaskProcessor(\n+                            executorFactoryProvider,\n+                            taskStatsCollector,\n+                            toTaskProcessorBroadcastInputs(broadcastInputs)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e6295347080e36a5ce956be7009876a30c08636b"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExNDc0ODAz", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-411474803", "createdAt": "2020-05-14T05:29:35Z", "commit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNToyOTozNVrOGVMkRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNTozNzo1MFrOGVMtJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3OTE3Mg==", "bodyText": "My understanding Spark Java API avoids directly using scala.reflect.ClassTag$.MODULE$,  and that's why JavaRDDLike is introduced with automatic wrap of class tags, e.g. https://github.com/apache/spark/blob/fd2d55c9919ece5463377bc6f45f2cdb8bf90515/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala#L93\nIn fact, in JavaRDDLike, it use \"fakeClassTag\"  to \"keep ClassTags out of the external Java API\", as explained here: https://github.com/apache/spark/blob/fd2d55c9919ece5463377bc6f45f2cdb8bf90515/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala#L754-L764\n   * Produces a ClassTag[T], which is actually just a casted ClassTag[AnyRef].\n   *\n   * This method is used to keep ClassTags out of the external Java API, as the Java compiler\n   * cannot produce them automatically. While this ClassTag-faking does please the compiler,\n   * it can cause problems at runtime if the Scala API relies on ClassTags for correctness.\n   *\n   * Often, though, a ClassTag[AnyRef] will not lead to incorrect behavior, just worse performance\n   * or security issues. For instance, an Array[AnyRef] can hold any type T, but may lose primitive\n   * specialization.\n\ncc @sameeragarwal", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r424879172", "createdAt": "2020-05-14T05:29:35Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);\n-        }\n-        if (rddInputs.size() == 2) {\n-            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n-            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()\n-                    .map(rddInputs::get)\n-                    .collect(toImmutableList());\n-            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            fragmentIds.get(0).toString(),\n-                            fragmentIds.get(1).toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return JavaPairRDD.fromJavaRDD(\n-                    rdds.get(0).zipPartitions(\n-                            rdds.get(1),\n-                            taskProcessor));\n+\n+        ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        for (Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n+            fragmentIds.add(input.getKey().toString());\n+            rdds.add(input.getValue().rdd());\n         }\n \n-        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+                executorFactoryProvider,\n+                serializedTaskDescriptor,\n+                fragmentIds.build(),\n+                taskStatsCollector,\n+                toTaskProcessorBroadcastInputs(broadcastInputs));\n+\n+        return JavaPairRDD.fromRDD(\n+                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                classTag(Integer.class),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MTQ0Nw==", "bodyText": "ZippedPartitionsBaseRDD seems to be a private[static] class : https://github.com/apache/spark/blob/fd2d55c9919ece5463377bc6f45f2cdb8bf90515/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala#L45  Shall we extend it?\ncc @sameeragarwal", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r424881447", "createdAt": "2020-05-14T05:37:50Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMTM1Njg4", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413135688", "createdAt": "2020-05-17T07:40:18Z", "commit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwNzo0MDoxOFrOGWe9Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwODowNTo0NVrOGWfFjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTA3MA==", "bodyText": "nit: I think we usually use Map.Entry<...> ?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426229070", "createdAt": "2020-05-17T07:40:18Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);\n-        }\n-        if (rddInputs.size() == 2) {\n-            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n-            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()\n-                    .map(rddInputs::get)\n-                    .collect(toImmutableList());\n-            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            fragmentIds.get(0).toString(),\n-                            fragmentIds.get(1).toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return JavaPairRDD.fromJavaRDD(\n-                    rdds.get(0).zipPartitions(\n-                            rdds.get(1),\n-                            taskProcessor));\n+\n+        ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        for (Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTI1OA==", "bodyText": "nit: for line 401 to 409, we could also consider use an imperative way (instead of using stream API) -- especially collectAndDestroyDependencies has side effect . No strong opinion here.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426229258", "createdAt": "2020-05-17T07:42:29Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -400,14 +398,20 @@ private PrestoSparkQueryExecution(\n                         tableWriteInfo);\n                 SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-                SubPlan child = getOnlyElement(root.getChildren());\n-                RddAndMore rdd = createRdd(child);\n-                List<Tuple2<Integer, PrestoSparkRow>> sparkDriverInput = rdd.collectAndDestroyDependencies();\n+                Map<PlanFragmentId, RddAndMore> inputRdds = root.getChildren().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTY5Mg==", "bodyText": "int: Maybe just using for-loop + ArrayList here? My feeling is using Stream API here doesn't help make the code easy to read or reasoning (you still need to use i which is almost for-loop). But it's just a personal preference.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426229692", "createdAt": "2020-05-17T07:47:56Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds;\n+    private Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> function;\n+\n+    public PrestoSparkZipRdd(\n+            SparkContext context,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds,\n+            Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> function)\n+    {\n+        super(\n+                context,\n+                getRDDSequence(requireNonNull(rdds, \"rdds is null\")),\n+                false,\n+                scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class));\n+        this.rdds = rdds;\n+        this.function = context.clean(requireNonNull(function, \"function is null\"), true);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Seq<RDD<?>> getRDDSequence(List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds)\n+    {\n+        return asScalaBuffer((List<RDD<?>>) (List<?>) new ArrayList<>(rdds)).toSeq();\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        List<Iterator<Tuple2<Integer, PrestoSparkRow>>> iterators = unmodifiableList(IntStream.range(0, rdds.size())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMDU3MA==", "bodyText": "Note you are also using PrestoSparkZipRdd even there is only one RDD input. I am wondering if we should only use PrestoSparkZipRdd when there are more than two inputs?  Since it seems reasonable to specialize when there is only one RDD input.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426230570", "createdAt": "2020-05-17T07:58:39Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMTE4Mg==", "bodyText": "Instead of using a general function type. I am wondering if we can have FlatMapFunctionNSameInputType<Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>>? Essentially thinking about generalizing FlatMapFunction2 to N inputs with same type :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426231182", "createdAt": "2020-05-17T08:05:45Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -62,59 +62,37 @@ private TaskProcessors() {}\n         };\n     }\n \n-    public static PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> createTaskProcessor(\n+    public static Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> createTaskProcessor(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjgxMDk3", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413281097", "createdAt": "2020-05-18T05:32:13Z", "commit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjgyODA1", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413282805", "createdAt": "2020-05-18T05:37:43Z", "commit": {"oid": "9352a0d6db268ce76afd8bf68d5786e475df15eb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwNTozNzo0NFrOGWoGow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwNTozNzo0NFrOGWoGow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM3ODkxNQ==", "bodyText": "Before, when will this branch being triggered? -- one case I can think is to gather all the table write information ?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426378915", "createdAt": "2020-05-18T05:37:44Z", "author": {"login": "wenleix"}, "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "diffHunk": "@@ -1202,9 +1205,30 @@ public PlanWithProperties visitUnion(UnionNode node, PreferredProperties parentP\n                 // children partitioning and don't GATHER partitioned inputs\n                 // TODO: add FIXED_ARBITRARY_DISTRIBUTION support on non empty singleNodeChildren\n                 if (!parentPartitioningPreference.isPresent() || parentPartitioningPreference.get().isDistributed()) {\n-                    return arbitraryDistributeUnion(node, distributedChildren, distributedOutputLayouts);\n+                    // TODO: can we insert LOCAL exchange for one child SOURCE distributed and another HASH distributed?\n+                    if (getNumberOfTableScans(distributedChildren) == 0 && isSameOrSystemCompatiblePartitions(extractRemoteExchangePartitioningHandles(distributedChildren))) {\n+                        // No source distributed child, we can use insert LOCAL exchange\n+                        // TODO: if all children have the same partitioning, pass this partitioning to the parent\n+                        // instead of \"arbitraryPartition\".\n+                        return new PlanWithProperties(node.replaceChildren(distributedChildren));\n+                    }\n+                    else if (preferDistributedUnion) {\n+                        // Presto currently can not execute stage that has multiple table scans, so in that case\n+                        // we have to insert REMOTE exchange with FIXED_ARBITRARY_DISTRIBUTION instead of local exchange\n+                        return new PlanWithProperties(\n+                                new ExchangeNode(\n+                                        idAllocator.getNextId(),\n+                                        REPARTITION,\n+                                        REMOTE_STREAMING,\n+                                        new PartitioningScheme(Partitioning.create(FIXED_ARBITRARY_DISTRIBUTION, ImmutableList.of()), node.getOutputVariables()),\n+                                        distributedChildren,\n+                                        distributedOutputLayouts,\n+                                        false,\n+                                        Optional.empty()));\n+                    }\n                 }\n-\n+                // TODO: We should support multiple table scans in a single fragment for efficient union implementation\n+                // TODO: Multiple table scans are already partially supported (for partitioned (bucketed) tables)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9352a0d6db268ce76afd8bf68d5786e475df15eb"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjg0MTc3", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413284177", "createdAt": "2020-05-18T05:42:18Z", "commit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjg2MDk5", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413286099", "createdAt": "2020-05-18T05:48:13Z", "commit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjg2ODY3", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413286867", "createdAt": "2020-05-18T05:50:24Z", "commit": {"oid": "50535a959c402218966f62b4c6f7165f41c03ee2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwNTo1MDoyNFrOGWoUkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwNTo1MDoyNFrOGWoUkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM4MjQ4Mw==", "bodyText": "There is no Airlift MoreFutures utility methods to do this? :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426382483", "createdAt": "2020-05-18T05:50:24Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -506,6 +517,34 @@ private QueryInfo createQueryInfo(Optional<Throwable> failure)\n             // TODO: create query info\n             return null;\n         }\n+\n+        private static <T> void waitFor(Collection<Future<T>> futures)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50535a959c402218966f62b4c6f7165f41c03ee2"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjg3NDg2", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413287486", "createdAt": "2020-05-18T05:52:11Z", "commit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjg3Nzcz", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-413287773", "createdAt": "2020-05-18T05:53:02Z", "commit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d856452d6c2caa1c65a6df21dd732ae36d40b297", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/d856452d6c2caa1c65a6df21dd732ae36d40b297", "committedDate": "2020-05-13T22:29:27Z", "message": "Do not close testing SparkContext that is still in use"}, "afterCommit": {"oid": "2d9fa7aa5a3acc512d6d3b0490b96c38d85a9db9", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/2d9fa7aa5a3acc512d6d3b0490b96c38d85a9db9", "committedDate": "2020-05-19T00:19:21Z", "message": "Do not close testing SparkContext that is still in use"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0MTE5MjEy", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-414119212", "createdAt": "2020-05-19T05:50:03Z", "commit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MDowNFrOGXQvEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MDowNFrOGXQvEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NDYyNA==", "bodyText": "Maybe explain now there are two situations UNION will be executed by gathering into the same worker\n\nParent node doesn't prefer distributed partitioning (e.g. TableFinish)\nParent node prefers distributed partitioning  but prefer_distributed_union is disabled\n\nFor the second situation, we should support multiple table scans in a single fragment.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427044624", "createdAt": "2020-05-19T05:50:04Z", "author": {"login": "wenleix"}, "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "diffHunk": "@@ -1202,9 +1205,30 @@ public PlanWithProperties visitUnion(UnionNode node, PreferredProperties parentP\n                 // children partitioning and don't GATHER partitioned inputs\n                 // TODO: add FIXED_ARBITRARY_DISTRIBUTION support on non empty singleNodeChildren\n                 if (!parentPartitioningPreference.isPresent() || parentPartitioningPreference.get().isDistributed()) {\n-                    return arbitraryDistributeUnion(node, distributedChildren, distributedOutputLayouts);\n+                    // TODO: can we insert LOCAL exchange for one child SOURCE distributed and another HASH distributed?\n+                    if (getNumberOfTableScans(distributedChildren) == 0 && isSameOrSystemCompatiblePartitions(extractRemoteExchangePartitioningHandles(distributedChildren))) {\n+                        // No source distributed child, we can use insert LOCAL exchange\n+                        // TODO: if all children have the same partitioning, pass this partitioning to the parent\n+                        // instead of \"arbitraryPartition\".\n+                        return new PlanWithProperties(node.replaceChildren(distributedChildren));\n+                    }\n+                    else if (preferDistributedUnion) {\n+                        // Presto currently can not execute stage that has multiple table scans, so in that case\n+                        // we have to insert REMOTE exchange with FIXED_ARBITRARY_DISTRIBUTION instead of local exchange\n+                        return new PlanWithProperties(\n+                                new ExchangeNode(\n+                                        idAllocator.getNextId(),\n+                                        REPARTITION,\n+                                        REMOTE_STREAMING,\n+                                        new PartitioningScheme(Partitioning.create(FIXED_ARBITRARY_DISTRIBUTION, ImmutableList.of()), node.getOutputVariables()),\n+                                        distributedChildren,\n+                                        distributedOutputLayouts,\n+                                        false,\n+                                        Optional.empty()));\n+                    }\n                 }\n-\n+                // TODO: We should support multiple table scans in a single fragment for efficient union implementation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0MTE5NjYz", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-414119663", "createdAt": "2020-05-19T05:51:12Z", "commit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MToxMlrOGXQwoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MToxMlrOGXQwoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTAyNQ==", "bodyText": "we can set this property to be hidden :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427045025", "createdAt": "2020-05-19T05:51:12Z", "author": {"login": "wenleix"}, "path": "presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java", "diffHunk": "@@ -762,6 +763,11 @@ public SystemSessionProperties(\n                         OPTIMIZE_COMMON_SUB_EXPRESSIONS,\n                         \"Extract and compute common sub-expressions in projection\",\n                         featuresConfig.isOptimizeCommonSubExpressions(),\n+                        false),\n+                booleanProperty(\n+                        PREFER_DISTRIBUTED_UNION,\n+                        \"Prefer distributed union\",\n+                        featuresConfig.isPreferDistributedUnion(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0MTIwNTAw", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-414120500", "createdAt": "2020-05-19T05:53:21Z", "commit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MzoyMlrOGXQzUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MzoyMlrOGXQzUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTcxNA==", "bodyText": "I am actually thinking to set PREFER_DISTRIBUTED_UNION  to be false in production -- this would fail user query loudly instead of make it run but somehow very slow...\nPREFER_DISTRIBUTED_UNION should only be used to entertain test.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427045714", "createdAt": "2020-05-19T05:53:22Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkSettingsRequirements.java", "diffHunk": "@@ -49,6 +50,7 @@ public void verify(SparkContext sparkContext, Session session)\n                 \"grouped execution is not supported\");\n         verify(!isRedistributeWrites(session), \"redistribute writes is not supported\");\n         verify(!isScaleWriters(session), \"scale writes is not supported\");\n+        verify(!isPreferDistributedUnion(session), \"prefer distributed union is expected to be disabled\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0OTg0MTk2", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-414984196", "createdAt": "2020-05-20T04:29:55Z", "commit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDoyOTo1NVrOGX63DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDo0Nzo1MVrOGX7HAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNDc5Nw==", "bodyText": "So wrap the encoded execution info with \"|\" is to allow being parsed out in PrestoSparkExecutionExceptionFactory ?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427734797", "createdAt": "2020-05-20T04:29:55Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionException.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+public abstract class PrestoSparkExecutionException\n+        extends RuntimeException\n+{\n+    protected PrestoSparkExecutionException(String message, String encodedExecutionFailureInfo, Throwable cause)\n+    {\n+        super(formatExceptionMessage(message, encodedExecutionFailureInfo), cause);\n+    }\n+\n+    private static String formatExceptionMessage(String message, String encodedExecutionFailureInfo)\n+    {\n+        return message + \" | ExecutionFailureInfo[\" + encodedExecutionFailureInfo + \"] |\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjI1Mw==", "bodyText": "nit: What about toPrestoSparkExecutionException?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427736253", "createdAt": "2020-05-20T04:36:19Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjM1NQ==", "bodyText": "is compression necessary?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427736355", "createdAt": "2020-05-20T04:36:48Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjcwNA==", "bodyText": "nit: what about extractExecutionFailureInfo?  -- ditto for other 2 tryDecode method.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427736704", "createdAt": "2020-05-20T04:38:30Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);\n+        String encoded = Base64.getEncoder().encodeToString(compressed);\n+        if (isRetryable(failureInfo)) {\n+            return new PrestoSparkRetryableExecutionException(throwable.getMessage(), encoded, throwable);\n+        }\n+        else {\n+            return new PrestoSparkNonRetryableExecutionException(throwable.getMessage(), encoded, throwable);\n+        }\n+    }\n+\n+    public Optional<ExecutionFailureInfo> tryDecode(SparkException sparkException)\n+    {\n+        return tryDecode(sparkException.getMessage());\n+    }\n+\n+    public Optional<ExecutionFailureInfo> tryDecode(PrestoSparkExecutionException executionException)\n+    {\n+        return tryDecode(executionException.getMessage());\n+    }\n+\n+    private Optional<ExecutionFailureInfo> tryDecode(String message)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNzM3NQ==", "bodyText": "nit: encodedExecutionFailureInfo", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427737375", "createdAt": "2020-05-20T04:41:29Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);\n+        String encoded = Base64.getEncoder().encodeToString(compressed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczODg4Mw==", "bodyText": "Instead of doing instanceof here, shouldn't we try to catch SparkException and PrestoSparkExecutionException in line 357? -- is that the more \"Java\" way to handle exceptions?\nIn order to do that, you probably need to refactor line 358 to 370 into something like tryRollbackAndLog", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427738883", "createdAt": "2020-05-20T04:47:51Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -341,24 +354,36 @@ private PrestoSparkQueryExecution(\n                 rddResults = doExecute(plan);\n                 commit(session, transactionManager);\n             }\n-            catch (RuntimeException executionFailure) {\n+            catch (Exception executionFailure) {\n                 try {\n                     rollback(session, transactionManager);\n                 }\n                 catch (RuntimeException rollbackFailure) {\n-                    if (executionFailure != rollbackFailure) {\n-                        executionFailure.addSuppressed(rollbackFailure);\n-                    }\n+                    log.error(rollbackFailure, \"Encountered error when performing rollback\");\n                 }\n+\n                 try {\n                     queryCompletedEvent(Optional.of(executionFailure));\n                 }\n                 catch (RuntimeException eventFailure) {\n-                    if (executionFailure != eventFailure) {\n-                        executionFailure.addSuppressed(eventFailure);\n+                    log.error(eventFailure, \"Error publishing query completed event\");\n+                }\n+\n+                if (executionFailure instanceof SparkException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 143}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d5e65d0f143d601748ad250a8d98a81f01f3a502", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/d5e65d0f143d601748ad250a8d98a81f01f3a502", "committedDate": "2020-05-20T14:48:41Z", "message": "Implement values in Presto on Spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40173ef09eca37f3b28d015c282281a3fe64746d", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/40173ef09eca37f3b28d015c282281a3fe64746d", "committedDate": "2020-05-20T14:48:41Z", "message": "Implement N-Way join and UNION ALL"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "878f76ee066a8f25d3e9f57461184b44ac0827e9", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/878f76ee066a8f25d3e9f57461184b44ac0827e9", "committedDate": "2020-05-20T15:48:16Z", "message": "Add PrestoSparkSettingsRequirements"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9d94ea94aa73bf577bf7be76d5acfcfb8ce2242", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/e9d94ea94aa73bf577bf7be76d5acfcfb8ce2242", "committedDate": "2020-05-20T15:48:56Z", "message": "Support broadcast union\n\nPresto on Spark does not support round robin partitioning.\nRound robin partitioning is very expensive in Spark and should be avoided.\nFor now unions are collected into a single node. Given there's not that many\nquery shapes that would result in such plan it should be fine as a temporary\nsolution. However long term we should support multiple table scans in the\nfragment to avoid such suboptimal union planning."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4740a10cb3fab3d3536564c504402d79db5fdfea", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/4740a10cb3fab3d3536564c504402d79db5fdfea", "committedDate": "2020-05-20T15:48:58Z", "message": "Support nulls in results for Presto on Spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5161a011737217a3e1482babd45c0c4f93d1ddb3", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/5161a011737217a3e1482babd45c0c4f93d1ddb3", "committedDate": "2020-05-20T15:57:01Z", "message": "Propagate exceptions from executor to driver\n\nSpark embeds the exception thrown on executor into SparkException as a message.\nSo we have to manually encode and decode the actual Presto exception."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d9cdfdb8a6af2037112b3315aefc9868de88fe7", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/8d9cdfdb8a6af2037112b3315aefc9868de88fe7", "committedDate": "2020-05-20T15:57:04Z", "message": "Run AbstractTestQueries suite with Presto on Spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "570c9d3f9d178de1e1def2ce44fcb6a716918daa", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/570c9d3f9d178de1e1def2ce44fcb6a716918daa", "committedDate": "2020-05-20T15:57:04Z", "message": "Collect input rdds in parallel"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "90a2798df8d7a1f8e3901e6ee124492050e63e4d", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/90a2798df8d7a1f8e3901e6ee124492050e63e4d", "committedDate": "2020-05-20T15:57:04Z", "message": "Do not close testing SparkContext that is still in use\n\nMultiple SparkContext per JVM is not supported, thus we have to\nmaintain a single SparkContext. This commit prevents the context\nfrom being closed while it is still in use by a SparkQueryRunner\ninstance."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2d9fa7aa5a3acc512d6d3b0490b96c38d85a9db9", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/2d9fa7aa5a3acc512d6d3b0490b96c38d85a9db9", "committedDate": "2020-05-19T00:19:21Z", "message": "Do not close testing SparkContext that is still in use"}, "afterCommit": {"oid": "90a2798df8d7a1f8e3901e6ee124492050e63e4d", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/90a2798df8d7a1f8e3901e6ee124492050e63e4d", "committedDate": "2020-05-20T15:57:04Z", "message": "Do not close testing SparkContext that is still in use\n\nMultiple SparkContext per JVM is not supported, thus we have to\nmaintain a single SparkContext. This commit prevents the context\nfrom being closed while it is still in use by a SparkQueryRunner\ninstance."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2Njk1NTEz", "url": "https://github.com/prestodb/presto/pull/14515#pullrequestreview-416695513", "createdAt": "2020-05-22T07:13:30Z", "commit": {"oid": "90a2798df8d7a1f8e3901e6ee124492050e63e4d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1626, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}