{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY1MTIxODg1", "number": 14995, "title": "Add support for druid data ingestion", "bodyText": "Add support for 'insert into select'\nAdd support for CTAS\nAdd support for native batch (parallel) ingestion\n\nIngest data from a local holder\n\n\n\nIngest by CTAS\ncreate table new_dataset7 as select localtimestamp as __time, 'beinan' as name, bigint '18' as age, double '188.88' as height;\npresto:druid> desc new_dataset7;\n Column |   Type    | Extra | Comment\n--------+-----------+-------+---------\n __time | timestamp |       |\n age    | bigint    |       |\n height | double    |       |\n name   | varchar   |       |\n\nIngest by Insert:\n insert into new_dataset7 select localtimestamp as __time, bigint '18' as age, double '288.88' as height, \"Mike\" as name;\nor even more\ninsert into new_dataset7 SELECT cast(date_column as timestamp),bigint '18' as age, double '188.88' as height, 'aaa' as name FROM (VALUES (SEQUENCE(FROM_ISO8601_DATE('2010-01-01'), FROM_ISO8601_DATE('2020-12-31'), INTERVAL '1' DAY) ) ) AS t1(date_array) CROSS JOIN UNNEST(date_array) AS t2(date_column);\nLimitation:\nCurrently only supports limited data types such as timestamp, varchar, bigint, double and float.\n== RELEASE NOTES ==\n\nDruid Changes\n* Add support for data ingestion", "createdAt": "2020-08-09T08:05:03Z", "url": "https://github.com/prestodb/presto/pull/14995", "merged": true, "mergeCommit": {"oid": "de096455a8990263b4bd89e78be6d4d3aea524c4"}, "closed": true, "closedAt": "2020-08-14T18:19:11Z", "author": {"login": "beinan"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc80YkLAH2gAyNDY1MTIxODg1OmJjNjZmY2MyNTlmMGU0ODM2Y2ZmYmNhYzk4YTU0ZTBkNzVlYjY0OTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-aTVNAFqTQ2NjQ4NjEyNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "bc66fcc259f0e4836cffbcac98a54e0d75eb6493", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/bc66fcc259f0e4836cffbcac98a54e0d75eb6493", "committedDate": "2020-08-08T08:03:26Z", "message": "Add druid table insert/ingestion skeleton code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b0d139453a6a714aaef44df259729a191c4877d", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/6b0d139453a6a714aaef44df259729a191c4877d", "committedDate": "2020-08-08T08:03:26Z", "message": "Add ingestion storage path to DruidConfig"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08e66d5b2ebbc3f01c0c60d04acd24c1e15afa9d", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/08e66d5b2ebbc3f01c0c60d04acd24c1e15afa9d", "committedDate": "2020-08-08T08:03:26Z", "message": "Write druid page data to gzip files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "043581ebf193294d3940fcef89fa414443e64850", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/043581ebf193294d3940fcef89fa414443e64850", "committedDate": "2020-08-09T07:55:21Z", "message": "Implement sending ingestion task to druid"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b41372506afc7105062cd01f3ff428024aa76a3", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/8b41372506afc7105062cd01f3ff428024aa76a3", "committedDate": "2020-08-10T03:23:15Z", "message": "Implement druid ingestion by CTAS"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MzQ1NjU5", "url": "https://github.com/prestodb/presto/pull/14995#pullrequestreview-466345659", "createdAt": "2020-08-12T23:36:01Z", "commit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMzozNjowMVrOG_2aJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMzozNjoyMlrOG_2ajw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNDkwMg==", "bodyText": "s/DruidIngestInputSourceLocal/DruidIngestLocalInput/g", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469604902", "createdAt": "2020-08-12T23:36:01Z", "author": {"login": "zhenxiao"}, "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYwNTAwNw==", "bodyText": "s/DruidIngestInputSourceHDFS/DruidIngestHDFSInput/g", "url": "https://github.com/prestodb/presto/pull/14995#discussion_r469605007", "createdAt": "2020-08-12T23:36:22Z", "author": {"login": "zhenxiao"}, "path": "presto-druid/src/main/java/com/facebook/presto/druid/ingestion/DruidIngestTask.java", "diffHunk": "@@ -0,0 +1,351 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.druid.ingestion;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.List;\n+\n+public class DruidIngestTask\n+{\n+    public static final String TASK_TYPE_INDEX_PARALLEL = \"index_parallel\";\n+    public static final String INPUT_FORMAT_JSON = \"json\";\n+    public static final String DEFAULT_INPUT_FILE_FILTER = \"*.json.gz\";\n+\n+    private final String type;\n+    private final DruidIngestSpec spec;\n+\n+    private DruidIngestTask(String type, DruidIngestSpec spec)\n+    {\n+        this.type = type;\n+        this.spec = spec;\n+    }\n+\n+    public static class Builder\n+    {\n+        private String dataSource;\n+        private String timestampColumn;\n+        private List<DruidIngestDimension> dimentions;\n+        private DruidIngestInputSource inputSource;\n+        private boolean appendToExisting;\n+\n+        public Builder withDataSource(String dataSource)\n+        {\n+            this.dataSource = dataSource;\n+            return this;\n+        }\n+\n+        public Builder withTimestampColumn(String timestampColumn)\n+        {\n+            this.timestampColumn = timestampColumn;\n+            return this;\n+        }\n+\n+        public Builder withDimensions(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimentions = dimensions;\n+            return this;\n+        }\n+\n+        public Builder withInputSource(Path baseDir, List<String> dataFileList)\n+        {\n+            switch (baseDir.toUri().getScheme()) {\n+                case \"file\":\n+                    inputSource = new DruidIngestInputSourceLocal(\"local\", baseDir.toString(), DEFAULT_INPUT_FILE_FILTER);\n+                    break;\n+                case \"hdfs\":\n+                    inputSource = new DruidIngestInputSourceHDFS(\"hdfs\", dataFileList);\n+                    break;\n+                default:\n+                    throw new IllegalArgumentException(\"Unsupported ingestion input source:\" + baseDir.toUri().getScheme());\n+            }\n+\n+            return this;\n+        }\n+\n+        public Builder withAppendToExisting(boolean appendToExisting)\n+        {\n+            this.appendToExisting = appendToExisting;\n+            return this;\n+        }\n+\n+        public DruidIngestTask build()\n+        {\n+            DruidIngestDataSchema dataSchema = new DruidIngestDataSchema(\n+                    dataSource,\n+                    new DruidIngestTimestampSpec(timestampColumn),\n+                    new DruidIngestDimensionsSpec(dimentions));\n+            DruidIngestIOConfig ioConfig = new DruidIngestIOConfig(\n+                    TASK_TYPE_INDEX_PARALLEL,\n+                    inputSource,\n+                    new DruidIngestInputFormat(INPUT_FORMAT_JSON),\n+                    appendToExisting);\n+            DruidIngestSpec spec = new DruidIngestSpec(dataSchema, ioConfig);\n+            return new DruidIngestTask(TASK_TYPE_INDEX_PARALLEL, spec);\n+        }\n+    }\n+\n+    @JsonProperty(\"type\")\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty(\"spec\")\n+    public DruidIngestSpec getSpec()\n+    {\n+        return spec;\n+    }\n+\n+    public String toJson()\n+    {\n+        return JsonCodec.jsonCodec(DruidIngestTask.class).toJson(this);\n+    }\n+\n+    public static class DruidIngestSpec\n+    {\n+        private final DruidIngestDataSchema dataSchema;\n+        private final DruidIngestIOConfig ioConfig;\n+\n+        public DruidIngestSpec(DruidIngestDataSchema dataSchema, DruidIngestIOConfig ioConfig)\n+        {\n+            this.dataSchema = dataSchema;\n+            this.ioConfig = ioConfig;\n+        }\n+\n+        @JsonProperty(\"dataSchema\")\n+        public DruidIngestDataSchema getDataSchema()\n+        {\n+            return dataSchema;\n+        }\n+\n+        @JsonProperty(\"ioConfig\")\n+        public DruidIngestIOConfig getIoConfig()\n+        {\n+            return ioConfig;\n+        }\n+    }\n+\n+    public static class DruidIngestDataSchema\n+    {\n+        private final String dataSource;\n+        private final DruidIngestTimestampSpec timestampSpec;\n+        private final DruidIngestDimensionsSpec dimensionsSpec;\n+\n+        public DruidIngestDataSchema(String dataSource, DruidIngestTimestampSpec timestampSpec, DruidIngestDimensionsSpec dimensionsSpec)\n+        {\n+            this.dataSource = dataSource;\n+            this.timestampSpec = timestampSpec;\n+            this.dimensionsSpec = dimensionsSpec;\n+        }\n+\n+        @JsonProperty(\"dataSource\")\n+        public String getDataSource()\n+        {\n+            return dataSource;\n+        }\n+\n+        @JsonProperty(\"timestampSpec\")\n+        public DruidIngestTimestampSpec getTimestampSpec()\n+        {\n+            return timestampSpec;\n+        }\n+\n+        @JsonProperty(\"dimensionsSpec\")\n+        public DruidIngestDimensionsSpec getDimensionsSpec()\n+        {\n+            return dimensionsSpec;\n+        }\n+    }\n+\n+    public static class DruidIngestTimestampSpec\n+    {\n+        private final String column;\n+\n+        public DruidIngestTimestampSpec(String column)\n+        {\n+            this.column = column;\n+        }\n+\n+        @JsonProperty(\"column\")\n+        public String getColumn()\n+        {\n+            return column;\n+        }\n+    }\n+\n+    public static class DruidIngestDimensionsSpec\n+    {\n+        private final List<DruidIngestDimension> dimensions;\n+\n+        public DruidIngestDimensionsSpec(List<DruidIngestDimension> dimensions)\n+        {\n+            this.dimensions = dimensions;\n+        }\n+\n+        @JsonProperty(\"dimensions\")\n+        public List<DruidIngestDimension> getDimensions()\n+        {\n+            return dimensions;\n+        }\n+    }\n+\n+    public static class DruidIngestDimension\n+    {\n+        private final String type;\n+        private final String name;\n+\n+        public DruidIngestDimension(String type, String name)\n+        {\n+            this.type = type;\n+            this.name = name;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"name\")\n+        public String getName()\n+        {\n+            return name;\n+        }\n+    }\n+\n+    public static class DruidIngestIOConfig\n+    {\n+        private final String type;\n+        private final DruidIngestInputSource inputSource;\n+        private final DruidIngestInputFormat inputFormat;\n+        private final boolean appendToExisting;\n+\n+        public DruidIngestIOConfig(\n+                String type,\n+                DruidIngestInputSource inputSource,\n+                DruidIngestInputFormat inputFormat,\n+                boolean appendToExisting)\n+        {\n+            this.type = type;\n+            this.inputSource = inputSource;\n+            this.inputFormat = inputFormat;\n+            this.appendToExisting = appendToExisting;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"inputSource\")\n+        public DruidIngestInputSource getInputSource()\n+        {\n+            return inputSource;\n+        }\n+\n+        @JsonProperty(\"inputFormat\")\n+        public DruidIngestInputFormat getInputFormat()\n+        {\n+            return inputFormat;\n+        }\n+\n+        @JsonProperty(\"appendToExisting\")\n+        public boolean isAppendToExisting()\n+        {\n+            return appendToExisting;\n+        }\n+    }\n+\n+    public interface DruidIngestInputSource\n+    {\n+    }\n+\n+    public static class DruidIngestInputSourceLocal\n+            implements DruidIngestInputSource\n+    {\n+        private final String type;\n+        private final String baseDir;\n+        private final String filter;\n+\n+        public DruidIngestInputSourceLocal(String type, String baseDir, String filter)\n+        {\n+            this.type = type;\n+            this.baseDir = baseDir;\n+            this.filter = filter;\n+        }\n+\n+        @JsonProperty(\"type\")\n+        public String getType()\n+        {\n+            return type;\n+        }\n+\n+        @JsonProperty(\"baseDir\")\n+        public String getBaseDir()\n+        {\n+            return baseDir;\n+        }\n+\n+        @JsonProperty(\"filter\")\n+        public String getFilter()\n+        {\n+            return filter;\n+        }\n+    }\n+\n+    public static class DruidIngestInputSourceHDFS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d"}, "originalPosition": 311}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06eaf54b1c683f7d250f63bfb16087cd2565fd18", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/06eaf54b1c683f7d250f63bfb16087cd2565fd18", "committedDate": "2020-08-13T06:09:21Z", "message": "Implement hdfs input source for druid ingestion"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "927a9d68076cc23f7a02b79c314029fa3dc6805d", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/927a9d68076cc23f7a02b79c314029fa3dc6805d", "committedDate": "2020-08-11T01:09:34Z", "message": "Implement hdfs input source for druid ingestion"}, "afterCommit": {"oid": "06eaf54b1c683f7d250f63bfb16087cd2565fd18", "author": {"user": {"login": "beinan", "name": "Beinan"}}, "url": "https://github.com/prestodb/presto/commit/06eaf54b1c683f7d250f63bfb16087cd2565fd18", "committedDate": "2020-08-13T06:09:21Z", "message": "Implement hdfs input source for druid ingestion"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2NDg2MTI2", "url": "https://github.com/prestodb/presto/pull/14995#pullrequestreview-466486126", "createdAt": "2020-08-13T06:48:02Z", "commit": {"oid": "06eaf54b1c683f7d250f63bfb16087cd2565fd18"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1128, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}