{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwMzc2MTE0", "number": 14559, "title": "Implement bucketed tables for Presto on Spark", "bodyText": "== NO RELEASE NOTE ==", "createdAt": "2020-05-19T21:30:29Z", "url": "https://github.com/prestodb/presto/pull/14559", "merged": true, "mergeCommit": {"oid": "4b0f7c192280a6ffb046766c2fb263ba1f0a4614"}, "closed": true, "closedAt": "2020-06-15T19:51:04Z", "author": {"login": "arhimondr"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABci_lf0gBqjMzNTQ0MzA2Nzk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcrj8BoABqjM0NDU0MDAzNTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4c337b331d248b5b5cef14803011dd31640a3d47", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/4c337b331d248b5b5cef14803011dd31640a3d47", "committedDate": "2020-05-19T19:46:30Z", "message": "Enable collocated join for Presto on Spark"}, "afterCommit": {"oid": "a5297c339800fd28016ddaac141d98258333e1a3", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/a5297c339800fd28016ddaac141d98258333e1a3", "committedDate": "2020-05-20T02:24:00Z", "message": "Reduce spark integration tests memory footprint"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a5297c339800fd28016ddaac141d98258333e1a3", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/a5297c339800fd28016ddaac141d98258333e1a3", "committedDate": "2020-05-20T02:24:00Z", "message": "Reduce spark integration tests memory footprint"}, "afterCommit": {"oid": "f73c3821001ed3e069c2a8e9f301cb8390c596a1", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/f73c3821001ed3e069c2a8e9f301cb8390c596a1", "committedDate": "2020-05-20T02:31:46Z", "message": "Reduce spark integration tests memory footprint"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f73c3821001ed3e069c2a8e9f301cb8390c596a1", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/f73c3821001ed3e069c2a8e9f301cb8390c596a1", "committedDate": "2020-05-20T02:31:46Z", "message": "Reduce spark integration tests memory footprint"}, "afterCommit": {"oid": "76537e8d3d2f427c8048132ce2a35a18202bf3ad", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/76537e8d3d2f427c8048132ce2a35a18202bf3ad", "committedDate": "2020-05-22T21:41:52Z", "message": "Enable collocated join for Presto on Spark"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "76537e8d3d2f427c8048132ce2a35a18202bf3ad", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/76537e8d3d2f427c8048132ce2a35a18202bf3ad", "committedDate": "2020-05-22T21:41:52Z", "message": "Enable collocated join for Presto on Spark"}, "afterCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/ecb7c56cc35eb5f2468c837483e8db92e0a19de2", "committedDate": "2020-06-02T18:30:30Z", "message": "Enable collocated join for Presto on Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NzkzMDA2", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-425793006", "createdAt": "2020-06-07T07:01:14Z", "commit": {"oid": "d3d04114b4123d0d3abb7bf84a090f25f543ad0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzowMToxNVrOGgHoCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzowMToxNVrOGgHoCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMjU1Mg==", "bodyText": "One theoretic caveat here is the number of buckets, returned by getBucketNodeMap and getBucketCount can be different if the number of nodes changed during the two method calls. Since nodeManager.getRequiredWorkerNodes() will return different list.\nHowever, this would only happen for 3 testing connector: BlackHole, TPCDS, TPCH as the \"partitioning handle\" is somewhat artificial in these connectors. For \"real\" connector partitioning handle, the number of buckets should be a constant property in the partitioning handle, thus. it's not a problem.\nI think the right long term solution is to allow connector partitioning handle to return the number of buckets :). So the current implementation looks OK.\nBTW: can we just throw in getBucketCount for BlackHole, TPCDS and TPCH connector?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436332552", "createdAt": "2020-06-07T07:01:15Z", "author": {"login": "wenleix"}, "path": "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsNodePartitioningProvider.java", "diffHunk": "@@ -70,4 +70,12 @@ public BucketFunction getBucketFunction(ConnectorTransactionHandle transactionHa\n     {\n         throw new UnsupportedOperationException();\n     }\n+\n+    @Override\n+    public int getBucketCount(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorPartitioningHandle partitioningHandle)\n+    {\n+        Set<Node> nodes = nodeManager.getRequiredWorkerNodes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3d04114b4123d0d3abb7bf84a090f25f543ad0c"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NzkzNTE0", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-425793514", "createdAt": "2020-06-07T07:10:56Z", "commit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxMDo1NlrOGgHqtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxMDo1NlrOGgHqtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMzIzOA==", "bodyText": "nit: maybe call it SerializedPrestoSparkTaskSource? -- so when other Presto developers search for TaskSource, they don't need to worry about \"SerializedTaskSource\":", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436333238", "createdAt": "2020-06-07T07:10:56Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/SerializedTaskSource.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import java.io.Serializable;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class SerializedTaskSource", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NzkzNzI5", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-425793729", "createdAt": "2020-06-07T07:14:46Z", "commit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxNDo0NlrOGgHrzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozOToyN1rOGgHykw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMzUxNw==", "bodyText": "nit: do you mean set the number of output partitions?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436333517", "createdAt": "2020-06-07T07:14:46Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -153,20 +152,10 @@ public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonC\n         // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n         checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-        int hashPartitionCount = getHashPartitionCount(session);\n-\n-        // configure number of output partitions\n-        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-        }\n-\n-        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n-            checkArgument(\n-                    fragment.getTableScanSchedulingOrder().isEmpty(),\n-                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n-                    fragment.getId(),\n-                    fragment.getPartitioning());\n+        // set's the number of output partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg==", "bodyText": "As you might guess, I will use more \"plain\" style if,list,for-loop for line 290-303 \ud83d\ude03 . But it's personal taste.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334102", "createdAt": "2020-06-07T07:23:08Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEyNw==", "bodyText": "Add a comment explain why eagerly remove the task sources (GC, right? )", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334127", "createdAt": "2020-06-07T07:23:36Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions\n+                .map(integer -> IntStream.range(0, integer))\n+                .orElseGet(() -> new ArrayList<>(taskSourcesMap.keySet()).stream().mapToInt(Integer::intValue));\n+\n+        List<List<SerializedTaskSource>> partitionedTaskSources = new ArrayList<>();\n+        partitions.forEach(partition -> {\n+            List<TaskSource> taskSources = taskSourcesMap.removeAll(partition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 325}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDM2NA==", "bodyText": "curious: why using SetMultimap instead of ListMultimap?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334364", "createdAt": "2020-06-07T07:27:17Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -330,49 +324,24 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         return splits;\n     }\n \n-    private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> splits, int numTasks)\n+    private SetMultimap<Integer, ScheduledSplit> assignSplitsToTasks(Session session, PartitioningHandle partitioning, List<ScheduledSplit> splits)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDQ1Ng==", "bodyText": "nit: why not just using a for-each loop? ;)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334456", "createdAt": "2020-06-07T07:28:38Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 301}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDg2OA==", "bodyText": "So now the anonymous lambda created in legacy TaskProcessors now has dedicated named class, this is great.\nCurious why this class has to stay in classloader_interface ?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334868", "createdAt": "2020-06-07T07:33:47Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDg3NQ==", "bodyText": "maybe call it process? :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334875", "createdAt": "2020-06-07T07:33:56Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDk3NQ==", "bodyText": "should there be a check that fragmentIds.size() == inputs.size() ?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334975", "createdAt": "2020-06-07T07:35:30Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();\n+        for (int i = 0; i < fragmentIds.size(); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNTI1MQ==", "bodyText": "For now Looks like it's a wrapper over ParallelCollectionPartition . I assume it will have non-trivial functionality when colocated join is supported :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436335251", "createdAt": "2020-06-07T07:39:27Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2NzkzNzc5", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-426793779", "createdAt": "2020-06-09T05:14:41Z", "commit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNToxNDo0MlrOGg5BmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNjoxMToxNFrOGg6F5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MTkxMw==", "bodyText": "nit: what about rename rddInputs to shuffleInputs?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437141913", "createdAt": "2020-06-09T05:14:42Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzUxMQ==", "bodyText": "My understanding is this is for special case that the fragment only has VALUES input? -- still we need a dumb PrestoSparkTaskSourceRdd with empty list (instead of making taskSourceRdd to be Optional.empty(). This can be a bit confusing to read.\nIdeally we want to eliminate this dumb PrestoSparkTaskSourceRdd . But if it's required, my suggestions are:\n\nAdd comment explain why we cannot make taskSourceRdd to be simply Optional.empty\nMaybe have a factory method such as PrestoSparkTaskSourceRdd.createEmptyTaskSourceRdd?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437143511", "createdAt": "2020-06-09T05:20:36Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NDg1OA==", "bodyText": "nit : what about getShuffleInputIterators", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437144858", "createdAt": "2020-06-09T05:25:33Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODAxOA==", "bodyText": "why not just using PrestoSparkTaskSourceRdd here?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437148018", "createdAt": "2020-06-09T05:36:32Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODg0OA==", "bodyText": "unpartitioned split / partitioned split is kind of overloaded in Presto. Since in the task execution context, it often means \"splits from table scan source\" vs. \"splits from exchange source\".\nWhat about saying \"splits from unbucketed table\" vs. \"splits from bucketed table\"", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437148848", "createdAt": "2020-06-09T05:39:11Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -330,49 +337,61 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         return splits;\n     }\n \n-    private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> splits, int numTasks)\n+    private SetMultimap<Integer, ScheduledSplit> assignSplitsToTasks(Session session, PartitioningHandle partitioning, List<ScheduledSplit> splits)\n     {\n-        checkArgument(numTasks > 0, \"numTasks must be greater then zero\");\n-        List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-        for (int i = 0; i < numTasks; i++) {\n-            assignedSplits.add(new ArrayList<>());\n+        // unpartitioned splits", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MTE1NQ==", "bodyText": "Add a comment explain the structure of partitionedTaskSources, also may consider rename it to taskSourcesByTaskId:\n// each element in taskSourcesByTaskId is a list of task sources assigned to the same Spark task. \n// When input tables are unbucketed, task sources are distributed randomly across all tasks. \n// When input tables are bucketed, each bucket in task sources will be assigned to one Spark task, and the assignment is compatible to potential shuffle inputs.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437151155", "createdAt": "2020-06-09T05:46:29Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd\n+        extends RDD<SerializedTaskSource>\n+{\n+    private List<List<SerializedTaskSource>> partitionedTaskSources;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzMxMQ==", "bodyText": "This partitions.get(0) is not easy to understand :).\nI have two thoughts:\n\nJust inline this getTaskSourceIterator method.\nAnother possibility is to consider change the signature of this method to be getTaskSourceIterator(Optional<Partition> partitions, TaskContext context). And let compute to decide whether to pass in Optional.empty() or partitions.get(0).", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437153311", "createdAt": "2020-06-09T05:53:14Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1Njg3Ng==", "bodyText": "Let's add some comment for this class. Feel free to adapt/modify the following:\n/**\n * PrestoSparkTaskRdd represents execution of Presto stage, it contains:\n *  - a list of shuffleInputRdds, each of the corresponding to a child stage.\n *  - an optional taskSourceRdd, which represents *ALL* the table scan inputs in this stage.\n *\n *  Table scan presents when join bucketed table with unbucketed table, for example:\n *        Join\n *        /  \\\n *    Scan  Remote\n *  In this case, bucket to Spark partition mapping has to be consistent with the Spark shuffle partition.\n *\n *  shuffleInputRdds can also be empty when the stage partitioning is SINGLE_DISTRIBUTION.\n *  TODO: Consider have dedicated RDD implementation for SINGLE_DISTRIBUTION.\n *\n *  The broadcast input is encapsulated in taskProcessor.\n */\n\nLet me know if the TODO suggestion (TODO: Consider have dedicated RDD implementation for SINGLE_DISTRIBUTION) makes sense. Since I personally found it difficult to reason when both shuffleInputRdds and taskSourceRdd can be empty.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437156876", "createdAt": "2020-06-09T06:04:12Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NzQ3OQ==", "bodyText": "Here a implicit \"sublist\" is done and makes the code a bit difficult to follow (similar argument to getTaskSourceIterator). Would it make sense to do the partitions.subList(1, ...) in compute?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437157479", "createdAt": "2020-06-09T06:05:50Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)\n+    {\n+        if (inputRdds.isEmpty()) {\n+            return emptyList();\n+        }\n+        int startIndex = taskSourceRdd != null ? 1 : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1Nzc1OQ==", "bodyText": "nit: shuffleInputs?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437157759", "createdAt": "2020-06-09T06:06:35Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ==", "bodyText": "nit: shuffleInputsMap or fragmentIdToShuffleInput?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437159359", "createdAt": "2020-06-09T06:11:08Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM5OQ==", "bodyText": "So essentially, these two information are maintained in different classes:\n\nfragmentIds list is maintained in PrestoSparkTaskProcessor\nshuffleInput list is maintained in PrestoSparkTaskRdd.\n\nAnd these two lists are \"zipped\" when PrestoSparkTaskRdd#compute calls PrestoSparkTaskProcessor#apply.\nI am wondering if this \"zipping\" operation can be done in PrestoSparkTaskRdd#compute? -- Otherwise, it's a bit uneasy to reason the ordering of shuffleInput in PrestoSparkTaskRdd has to be consistent with the ordering of fragmentIds in PrestoSparkTaskProcessor...\nWhen I am reading the code, I have to traverse the code in the following way:\n\nOh, how to guarantee the ordering in this two list are consistent?\nok , shuffleInput is from PrestoSparkTaskRdd, and it's from PrestoSparkRddFactory\nAnd  the same PrestoSparkRddFactory provides the fragment id list when construct PrestoSparkTaskProcessor\nOK so these two lists are consistent...", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437159399", "createdAt": "2020-06-09T06:11:14Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NzA3OTM1", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-427707935", "createdAt": "2020-06-10T04:48:19Z", "commit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NzIwMzQz", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-427720343", "createdAt": "2020-06-10T05:27:10Z", "commit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToyNzoxMFrOGhlWLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToyNzoxMFrOGhlWLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODA3Ng==", "bodyText": "Two questions:\n\nIs it possible some partition is missing from the taskSourcesMap? (e.g. table contains empty bucket)\nWill it happen that expectedNumberOfPartitions is present but different from the number of partitions in taskSourcesMap?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437868076", "createdAt": "2020-06-10T05:27:10Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions\n+                .map(integer -> IntStream.range(0, integer))\n+                .orElseGet(() -> new ArrayList<>(taskSourcesMap.keySet()).stream().mapToInt(Integer::intValue));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 321}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NzIwODM2", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-427720836", "createdAt": "2020-06-10T05:28:35Z", "commit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToyODozNVrOGhlXmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToyODozNVrOGhlXmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODQ0MA==", "bodyText": "Curious what does this numberOfInputPartitions mean? -- Looks like it's decided by the partition number of other RDDs?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437868440", "createdAt": "2020-06-10T05:28:35Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 216}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NzIxNzY1", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-427721765", "createdAt": "2020-06-10T05:31:25Z", "commit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNTozMToyNlrOGhlalw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNTozMTozNlrOGhlayA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2OTIwNw==", "bodyText": "nit: maybe say \"only probe side table is bucketed\"", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437869207", "createdAt": "2020-06-10T05:31:26Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkQueryRunner.java", "diffHunk": "@@ -55,20 +55,77 @@ public void testTableWrite()\n                         \"FROM orders\");\n     }\n \n+    @Test\n+    public void testBucketedTableWrite()\n+    {\n+        // create from bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_1 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders_bucketed\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_1 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+\n+        // create from non bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_2 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_2 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+    }\n+\n     @Test\n     public void testAggregation()\n     {\n         assertQuery(\"select partkey, count(*) c from lineitem where partkey % 10 = 1 group by partkey having count(*) = 42\");\n     }\n \n+    @Test\n+    public void testBucketedAggregation()\n+    {\n+        assertBucketedQuery(\"SELECT orderkey, count(*) c FROM lineitem_bucketed WHERE partkey % 10 = 1 GROUP BY orderkey\");\n+    }\n+\n     @Test\n     public void testJoin()\n     {\n-        assertQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n-                \"FROM lineitem l \" +\n+        assertQuery(\"SELECT l.orderkey, l.linenumber, p.brand \" +\n+                \"FROM lineitem l, part p \" +\n+                \"WHERE l.partkey = p.partkey\");\n+    }\n+\n+    @Test\n+    public void testBucketedJoin()\n+    {\n+        // both tables are bucketed\n+        assertBucketedQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n+                \"FROM lineitem_bucketed l \" +\n+                \"JOIN orders_bucketed o \" +\n+                \"ON l.orderkey = o.orderkey \" +\n+                \"WHERE l.orderkey % 223 = 42 AND l.linenumber = 4 and o.orderstatus = 'O'\");\n+\n+        // only one table is bucketed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2OTI1Ng==", "bodyText": "ditto, \"only build side table is bucketed\"", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437869256", "createdAt": "2020-06-10T05:31:36Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkQueryRunner.java", "diffHunk": "@@ -55,20 +55,77 @@ public void testTableWrite()\n                         \"FROM orders\");\n     }\n \n+    @Test\n+    public void testBucketedTableWrite()\n+    {\n+        // create from bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_1 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders_bucketed\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_1 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+\n+        // create from non bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_2 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_2 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+    }\n+\n     @Test\n     public void testAggregation()\n     {\n         assertQuery(\"select partkey, count(*) c from lineitem where partkey % 10 = 1 group by partkey having count(*) = 42\");\n     }\n \n+    @Test\n+    public void testBucketedAggregation()\n+    {\n+        assertBucketedQuery(\"SELECT orderkey, count(*) c FROM lineitem_bucketed WHERE partkey % 10 = 1 GROUP BY orderkey\");\n+    }\n+\n     @Test\n     public void testJoin()\n     {\n-        assertQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n-                \"FROM lineitem l \" +\n+        assertQuery(\"SELECT l.orderkey, l.linenumber, p.brand \" +\n+                \"FROM lineitem l, part p \" +\n+                \"WHERE l.partkey = p.partkey\");\n+    }\n+\n+    @Test\n+    public void testBucketedJoin()\n+    {\n+        // both tables are bucketed\n+        assertBucketedQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n+                \"FROM lineitem_bucketed l \" +\n+                \"JOIN orders_bucketed o \" +\n+                \"ON l.orderkey = o.orderkey \" +\n+                \"WHERE l.orderkey % 223 = 42 AND l.linenumber = 4 and o.orderstatus = 'O'\");\n+\n+        // only one table is bucketed\n+        assertBucketedQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n+                \"FROM lineitem_bucketed l \" +\n                 \"JOIN orders o \" +\n                 \"ON l.orderkey = o.orderkey \" +\n                 \"WHERE l.orderkey % 223 = 42 AND l.linenumber = 4 and o.orderstatus = 'O'\");\n+\n+        // only one table is bucketed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 71}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/ecb7c56cc35eb5f2468c837483e8db92e0a19de2", "committedDate": "2020-06-02T18:30:30Z", "message": "Enable collocated join for Presto on Spark"}, "afterCommit": {"oid": "af4b4ef3cbaf43b3ad3da003d938ccf862866ae7", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/af4b4ef3cbaf43b3ad3da003d938ccf862866ae7", "committedDate": "2020-06-11T22:46:41Z", "message": "Enable collocated join for Presto on Spark"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "af4b4ef3cbaf43b3ad3da003d938ccf862866ae7", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/af4b4ef3cbaf43b3ad3da003d938ccf862866ae7", "committedDate": "2020-06-11T22:46:41Z", "message": "Enable collocated join for Presto on Spark"}, "afterCommit": {"oid": "44b0d924aea643efdafd6b7e7987c546fc82dc5c", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/44b0d924aea643efdafd6b7e7987c546fc82dc5c", "committedDate": "2020-06-12T12:15:49Z", "message": "Enable collocated join for Presto on Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMTg3OTQ1", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-430187945", "createdAt": "2020-06-14T05:09:27Z", "commit": {"oid": "c6bae2da1afcc0b42d69c95244dbaccf756e899e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwNTowOToyN1rOGjazEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwNTowOToyN1rOGjazEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MjQwMw==", "bodyText": "Maybe emphasize both taskSourceRdd and shuffleInputRdds will be empty when the stage partitioning is SINGLE_DISTRIBUTION ?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439792403", "createdAt": "2020-06-14T05:09:27Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+/**\n+ * PrestoSparkTaskRdd represents execution of Presto stage, it contains:\n+ * - A list of shuffleInputRdds, each of the corresponding to a child stage.\n+ * - An optional taskSourceRdd, which represents ALL table scan inputs in this stage.\n+ * <p>\n+ * Table scan is present when joining a bucketed table with an unbucketed table, for example:\n+ * Join\n+ * /  \\\n+ * Scan  Remote Source\n+ * <p>\n+ * In this case, bucket to Spark partition mapping has to be consistent with the Spark shuffle partition.\n+ * <p>\n+ * shuffleInputRdds can also be empty when the stage partitioning is SINGLE_DISTRIBUTION.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6bae2da1afcc0b42d69c95244dbaccf756e899e"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMTg4NDkw", "url": "https://github.com/prestodb/presto/pull/14559#pullrequestreview-430188490", "createdAt": "2020-06-14T05:22:27Z", "commit": {"oid": "44b0d924aea643efdafd6b7e7987c546fc82dc5c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04a5229d376640a099e909336730aa618c8aec99", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/04a5229d376640a099e909336730aa618c8aec99", "committedDate": "2020-06-15T15:48:08Z", "message": "Add ConnectorNodePartitioningProvider#getBucketCount"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5943042f89e42a1bfe238046b47a892296a7a69", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/f5943042f89e42a1bfe238046b47a892296a7a69", "committedDate": "2020-06-15T17:15:24Z", "message": "Refactor PrestoSparkRddFactory\n\nConsolidate rdd creation for intermediate and source tasks.\n\nExtend PrestoSparkZipRdd into PrestoSparkTaskRdd.\nThe PrestoSparkTaskRdd now accepts PrestoSparkTaskProcessor and\nPrestoSparkTaskSourceRdd as its inputs.\n\nThis refactor is needed to support bucketed tables, as bucketed\nsplits have to be supplied to a task together with the shuffle inputs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a92f02a3e1515e625b4416c6c2699ff81aeef99", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/1a92f02a3e1515e625b4416c6c2699ff81aeef99", "committedDate": "2020-06-15T17:15:24Z", "message": "Add support for bucketed tables in Presto on Spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f0c897bca86fe97c73ab141fb13fea02a48cab8", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/1f0c897bca86fe97c73ab141fb13fea02a48cab8", "committedDate": "2020-06-15T17:15:24Z", "message": "Enable collocated join for Presto on Spark"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "44b0d924aea643efdafd6b7e7987c546fc82dc5c", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/44b0d924aea643efdafd6b7e7987c546fc82dc5c", "committedDate": "2020-06-12T12:15:49Z", "message": "Enable collocated join for Presto on Spark"}, "afterCommit": {"oid": "1f0c897bca86fe97c73ab141fb13fea02a48cab8", "author": {"user": {"login": "arhimondr", "name": "Andrii Rosa"}}, "url": "https://github.com/prestodb/presto/commit/1f0c897bca86fe97c73ab141fb13fea02a48cab8", "committedDate": "2020-06-15T17:15:24Z", "message": "Enable collocated join for Presto on Spark"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1692, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}