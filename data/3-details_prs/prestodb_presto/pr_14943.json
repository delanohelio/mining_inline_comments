{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYxOTMyMDQw", "number": 14943, "title": "Add max Spark input partition count for auto tune", "bodyText": "When scanning over very large tables (hundreds of TBs or even PBs),\nusing the default spark.max-splits-data-size-per-partition might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\nThe long term solution is to set\nspark.max-splits-data-size-per-partition to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration.\nTest plan - Tested with a large-scale Spark job.\n== NO RELEASE NOTE ==", "createdAt": "2020-08-03T04:19:22Z", "url": "https://github.com/prestodb/presto/pull/14943", "merged": true, "mergeCommit": {"oid": "86914fbd787a254c9691759cf274d133b2e7a332"}, "closed": true, "closedAt": "2020-08-24T23:30:44Z", "author": {"login": "wenleix"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc78IKZAFqTQ2MTcyMjQ4Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdCGz0jgBqjM2ODY2NzQ2MTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxNzIyNDg3", "url": "https://github.com/prestodb/presto/pull/14943#pullrequestreview-461722487", "createdAt": "2020-08-05T14:30:50Z", "commit": {"oid": "7a763d6f36b8981eb87f65ff5e1706ac6cfaea5d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxNzc1ODQ3", "url": "https://github.com/prestodb/presto/pull/14943#pullrequestreview-461775847", "createdAt": "2020-08-05T15:26:39Z", "commit": {"oid": "7a763d6f36b8981eb87f65ff5e1706ac6cfaea5d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxNToyNjozOVrOG8O13g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxNToyNjozOVrOG8O13g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMDkxMA==", "bodyText": "When auto tune is not enabled, do we also want to set a max limit ? actual partition count = min(count_set_by_session_property,  hard_limit_set_by_this_config)", "url": "https://github.com/prestodb/presto/pull/14943#discussion_r465810910", "createdAt": "2020-08-05T15:26:39Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -38,6 +39,19 @@ public PrestoSparkConfig setSparkPartitionCountAutoTuneEnabled(boolean sparkPart\n         return this;\n     }\n \n+    @Config(\"spark.max-spark-input-partition-count-for-auto-tune\")\n+    @ConfigDescription(\"Max Spark input partition count when Spark partition auto tune is enabled\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a763d6f36b8981eb87f65ff5e1706ac6cfaea5d"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7a763d6f36b8981eb87f65ff5e1706ac6cfaea5d", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/7a763d6f36b8981eb87f65ff5e1706ac6cfaea5d", "committedDate": "2020-08-03T04:18:13Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}, "afterCommit": {"oid": "4d4b2d085ec0d730945897e2b9b1f9e02d454128", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/4d4b2d085ec0d730945897e2b9b1f9e02d454128", "committedDate": "2020-08-24T05:39:37Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMTcwNDgw", "url": "https://github.com/prestodb/presto/pull/14943#pullrequestreview-473170480", "createdAt": "2020-08-24T07:09:26Z", "commit": {"oid": "4d4b2d085ec0d730945897e2b9b1f9e02d454128"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNzowOToyNlrOHFXLJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNzowOToyNlrOHFXLJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4NDYxMg==", "bodyText": "Just curious, will this be the new default to replace 2000 in production? If not,  maybe keep it as 16 to make it easier for PrestoQueryRunner", "url": "https://github.com/prestodb/presto/pull/14943#discussion_r475384612", "createdAt": "2020-08-24T07:09:26Z", "author": {"login": "viczhang861"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -25,7 +25,8 @@\n public class PrestoSparkConfig\n {\n     private boolean sparkPartitionCountAutoTuneEnabled = true;\n-    private int initialSparkPartitionCount = 16;\n+    private int maxSparkInputPartitionCountForAutoTune = 1000;\n+    private int initialSparkPartitionCount = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d4b2d085ec0d730945897e2b9b1f9e02d454128"}, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4d4b2d085ec0d730945897e2b9b1f9e02d454128", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/4d4b2d085ec0d730945897e2b9b1f9e02d454128", "committedDate": "2020-08-24T05:39:37Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}, "afterCommit": {"oid": "2a714869435ec54a86bcdc9dc4903d02167bad88", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/2a714869435ec54a86bcdc9dc4903d02167bad88", "committedDate": "2020-08-24T17:21:10Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b08f76250ac93a7989ca797f700e97be9889202", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/4b08f76250ac93a7989ca797f700e97be9889202", "committedDate": "2020-08-24T18:20:55Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2a714869435ec54a86bcdc9dc4903d02167bad88", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/2a714869435ec54a86bcdc9dc4903d02167bad88", "committedDate": "2020-08-24T17:21:10Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}, "afterCommit": {"oid": "4b08f76250ac93a7989ca797f700e97be9889202", "author": {"user": {"login": "wenleix", "name": "Wenlei Xie"}}, "url": "https://github.com/prestodb/presto/commit/4b08f76250ac93a7989ca797f700e97be9889202", "committedDate": "2020-08-24T18:20:55Z", "message": "Add max Spark input partition count for auto tune\n\nWhen scanning over very large tables (hundreds of TBs or even PBs),\nusing the default `spark.max-splits-data-size-per-partition` might\nresult in too many Spark executors (e.g. >50K) and cause jobs to be\nunstable.\n\nThe long term solution is to set\n`spark.max-splits-data-size-per-partition` to be reasonably large, and\nleverage historic based optimizer to figure out the optimal split size\nper partition. In a short term, having a max Spark input partition count\nmakes it makes it easier for job auto migration."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 406, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}