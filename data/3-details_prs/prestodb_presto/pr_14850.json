{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUwNTY2MzE1", "number": 14850, "title": "Distribute splits to presto on spark tasks based on size", "bodyText": "== NO RELEASE NOTE ==", "createdAt": "2020-07-16T22:30:38Z", "url": "https://github.com/prestodb/presto/pull/14850", "merged": true, "mergeCommit": {"oid": "63547278dd82157491c88d689ce0f9247cae10b7"}, "closed": true, "closedAt": "2020-07-23T16:09:38Z", "author": {"login": "viczhang861"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc12GjbAFqTQ1MDc4MTQ2Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc3mzWkABqjM1NzgyOTIzMDI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwNzgxNDY2", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-450781466", "createdAt": "2020-07-17T16:06:06Z", "commit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNjowNjowNlrOGzYvdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNjowNjowNlrOGzYvdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUzNTkyNA==", "bodyText": "nit: use OptionalLong", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456535924", "createdAt": "2020-07-17T16:06:06Z", "author": {"login": "wenleix"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java", "diffHunk": "@@ -282,6 +282,12 @@ public Object getInfo()\n                 .build();\n     }\n \n+    @Override\n+    public Optional<Long> getSplitSizeInBytes()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwODQ1Mzkw", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-450845390", "createdAt": "2020-07-17T17:44:58Z", "commit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo0NDo1OFrOGzbvqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo0OTowNVrOGzb36Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTEyOA==", "bodyText": "Let's return Optional.empty() here.", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585128", "createdAt": "2020-07-17T17:44:58Z", "author": {"login": "arhimondr"}, "path": "presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java", "diffHunk": "@@ -37,4 +38,9 @@\n     List<HostAddress> getPreferredNodes(List<HostAddress> sortedCandidates);\n \n     Object getInfo();\n+\n+    default Optional<Long> getSplitSizeInBytes()\n+    {\n+        throw new UnsupportedOperationException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTQ5Mw==", "bodyText": "add checkArgument(minSplitsPerSparkPartition > 0 , \"minSplitsPerSparkPartition must be greater than zero: %s\", minSplitsPerSparkPartition)", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585493", "createdAt": "2020-07-17T17:45:37Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTgzOQ==", "bodyText": "ditto here", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585839", "createdAt": "2020-07-17T17:46:19Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n+            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n+\n+            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjIyMQ==", "bodyText": "nit: move it inside the loop", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586221", "createdAt": "2020-07-17T17:47:03Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n+            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n+\n+            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().get())\n+                    .sum();\n+            int taskCountBySplitsDataSizePerPartition = max(1, toIntExact(totalSizeInBytes / estimatedSizeInBytesPerPartition));\n+\n+            int taskCount = min(taskCountBySplitsPerPartition, taskCountBySplitsDataSizePerPartition);\n+\n+            PriorityQueue<SparkPartition> pq = new PriorityQueue();\n+            int partitionId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjM2NA==", "bodyText": "maybe assignSplit?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586364", "createdAt": "2020-07-17T17:47:21Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void addSplit(ScheduledSplit scheduledSplit)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA==", "bodyText": "I would simply sort it by size. We should always add a next split to the partition that has the least data assigned (not the least number of splits).", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586940", "createdAt": "2020-07-17T17:48:30Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NzI0MQ==", "bodyText": "splitSize is not available", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456587241", "createdAt": "2020-07-17T17:49:05Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void addSplit(ScheduledSplit scheduledSplit)\n+        {\n+            usedSplitSlot = min(usedSplitSlot + 1, maxSplitSlot);\n+            Optional<Long> splitSize = scheduledSplit.getSplit().getConnectorSplit().getSplitSizeInBytes();\n+            splitsSizeInBytes += splitSize.orElseThrow(() -> new IllegalArgumentException(\"splitSizeInBytes not available\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 115}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/ca11baba29aef6b8eaa743aa4369e64083a5c202", "committedDate": "2020-07-16T22:19:53Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size."}, "afterCommit": {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "committedDate": "2020-07-18T00:52:49Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxNzM0OTMy", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-451734932", "createdAt": "2020-07-20T15:47:52Z", "commit": {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNTo0Nzo1MlrOG0UQ1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNTo0Nzo1MlrOG0UQ1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMTEyNg==", "bodyText": "nit: should this be allMatch? -- Since if some split has size, some other splits don't, we still cannot do auto tune right?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457511126", "createdAt": "2020-07-20T15:47:52Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +389,50 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxNzM1OTY0", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-451735964", "createdAt": "2020-07-20T15:48:59Z", "commit": {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNTo0ODo1OVrOG0UUWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNTo0ODo1OVrOG0UUWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMjAyNQ==", "bodyText": "Even the partition count is not decided automatically, we can still use the greedy algorithm to have a balanced split assignment.", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457512025", "createdAt": "2020-07-20T15:48:59Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +389,50 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!autoTunePartitionCount) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxNzQwNDQz", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-451740443", "createdAt": "2020-07-20T15:53:58Z", "commit": {"oid": "2303a390bd13591864af0cc311277b407833e532"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNTo1Mzo1OFrOG0Ukdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNTo1Mzo1OFrOG0Ukdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ==", "bodyText": "Curious: Will this be the total number of physical cores on the Spark workers (instead of Spark container CPUs? ). (E.g. the worker has 16 cores, but the Spark container only has 4 CPUs, and this will set minSplitsPerSparkPartition to 16? )", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457516151", "createdAt": "2020-07-20T15:53:58Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -15,10 +15,29 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import io.airlift.units.DataSize;\n+\n+import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n+    private boolean autoTuneSparkPartitionCount = true;\n     private int initialSparkPartitionCount = 16;\n+    private int minSplitsPerSparkPartition = Runtime.getRuntime().availableProcessors();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2303a390bd13591864af0cc311277b407833e532"}, "originalPosition": 12}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "committedDate": "2020-07-18T00:52:49Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size."}, "afterCommit": {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/7f267d86eb8656c8f852c931baffb0d0b00b8025", "committedDate": "2020-07-20T17:14:57Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxOTE3Mzk4", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-451917398", "createdAt": "2020-07-20T20:00:58Z", "commit": {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDowMDo1OFrOG0dQlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDowNToxMVrOG0dY0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1ODUxOQ==", "bodyText": "maybe sparkInitialPartitionCountAutoTuneEnabled?\ncc: @wenleix", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457658519", "createdAt": "2020-07-20T20:00:58Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -15,10 +15,29 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import io.airlift.units.DataSize;\n+\n+import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n+    private boolean autoTuneSparkPartitionCount = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1OTQwNQ==", "bodyText": "Let's extract this into a variable\nboolean splitsDataSizeAvailable  = splits.stream() .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent())\nThen simply boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session) && splitsDataSizeAvailable", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457659405", "createdAt": "2020-07-20T20:02:49Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +389,51 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        int taskCount;\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && splits.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDMzOQ==", "bodyText": "simply this. splitsSizeInBytes  - that.splitsSizeInBytes", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457660339", "createdAt": "2020-07-20T20:04:39Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}, "originalCommit": {"oid": "ca11baba29aef6b8eaa743aa4369e64083a5c202"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDYyNQ==", "bodyText": "This breakes the non auto tune path. It may start throwing if the split size is not available", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457660625", "createdAt": "2020-07-20T20:05:11Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +523,35 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int partitionId;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId)\n+        {\n+            this.partitionId = partitionId;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void assignSplit(ScheduledSplit scheduledSplit)\n+        {\n+            OptionalLong splitSizeInBytes = scheduledSplit.getSplit().getConnectorSplit().getSplitSizeInBytes();\n+            splitsSizeInBytes += splitSizeInBytes.orElseThrow(() -> new IllegalArgumentException(\"splitSizeInBytes is not present\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025"}, "originalPosition": 116}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/7f267d86eb8656c8f852c931baffb0d0b00b8025", "committedDate": "2020-07-20T17:14:57Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size."}, "afterCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "committedDate": "2020-07-21T02:18:20Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMDg0NTM4", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-452084538", "createdAt": "2020-07-21T02:33:22Z", "commit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMjozMzoyMlrOG0l51Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMjozOTozNlrOG0mAqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMDE0OQ==", "bodyText": "(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457800149", "createdAt": "2020-07-21T02:33:22Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMDQwMw==", "bodyText": "Can splits be immutable here?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457800403", "createdAt": "2020-07-21T02:34:14Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw==", "bodyText": "What if split is larger? Does it mean that we are not going to schedule it? We are going to break correctness.", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801177", "createdAt": "2020-07-21T02:37:04Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA==", "bodyText": "Should this partition be placed into the queue?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801298", "createdAt": "2020-07-21T02:37:29Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTQ1Mw==", "bodyText": "This code is getting non trivial. I would strongly suggest adding a unit test.", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801453", "createdAt": "2020-07-21T02:38:05Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);\n+                        newPartition.assignSplitWithSize(splitSizeInBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTU3MQ==", "bodyText": "why not simply this. availableCapacityInBytes  - that. availableCapacityInBytes ? (or the vice versa, depending on the order you are trying to achieve)", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801571", "createdAt": "2020-07-21T02:38:30Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +537,40 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int partitionId;\n+        private long availableCapacityInBytes;\n+\n+        public SparkPartition(int partitionId, long availableCapacityInBytes)\n+        {\n+            this.partitionId = partitionId;\n+            this.availableCapacityInBytes = availableCapacityInBytes;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            return availableCapacityInBytes == o.availableCapacityInBytes ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTg5OA==", "bodyText": "You can simply do o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() - o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() (or the vice versa, depending on the order you are trying to achieve)", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801898", "createdAt": "2020-07-21T02:39:36Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 63}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "committedDate": "2020-07-21T02:18:20Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}, "afterCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/fcf8a651fd84280b60aca8b777ef32c26217b986", "committedDate": "2020-07-22T05:37:06Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzMzgzNzM1", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-453383735", "createdAt": "2020-07-22T14:38:56Z", "commit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNDozODo1NlrOG1liyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNDo1MDozMlrOG1mGug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0MjgyNQ==", "bodyText": "nit: reformat", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458842825", "createdAt": "2020-07-22T14:38:56Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/pom.xml", "diffHunk": "@@ -175,6 +175,12 @@\n             <scope>test</scope>\n         </dependency>\n \n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-hive-common</artifactId>\n+            <scope>test</scope>\n+\t</dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NTYxMQ==", "bodyText": "Oh, nevermind. I missread", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458845611", "createdAt": "2020-07-22T14:42:38Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NzU2Nw==", "bodyText": "Could you please elaborate how it works?\nfrom what I see in the upper branch we check if !queue.isEmpty(), so if queue is empty it will not enter the upper branch. But we never place anything into the queue outside of the upper branch?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458847567", "createdAt": "2020-07-22T14:45:05Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0ODYyMQ==", "bodyText": "So basically if the split size is larger than the split size per partition theres no need to add the partition to the queue, as it will get full after adding this split, right?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458848621", "createdAt": "2020-07-22T14:46:24Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}, "originalCommit": {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MDU2OA==", "bodyText": "You don't have to create a HiveSplit. You can create a private class MockSplit that only implement the getSplitSizeInBytes method", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458850568", "createdAt": "2020-07-22T14:48:45Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n+            totalSizeInBytes += splitSizeInBytes;\n+\n+            HiveSplit hiveSplit = new HiveSplit(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MTA1MQ==", "bodyText": "It doesn't feel like this algorithm is worth to be tested with a fuzz testing. Instead i would recommend adding a number of deterministic test cases to verify all possible corner cases of the algorithm.", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458851051", "createdAt": "2020-07-22T14:49:20Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MjAyNg==", "bodyText": "Instead of having very generic assertions I would recommend adding more concrete assertions", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458852026", "createdAt": "2020-07-22T14:50:32Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n+            totalSizeInBytes += splitSizeInBytes;\n+\n+            HiveSplit hiveSplit = new HiveSplit(\n+                    \"test_schema\",\n+                    \"test_table\",\n+                    \"\",\n+                    \"path\",\n+                    0,\n+                    splitSizeInBytes,\n+                    splitSizeInBytes,\n+                    new Storage(\n+                            StorageFormat.create(\"serde\", \"input\", \"output\"),\n+                            \"location\",\n+                            Optional.empty(),\n+                            false,\n+                            ImmutableMap.of(),\n+                            ImmutableMap.of()),\n+                    ImmutableList.of(),\n+                    ImmutableList.of(),\n+                    OptionalInt.empty(),\n+                    OptionalInt.empty(),\n+                    NO_PREFERENCE,\n+                    1,\n+                    ImmutableMap.of(),\n+                    Optional.empty(),\n+                    false,\n+                    Optional.empty(),\n+                    NO_CACHE_REQUIREMENT,\n+                    Optional.empty());\n+            Split testSplit = new Split(new ConnectorId(\"test\"), TestingTransactionHandle.create(), hiveSplit);\n+            ScheduledSplit scheduledSplit = new ScheduledSplit(0, new PlanNodeId(\"source\"), testSplit);\n+            splits.add(scheduledSplit);\n+        }\n+\n+        SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSourceDistributionSplits(session, splits);\n+        asMap(assignedSplits).forEach((partitionId, scheduledSplits) -> {\n+            if (scheduledSplits.size() > 1) {\n+                long totalPartitionSize = scheduledSplits.stream()\n+                        .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                        .sum();\n+                assertTrue(totalPartitionSize <= maxSplitsSize, format(\"Total size for splits in one partition should be less than %d\", maxSplitsSize));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNjU1MDk0", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-453655094", "createdAt": "2020-07-22T20:21:16Z", "commit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQyMDoyMToxNlrOG1yx3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQyMDoyMToxNlrOG1yx3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTY3OQ==", "bodyText": "So if we look at this if-statement, it's like this:\n    if (queue is not empty and some other cnoditions) {\n        Do something\n        Add the partition into the queue\n    }\n    else {\n         // queue is empty\n         Do something, but doesn't append anything into the queue\n    }\n\nAs the queue is empty at the beginning, it looks like nothing will be added into the queue. Is there anything I am missing in the code flow? ~ -- or maybe just debug through it to see if the \"else\" branch will ever get triggered?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459059679", "createdAt": "2020-07-22T20:21:16Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -379,14 +385,72 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact((totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) -> {\n+                long size1 = o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+                long size2 = o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+                return size1 == size2 ? 0 : size1 > size2 ? -1 : 1;\n+            });\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986"}, "originalPosition": 87}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "committedDate": "2020-07-22T23:27:13Z", "message": "Add properties for Presto-on-Spark split distribution"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/fcf8a651fd84280b60aca8b777ef32c26217b986", "committedDate": "2020-07-22T05:37:06Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}, "afterCommit": {"oid": "cb194a543593c4d344f68d2325672c972e99b30c", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/cb194a543593c4d344f68d2325672c972e99b30c", "committedDate": "2020-07-23T01:05:05Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzODAwMzg1", "url": "https://github.com/prestodb/presto/pull/14850#pullrequestreview-453800385", "createdAt": "2020-07-23T02:15:47Z", "commit": {"oid": "cb194a543593c4d344f68d2325672c972e99b30c"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjoxNTo0N1rOG16NVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMjoyMTo0NlrOG16S1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTM5OQ==", "bodyText": "nit: Let's simply return result.build(); instead of adding one more level of indentation", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459181399", "createdAt": "2020-07-23T02:15:47Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -379,14 +383,85 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+        int initialPartitionCount = getSparkInitialPartitionCount(session);\n+        checkArgument(initialPartitionCount > 0,\n+                \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb194a543593c4d344f68d2325672c972e99b30c"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MjgwNA==", "bodyText": "maxPartitionSize?", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459182804", "createdAt": "2020-07-23T02:21:46Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +178,158 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        // black box test\n+        testAssignSplitsToPartitionWithRandomSplitsSize(3);\n+\n+        // auto tune partition + splits with mixed size\n+        List<Long> testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L));\n+        Collections.shuffle(testSplitsSize);\n+        Map<Integer, List<Long>> actualResult = assignSplitsToPartition(true, 10, testSplitsSize);\n+\n+        Map<Integer, List<Long>> expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(11L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(10L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(9L)));\n+        expectedResult.put(3, new ArrayList(Arrays.asList(8L, 1L)));\n+        expectedResult.put(4, new ArrayList(Arrays.asList(7L, 2L)));\n+        expectedResult.put(5, new ArrayList(Arrays.asList(6L, 3L)));\n+        expectedResult.put(6, new ArrayList(Arrays.asList(5L, 4L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // enable auto tune partition + small splits\n+        testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L));\n+        Collections.shuffle(testSplitsSize);\n+        actualResult = assignSplitsToPartition(true, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(6L, 3L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(5L, 4L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(2L, 1L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // disable auto tune partition + small splits\n+        testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L));\n+        actualResult = assignSplitsToPartition(false, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(6L, 1L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(5L, 2L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(4L, 3L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // disable auto tune partition + large splits\n+        testSplitsSize = new ArrayList(Arrays.asList(5L, 6L, 7L, 8L, 9L, 10L));\n+        Collections.shuffle(testSplitsSize);\n+        actualResult = assignSplitsToPartition(false, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(10L, 5L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(9L, 6L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(8L, 7L)));\n+        assertEquals(actualResult, expectedResult);\n+    }\n+\n+    private void testAssignSplitsToPartitionWithRandomSplitsSize(int repeatedTimes)\n+    {\n+        int maxSplitSizeInBytes = 2048;\n+        for (int i = 0; i < repeatedTimes; ++i) {\n+            List<Long> splitsSize = new ArrayList<>(1000);\n+            for (int j = 0; j < splitsSize.size(); j++) {\n+                splitsSize.set(j, ThreadLocalRandom.current().nextLong((long) (maxSplitSizeInBytes * 1.2)));\n+            }\n+            assignSplitsToPartition(true, maxSplitSizeInBytes, splitsSize);\n+            assignSplitsToPartition(false, maxSplitSizeInBytes, splitsSize);\n+        }\n+    }\n+\n+    private Map<Integer, List<Long>> assignSplitsToPartition(\n+            boolean autoTunePartitionCount,\n+            long maxSplitsSize,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb194a543593c4d344f68d2325672c972e99b30c"}, "originalPosition": 112}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/508b1f86bac2451d7dd8be16de975b3415e2bf67", "committedDate": "2020-07-23T03:21:10Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cb194a543593c4d344f68d2325672c972e99b30c", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/cb194a543593c4d344f68d2325672c972e99b30c", "committedDate": "2020-07-23T01:05:05Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}, "afterCommit": {"oid": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "author": {"user": {"login": "viczhang861", "name": "Vic Zhang"}}, "url": "https://github.com/prestodb/presto/commit/508b1f86bac2451d7dd8be16de975b3415e2bf67", "committedDate": "2020-07-23T03:21:10Z", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1287, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}