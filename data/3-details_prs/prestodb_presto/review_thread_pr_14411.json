{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NzM2ODY4", "number": 14411, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOTowODozNVrOD067bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTowNDo0OFrOD1l6zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2ODE4MDI4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOTowODozNVrOGKGmog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOTowODozNVrOGKGmog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0NzEzOA==", "bodyText": "static import HiveSessionProperties.getParquetWriterPageSize and HiveSessionProperties.getParquetWriterBlockSize", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413247138", "createdAt": "2020-04-22T19:08:35Z", "author": {"login": "zhenxiao"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -97,6 +93,11 @@ public ParquetFileWriterFactory(\n             return Optional.empty();\n         }\n \n+        ParquetWriterOptions parquetWriterOptions = ParquetWriterOptions.builder()\n+                .setMaxPageSize(HiveSessionProperties.getParquetWriterPageSize(session))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2ODE4OTAwOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMDo0NlrOGKGr3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMDo0NlrOGKGr3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODQ3OQ==", "bodyText": "not needed? already named ignored", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413248479", "createdAt": "2020-04-22T19:10:46Z", "author": {"login": "zhenxiao"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.Page;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.block.Block;\n+import com.facebook.presto.spi.block.BlockBuilder;\n+import com.facebook.presto.spi.block.RunLengthEncodedBlock;\n+import com.facebook.presto.spi.type.Type;\n+import com.google.common.collect.ImmutableList;\n+import org.openjdk.jol.info.ClassLayout;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_CLOSE_ERROR;\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_DATA_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ParquetFileWriter\n+        implements HiveFileWriter\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(ParquetFileWriter.class).instanceSize();\n+\n+    private final ParquetWriter parquetWriter;\n+    private final Callable<Void> rollbackAction;\n+    private final int[] fileInputColumnIndexes;\n+    private final List<Block> nullBlocks;\n+\n+    public ParquetFileWriter(\n+            OutputStream outputStream,\n+            Callable<Void> rollbackAction,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ParquetWriterOptions parquetWriterOptions,\n+            int[] fileInputColumnIndexes,\n+            CompressionCodecName compressionCodecName)\n+    {\n+        requireNonNull(outputStream, \"outputStream is null\");\n+\n+        this.parquetWriter = new ParquetWriter(\n+                outputStream,\n+                columnNames,\n+                fileColumnTypes,\n+                parquetWriterOptions,\n+                compressionCodecName.getHadoopCompressionCodecClassName());\n+\n+        this.rollbackAction = requireNonNull(rollbackAction, \"rollbackAction is null\");\n+        this.fileInputColumnIndexes = requireNonNull(fileInputColumnIndexes, \"fileInputColumnIndexes is null\");\n+\n+        ImmutableList.Builder<Block> nullBlocks = ImmutableList.builder();\n+        for (Type fileColumnType : fileColumnTypes) {\n+            BlockBuilder blockBuilder = fileColumnType.createBlockBuilder(null, 1, 0);\n+            blockBuilder.appendNull();\n+            nullBlocks.add(blockBuilder.build());\n+        }\n+        this.nullBlocks = nullBlocks.build();\n+    }\n+\n+    @Override\n+    public long getWrittenBytes()\n+    {\n+        return parquetWriter.getWrittenBytes();\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return INSTANCE_SIZE + parquetWriter.getRetainedBytes();\n+    }\n+\n+    @Override\n+    public void appendRows(Page dataPage)\n+    {\n+        Block[] blocks = new Block[fileInputColumnIndexes.length];\n+        for (int i = 0; i < fileInputColumnIndexes.length; i++) {\n+            int inputColumnIndex = fileInputColumnIndexes[i];\n+            if (inputColumnIndex < 0) {\n+                blocks[i] = new RunLengthEncodedBlock(nullBlocks.get(i), dataPage.getPositionCount());\n+            }\n+            else {\n+                blocks[i] = dataPage.getBlock(inputColumnIndex);\n+            }\n+        }\n+        Page page = new Page(dataPage.getPositionCount(), blocks);\n+        try {\n+            parquetWriter.write(page);\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            throw new PrestoException(HIVE_WRITER_DATA_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public void commit()\n+    {\n+        try {\n+            parquetWriter.close();\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            try {\n+                rollbackAction.call();\n+            }\n+            catch (Exception ignored) {\n+                // ignore", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2ODE5MTE1OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMToxM1rOGKGtDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMToxM1rOGKGtDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODc4MQ==", "bodyText": "...writing parquet to Hive", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413248781", "createdAt": "2020-04-22T19:11:13Z", "author": {"login": "zhenxiao"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriter;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.Page;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.block.Block;\n+import com.facebook.presto.spi.block.BlockBuilder;\n+import com.facebook.presto.spi.block.RunLengthEncodedBlock;\n+import com.facebook.presto.spi.type.Type;\n+import com.google.common.collect.ImmutableList;\n+import org.openjdk.jol.info.ClassLayout;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_CLOSE_ERROR;\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_DATA_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class ParquetFileWriter\n+        implements HiveFileWriter\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(ParquetFileWriter.class).instanceSize();\n+\n+    private final ParquetWriter parquetWriter;\n+    private final Callable<Void> rollbackAction;\n+    private final int[] fileInputColumnIndexes;\n+    private final List<Block> nullBlocks;\n+\n+    public ParquetFileWriter(\n+            OutputStream outputStream,\n+            Callable<Void> rollbackAction,\n+            List<String> columnNames,\n+            List<Type> fileColumnTypes,\n+            ParquetWriterOptions parquetWriterOptions,\n+            int[] fileInputColumnIndexes,\n+            CompressionCodecName compressionCodecName)\n+    {\n+        requireNonNull(outputStream, \"outputStream is null\");\n+\n+        this.parquetWriter = new ParquetWriter(\n+                outputStream,\n+                columnNames,\n+                fileColumnTypes,\n+                parquetWriterOptions,\n+                compressionCodecName.getHadoopCompressionCodecClassName());\n+\n+        this.rollbackAction = requireNonNull(rollbackAction, \"rollbackAction is null\");\n+        this.fileInputColumnIndexes = requireNonNull(fileInputColumnIndexes, \"fileInputColumnIndexes is null\");\n+\n+        ImmutableList.Builder<Block> nullBlocks = ImmutableList.builder();\n+        for (Type fileColumnType : fileColumnTypes) {\n+            BlockBuilder blockBuilder = fileColumnType.createBlockBuilder(null, 1, 0);\n+            blockBuilder.appendNull();\n+            nullBlocks.add(blockBuilder.build());\n+        }\n+        this.nullBlocks = nullBlocks.build();\n+    }\n+\n+    @Override\n+    public long getWrittenBytes()\n+    {\n+        return parquetWriter.getWrittenBytes();\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return INSTANCE_SIZE + parquetWriter.getRetainedBytes();\n+    }\n+\n+    @Override\n+    public void appendRows(Page dataPage)\n+    {\n+        Block[] blocks = new Block[fileInputColumnIndexes.length];\n+        for (int i = 0; i < fileInputColumnIndexes.length; i++) {\n+            int inputColumnIndex = fileInputColumnIndexes[i];\n+            if (inputColumnIndex < 0) {\n+                blocks[i] = new RunLengthEncodedBlock(nullBlocks.get(i), dataPage.getPositionCount());\n+            }\n+            else {\n+                blocks[i] = dataPage.getBlock(inputColumnIndex);\n+            }\n+        }\n+        Page page = new Page(dataPage.getPositionCount(), blocks);\n+        try {\n+            parquetWriter.write(page);\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            throw new PrestoException(HIVE_WRITER_DATA_ERROR, e);\n+        }\n+    }\n+\n+    @Override\n+    public void commit()\n+    {\n+        try {\n+            parquetWriter.close();\n+        }\n+        catch (IOException | UncheckedIOException e) {\n+            try {\n+                rollbackAction.call();\n+            }\n+            catch (Exception ignored) {\n+                // ignore\n+            }\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error committing write parquet to Hive\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void rollback()\n+    {\n+        try {\n+            try {\n+                parquetWriter.close();\n+            }\n+            finally {\n+                rollbackAction.call();\n+            }\n+        }\n+        catch (Exception e) {\n+            throw new PrestoException(HIVE_WRITER_CLOSE_ERROR, \"Error rolling back write parquet to Hive\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50"}, "originalPosition": 143}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2ODE5MjU0OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMTozNVrOGKGt4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQwNDozNDo0NFrOGZsl6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODk5Mg==", "bodyText": "is this indentation correct?", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413248992", "createdAt": "2020-04-22T19:11:35Z", "author": {"login": "zhenxiao"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.HiveSessionProperties;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.ParquetOutputFormat;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;\n+\n+    @Inject\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            HiveClientConfig hiveConfig)\n+    {\n+        this(\n+                hdfsEnvironment,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5ODE4NQ==", "bodyText": "yeah. I think it's correct.", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r429598185", "createdAt": "2020-05-24T04:34:44Z", "author": {"login": "qqibrow"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.HiveSessionProperties;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.ParquetOutputFormat;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;\n+\n+    @Inject\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            HiveClientConfig hiveConfig)\n+    {\n+        this(\n+                hdfsEnvironment,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0ODk5Mg=="}, "originalCommit": {"oid": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2ODE5NTA2OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMjoxMVrOGKGvTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQxOToxMjoxMVrOGKGvTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzI0OTM1OA==", "bodyText": "static import HiveSessionProperties.isParquetOptimizedWriterEnabled", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r413249358", "createdAt": "2020-04-22T19:12:11Z", "author": {"login": "zhenxiao"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.HiveSessionProperties;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.ParquetOutputFormat;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;\n+\n+    @Inject\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            HiveClientConfig hiveConfig)\n+    {\n+        this(\n+                hdfsEnvironment,\n+                typeManager,\n+                nodeVersion,\n+                requireNonNull(hiveConfig, \"hiveConfig is null\").getDateTimeZone());\n+    }\n+\n+    public ParquetFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            NodeVersion nodeVersion,\n+            DateTimeZone hiveStorageTimeZone)\n+    {\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.nodeVersion = requireNonNull(nodeVersion, \"nodeVersion is null\");\n+        this.hiveStorageTimeZone = requireNonNull(hiveStorageTimeZone, \"hiveStorageTimeZone is null\");\n+    }\n+\n+    @Override\n+    public Optional<HiveFileWriter> createFileWriter(Path path, List<String> inputColumnNames, StorageFormat storageFormat, Properties schema, JobConf conf, ConnectorSession session)\n+    {\n+        if (!HiveSessionProperties.isParquetOptimizedWriterEnabled(session)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "066d4fdfa4cd23d628109fb3a3e3f7ea5f378f50"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTE4NDU1OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNDo0Nzo1M1rOGLGPHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNDo0Nzo1M1rOGLGPHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI4OTY5NA==", "bodyText": "This is not used.", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r414289694", "createdAt": "2020-04-24T04:47:53Z", "author": {"login": "viczhang861"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6232bbacdd2cc9c73e86165537531bfcf4e7f1c1"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTE4NTQ4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNDo0ODoxM1rOGLGPkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNDo0ODoxM1rOGLGPkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI4OTgxMA==", "bodyText": "This is not used.", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r414289810", "createdAt": "2020-04-24T04:48:13Z", "author": {"login": "viczhang861"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive.parquet;\n+\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveFileWriter;\n+import com.facebook.presto.hive.HiveFileWriterFactory;\n+import com.facebook.presto.hive.NodeVersion;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.type.Type;\n+import com.facebook.presto.spi.type.TypeManager;\n+import com.google.common.base.Splitter;\n+import com.google.inject.Inject;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.joda.time.DateTimeZone;\n+import parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.Callable;\n+\n+import static com.facebook.presto.hive.HiveErrorCode.HIVE_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.hive.HiveType.toHiveTypes;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMNS;\n+import static org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_COLUMN_TYPES;\n+\n+public class ParquetFileWriterFactory\n+        implements HiveFileWriterFactory\n+{\n+    private final DateTimeZone hiveStorageTimeZone;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final NodeVersion nodeVersion;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6232bbacdd2cc9c73e86165537531bfcf4e7f1c1"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3NTIyMzgyOnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwNTowNDo0OFrOGLGkZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQwNDozMTowOVrOGZslfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI5NTE0Mw==", "bodyText": "Just curious,  can we read parquet compression directly using HiveSessionProperties::getCompressionCodec(ConnectorSession session)", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r414295143", "createdAt": "2020-04-24T05:04:48Z", "author": {"login": "viczhang861"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -96,6 +97,8 @@ public ParquetFileWriterFactory(\n             return Optional.empty();\n         }\n \n+        CompressionCodecName compressionCodecName = getCompression(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a5dcf973135234d497d1b4d093c41b804bcfebc7"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5ODA3OQ==", "bodyText": "When I made the code change, the source of truth for compression is from config and the conf is set through ConfigurationUtils::configureCompression. Maybe your way would also work.", "url": "https://github.com/prestodb/presto/pull/14411#discussion_r429598079", "createdAt": "2020-05-24T04:31:09Z", "author": {"login": "qqibrow"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/parquet/ParquetFileWriterFactory.java", "diffHunk": "@@ -96,6 +97,8 @@ public ParquetFileWriterFactory(\n             return Optional.empty();\n         }\n \n+        CompressionCodecName compressionCodecName = getCompression(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI5NTE0Mw=="}, "originalCommit": {"oid": "a5dcf973135234d497d1b4d093c41b804bcfebc7"}, "originalPosition": 12}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2805, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}