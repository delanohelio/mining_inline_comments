{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE2ODA0MzI0", "number": 14515, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNToxMzo0MVrOD8Uxdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDo0Nzo1MFrOD-DoDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTgxNDk1OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNToxMzo0MVrOGVMTNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNjozMzoxNFrOGVktyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3NDgwNw==", "bodyText": "When the fragment doesn't have any child, it cannot have broadcast input as well right?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r424874807", "createdAt": "2020-05-14T05:13:41Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -225,7 +227,15 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n         SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-        if (rddInputs.size() == 1) {\n+        if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n+                    .mapPartitionsToPair(createTaskProcessor(\n+                            executorFactoryProvider,\n+                            taskStatsCollector,\n+                            toTaskProcessorBroadcastInputs(broadcastInputs)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e6295347080e36a5ce956be7009876a30c08636b"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI3NDgyNw==", "bodyText": "Theoretically it is possible to get such plans. Think of a broadcast join with VALUES. e.g.: (VALUES JOIN table)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r425274827", "createdAt": "2020-05-14T16:33:14Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -225,7 +227,15 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n         SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-        if (rddInputs.size() == 1) {\n+        if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n+                    .mapPartitionsToPair(createTaskProcessor(\n+                            executorFactoryProvider,\n+                            taskStatsCollector,\n+                            toTaskProcessorBroadcastInputs(broadcastInputs)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3NDgwNw=="}, "originalCommit": {"oid": "e6295347080e36a5ce956be7009876a30c08636b"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTg0MjAwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNToyOTozNVrOGVMkRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwMDo1MzowMVrOGWXGvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3OTE3Mg==", "bodyText": "My understanding Spark Java API avoids directly using scala.reflect.ClassTag$.MODULE$,  and that's why JavaRDDLike is introduced with automatic wrap of class tags, e.g. https://github.com/apache/spark/blob/fd2d55c9919ece5463377bc6f45f2cdb8bf90515/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala#L93\nIn fact, in JavaRDDLike, it use \"fakeClassTag\"  to \"keep ClassTags out of the external Java API\", as explained here: https://github.com/apache/spark/blob/fd2d55c9919ece5463377bc6f45f2cdb8bf90515/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala#L754-L764\n   * Produces a ClassTag[T], which is actually just a casted ClassTag[AnyRef].\n   *\n   * This method is used to keep ClassTags out of the external Java API, as the Java compiler\n   * cannot produce them automatically. While this ClassTag-faking does please the compiler,\n   * it can cause problems at runtime if the Scala API relies on ClassTags for correctness.\n   *\n   * Often, though, a ClassTag[AnyRef] will not lead to incorrect behavior, just worse performance\n   * or security issues. For instance, an Array[AnyRef] can hold any type T, but may lose primitive\n   * specialization.\n\ncc @sameeragarwal", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r424879172", "createdAt": "2020-05-14T05:29:35Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);\n-        }\n-        if (rddInputs.size() == 2) {\n-            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n-            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()\n-                    .map(rddInputs::get)\n-                    .collect(toImmutableList());\n-            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            fragmentIds.get(0).toString(),\n-                            fragmentIds.get(1).toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return JavaPairRDD.fromJavaRDD(\n-                    rdds.get(0).zipPartitions(\n-                            rdds.get(1),\n-                            taskProcessor));\n+\n+        ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        for (Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n+            fragmentIds.add(input.getKey().toString());\n+            rdds.add(input.getValue().rdd());\n         }\n \n-        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+                executorFactoryProvider,\n+                serializedTaskDescriptor,\n+                fragmentIds.build(),\n+                taskStatsCollector,\n+                toTaskProcessorBroadcastInputs(broadcastInputs));\n+\n+        return JavaPairRDD.fromRDD(\n+                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                classTag(Integer.class),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI3MjEzNA==", "bodyText": "$.MODULE$ Is the way Scala implements lazy singleton objects. Although this is not very \"public\" API, given the legacy I don't expect it to change.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r425272134", "createdAt": "2020-05-14T16:29:09Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);\n-        }\n-        if (rddInputs.size() == 2) {\n-            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n-            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()\n-                    .map(rddInputs::get)\n-                    .collect(toImmutableList());\n-            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            fragmentIds.get(0).toString(),\n-                            fragmentIds.get(1).toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return JavaPairRDD.fromJavaRDD(\n-                    rdds.get(0).zipPartitions(\n-                            rdds.get(1),\n-                            taskProcessor));\n+\n+        ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        for (Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n+            fragmentIds.add(input.getKey().toString());\n+            rdds.add(input.getValue().rdd());\n         }\n \n-        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+                executorFactoryProvider,\n+                serializedTaskDescriptor,\n+                fragmentIds.build(),\n+                taskStatsCollector,\n+                toTaskProcessorBroadcastInputs(broadcastInputs));\n+\n+        return JavaPairRDD.fromRDD(\n+                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                classTag(Integer.class),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3OTE3Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEwMDQxNQ==", "bodyText": "@arhimondr : The fundamental question is how to interact between Java/Scala API in such case. I assume Databricks has done in-depth investigation into this when they design the Spark Java API. Thus there might be reason why they use fakeClassTag instead of scala.reflect.\nI will check this offline with @sameeragarwal . In the meanwhile, we can continue with what is implemented in your PR. Just keep in mind we might need revisit in the future about Java-Scala interaction (which I heard can be tricky in some cases due to Scala has many fancy features done by Scala Compiler \ud83d\ude03 ) and pay back tech-debt, if there is \ud83d\ude03", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426100415", "createdAt": "2020-05-16T00:53:01Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);\n-        }\n-        if (rddInputs.size() == 2) {\n-            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n-            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()\n-                    .map(rddInputs::get)\n-                    .collect(toImmutableList());\n-            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            fragmentIds.get(0).toString(),\n-                            fragmentIds.get(1).toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return JavaPairRDD.fromJavaRDD(\n-                    rdds.get(0).zipPartitions(\n-                            rdds.get(1),\n-                            taskProcessor));\n+\n+        ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        for (Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n+            fragmentIds.add(input.getKey().toString());\n+            rdds.add(input.getValue().rdd());\n         }\n \n-        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+                executorFactoryProvider,\n+                serializedTaskDescriptor,\n+                fragmentIds.build(),\n+                taskStatsCollector,\n+                toTaskProcessorBroadcastInputs(broadcastInputs));\n+\n+        return JavaPairRDD.fromRDD(\n+                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                classTag(Integer.class),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg3OTE3Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0NTg1NjM4OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNTozNzo1MFrOGVMtJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNDo1NToyM1rOGXP0qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MTQ0Nw==", "bodyText": "ZippedPartitionsBaseRDD seems to be a private[static] class : https://github.com/apache/spark/blob/fd2d55c9919ece5463377bc6f45f2cdb8bf90515/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala#L45  Shall we extend it?\ncc @sameeragarwal", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r424881447", "createdAt": "2020-05-14T05:37:50Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTI3NDMzOQ==", "bodyText": "Ideally - no. This is a private API. However public API does not allow zipping of more than 4 RDD's. That's why we are making this tradeoff.\nI acknowledge that we might looks forward compatibility at some point due to using this private API. However unfortunately I don't see how can we zip more than 4 partition in a way other than that.\nThis is tested with Spark 2.0 and Spark 2.4.3 and it works. If at some point we discover that compatibility is broken for some version of Spark we will try to figure out how to fix it.\nMeanwhile we can also try to contribute N-way ZIP partition to open source Spark.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r425274339", "createdAt": "2020-05-14T16:32:28Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MTQ0Nw=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA5OTg0Mg==", "bodyText": "@arhimondr :\n\nMeanwhile we can also try to contribute N-way ZIP partition to open source Spark.\n\nThat could be the long-term solution. Another worthwhile investigation next half could be to understand how does SparkSQL implement it without N-way ZIP partition. I also see SparkSQL implements some specialized SparkSQL RDD (unfortunately, in Scala). Thus they don't have this Java-Scala interaction design problem :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426099842", "createdAt": "2020-05-16T00:48:27Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MTQ0Nw=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MzE5OA==", "bodyText": "I suspect that they implemented it by calling zip partitions (or join / etc:) multiple times, as they don't really need to have 1 task per stage, since the exchange is row based anyway.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426693198", "createdAt": "2020-05-18T15:03:42Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MTQ0Nw=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyOTY3Mw==", "bodyText": "@arhimondr : Even SparkSQL is already row-based, calling zipPartition might also cause unnecessary materialization (depends on implementation). I also suspect it could limit the efficiency of whole stage codegen without having all the zipped partitions all in once...", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427029673", "createdAt": "2020-05-19T04:55:23Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg4MTQ0Nw=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDQxMjMzOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwNzo0MDoxOFrOGWe9Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwNzo0MDoxOFrOGWe9Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTA3MA==", "bodyText": "nit: I think we usually use Map.Entry<...> ?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426229070", "createdAt": "2020-05-17T07:40:18Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);\n-        }\n-        if (rddInputs.size() == 2) {\n-            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n-            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()\n-                    .map(rddInputs::get)\n-                    .collect(toImmutableList());\n-            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            fragmentIds.get(0).toString(),\n-                            fragmentIds.get(1).toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return JavaPairRDD.fromJavaRDD(\n-                    rdds.get(0).zipPartitions(\n-                            rdds.get(1),\n-                            taskProcessor));\n+\n+        ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        for (Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDQxMzYxOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwNzo0MjoyOVrOGWe-Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMDowNDo1NFrOGXLXUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTI1OA==", "bodyText": "nit: for line 401 to 409, we could also consider use an imperative way (instead of using stream API) -- especially collectAndDestroyDependencies has side effect . No strong opinion here.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426229258", "createdAt": "2020-05-17T07:42:29Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -400,14 +398,20 @@ private PrestoSparkQueryExecution(\n                         tableWriteInfo);\n                 SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-                SubPlan child = getOnlyElement(root.getChildren());\n-                RddAndMore rdd = createRdd(child);\n-                List<Tuple2<Integer, PrestoSparkRow>> sparkDriverInput = rdd.collectAndDestroyDependencies();\n+                Map<PlanFragmentId, RddAndMore> inputRdds = root.getChildren().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk1NjYyNA==", "bodyText": "I tried converting it to loops, but it becomes more wordy, as you have to initialize the builders first for rdd's, then for the futures, then for the results. I would like to keep it as is if you don't mind.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426956624", "createdAt": "2020-05-19T00:04:54Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -400,14 +398,20 @@ private PrestoSparkQueryExecution(\n                         tableWriteInfo);\n                 SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-                SubPlan child = getOnlyElement(root.getChildren());\n-                RddAndMore rdd = createRdd(child);\n-                List<Tuple2<Integer, PrestoSparkRow>> sparkDriverInput = rdd.collectAndDestroyDependencies();\n+                Map<PlanFragmentId, RddAndMore> inputRdds = root.getChildren().stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTI1OA=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDQxNjg0OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwNzo0Nzo1NlrOGWe_vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwMDowNjozN1rOGXLZJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTY5Mg==", "bodyText": "int: Maybe just using for-loop + ArrayList here? My feeling is using Stream API here doesn't help make the code easy to read or reasoning (you still need to use i which is almost for-loop). But it's just a personal preference.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426229692", "createdAt": "2020-05-17T07:47:56Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds;\n+    private Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> function;\n+\n+    public PrestoSparkZipRdd(\n+            SparkContext context,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds,\n+            Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> function)\n+    {\n+        super(\n+                context,\n+                getRDDSequence(requireNonNull(rdds, \"rdds is null\")),\n+                false,\n+                scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class));\n+        this.rdds = rdds;\n+        this.function = context.clean(requireNonNull(function, \"function is null\"), true);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Seq<RDD<?>> getRDDSequence(List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds)\n+    {\n+        return asScalaBuffer((List<RDD<?>>) (List<?>) new ArrayList<>(rdds)).toSeq();\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        List<Iterator<Tuple2<Integer, PrestoSparkRow>>> iterators = unmodifiableList(IntStream.range(0, rdds.size())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk1NzA5Mw==", "bodyText": "Makes sense. Converted to a loop.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426957093", "createdAt": "2020-05-19T00:06:37Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkZipRdd.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkZipRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds;\n+    private Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> function;\n+\n+    public PrestoSparkZipRdd(\n+            SparkContext context,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds,\n+            Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> function)\n+    {\n+        super(\n+                context,\n+                getRDDSequence(requireNonNull(rdds, \"rdds is null\")),\n+                false,\n+                scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class));\n+        this.rdds = rdds;\n+        this.function = context.clean(requireNonNull(function, \"function is null\"), true);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Seq<RDD<?>> getRDDSequence(List<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds)\n+    {\n+        return asScalaBuffer((List<RDD<?>>) (List<?>) new ArrayList<>(rdds)).toSeq();\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        List<Iterator<Tuple2<Integer, PrestoSparkRow>>> iterators = unmodifiableList(IntStream.range(0, rdds.size())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyOTY5Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDQyMzE0OnYy", "diffSide": "LEFT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwNzo1ODozOVrOGWfDKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0Njo1MVrOGYPeuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMDU3MA==", "bodyText": "Note you are also using PrestoSparkZipRdd even there is only one RDD input. I am wondering if we should only use PrestoSparkZipRdd when there are more than two inputs?  Since it seems reasonable to specialize when there is only one RDD input.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426230570", "createdAt": "2020-05-17T07:58:39Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NTU3OQ==", "bodyText": "Then we need to have 2 different task processors implemented, one for partitionBy and one for zipRdd. This results in code duplication.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426695579", "createdAt": "2020-05-18T15:07:01Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMDU3MA=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk1OTgyOA==", "bodyText": "Also I was thinking about extending PrestoSparkZipRdd to also accept an RDD of partitioned splits for bucketed tables. Thus I would prefer to have all exchanges implemented with PrestoSparkZipRdd.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426959828", "createdAt": "2020-05-19T00:16:58Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMDU3MA=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0MjcxNw==", "bodyText": "@arhimondr :\n\nThen we need to have 2 different task processors implemented, one for partitionBy and one for zipRdd. This results in code duplication.\n\nMy intuition is there might be some way to refactor the task processor to avoid code duplication. I raise this as originally, the code is clear about when to use the mapPartitions primitive and when to use zipPartitions primitive. Now zipPartitions becomes a bit omnipotent.\nBut I agree let's revisit these primitives after bucketed table is implemented (even after initial production use cases \ud83d\ude03 )", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427042717", "createdAt": "2020-05-19T05:43:47Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMDU3MA=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MjYzMg==", "bodyText": "I have a prototype for bucketed tables.\nI consolidated all RDD creation into the PrestoSparkTaskRdd, that accepts both, PrestoSparkRow RDD's and RDD containing splits: c018b12#diff-c96d6601d2e7713a8432fbd9702bdcb6R41\nThe PrestoSparkTaskRdd takes PrestoSparkTaskProcessor, that is also now consolidated into one:\nc018b12#diff-25f8e21cfd79d29a34d89b1f2758822bR32", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r428072632", "createdAt": "2020-05-20T14:46:51Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -235,38 +237,25 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                             taskStatsCollector,\n                             toTaskProcessorBroadcastInputs(broadcastInputs)));\n         }\n-        else if (rddInputs.size() == 1) {\n-            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n-            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n-                    createTaskProcessor(\n-                            executorFactoryProvider,\n-                            serializedTaskDescriptor,\n-                            input.getKey().toString(),\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs));\n-            return input.getValue()\n-                    .mapPartitionsToPair(taskProcessor);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMDU3MA=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDQyNzU1OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xN1QwODowNTo0NVrOGWfFjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTowMToyOFrOGXP62w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMTE4Mg==", "bodyText": "Instead of using a general function type. I am wondering if we can have FlatMapFunctionNSameInputType<Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>>? Essentially thinking about generalizing FlatMapFunction2 to N inputs with same type :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426231182", "createdAt": "2020-05-17T08:05:45Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -62,59 +62,37 @@ private TaskProcessors() {}\n         };\n     }\n \n-    public static PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> createTaskProcessor(\n+    public static Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> createTaskProcessor(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NjQxNA==", "bodyText": "Could you please elaborate a little more on this?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426696414", "createdAt": "2020-05-18T15:08:08Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -62,59 +62,37 @@ private TaskProcessors() {}\n         };\n     }\n \n-    public static PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> createTaskProcessor(\n+    public static Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> createTaskProcessor(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMTE4Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMDE2NQ==", "bodyText": "Actually never-mind. I think I got it. Let me have a look", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426700165", "createdAt": "2020-05-18T15:13:23Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -62,59 +62,37 @@ private TaskProcessors() {}\n         };\n     }\n \n-    public static PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> createTaskProcessor(\n+    public static Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> createTaskProcessor(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMTE4Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk1OTM5NA==", "bodyText": "Currently the functions argument is List<Iterator<Tuple2<Integer, PrestoSparkRow>>>. It's because the inputs are always Tuple2<Integer, PrestoSparkRow>.\nWhen I was thinking about bucketed tables implementation, I envisioned the second type of inputs, Tuple2<Integer, Split> -> (partition,split). Thus I would love to leave the flexibility to freely define signature, since it will not necessarily be accepting the inputs of the same type.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426959394", "createdAt": "2020-05-19T00:15:10Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -62,59 +62,37 @@ private TaskProcessors() {}\n         };\n     }\n \n-    public static PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> createTaskProcessor(\n+    public static Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> createTaskProcessor(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMTE4Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAzMTI1OQ==", "bodyText": "@arhimondr : Makes sense. We can revisit potential refactors once bucketed table join unbucketed table is implemented.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427031259", "createdAt": "2020-05-19T05:01:28Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -62,59 +62,37 @@ private TaskProcessors() {}\n         };\n     }\n \n-    public static PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> createTaskProcessor(\n+    public static Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> createTaskProcessor(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIzMTE4Mg=="}, "originalCommit": {"oid": "32653f1a4c24cd87557e23c7a515a5fa8c2c6b37"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NTQ3ODQ4OnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwNTozNzo0NFrOGWoGow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToxMDowOFrOGW7kUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM3ODkxNQ==", "bodyText": "Before, when will this branch being triggered? -- one case I can think is to gather all the table write information ?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426378915", "createdAt": "2020-05-18T05:37:44Z", "author": {"login": "wenleix"}, "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "diffHunk": "@@ -1202,9 +1205,30 @@ public PlanWithProperties visitUnion(UnionNode node, PreferredProperties parentP\n                 // children partitioning and don't GATHER partitioned inputs\n                 // TODO: add FIXED_ARBITRARY_DISTRIBUTION support on non empty singleNodeChildren\n                 if (!parentPartitioningPreference.isPresent() || parentPartitioningPreference.get().isDistributed()) {\n-                    return arbitraryDistributeUnion(node, distributedChildren, distributedOutputLayouts);\n+                    // TODO: can we insert LOCAL exchange for one child SOURCE distributed and another HASH distributed?\n+                    if (getNumberOfTableScans(distributedChildren) == 0 && isSameOrSystemCompatiblePartitions(extractRemoteExchangePartitioningHandles(distributedChildren))) {\n+                        // No source distributed child, we can use insert LOCAL exchange\n+                        // TODO: if all children have the same partitioning, pass this partitioning to the parent\n+                        // instead of \"arbitraryPartition\".\n+                        return new PlanWithProperties(node.replaceChildren(distributedChildren));\n+                    }\n+                    else if (preferDistributedUnion) {\n+                        // Presto currently can not execute stage that has multiple table scans, so in that case\n+                        // we have to insert REMOTE exchange with FIXED_ARBITRARY_DISTRIBUTION instead of local exchange\n+                        return new PlanWithProperties(\n+                                new ExchangeNode(\n+                                        idAllocator.getNextId(),\n+                                        REPARTITION,\n+                                        REMOTE_STREAMING,\n+                                        new PartitioningScheme(Partitioning.create(FIXED_ARBITRARY_DISTRIBUTION, ImmutableList.of()), node.getOutputVariables()),\n+                                        distributedChildren,\n+                                        distributedOutputLayouts,\n+                                        false,\n+                                        Optional.empty()));\n+                    }\n                 }\n-\n+                // TODO: We should support multiple table scans in a single fragment for efficient union implementation\n+                // TODO: Multiple table scans are already partially supported (for partitioned (bucketed) tables)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9352a0d6db268ce76afd8bf68d5786e475df15eb"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NzgwOA==", "bodyText": "Yeah, table write information is one of them. Another good example is sort.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426697808", "createdAt": "2020-05-18T15:10:08Z", "author": {"login": "arhimondr"}, "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "diffHunk": "@@ -1202,9 +1205,30 @@ public PlanWithProperties visitUnion(UnionNode node, PreferredProperties parentP\n                 // children partitioning and don't GATHER partitioned inputs\n                 // TODO: add FIXED_ARBITRARY_DISTRIBUTION support on non empty singleNodeChildren\n                 if (!parentPartitioningPreference.isPresent() || parentPartitioningPreference.get().isDistributed()) {\n-                    return arbitraryDistributeUnion(node, distributedChildren, distributedOutputLayouts);\n+                    // TODO: can we insert LOCAL exchange for one child SOURCE distributed and another HASH distributed?\n+                    if (getNumberOfTableScans(distributedChildren) == 0 && isSameOrSystemCompatiblePartitions(extractRemoteExchangePartitioningHandles(distributedChildren))) {\n+                        // No source distributed child, we can use insert LOCAL exchange\n+                        // TODO: if all children have the same partitioning, pass this partitioning to the parent\n+                        // instead of \"arbitraryPartition\".\n+                        return new PlanWithProperties(node.replaceChildren(distributedChildren));\n+                    }\n+                    else if (preferDistributedUnion) {\n+                        // Presto currently can not execute stage that has multiple table scans, so in that case\n+                        // we have to insert REMOTE exchange with FIXED_ARBITRARY_DISTRIBUTION instead of local exchange\n+                        return new PlanWithProperties(\n+                                new ExchangeNode(\n+                                        idAllocator.getNextId(),\n+                                        REPARTITION,\n+                                        REMOTE_STREAMING,\n+                                        new PartitioningScheme(Partitioning.create(FIXED_ARBITRARY_DISTRIBUTION, ImmutableList.of()), node.getOutputVariables()),\n+                                        distributedChildren,\n+                                        distributedOutputLayouts,\n+                                        false,\n+                                        Optional.empty()));\n+                    }\n                 }\n-\n+                // TODO: We should support multiple table scans in a single fragment for efficient union implementation\n+                // TODO: Multiple table scans are already partially supported (for partitioned (bucketed) tables)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM3ODkxNQ=="}, "originalCommit": {"oid": "9352a0d6db268ce76afd8bf68d5786e475df15eb"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NTUwMzMwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQwNTo1MDoyNFrOGWoUkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNToxMDo1N1rOGW7mqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM4MjQ4Mw==", "bodyText": "There is no Airlift MoreFutures utility methods to do this? :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426382483", "createdAt": "2020-05-18T05:50:24Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -506,6 +517,34 @@ private QueryInfo createQueryInfo(Optional<Throwable> failure)\n             // TODO: create query info\n             return null;\n         }\n+\n+        private static <T> void waitFor(Collection<Future<T>> futures)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50535a959c402218966f62b4c6f7165f41c03ee2"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODQxMA==", "bodyText": "They all operate on ListeableFuture. The Spark API returns just a regular Future =\\", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r426698410", "createdAt": "2020-05-18T15:10:57Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -506,6 +517,34 @@ private QueryInfo createQueryInfo(Optional<Throwable> failure)\n             // TODO: create query info\n             return null;\n         }\n+\n+        private static <T> void waitFor(Collection<Future<T>> futures)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM4MjQ4Mw=="}, "originalCommit": {"oid": "50535a959c402218966f62b4c6f7165f41c03ee2"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1OTY2NTEyOnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MDowNFrOGXQvEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MDowNFrOGXQvEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NDYyNA==", "bodyText": "Maybe explain now there are two situations UNION will be executed by gathering into the same worker\n\nParent node doesn't prefer distributed partitioning (e.g. TableFinish)\nParent node prefers distributed partitioning  but prefer_distributed_union is disabled\n\nFor the second situation, we should support multiple table scans in a single fragment.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427044624", "createdAt": "2020-05-19T05:50:04Z", "author": {"login": "wenleix"}, "path": "presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/AddExchanges.java", "diffHunk": "@@ -1202,9 +1205,30 @@ public PlanWithProperties visitUnion(UnionNode node, PreferredProperties parentP\n                 // children partitioning and don't GATHER partitioned inputs\n                 // TODO: add FIXED_ARBITRARY_DISTRIBUTION support on non empty singleNodeChildren\n                 if (!parentPartitioningPreference.isPresent() || parentPartitioningPreference.get().isDistributed()) {\n-                    return arbitraryDistributeUnion(node, distributedChildren, distributedOutputLayouts);\n+                    // TODO: can we insert LOCAL exchange for one child SOURCE distributed and another HASH distributed?\n+                    if (getNumberOfTableScans(distributedChildren) == 0 && isSameOrSystemCompatiblePartitions(extractRemoteExchangePartitioningHandles(distributedChildren))) {\n+                        // No source distributed child, we can use insert LOCAL exchange\n+                        // TODO: if all children have the same partitioning, pass this partitioning to the parent\n+                        // instead of \"arbitraryPartition\".\n+                        return new PlanWithProperties(node.replaceChildren(distributedChildren));\n+                    }\n+                    else if (preferDistributedUnion) {\n+                        // Presto currently can not execute stage that has multiple table scans, so in that case\n+                        // we have to insert REMOTE exchange with FIXED_ARBITRARY_DISTRIBUTION instead of local exchange\n+                        return new PlanWithProperties(\n+                                new ExchangeNode(\n+                                        idAllocator.getNextId(),\n+                                        REPARTITION,\n+                                        REMOTE_STREAMING,\n+                                        new PartitioningScheme(Partitioning.create(FIXED_ARBITRARY_DISTRIBUTION, ImmutableList.of()), node.getOutputVariables()),\n+                                        distributedChildren,\n+                                        distributedOutputLayouts,\n+                                        false,\n+                                        Optional.empty()));\n+                    }\n                 }\n-\n+                // TODO: We should support multiple table scans in a single fragment for efficient union implementation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1OTY2Nzg0OnYy", "diffSide": "RIGHT", "path": "presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MToxMlrOGXQwoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MToxMlrOGXQwoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTAyNQ==", "bodyText": "we can set this property to be hidden :)", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427045025", "createdAt": "2020-05-19T05:51:12Z", "author": {"login": "wenleix"}, "path": "presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java", "diffHunk": "@@ -762,6 +763,11 @@ public SystemSessionProperties(\n                         OPTIMIZE_COMMON_SUB_EXPRESSIONS,\n                         \"Extract and compute common sub-expressions in projection\",\n                         featuresConfig.isOptimizeCommonSubExpressions(),\n+                        false),\n+                booleanProperty(\n+                        PREFER_DISTRIBUTED_UNION,\n+                        \"Prefer distributed union\",\n+                        featuresConfig.isPreferDistributedUnion(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1OTY3MjE1OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkSettingsRequirements.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQwNTo1MzoyMlrOGXQzUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTo0NToxMlrOGYSNbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTcxNA==", "bodyText": "I am actually thinking to set PREFER_DISTRIBUTED_UNION  to be false in production -- this would fail user query loudly instead of make it run but somehow very slow...\nPREFER_DISTRIBUTED_UNION should only be used to entertain test.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427045714", "createdAt": "2020-05-19T05:53:22Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkSettingsRequirements.java", "diffHunk": "@@ -49,6 +50,7 @@ public void verify(SparkContext sparkContext, Session session)\n                 \"grouped execution is not supported\");\n         verify(!isRedistributeWrites(session), \"redistribute writes is not supported\");\n         verify(!isScaleWriters(session), \"scale writes is not supported\");\n+        verify(!isPreferDistributedUnion(session), \"prefer distributed union is expected to be disabled\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODExNzM1Ng==", "bodyText": "That makes sense. Let me change that.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r428117356", "createdAt": "2020-05-20T15:45:12Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkSettingsRequirements.java", "diffHunk": "@@ -49,6 +50,7 @@ public void verify(SparkContext sparkContext, Session session)\n                 \"grouped execution is not supported\");\n         verify(!isRedistributeWrites(session), \"redistribute writes is not supported\");\n         verify(!isScaleWriters(session), \"scale writes is not supported\");\n+        verify(!isPreferDistributedUnion(session), \"prefer distributed union is expected to be disabled\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTcxNA=="}, "originalCommit": {"oid": "afb0fe58126e30ac93dd867d5a837d82ced6db9f"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Mzk1MTQwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionException.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDoyOTo1NVrOGX63DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTo1MDo0NFrOGYSclQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNDc5Nw==", "bodyText": "So wrap the encoded execution info with \"|\" is to allow being parsed out in PrestoSparkExecutionExceptionFactory ?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427734797", "createdAt": "2020-05-20T04:29:55Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionException.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+public abstract class PrestoSparkExecutionException\n+        extends RuntimeException\n+{\n+    protected PrestoSparkExecutionException(String message, String encodedExecutionFailureInfo, Throwable cause)\n+    {\n+        super(formatExceptionMessage(message, encodedExecutionFailureInfo), cause);\n+    }\n+\n+    private static String formatExceptionMessage(String message, String encodedExecutionFailureInfo)\n+    {\n+        return message + \" | ExecutionFailureInfo[\" + encodedExecutionFailureInfo + \"] |\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEyMTIzNw==", "bodyText": "Also it helps to visually divide it when reading the error message from the executor", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r428121237", "createdAt": "2020-05-20T15:50:44Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionException.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+public abstract class PrestoSparkExecutionException\n+        extends RuntimeException\n+{\n+    protected PrestoSparkExecutionException(String message, String encodedExecutionFailureInfo, Throwable cause)\n+    {\n+        super(formatExceptionMessage(message, encodedExecutionFailureInfo), cause);\n+    }\n+\n+    private static String formatExceptionMessage(String message, String encodedExecutionFailureInfo)\n+    {\n+        return message + \" | ExecutionFailureInfo[\" + encodedExecutionFailureInfo + \"] |\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNDc5Nw=="}, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Mzk2MDgxOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDozNjoxOVrOGX68vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDozNjoxOVrOGX68vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjI1Mw==", "bodyText": "nit: What about toPrestoSparkExecutionException?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427736253", "createdAt": "2020-05-20T04:36:19Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Mzk2MTM2OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDozNjo0OFrOGX69Iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNzowNzowNVrOGZM3uA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjM1NQ==", "bodyText": "is compression necessary?", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427736355", "createdAt": "2020-05-20T04:36:48Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEyMjM0NA==", "bodyText": "Not necessary, but generally desired since we serialize to Json that is very verbose.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r428122344", "createdAt": "2020-05-20T15:52:18Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjM1NQ=="}, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3ODQ1Ng==", "bodyText": "@arhimondr : Ideally thrift encoding should be used then .But ExecutionFailureInfo contains recursive structure so it's not an option \ud83d\ude15", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r429078456", "createdAt": "2020-05-22T07:07:05Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjM1NQ=="}, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Mzk2MzU3OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDozODozMFrOGX6-gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTo1Mjo0NVrOGYSiQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjcwNA==", "bodyText": "nit: what about extractExecutionFailureInfo?  -- ditto for other 2 tryDecode method.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427736704", "createdAt": "2020-05-20T04:38:30Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);\n+        String encoded = Base64.getEncoder().encodeToString(compressed);\n+        if (isRetryable(failureInfo)) {\n+            return new PrestoSparkRetryableExecutionException(throwable.getMessage(), encoded, throwable);\n+        }\n+        else {\n+            return new PrestoSparkNonRetryableExecutionException(throwable.getMessage(), encoded, throwable);\n+        }\n+    }\n+\n+    public Optional<ExecutionFailureInfo> tryDecode(SparkException sparkException)\n+    {\n+        return tryDecode(sparkException.getMessage());\n+    }\n+\n+    public Optional<ExecutionFailureInfo> tryDecode(PrestoSparkExecutionException executionException)\n+    {\n+        return tryDecode(executionException.getMessage());\n+    }\n+\n+    private Optional<ExecutionFailureInfo> tryDecode(String message)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEyMjY5MA==", "bodyText": "Sounds great", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r428122690", "createdAt": "2020-05-20T15:52:45Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);\n+        String encoded = Base64.getEncoder().encodeToString(compressed);\n+        if (isRetryable(failureInfo)) {\n+            return new PrestoSparkRetryableExecutionException(throwable.getMessage(), encoded, throwable);\n+        }\n+        else {\n+            return new PrestoSparkNonRetryableExecutionException(throwable.getMessage(), encoded, throwable);\n+        }\n+    }\n+\n+    public Optional<ExecutionFailureInfo> tryDecode(SparkException sparkException)\n+    {\n+        return tryDecode(sparkException.getMessage());\n+    }\n+\n+    public Optional<ExecutionFailureInfo> tryDecode(PrestoSparkExecutionException executionException)\n+    {\n+        return tryDecode(executionException.getMessage());\n+    }\n+\n+    private Optional<ExecutionFailureInfo> tryDecode(String message)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNjcwNA=="}, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Mzk2Nzc4OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDo0MToyOVrOGX7BHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDo0MToyOVrOGX7BHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczNzM3NQ==", "bodyText": "nit: encodedExecutionFailureInfo", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427737375", "createdAt": "2020-05-20T04:41:29Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkExecutionExceptionFactory.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.execution.ExecutionFailureInfo;\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorType;\n+import org.apache.spark.SparkException;\n+\n+import javax.inject.Inject;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Base64;\n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.zip.DeflaterInputStream;\n+import java.util.zip.InflaterOutputStream;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.util.Failures.toFailure;\n+import static com.google.common.io.ByteStreams.toByteArray;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.regex.Pattern.DOTALL;\n+import static java.util.regex.Pattern.MULTILINE;\n+\n+public class PrestoSparkExecutionExceptionFactory\n+{\n+    private static final Pattern PATTERN = Pattern.compile(\".*\\\\| ExecutionFailureInfo\\\\[([^\\\\[\\\\]]+)\\\\] \\\\|.*\", MULTILINE | DOTALL);\n+\n+    private final JsonCodec<ExecutionFailureInfo> codec;\n+\n+    @Inject\n+    public PrestoSparkExecutionExceptionFactory(JsonCodec<ExecutionFailureInfo> codec)\n+    {\n+        this.codec = requireNonNull(codec, \"codec is null\");\n+    }\n+\n+    public PrestoSparkExecutionException translate(Throwable throwable)\n+    {\n+        ExecutionFailureInfo failureInfo = toFailure(throwable);\n+        byte[] serialized = codec.toJsonBytes(failureInfo);\n+        byte[] compressed = compress(serialized);\n+        String encoded = Base64.getEncoder().encodeToString(compressed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2Mzk3NzEwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDo0Nzo1MVrOGX7HAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNTo1NTo1MVrOGYSrBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczODg4Mw==", "bodyText": "Instead of doing instanceof here, shouldn't we try to catch SparkException and PrestoSparkExecutionException in line 357? -- is that the more \"Java\" way to handle exceptions?\nIn order to do that, you probably need to refactor line 358 to 370 into something like tryRollbackAndLog", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r427738883", "createdAt": "2020-05-20T04:47:51Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -341,24 +354,36 @@ private PrestoSparkQueryExecution(\n                 rddResults = doExecute(plan);\n                 commit(session, transactionManager);\n             }\n-            catch (RuntimeException executionFailure) {\n+            catch (Exception executionFailure) {\n                 try {\n                     rollback(session, transactionManager);\n                 }\n                 catch (RuntimeException rollbackFailure) {\n-                    if (executionFailure != rollbackFailure) {\n-                        executionFailure.addSuppressed(rollbackFailure);\n-                    }\n+                    log.error(rollbackFailure, \"Encountered error when performing rollback\");\n                 }\n+\n                 try {\n                     queryCompletedEvent(Optional.of(executionFailure));\n                 }\n                 catch (RuntimeException eventFailure) {\n-                    if (executionFailure != eventFailure) {\n-                        executionFailure.addSuppressed(eventFailure);\n+                    log.error(eventFailure, \"Error publishing query completed event\");\n+                }\n+\n+                if (executionFailure instanceof SparkException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEyNDkzMg==", "bodyText": "Instead of doing instanceof here, shouldn't we try to catch SparkException and PrestoSparkExecutionException in line 357?\n\nIt feels like more just a matter of taste. We can try to catch these two exceptions separately, but then we need to make sure we replicate the rollback and query event logic correctly. We can refactor that logic into a separate method, as you suggested, but still I would prefer to have only a single block of exception handling, as it feels like it is easier to follow.", "url": "https://github.com/prestodb/presto/pull/14515#discussion_r428124932", "createdAt": "2020-05-20T15:55:51Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -341,24 +354,36 @@ private PrestoSparkQueryExecution(\n                 rddResults = doExecute(plan);\n                 commit(session, transactionManager);\n             }\n-            catch (RuntimeException executionFailure) {\n+            catch (Exception executionFailure) {\n                 try {\n                     rollback(session, transactionManager);\n                 }\n                 catch (RuntimeException rollbackFailure) {\n-                    if (executionFailure != rollbackFailure) {\n-                        executionFailure.addSuppressed(rollbackFailure);\n-                    }\n+                    log.error(rollbackFailure, \"Encountered error when performing rollback\");\n                 }\n+\n                 try {\n                     queryCompletedEvent(Optional.of(executionFailure));\n                 }\n                 catch (RuntimeException eventFailure) {\n-                    if (executionFailure != eventFailure) {\n-                        executionFailure.addSuppressed(eventFailure);\n+                    log.error(eventFailure, \"Error publishing query completed event\");\n+                }\n+\n+                if (executionFailure instanceof SparkException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczODg4Mw=="}, "originalCommit": {"oid": "d70f28a22701873490635eaaf814edd808da22c1"}, "originalPosition": 143}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2617, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}