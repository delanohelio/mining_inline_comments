{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwMzc2MTE0", "number": 14559, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzowMToxNVrOEDNvgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwNTowOToyN1rOEFS0Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA2MzM4OnYy", "diffSide": "RIGHT", "path": "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsNodePartitioningProvider.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzowMToxNVrOGgHoCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwNjoyNzoxMFrOGi4f6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMjU1Mg==", "bodyText": "One theoretic caveat here is the number of buckets, returned by getBucketNodeMap and getBucketCount can be different if the number of nodes changed during the two method calls. Since nodeManager.getRequiredWorkerNodes() will return different list.\nHowever, this would only happen for 3 testing connector: BlackHole, TPCDS, TPCH as the \"partitioning handle\" is somewhat artificial in these connectors. For \"real\" connector partitioning handle, the number of buckets should be a constant property in the partitioning handle, thus. it's not a problem.\nI think the right long term solution is to allow connector partitioning handle to return the number of buckets :). So the current implementation looks OK.\nBTW: can we just throw in getBucketCount for BlackHole, TPCDS and TPCH connector?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436332552", "createdAt": "2020-06-07T07:01:15Z", "author": {"login": "wenleix"}, "path": "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsNodePartitioningProvider.java", "diffHunk": "@@ -70,4 +70,12 @@ public BucketFunction getBucketFunction(ConnectorTransactionHandle transactionHa\n     {\n         throw new UnsupportedOperationException();\n     }\n+\n+    @Override\n+    public int getBucketCount(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorPartitioningHandle partitioningHandle)\n+    {\n+        Set<Node> nodes = nodeManager.getRequiredWorkerNodes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3d04114b4123d0d3abb7bf84a090f25f543ad0c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAzNzA3NQ==", "bodyText": "One theoretic caveat here is the number of buckets, returned by getBucketNodeMap and getBucketCount can be different if the number of nodes changed during the two method calls. Since nodeManager.getRequiredWorkerNodes() will return different list.\n\nYeah, that is in theory possible. However it is also possible that getBucketNodeMap called multiple times may return different bucket count. But I agree, that is a little brittle.\n\nBTW: can we just throw in getBucketCount for BlackHole, TPCDS and TPCH connector?\n\nI would prefer to keep it as is, as it is sometimes useful to have TPC-DC and TPC-H connectors enabled in Presto on Spark.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439037075", "createdAt": "2020-06-11T20:04:29Z", "author": {"login": "arhimondr"}, "path": "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsNodePartitioningProvider.java", "diffHunk": "@@ -70,4 +70,12 @@ public BucketFunction getBucketFunction(ConnectorTransactionHandle transactionHa\n     {\n         throw new UnsupportedOperationException();\n     }\n+\n+    @Override\n+    public int getBucketCount(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorPartitioningHandle partitioningHandle)\n+    {\n+        Set<Node> nodes = nodeManager.getRequiredWorkerNodes();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMjU1Mg=="}, "originalCommit": {"oid": "d3d04114b4123d0d3abb7bf84a090f25f543ad0c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzMDQ0MA==", "bodyText": "Yeah, that is in theory possible. However it is also possible that getBucketNodeMap called multiple times may return different bucket count. But I agree, that is a little brittle.\n\nRight. Long long ago (before grouped execution), getBucketNodeMap is only called once in execution. Now we already called once in PlanFragmenter, once in execution.\nAnyhow. :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439230440", "createdAt": "2020-06-12T06:27:10Z", "author": {"login": "wenleix"}, "path": "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsNodePartitioningProvider.java", "diffHunk": "@@ -70,4 +70,12 @@ public BucketFunction getBucketFunction(ConnectorTransactionHandle transactionHa\n     {\n         throw new UnsupportedOperationException();\n     }\n+\n+    @Override\n+    public int getBucketCount(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorPartitioningHandle partitioningHandle)\n+    {\n+        Set<Node> nodes = nodeManager.getRequiredWorkerNodes();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMjU1Mg=="}, "originalCommit": {"oid": "d3d04114b4123d0d3abb7bf84a090f25f543ad0c"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA2ODg1OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/SerializedTaskSource.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxMDo1NlrOGgHqtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxMDo1NlrOGgHqtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMzIzOA==", "bodyText": "nit: maybe call it SerializedPrestoSparkTaskSource? -- so when other Presto developers search for TaskSource, they don't need to worry about \"SerializedTaskSource\":", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436333238", "createdAt": "2020-06-07T07:10:56Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/SerializedTaskSource.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import java.io.Serializable;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class SerializedTaskSource", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA3MTEwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxNDo0NlrOGgHrzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoxNDo0NlrOGgHrzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzMzUxNw==", "bodyText": "nit: do you mean set the number of output partitions?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436333517", "createdAt": "2020-06-07T07:14:46Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -153,20 +152,10 @@ public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonC\n         // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n         checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-        int hashPartitionCount = getHashPartitionCount(session);\n-\n-        // configure number of output partitions\n-        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-        }\n-\n-        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n-            checkArgument(\n-                    fragment.getTableScanSchedulingOrder().isEmpty(),\n-                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n-                    fragment.getId(),\n-                    fragment.getPartitioning());\n+        // set's the number of output partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA3NTY0OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoyMzowOFrOGgHuFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNjozNjoyN1rOGj6BzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg==", "bodyText": "As you might guess, I will use more \"plain\" style if,list,for-loop for line 290-303 \ud83d\ude03 . But it's personal taste.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334102", "createdAt": "2020-06-07T07:23:08Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0MTc5MQ==", "bodyText": "I actually wanted to use IntStream specifically to avoid materializing IntStream.range(0, integer)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439041791", "createdAt": "2020-06-11T20:11:34Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzMDA4Ng==", "bodyText": "I actually wanted to use IntStream specifically to avoid materializing IntStream.range(0, integer)\n\nWhy not just using for loop? ...", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439230086", "createdAt": "2020-06-12T06:26:08Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM2MjI2OA==", "bodyText": "You will have to loop either over the partitions from taskSourcesMap or over all partitions provided by expectedNumberOfPartitions . With simple for loop it seems like you would have to duplicate the loop body twice.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439362268", "createdAt": "2020-06-12T11:23:11Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MTEyOQ==", "bodyText": "@arhimondr : Stream API often yields non-trivial performance overhead (see, for example #11374) . And I don't think this part of code is performance sensitive? (it's just doing plan once on Driver?)\nI personally prefer not using Stream API unless it's pretty clear (e.g. the classic .map().filter().XXX pattern. This has also been discussed in #11374 (comment) . And I also have expressed similar ideas in my other code reviews.\nI understand a general policy discussion about Stream API should be done by TSC.\nIn this specific case, I found it make hard for me to follow the core logic with Stream API \ud83d\ude15", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439791129", "createdAt": "2020-06-14T04:42:07Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MzMyMQ==", "bodyText": "@arhimondr : Java Stream API can also have non-trivial memory allocation overhead, see #13984", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439793321", "createdAt": "2020-06-14T05:27:53Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMwNDA3Ng==", "bodyText": "Replaces with 2 branches and regular loops. Added comments.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r440304076", "createdAt": "2020-06-15T16:36:27Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEwMg=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 319}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA3NTgwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoyMzozNlrOGgHuLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDoxMjo1MFrOGitCsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEyNw==", "bodyText": "Add a comment explain why eagerly remove the task sources (GC, right? )", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334127", "createdAt": "2020-06-07T07:23:36Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions\n+                .map(integer -> IntStream.range(0, integer))\n+                .orElseGet(() -> new ArrayList<>(taskSourcesMap.keySet()).stream().mapToInt(Integer::intValue));\n+\n+        List<List<SerializedTaskSource>> partitionedTaskSources = new ArrayList<>();\n+        partitions.forEach(partition -> {\n+            List<TaskSource> taskSources = taskSourcesMap.removeAll(partition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 325}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0MjczNw==", "bodyText": "Added a comment", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439042737", "createdAt": "2020-06-11T20:12:50Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions\n+                .map(integer -> IntStream.range(0, integer))\n+                .orElseGet(() -> new ArrayList<>(taskSourcesMap.keySet()).stream().mapToInt(Integer::intValue));\n+\n+        List<List<SerializedTaskSource>> partitionedTaskSources = new ArrayList<>();\n+        partitions.forEach(partition -> {\n+            List<TaskSource> taskSources = taskSourcesMap.removeAll(partition);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDEyNw=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 325}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA3NzU2OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoyNzoxN1rOGgHvHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDoxMzoyNFrOGitEeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDM2NA==", "bodyText": "curious: why using SetMultimap instead of ListMultimap?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334364", "createdAt": "2020-06-07T07:27:17Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -330,49 +324,24 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         return splits;\n     }\n \n-    private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> splits, int numTasks)\n+    private SetMultimap<Integer, ScheduledSplit> assignSplitsToTasks(Session session, PartitioningHandle partitioning, List<ScheduledSplit> splits)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0MzE5Mw==", "bodyText": "TaskSource takes Set<ScheduledSplit> splits =\\", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439043193", "createdAt": "2020-06-11T20:13:24Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -330,49 +324,24 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         return splits;\n     }\n \n-    private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> splits, int numTasks)\n+    private SetMultimap<Integer, ScheduledSplit> assignSplitsToTasks(Session session, PartitioningHandle partitioning, List<ScheduledSplit> splits)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDM2NA=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 354}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA3ODI3OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzoyODozOFrOGgHveA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDoxNDozM1rOGitH0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDQ1Ng==", "bodyText": "nit: why not just using a for-each loop? ;)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334456", "createdAt": "2020-06-07T07:28:38Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 301}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0NDA1MQ==", "bodyText": "I like this style as you can give a key and a value a meaningful name\n\n(partitionId, splits) ->", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439044051", "createdAt": "2020-06-11T20:14:33Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDQ1Ng=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 301}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA4MTYwOnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozMzo0N1rOGgHxFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDoxNToyOVrOGitKsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDg2OA==", "bodyText": "So now the anonymous lambda created in legacy TaskProcessors now has dedicated named class, this is great.\nCurious why this class has to stay in classloader_interface ?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334868", "createdAt": "2020-06-07T07:33:47Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0NDc4Ng==", "bodyText": "It has to be serializable. If it is loaded not by the Spark class loader it cannot get serialized =\\", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439044786", "createdAt": "2020-06-11T20:15:29Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDg2OA=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA4MTY3OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozMzo1NlrOGgHxGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozMzo1NlrOGgHxGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDg3NQ==", "bodyText": "maybe call it process? :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334875", "createdAt": "2020-06-07T07:33:56Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA4MjQ0OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozNTozMFrOGgHxfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozNTozMFrOGgHxfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNDk3NQ==", "bodyText": "should there be a check that fragmentIds.size() == inputs.size() ?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436334975", "createdAt": "2020-06-07T07:35:30Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();\n+        for (int i = 0; i < fragmentIds.size(); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxODA4NDQ4OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwNzozOToyN1rOGgHykw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDoxODoyMVrOGitToA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNTI1MQ==", "bodyText": "For now Looks like it's a wrapper over ParallelCollectionPartition . I assume it will have non-trivial functionality when colocated join is supported :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436335251", "createdAt": "2020-06-07T07:39:27Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNjQyOQ==", "bodyText": "In fact I need to take a closer look into this. Looks like it's more than just a wrapper :) (especially this getInputIterators method)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r436336429", "createdAt": "2020-06-07T07:54:54Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNTI1MQ=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0NzA3Mg==", "bodyText": "For now Looks like it's a wrapper over ParallelCollectionPartition . I assume it will have non-trivial functionality when colocated join is supported :)\n\nYeah. It provides us with a flexibility to manually assign TaskSource to spark partitions.  ParallelRdd (the result of parallelize) just does round robin.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439047072", "createdAt": "2020-06-11T20:18:21Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMzNTI1MQ=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzI5MTUzOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNToxNDo0MlrOGg5BmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwNjoyOTowOVrOGi4imw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MTkxMw==", "bodyText": "nit: what about rename rddInputs to shuffleInputs?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437141913", "createdAt": "2020-06-09T05:14:42Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0NzgyOA==", "bodyText": "I don't feel particularly strong. I can do this rename in a separate PR if you wish, it's been like that before.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439047828", "createdAt": "2020-06-11T20:19:22Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MTkxMw=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzMTEzMQ==", "bodyText": "@arhimondr : Previously there is only one type of input (shuffle input). Now with two types of inputs, just saying rddInputs can be confusing when reading the code.\nFeel free to to it in a separate PR.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439231131", "createdAt": "2020-06-12T06:29:09Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MTkxMw=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzMwMTU0OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNToyMDozNlrOGg5H1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMTozOToyMlrOGjA6Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzUxMQ==", "bodyText": "My understanding is this is for special case that the fragment only has VALUES input? -- still we need a dumb PrestoSparkTaskSourceRdd with empty list (instead of making taskSourceRdd to be Optional.empty(). This can be a bit confusing to read.\nIdeally we want to eliminate this dumb PrestoSparkTaskSourceRdd . But if it's required, my suggestions are:\n\nAdd comment explain why we cannot make taskSourceRdd to be simply Optional.empty\nMaybe have a factory method such as PrestoSparkTaskSourceRdd.createEmptyTaskSourceRdd?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437143511", "createdAt": "2020-06-09T05:20:36Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA0OTc0NA==", "bodyText": "We need to somehow create exactly 1 partition, so exactly one task is scheduled. Thats why we need this single partition input RDD. I agree that this is a bit of a hack, and in theory it is possible to create a single \"dummy\" partition of the PrestoSparkTaskRdd itself. But that might be a little messy, so I'm kinda trying to chose a lesser evil.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439049744", "createdAt": "2020-06-11T20:21:56Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzUxMQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzNjM5OA==", "bodyText": "@arhimondr : If the RDD becomes special enough in the case of SINGLE_DISTRIBUTION , a dedicated RDD might not be a bad idea, but you might have different opinions :) .\nIn any case, for now just explain in comment when the list is empty, it means exactly 1 partition (this is not intuitive to me), and perhaps a TODO about figure out less hacky approach? (We have 20% time assigned to Better Engineering).\nStill , what about have a factory method such as PrestoSparkTaskSourceRdd.createRddWithSingleDistribution so you hide the detail that creating a PrestoSparkTaskSourceRdd with ImmutableList.of(ImmutableList.of()) as task source?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439236398", "createdAt": "2020-06-12T06:43:52Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzUxMQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM2ODI5MA==", "bodyText": "@arhimondr : If the RDD becomes special enough in the case of SINGLE_DISTRIBUTION , a dedicated RDD might not be a bad idea, but you might have different opinions :) .\n\nI would prefer to have a single way of creating a presto task. Otherwise we would have to maintain multiple ways. I think that's the main motivation to stick with  only PrestoSparkTaskSourceRdd , PrestoSparkTaskRdd.\n\nPrestoSparkTaskSourceRdd.createRddWithSingleDistribution\n\nThen maybe createRddWithSingleEmptyPartition. But that might also be quite confusing. Let me leave a comment instead.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439368290", "createdAt": "2020-06-12T11:39:22Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +230,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzUxMQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 285}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzMxMDE2OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNToyNTozM1rOGg5NGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNToyNTozM1rOGg5NGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NDg1OA==", "bodyText": "nit : what about getShuffleInputIterators", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437144858", "createdAt": "2020-06-09T05:25:33Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzMzMDI2OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTozNjozMlrOGg5Zcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDoyNzoyOFrOGitqlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODAxOA==", "bodyText": "why not just using PrestoSparkTaskSourceRdd here?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437148018", "createdAt": "2020-06-09T05:36:32Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA1Mjk1MQ==", "bodyText": "Good point", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439052951", "createdAt": "2020-06-11T20:27:28Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODAxOA=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzMzNTYzOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTozOToxMVrOGg5csA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTozOToxMVrOGg5csA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0ODg0OA==", "bodyText": "unpartitioned split / partitioned split is kind of overloaded in Presto. Since in the task execution context, it often means \"splits from table scan source\" vs. \"splits from exchange source\".\nWhat about saying \"splits from unbucketed table\" vs. \"splits from bucketed table\"", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437148848", "createdAt": "2020-06-09T05:39:11Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -330,49 +337,61 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         return splits;\n     }\n \n-    private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> splits, int numTasks)\n+    private SetMultimap<Integer, ScheduledSplit> assignSplitsToTasks(Session session, PartitioningHandle partitioning, List<ScheduledSplit> splits)\n     {\n-        checkArgument(numTasks > 0, \"numTasks must be greater then zero\");\n-        List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-        for (int i = 0; i < numTasks; i++) {\n-            assignedSplits.add(new ArrayList<>());\n+        // unpartitioned splits", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 392}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzM1MDgzOnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo0NjoyOVrOGg5lsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMDozMDozOFrOGitwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MTE1NQ==", "bodyText": "Add a comment explain the structure of partitionedTaskSources, also may consider rename it to taskSourcesByTaskId:\n// each element in taskSourcesByTaskId is a list of task sources assigned to the same Spark task. \n// When input tables are unbucketed, task sources are distributed randomly across all tasks. \n// When input tables are bucketed, each bucket in task sources will be assigned to one Spark task, and the assignment is compatible to potential shuffle inputs.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437151155", "createdAt": "2020-06-09T05:46:29Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd\n+        extends RDD<SerializedTaskSource>\n+{\n+    private List<List<SerializedTaskSource>> partitionedTaskSources;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA1NDM4Ng==", "bodyText": "Renames to taskSourcesByPartitionId, it feels like it fits better, as then in the PrestoSparkTaskSourceRdd we are creating partitions and iterating over partitions.\nAdded a comment as well.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439054386", "createdAt": "2020-06-11T20:30:38Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskSourceRdd.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Dependency;\n+import org.apache.spark.InterruptibleIterator;\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.ParallelCollectionPartition;\n+import org.apache.spark.rdd.RDD;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.stream.Collectors.toList;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+\n+public class PrestoSparkTaskSourceRdd\n+        extends RDD<SerializedTaskSource>\n+{\n+    private List<List<SerializedTaskSource>> partitionedTaskSources;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MTE1NQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzM2NDYwOnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1MzoxNFrOGg5uHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMTozMDoyM1rOGjAtLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzMxMQ==", "bodyText": "This partitions.get(0) is not easy to understand :).\nI have two thoughts:\n\nJust inline this getTaskSourceIterator method.\nAnother possibility is to consider change the signature of this method to be getTaskSourceIterator(Optional<Partition> partitions, TaskContext context). And let compute to decide whether to pass in Optional.empty() or partitions.get(0).", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437153311", "createdAt": "2020-06-09T05:53:14Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA1NjMxMA==", "bodyText": "Yeah, generally this class is a little messy, as it has to multiplex both, inputs with splits with inputs with rows. I was trying to meddle it around, but it seems that it is getting even worse.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439056310", "createdAt": "2020-06-11T20:34:44Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzMxMQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzMjY5MQ==", "bodyText": "@arhimondr : What about the second option (let compute to decide whether to pass in Optional.empty() or partitions.get(0)\nIf it doesn't work (similar to #14559 (comment)), maybe add a comment about refactor and I can see if there is anything I can help later?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439232691", "createdAt": "2020-06-12T06:33:30Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzMxMQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM2NDkwOQ==", "bodyText": "Actually let me change this code to store source partitions as a last element. It might be easier to read it then.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439364909", "createdAt": "2020-06-12T11:30:23Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzMxMQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzM4NzI2OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNjowNDoxMlrOGg58DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwNDo0Mzo1MFrOGjaueQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1Njg3Ng==", "bodyText": "Let's add some comment for this class. Feel free to adapt/modify the following:\n/**\n * PrestoSparkTaskRdd represents execution of Presto stage, it contains:\n *  - a list of shuffleInputRdds, each of the corresponding to a child stage.\n *  - an optional taskSourceRdd, which represents *ALL* the table scan inputs in this stage.\n *\n *  Table scan presents when join bucketed table with unbucketed table, for example:\n *        Join\n *        /  \\\n *    Scan  Remote\n *  In this case, bucket to Spark partition mapping has to be consistent with the Spark shuffle partition.\n *\n *  shuffleInputRdds can also be empty when the stage partitioning is SINGLE_DISTRIBUTION.\n *  TODO: Consider have dedicated RDD implementation for SINGLE_DISTRIBUTION.\n *\n *  The broadcast input is encapsulated in taskProcessor.\n */\n\nLet me know if the TODO suggestion (TODO: Consider have dedicated RDD implementation for SINGLE_DISTRIBUTION) makes sense. Since I personally found it difficult to reason when both shuffleInputRdds and taskSourceRdd can be empty.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437156876", "createdAt": "2020-06-09T06:04:12Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5NTIxNw==", "bodyText": "(TODO: Consider have dedicated RDD implementation for SINGLE_DISTRIBUTION)\n\nI'm not particularly convinced that a separate RDD specifically for single distribution makes a lot of sense, especially if sometimes single distributed fragment may still have inputs. It might make things even more confusing.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439095217", "createdAt": "2020-06-11T21:59:04Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1Njg3Ng=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MTIyNQ==", "bodyText": "@arhimondr : Yeah, let's make how to handle SINGLE_DISTRIBUTION as a future work :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439791225", "createdAt": "2020-06-14T04:43:50Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1Njg3Ng=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzM5MTE3OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNjowNTo1MFrOGg5-Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMTozMTowMVrOGjAuHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NzQ3OQ==", "bodyText": "Here a implicit \"sublist\" is done and makes the code a bit difficult to follow (similar argument to getTaskSourceIterator). Would it make sense to do the partitions.subList(1, ...) in compute?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437157479", "createdAt": "2020-06-09T06:05:50Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)\n+    {\n+        if (inputRdds.isEmpty()) {\n+            return emptyList();\n+        }\n+        int startIndex = taskSourceRdd != null ? 1 : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5NTY3Mw==", "bodyText": "I remember I was trying to do something like that at the beginning, but it was still messy + it was more code. I would prefer to keep it as is for now. Let me know what you think?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439095673", "createdAt": "2020-06-11T22:00:11Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)\n+    {\n+        if (inputRdds.isEmpty()) {\n+            return emptyList();\n+        }\n+        int startIndex = taskSourceRdd != null ? 1 : 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NzQ3OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzNjgyNA==", "bodyText": "@arhimondr : I see. Maybe add a TODO about refactor and maybe I can look into that later? :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439236824", "createdAt": "2020-06-12T06:45:02Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)\n+    {\n+        if (inputRdds.isEmpty()) {\n+            return emptyList();\n+        }\n+        int startIndex = taskSourceRdd != null ? 1 : 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NzQ3OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM2NTE0OA==", "bodyText": "Let me try to store taskSourceRdd  as a last element. It might make things a little bit less confusing.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439365148", "createdAt": "2020-06-12T11:31:01Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+public class PrestoSparkTaskRdd\n+        extends ZippedPartitionsBaseRDD<Tuple2<Integer, PrestoSparkRow>>\n+{\n+    private RDD<SerializedTaskSource> taskSourceRdd;\n+    private List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds;\n+    private PrestoSparkTaskProcessor taskProcessor;\n+\n+    public PrestoSparkTaskRdd(\n+            SparkContext context,\n+            Optional<RDD<SerializedTaskSource>> taskSourceRdd,\n+            List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds,\n+            PrestoSparkTaskProcessor taskProcessor)\n+    {\n+        super(context, getRDDSequence(taskSourceRdd, inputRdds), false, fakeClassTag());\n+        // Optional is not Java Serializable\n+        this.taskSourceRdd = taskSourceRdd.orElse(null);\n+        this.inputRdds = unmodifiableList(new ArrayList<>(inputRdds));\n+        this.taskProcessor = context.clean(requireNonNull(taskProcessor, \"taskProcessor is null\"), true);\n+    }\n+\n+    private static Seq<RDD<?>> getRDDSequence(Optional<RDD<SerializedTaskSource>> taskSourceRdd, List<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds)\n+    {\n+        requireNonNull(taskSourceRdd, \"taskSourceRdd is null\");\n+        requireNonNull(inputRdds, \"inputRdds is null\");\n+        List<RDD<?>> list = new ArrayList<>();\n+        taskSourceRdd.ifPresent(list::add);\n+        list.addAll(inputRdds);\n+        return asScalaBuffer(list).toSeq();\n+    }\n+\n+    private static <T> ClassTag<T> fakeClassTag()\n+    {\n+        return scala.reflect.ClassTag$.MODULE$.apply(Tuple2.class);\n+    }\n+\n+    @Override\n+    public scala.collection.Iterator<Tuple2<Integer, PrestoSparkRow>> compute(Partition split, TaskContext context)\n+    {\n+        List<Partition> partitions = seqAsJavaList(((ZippedPartitionsPartition) split).partitions());\n+        int expectedPartitionsSize = (taskSourceRdd != null ? 1 : 0) + inputRdds.size();\n+        checkArgument(\n+                partitions.size() == expectedPartitionsSize,\n+                \"Unexpected partitions size. Expected: %s. Actual: %s.\",\n+                expectedPartitionsSize, partitions.size());\n+        return asScalaIterator(taskProcessor.apply(\n+                getTaskSourceIterator(partitions, context),\n+                getInputIterators(partitions, context)));\n+    }\n+\n+    private Iterator<SerializedTaskSource> getTaskSourceIterator(List<Partition> partitions, TaskContext context)\n+    {\n+        if (taskSourceRdd != null) {\n+            return asJavaIterator(taskSourceRdd.iterator(partitions.get(0), context));\n+        }\n+        return emptyIterator();\n+    }\n+\n+    private List<Iterator<Tuple2<Integer, PrestoSparkRow>>> getInputIterators(List<Partition> partitions, TaskContext context)\n+    {\n+        if (inputRdds.isEmpty()) {\n+            return emptyList();\n+        }\n+        int startIndex = taskSourceRdd != null ? 1 : 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NzQ3OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzM5MjczOnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNjowNjozNVrOGg5_fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNjowNjozNVrOGg5_fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1Nzc1OQ==", "bodyText": "nit: shuffleInputs?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437157759", "createdAt": "2020-06-09T06:06:35Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzQwMjUzOnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNjoxMTowOFrOGg6Fvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNjo1NjowOVrOGj6ulA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ==", "bodyText": "nit: shuffleInputsMap or fragmentIdToShuffleInput?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437159359", "createdAt": "2020-06-09T06:11:08Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM5OQ==", "bodyText": "So essentially, these two information are maintained in different classes:\n\nfragmentIds list is maintained in PrestoSparkTaskProcessor\nshuffleInput list is maintained in PrestoSparkTaskRdd.\n\nAnd these two lists are \"zipped\" when PrestoSparkTaskRdd#compute calls PrestoSparkTaskProcessor#apply.\nI am wondering if this \"zipping\" operation can be done in PrestoSparkTaskRdd#compute? -- Otherwise, it's a bit uneasy to reason the ordering of shuffleInput in PrestoSparkTaskRdd has to be consistent with the ordering of fragmentIds in PrestoSparkTaskProcessor...\nWhen I am reading the code, I have to traverse the code in the following way:\n\nOh, how to guarantee the ordering in this two list are consistent?\nok , shuffleInput is from PrestoSparkTaskRdd, and it's from PrestoSparkRddFactory\nAnd  the same PrestoSparkRddFactory provides the fragment id list when construct PrestoSparkTaskProcessor\nOK so these two lists are consistent...", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437159399", "createdAt": "2020-06-09T06:11:14Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5NzM1NQ==", "bodyText": "I am wondering if this \"zipping\" operation can be done in PrestoSparkTaskRdd#compute?\n\nCould you please elaborate a little bit more. I believe I'm missing something. The list of shuffleInput  is only available as a parameter of process in PrestoSparkTaskProcessor. What zipping you think can be done in PrestoSparkTaskRdd?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439097355", "createdAt": "2020-06-11T22:04:42Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIzNzQxMA==", "bodyText": "@arhimondr :\n\nCould you please elaborate a little bit more. I believe I'm missing something. The list of shuffleInput is only available as a parameter of process in PrestoSparkTaskProcessor.\n\nSo essentially, you are zipping fragmentIds and inputs into inputsMap. And fragmentIds is a field in PrestoSparkTaskProcessor, while inputs is passed as a parameter into PrestoSparkTaskProcessor#process", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439237410", "createdAt": "2020-06-12T06:46:37Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM2NTU2Ng==", "bodyText": "Yup", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439365566", "createdAt": "2020-06-12T11:32:13Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MTUyOA==", "bodyText": "@arhimondr :\n\nYup\n\nBut it's not fixed. I mean, the \"zipped inputs\" are from two different source (one stay in PrestoSparkTaskProcessor as a field member, one provided through PrestoSparkTaskProcessor#process). And it's quite confusing. For example, can they both be provided from PrestoSparkTaskProcessor#process?\nI would leave this as a future discussion for now.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439791528", "createdAt": "2020-06-14T04:50:04Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMxNTU0MA==", "bodyText": "Nevermind, I'm being slow. I think I got your point now. Refactored. Good catch.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r440315540", "createdAt": "2020-06-15T16:56:09Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskProcessor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableList;\n+import static java.util.Collections.unmodifiableMap;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkTaskProcessor\n+        implements Serializable\n+{\n+    private final PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider;\n+    private final SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor;\n+    private final List<String> fragmentIds;\n+    private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n+    // fragmentId -> Broadcast\n+    private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+\n+    public PrestoSparkTaskProcessor(\n+            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n+            SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor,\n+            List<String> fragmentIds,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        this.taskExecutorFactoryProvider = requireNonNull(taskExecutorFactoryProvider, \"taskExecutorFactoryProvider is null\");\n+        this.serializedTaskDescriptor = requireNonNull(serializedTaskDescriptor, \"serializedTaskDescriptor is null\");\n+        this.fragmentIds = unmodifiableList(new ArrayList<>(requireNonNull(fragmentIds, \"fragmentIds is null\")));\n+        this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n+        this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+    }\n+\n+    public Iterator<Tuple2<Integer, PrestoSparkRow>> apply(\n+            Iterator<SerializedTaskSource> serializedTaskSources,\n+            List<Iterator<Tuple2<Integer, PrestoSparkRow>>> inputs)\n+    {\n+        int partitionId = TaskContext.get().partitionId();\n+        int attemptNumber = TaskContext.get().attemptNumber();\n+        Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> inputsMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1OTM1OQ=="}, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNzgxOTUyOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToyNzoxMFrOGhlWLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMjowNzoxNVrOGiwbtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODA3Ng==", "bodyText": "Two questions:\n\nIs it possible some partition is missing from the taskSourcesMap? (e.g. table contains empty bucket)\nWill it happen that expectedNumberOfPartitions is present but different from the number of partitions in taskSourcesMap?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437868076", "createdAt": "2020-06-10T05:27:10Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions\n+                .map(integer -> IntStream.range(0, integer))\n+                .orElseGet(() -> new ArrayList<>(taskSourcesMap.keySet()).stream().mapToInt(Integer::intValue));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 321}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5ODI5NQ==", "bodyText": "Is it possible some partition is missing from the taskSourcesMap? (e.g. table contains empty bucket)\n\nYes, it is possible. taskSourcesMap.removeAll(partition) would return an empty list then. Even if the splits are empty, we still need to provide a partition, as the number of partitions should be the same for all the inputs that are zipped for consistency.\n\nWill it happen that expectedNumberOfPartitions is present but different from the number of partitions in taskSourcesMap?\n\nIf it happens it means that the fragmented plan is wrong. The partitioning of the fragment should be consistent with partitioning of the TableScan.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439098295", "createdAt": "2020-06-11T22:07:15Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();\n         ImmutableList.Builder<String> fragmentIds = ImmutableList.builder();\n-        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> rdds = ImmutableList.builder();\n+        ImmutableList.Builder<RDD<Tuple2<Integer, PrestoSparkRow>>> inputRdds = ImmutableList.builder();\n         for (Map.Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input : rddInputs.entrySet()) {\n             fragmentIds.add(input.getKey().toString());\n-            rdds.add(input.getValue().rdd());\n+            RDD<Tuple2<Integer, PrestoSparkRow>> rdd = input.getValue().rdd();\n+            inputRdds.add(rdd);\n+            if (!numberOfInputPartitions.isPresent()) {\n+                numberOfInputPartitions = Optional.of(rdd.getNumPartitions());\n+            }\n+            else {\n+                checkArgument(\n+                        numberOfInputPartitions.get() == rdd.getNumPartitions(),\n+                        \"Incompatible number of input partitions: %s != %s\",\n+                        numberOfInputPartitions.get(),\n+                        rdd.getNumPartitions());\n+            }\n         }\n \n-        Function<List<Iterator<Tuple2<Integer, PrestoSparkRow>>>, Iterator<Tuple2<Integer, PrestoSparkRow>>> taskProcessor = createTaskProcessor(\n+        PrestoSparkTaskProcessor taskProcessor = new PrestoSparkTaskProcessor(\n                 executorFactoryProvider,\n                 serializedTaskDescriptor,\n                 fragmentIds.build(),\n                 taskStatsCollector,\n                 toTaskProcessorBroadcastInputs(broadcastInputs));\n \n+        Optional<RDD<SerializedTaskSource>> taskSourceRdd;\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        if (!tableScans.isEmpty()) {\n+            PartitioningHandle partitioning = fragment.getPartitioning();\n+            taskSourceRdd = Optional.of(createTaskSourcesRdd(sparkContext, session, partitioning, tableScans, numberOfInputPartitions).rdd());\n+        }\n+        else if (rddInputs.size() == 0) {\n+            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n+            taskSourceRdd = Optional.of(new PrestoSparkTaskSourceRdd(sparkContext.sc(), ImmutableList.of(ImmutableList.of())));\n+        }\n+        else {\n+            taskSourceRdd = Optional.empty();\n+        }\n+\n         return JavaPairRDD.fromRDD(\n-                new PrestoSparkZipRdd(sparkContext.sc(), rdds.build(), taskProcessor),\n+                new PrestoSparkTaskRdd(sparkContext.sc(), taskSourceRdd, inputRdds.build(), taskProcessor),\n                 classTag(Integer.class),\n                 classTag(PrestoSparkRow.class));\n     }\n \n-    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+    private JavaRDD<SerializedTaskSource> createTaskSourcesRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PlanFragment fragment,\n-            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo,\n-            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            PartitioningHandle partitioning,\n+            List<TableScanNode> tableScans,\n+            Optional<Integer> expectedNumberOfPartitions)\n     {\n-        checkInputs(fragment.getRemoteSourceNodes(), ImmutableMap.of(), broadcastInputs);\n-\n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        checkArgument(\n-                tableScans.size() == 1,\n-                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n-                fragment.getId(),\n-                tableScans.size());\n-\n-        TableScanNode tableScan = getOnlyElement(tableScans);\n-\n-        List<ScheduledSplit> splits = getSplits(session, tableScan);\n-        shuffle(splits);\n-        int initialPartitionCount = getSparkInitialPartitionCount(session);\n-        int numTasks = Math.min(splits.size(), initialPartitionCount);\n-        if (numTasks == 0) {\n-            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        ListMultimap<Integer, TaskSource> taskSourcesMap = ArrayListMultimap.create();\n+        for (TableScanNode tableScan : tableScans) {\n+            List<ScheduledSplit> scheduledSplits = getSplits(session, tableScan);\n+            shuffle(scheduledSplits);\n+            SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSplitsToTasks(session, partitioning, scheduledSplits);\n+            asMap(assignedSplits).forEach((partitionId, splits) ->\n+                    taskSourcesMap.put(partitionId, new TaskSource(tableScan.getId(), splits, true)));\n         }\n \n-        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n-\n-        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-        splits = null;\n-\n-        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n-        for (int i = 0; i < assignedSplits.size(); i++) {\n-            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n-            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n-            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n-            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n-            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n-            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n-            assignedSplits.set(i, null);\n+        IntStream partitions = expectedNumberOfPartitions\n+                .map(integer -> IntStream.range(0, integer))\n+                .orElseGet(() -> new ArrayList<>(taskSourcesMap.keySet()).stream().mapToInt(Integer::intValue));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODA3Ng=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 321}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNzgyMTgyOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNToyODozNVrOGhlXmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwNDo0ODowNlrOGjavSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODQ0MA==", "bodyText": "Curious what does this numberOfInputPartitions mean? -- Looks like it's decided by the partition number of other RDDs?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437868440", "createdAt": "2020-06-10T05:28:35Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA5OTQyMg==", "bodyText": "It represents number of partitions that are expected by the rdd based on it's inputs. The number of partitions should be equal for all input rdds. If there's no shuffle inputs we don't have to create empty partitions if the splits are missing for some buckets. Think of it as of an optimization for a query like\nselect col, count(*)\nfrom table_bucketed_by_col\ngroup by col\n\nIf some buckets are missing we are free to not to create tasks for buckets that are missing.\nBut for a query like\nselect col, count(*)\nfrom table_bucketed_by_col t1\nRIGHT OUTER JOIN non_bucketed_table t2\nWHERE t1.col = t2.col\n\neven if some buckets are missing for t1 we still need to create a task for correctness", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439099422", "createdAt": "2020-06-11T22:10:21Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODQ0MA=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTI1MDIyMg==", "bodyText": "@arhimondr : I see. So essentially numberOfInputPartitions comes from the left-most join table right ?\ni.e. considering\n            JOIN\n          /   |   \\\n        A    B    C\n\nOnce rddA is created, the number of partitions in rddA will be populated to B and C as  the expected number of partitions?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439250222", "createdAt": "2020-06-12T07:19:42Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODQ0MA=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM2MTcwNA==", "bodyText": "I see. So essentially numberOfInputPartitions comes from the left-most join table right ?\n\nNo. It should be determined by the partitions produced by shuffle.\n           JOIN\n          /   |   \\\n        A    B    C\n\nIf table A and C are bucketed, but table B is non bucketed - we will have to shuffle table B. Thus the number of partitions will be present, and will be set to the number of partitions produced by the shuffle over B.\nIf all tables A, B and C are bucketed, and if bucket 0 is missing for all of them - there's no need to schedule a task for that bucket. This optimization cannot be done if at least one table is non bucketed, as there's no way to know in advance if there will be any rows for that bucket produced by the shuffle.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439361704", "createdAt": "2020-06-12T11:21:37Z", "author": {"login": "arhimondr"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODQ0MA=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MTQzMw==", "bodyText": "@arhimondr : Maybe add some comments about the intention ofexpectedNumberOfPartitions (used to eliminate tasks when there are missing buckets), and its behavior when there are both bucketed table input and shuffle input, since it's not intuitive at first glance :)", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439791433", "createdAt": "2020-06-14T04:48:06Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -230,90 +217,97 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n     {\n         checkInputs(fragment.getRemoteSourceNodes(), rddInputs, broadcastInputs);\n \n-        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n-        verify(tableScans.isEmpty(), \"no table scans is expected\");\n-\n-        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n-        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-        if (rddInputs.size() == 0) {\n-            checkArgument(fragment.getPartitioning().equals(SINGLE_DISTRIBUTION), \"SINGLE_DISTRIBUTION partitioning is expected: %s\", fragment.getPartitioning());\n-            return sparkContext.parallelize(ImmutableList.of(serializedTaskDescriptor), 1)\n-                    .mapPartitionsToPair(createTaskProcessor(\n-                            executorFactoryProvider,\n-                            taskStatsCollector,\n-                            toTaskProcessorBroadcastInputs(broadcastInputs)));\n-        }\n+        PrestoSparkTaskDescriptor taskDescriptor = new PrestoSparkTaskDescriptor(\n+                session.toSessionRepresentation(),\n+                session.getIdentity().getExtraCredentials(),\n+                fragment,\n+                tableWriteInfo);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(\n+                taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n+        Optional<Integer> numberOfInputPartitions = Optional.empty();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2ODQ0MA=="}, "originalCommit": {"oid": "0ae646de13bb51d9f29786b85fb589ddca30b1a2"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNzgyNjYwOnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkQueryRunner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNTozMToyNlrOGhlalw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNTozMToyNlrOGhlalw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2OTIwNw==", "bodyText": "nit: maybe say \"only probe side table is bucketed\"", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437869207", "createdAt": "2020-06-10T05:31:26Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkQueryRunner.java", "diffHunk": "@@ -55,20 +55,77 @@ public void testTableWrite()\n                         \"FROM orders\");\n     }\n \n+    @Test\n+    public void testBucketedTableWrite()\n+    {\n+        // create from bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_1 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders_bucketed\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_1 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+\n+        // create from non bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_2 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_2 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+    }\n+\n     @Test\n     public void testAggregation()\n     {\n         assertQuery(\"select partkey, count(*) c from lineitem where partkey % 10 = 1 group by partkey having count(*) = 42\");\n     }\n \n+    @Test\n+    public void testBucketedAggregation()\n+    {\n+        assertBucketedQuery(\"SELECT orderkey, count(*) c FROM lineitem_bucketed WHERE partkey % 10 = 1 GROUP BY orderkey\");\n+    }\n+\n     @Test\n     public void testJoin()\n     {\n-        assertQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n-                \"FROM lineitem l \" +\n+        assertQuery(\"SELECT l.orderkey, l.linenumber, p.brand \" +\n+                \"FROM lineitem l, part p \" +\n+                \"WHERE l.partkey = p.partkey\");\n+    }\n+\n+    @Test\n+    public void testBucketedJoin()\n+    {\n+        // both tables are bucketed\n+        assertBucketedQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n+                \"FROM lineitem_bucketed l \" +\n+                \"JOIN orders_bucketed o \" +\n+                \"ON l.orderkey = o.orderkey \" +\n+                \"WHERE l.orderkey % 223 = 42 AND l.linenumber = 4 and o.orderstatus = 'O'\");\n+\n+        // only one table is bucketed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyNzgyNjg0OnYy", "diffSide": "RIGHT", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkQueryRunner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNTozMTozNlrOGhlayA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwNTozMTozNlrOGhlayA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg2OTI1Ng==", "bodyText": "ditto, \"only build side table is bucketed\"", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r437869256", "createdAt": "2020-06-10T05:31:36Z", "author": {"login": "wenleix"}, "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkQueryRunner.java", "diffHunk": "@@ -55,20 +55,77 @@ public void testTableWrite()\n                         \"FROM orders\");\n     }\n \n+    @Test\n+    public void testBucketedTableWrite()\n+    {\n+        // create from bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_1 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders_bucketed\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_1 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+\n+        // create from non bucketed table\n+        assertUpdate(\n+                \"CREATE TABLE hive.hive_test.hive_orders_bucketed_2 WITH (bucketed_by=array['orderkey'], bucket_count=11) AS \" +\n+                        \"SELECT orderkey, custkey, orderstatus, totalprice, orderdate, orderpriority, clerk, shippriority, comment \" +\n+                        \"FROM orders\",\n+                15000);\n+        assertQuery(\n+                \"SELECT count(*) \" +\n+                        \"FROM hive.hive_test.hive_orders_bucketed_2 \" +\n+                        \"WHERE \\\"$bucket\\\" = 1\",\n+                \"SELECT 1365\");\n+    }\n+\n     @Test\n     public void testAggregation()\n     {\n         assertQuery(\"select partkey, count(*) c from lineitem where partkey % 10 = 1 group by partkey having count(*) = 42\");\n     }\n \n+    @Test\n+    public void testBucketedAggregation()\n+    {\n+        assertBucketedQuery(\"SELECT orderkey, count(*) c FROM lineitem_bucketed WHERE partkey % 10 = 1 GROUP BY orderkey\");\n+    }\n+\n     @Test\n     public void testJoin()\n     {\n-        assertQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n-                \"FROM lineitem l \" +\n+        assertQuery(\"SELECT l.orderkey, l.linenumber, p.brand \" +\n+                \"FROM lineitem l, part p \" +\n+                \"WHERE l.partkey = p.partkey\");\n+    }\n+\n+    @Test\n+    public void testBucketedJoin()\n+    {\n+        // both tables are bucketed\n+        assertBucketedQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n+                \"FROM lineitem_bucketed l \" +\n+                \"JOIN orders_bucketed o \" +\n+                \"ON l.orderkey = o.orderkey \" +\n+                \"WHERE l.orderkey % 223 = 42 AND l.linenumber = 4 and o.orderstatus = 'O'\");\n+\n+        // only one table is bucketed\n+        assertBucketedQuery(\"SELECT l.orderkey, l.linenumber, o.orderstatus \" +\n+                \"FROM lineitem_bucketed l \" +\n                 \"JOIN orders o \" +\n                 \"ON l.orderkey = o.orderkey \" +\n                 \"WHERE l.orderkey % 223 = 42 AND l.linenumber = 4 and o.orderstatus = 'O'\");\n+\n+        // only one table is bucketed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecb7c56cc35eb5f2468c837483e8db92e0a19de2"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTg2NjU4OnYy", "diffSide": "RIGHT", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwNTowOToyN1rOGjazEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNzowMzozN1rOGj6_jQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MjQwMw==", "bodyText": "Maybe emphasize both taskSourceRdd and shuffleInputRdds will be empty when the stage partitioning is SINGLE_DISTRIBUTION ?", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r439792403", "createdAt": "2020-06-14T05:09:27Z", "author": {"login": "wenleix"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+/**\n+ * PrestoSparkTaskRdd represents execution of Presto stage, it contains:\n+ * - A list of shuffleInputRdds, each of the corresponding to a child stage.\n+ * - An optional taskSourceRdd, which represents ALL table scan inputs in this stage.\n+ * <p>\n+ * Table scan is present when joining a bucketed table with an unbucketed table, for example:\n+ * Join\n+ * /  \\\n+ * Scan  Remote Source\n+ * <p>\n+ * In this case, bucket to Spark partition mapping has to be consistent with the Spark shuffle partition.\n+ * <p>\n+ * shuffleInputRdds can also be empty when the stage partitioning is SINGLE_DISTRIBUTION.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6bae2da1afcc0b42d69c95244dbaccf756e899e"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMxOTg4NQ==", "bodyText": "Changed the comment to\n * When the stage partitioning is SINGLE_DISTRIBUTION and the shuffleInputRdds is empty,\n * the taskSourceRdd is expected to be present and contain exactly one empty partition.", "url": "https://github.com/prestodb/presto/pull/14559#discussion_r440319885", "createdAt": "2020-06-15T17:03:37Z", "author": {"login": "arhimondr"}, "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.Partition;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.rdd.ZippedPartitionsBaseRDD;\n+import org.apache.spark.rdd.ZippedPartitionsPartition;\n+import scala.Tuple2;\n+import scala.collection.Seq;\n+import scala.reflect.ClassTag;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static java.lang.String.format;\n+import static java.util.Collections.emptyIterator;\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asJavaIterator;\n+import static scala.collection.JavaConversions.asScalaBuffer;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+import static scala.collection.JavaConversions.seqAsJavaList;\n+\n+/**\n+ * PrestoSparkTaskRdd represents execution of Presto stage, it contains:\n+ * - A list of shuffleInputRdds, each of the corresponding to a child stage.\n+ * - An optional taskSourceRdd, which represents ALL table scan inputs in this stage.\n+ * <p>\n+ * Table scan is present when joining a bucketed table with an unbucketed table, for example:\n+ * Join\n+ * /  \\\n+ * Scan  Remote Source\n+ * <p>\n+ * In this case, bucket to Spark partition mapping has to be consistent with the Spark shuffle partition.\n+ * <p>\n+ * shuffleInputRdds can also be empty when the stage partitioning is SINGLE_DISTRIBUTION.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc5MjQwMw=="}, "originalCommit": {"oid": "c6bae2da1afcc0b42d69c95244dbaccf756e899e"}, "originalPosition": 51}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2673, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}