{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk2Njg4MDIx", "number": 14321, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QyMDozMzo0MFrODuk9iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNzowMjo1OVrODvqAXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMTY2NjY1OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveWriterFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QyMDozMzo0MFrOGAoSrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQyMDo1NzowMlrOGBplXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzMxMzMyNA==", "bodyText": "The method calling this already calculating it differently? 522248f#diff-bd92d40b327dea510bb55e3f197aa911R302\nSeems like we can either generate it from the name or from the column values. Would be nice to be consistent and not do it twice?", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r403313324", "createdAt": "2020-04-03T20:33:40Z", "author": {"login": "aweisberg"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveWriterFactory.java", "diffHunk": "@@ -308,38 +309,107 @@ public HiveWriter createWriter(Page partitionColumns, int position, OptionalInt\n             partitionName = Optional.empty();\n         }\n \n+        WriterParameters writerParameters = getWriterParameters(partitionName, bucketNumber);\n+\n+        validateSchema(partitionName, writerParameters.getSchema());\n+\n+        String extension = getFileExtension(writerParameters.getOutputStorageFormat(), compressionCodec);\n+        String targetFileName;\n+        if (bucketNumber.isPresent()) {\n+            targetFileName = computeBucketedFileName(filePrefix, bucketNumber.getAsInt()) + extension;\n+        }\n+        else {\n+            targetFileName = filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+\n+        String writeFileName;\n+        if (partitionCommitRequired) {\n+            writeFileName = \".tmp.presto.\" + filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+        else {\n+            writeFileName = targetFileName;\n+        }\n+\n+        Path path = new Path(writerParameters.getWriteInfo().getWritePath(), writeFileName);\n+\n+        HiveFileWriter hiveFileWriter = null;\n+        for (HiveFileWriterFactory fileWriterFactory : fileWriterFactories) {\n+            Optional<HiveFileWriter> fileWriter = fileWriterFactory.createFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    conf,\n+                    session);\n+            if (fileWriter.isPresent()) {\n+                hiveFileWriter = fileWriter.get();\n+                break;\n+            }\n+        }\n+\n+        if (hiveFileWriter == null) {\n+            hiveFileWriter = new RecordFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    partitionStorageFormat.getEstimatedWriterSystemMemoryUsage(),\n+                    conf,\n+                    typeManager,\n+                    session);\n+        }\n+\n+        if (sortingFileWriterFactory.isPresent()) {\n+            checkState(bucketNumber.isPresent(), \"missing bucket number for sorted table write\");\n+            hiveFileWriter = sortingFileWriterFactory.get().createSortingFileWriter(\n+                    path,\n+                    hiveFileWriter,\n+                    bucketNumber.getAsInt(),\n+                    writerParameters.getWriteInfo().getTempPath());\n+        }\n+\n+        return new HiveWriter(\n+                hiveFileWriter,\n+                partitionName,\n+                writerParameters.getUpdateMode(),\n+                new FileWriteInfo(writeFileName, targetFileName),\n+                writerParameters.getWriteInfo().getWritePath().toString(),\n+                writerParameters.getWriteInfo().getTargetPath().toString(),\n+                createCommitEventListener(path, partitionName, hiveFileWriter, writerParameters),\n+                hiveWriterStats);\n+    }\n+\n+    private WriterParameters getWriterParameters(Optional<String> partitionName, OptionalInt bucketNumber)\n+    {\n         // attempt to get the existing partition (if this is an existing partitioned table)\n         Optional<Partition> partition = Optional.empty();\n-        if (!partitionValues.isEmpty() && table != null) {\n-            partition = pageSinkMetadataProvider.getPartition(partitionValues);\n+        if (partitionName.isPresent() && table != null) {\n+            partition = pageSinkMetadataProvider.getPartition(toPartitionValues(partitionName.get()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "522248f158838926751ef340aa72bf154961cd59"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDM4MzA3MQ==", "bodyText": "I just wanted to avoid passing both, Optional<PartitionName> and Optional<List<String>> partitionValues to keep the signature simple.", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r404383071", "createdAt": "2020-04-06T20:57:02Z", "author": {"login": "arhimondr"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveWriterFactory.java", "diffHunk": "@@ -308,38 +309,107 @@ public HiveWriter createWriter(Page partitionColumns, int position, OptionalInt\n             partitionName = Optional.empty();\n         }\n \n+        WriterParameters writerParameters = getWriterParameters(partitionName, bucketNumber);\n+\n+        validateSchema(partitionName, writerParameters.getSchema());\n+\n+        String extension = getFileExtension(writerParameters.getOutputStorageFormat(), compressionCodec);\n+        String targetFileName;\n+        if (bucketNumber.isPresent()) {\n+            targetFileName = computeBucketedFileName(filePrefix, bucketNumber.getAsInt()) + extension;\n+        }\n+        else {\n+            targetFileName = filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+\n+        String writeFileName;\n+        if (partitionCommitRequired) {\n+            writeFileName = \".tmp.presto.\" + filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+        else {\n+            writeFileName = targetFileName;\n+        }\n+\n+        Path path = new Path(writerParameters.getWriteInfo().getWritePath(), writeFileName);\n+\n+        HiveFileWriter hiveFileWriter = null;\n+        for (HiveFileWriterFactory fileWriterFactory : fileWriterFactories) {\n+            Optional<HiveFileWriter> fileWriter = fileWriterFactory.createFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    conf,\n+                    session);\n+            if (fileWriter.isPresent()) {\n+                hiveFileWriter = fileWriter.get();\n+                break;\n+            }\n+        }\n+\n+        if (hiveFileWriter == null) {\n+            hiveFileWriter = new RecordFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    partitionStorageFormat.getEstimatedWriterSystemMemoryUsage(),\n+                    conf,\n+                    typeManager,\n+                    session);\n+        }\n+\n+        if (sortingFileWriterFactory.isPresent()) {\n+            checkState(bucketNumber.isPresent(), \"missing bucket number for sorted table write\");\n+            hiveFileWriter = sortingFileWriterFactory.get().createSortingFileWriter(\n+                    path,\n+                    hiveFileWriter,\n+                    bucketNumber.getAsInt(),\n+                    writerParameters.getWriteInfo().getTempPath());\n+        }\n+\n+        return new HiveWriter(\n+                hiveFileWriter,\n+                partitionName,\n+                writerParameters.getUpdateMode(),\n+                new FileWriteInfo(writeFileName, targetFileName),\n+                writerParameters.getWriteInfo().getWritePath().toString(),\n+                writerParameters.getWriteInfo().getTargetPath().toString(),\n+                createCommitEventListener(path, partitionName, hiveFileWriter, writerParameters),\n+                hiveWriterStats);\n+    }\n+\n+    private WriterParameters getWriterParameters(Optional<String> partitionName, OptionalInt bucketNumber)\n+    {\n         // attempt to get the existing partition (if this is an existing partitioned table)\n         Optional<Partition> partition = Optional.empty();\n-        if (!partitionValues.isEmpty() && table != null) {\n-            partition = pageSinkMetadataProvider.getPartition(partitionValues);\n+        if (partitionName.isPresent() && table != null) {\n+            partition = pageSinkMetadataProvider.getPartition(toPartitionValues(partitionName.get()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzMxMzMyNA=="}, "originalCommit": {"oid": "522248f158838926751ef340aa72bf154961cd59"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMTY3NTI3OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveWriterFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QyMDozNToxOVrOGAoXUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNjozODozMFrOGCMX7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzMxNDUxNQ==", "bodyText": "Intentional line break?", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r403314515", "createdAt": "2020-04-03T20:35:19Z", "author": {"login": "aweisberg"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveWriterFactory.java", "diffHunk": "@@ -308,38 +309,107 @@ public HiveWriter createWriter(Page partitionColumns, int position, OptionalInt\n             partitionName = Optional.empty();\n         }\n \n+        WriterParameters writerParameters = getWriterParameters(partitionName, bucketNumber);\n+\n+        validateSchema(partitionName, writerParameters.getSchema());\n+\n+        String extension = getFileExtension(writerParameters.getOutputStorageFormat(), compressionCodec);\n+        String targetFileName;\n+        if (bucketNumber.isPresent()) {\n+            targetFileName = computeBucketedFileName(filePrefix, bucketNumber.getAsInt()) + extension;\n+        }\n+        else {\n+            targetFileName = filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+\n+        String writeFileName;\n+        if (partitionCommitRequired) {\n+            writeFileName = \".tmp.presto.\" + filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+        else {\n+            writeFileName = targetFileName;\n+        }\n+\n+        Path path = new Path(writerParameters.getWriteInfo().getWritePath(), writeFileName);\n+\n+        HiveFileWriter hiveFileWriter = null;\n+        for (HiveFileWriterFactory fileWriterFactory : fileWriterFactories) {\n+            Optional<HiveFileWriter> fileWriter = fileWriterFactory.createFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    conf,\n+                    session);\n+            if (fileWriter.isPresent()) {\n+                hiveFileWriter = fileWriter.get();\n+                break;\n+            }\n+        }\n+\n+        if (hiveFileWriter == null) {\n+            hiveFileWriter = new RecordFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    partitionStorageFormat.getEstimatedWriterSystemMemoryUsage(),\n+                    conf,\n+                    typeManager,\n+                    session);\n+        }\n+\n+        if (sortingFileWriterFactory.isPresent()) {\n+            checkState(bucketNumber.isPresent(), \"missing bucket number for sorted table write\");\n+            hiveFileWriter = sortingFileWriterFactory.get().createSortingFileWriter(\n+                    path,\n+                    hiveFileWriter,\n+                    bucketNumber.getAsInt(),\n+                    writerParameters.getWriteInfo().getTempPath());\n+        }\n+\n+        return new HiveWriter(\n+                hiveFileWriter,\n+                partitionName,\n+                writerParameters.getUpdateMode(),\n+                new FileWriteInfo(writeFileName, targetFileName),\n+                writerParameters.getWriteInfo().getWritePath().toString(),\n+                writerParameters.getWriteInfo().getTargetPath().toString(),\n+                createCommitEventListener(path, partitionName, hiveFileWriter, writerParameters),\n+                hiveWriterStats);\n+    }\n+\n+    private WriterParameters getWriterParameters(Optional<String> partitionName, OptionalInt bucketNumber)\n+    {\n         // attempt to get the existing partition (if this is an existing partitioned table)\n         Optional<Partition> partition = Optional.empty();\n-        if (!partitionValues.isEmpty() && table != null) {\n-            partition = pageSinkMetadataProvider.getPartition(partitionValues);\n+        if (partitionName.isPresent() && table != null) {\n+            partition = pageSinkMetadataProvider.getPartition(toPartitionValues(partitionName.get()));\n         }\n \n         UpdateMode updateMode;\n         Properties schema;\n         WriteInfo writeInfo;\n         StorageFormat outputStorageFormat;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "522248f158838926751ef340aa72bf154961cd59"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk1MzA3MQ==", "bodyText": "Yeah maybe let's remove this line break.", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r404953071", "createdAt": "2020-04-07T16:38:30Z", "author": {"login": "shixuan-fan"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveWriterFactory.java", "diffHunk": "@@ -308,38 +309,107 @@ public HiveWriter createWriter(Page partitionColumns, int position, OptionalInt\n             partitionName = Optional.empty();\n         }\n \n+        WriterParameters writerParameters = getWriterParameters(partitionName, bucketNumber);\n+\n+        validateSchema(partitionName, writerParameters.getSchema());\n+\n+        String extension = getFileExtension(writerParameters.getOutputStorageFormat(), compressionCodec);\n+        String targetFileName;\n+        if (bucketNumber.isPresent()) {\n+            targetFileName = computeBucketedFileName(filePrefix, bucketNumber.getAsInt()) + extension;\n+        }\n+        else {\n+            targetFileName = filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+\n+        String writeFileName;\n+        if (partitionCommitRequired) {\n+            writeFileName = \".tmp.presto.\" + filePrefix + \"_\" + randomUUID() + extension;\n+        }\n+        else {\n+            writeFileName = targetFileName;\n+        }\n+\n+        Path path = new Path(writerParameters.getWriteInfo().getWritePath(), writeFileName);\n+\n+        HiveFileWriter hiveFileWriter = null;\n+        for (HiveFileWriterFactory fileWriterFactory : fileWriterFactories) {\n+            Optional<HiveFileWriter> fileWriter = fileWriterFactory.createFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    conf,\n+                    session);\n+            if (fileWriter.isPresent()) {\n+                hiveFileWriter = fileWriter.get();\n+                break;\n+            }\n+        }\n+\n+        if (hiveFileWriter == null) {\n+            hiveFileWriter = new RecordFileWriter(\n+                    path,\n+                    dataColumns.stream()\n+                            .map(DataColumn::getName)\n+                            .collect(toList()),\n+                    writerParameters.getOutputStorageFormat(),\n+                    writerParameters.getSchema(),\n+                    partitionStorageFormat.getEstimatedWriterSystemMemoryUsage(),\n+                    conf,\n+                    typeManager,\n+                    session);\n+        }\n+\n+        if (sortingFileWriterFactory.isPresent()) {\n+            checkState(bucketNumber.isPresent(), \"missing bucket number for sorted table write\");\n+            hiveFileWriter = sortingFileWriterFactory.get().createSortingFileWriter(\n+                    path,\n+                    hiveFileWriter,\n+                    bucketNumber.getAsInt(),\n+                    writerParameters.getWriteInfo().getTempPath());\n+        }\n+\n+        return new HiveWriter(\n+                hiveFileWriter,\n+                partitionName,\n+                writerParameters.getUpdateMode(),\n+                new FileWriteInfo(writeFileName, targetFileName),\n+                writerParameters.getWriteInfo().getWritePath().toString(),\n+                writerParameters.getWriteInfo().getTargetPath().toString(),\n+                createCommitEventListener(path, partitionName, hiveFileWriter, writerParameters),\n+                hiveWriterStats);\n+    }\n+\n+    private WriterParameters getWriterParameters(Optional<String> partitionName, OptionalInt bucketNumber)\n+    {\n         // attempt to get the existing partition (if this is an existing partitioned table)\n         Optional<Partition> partition = Optional.empty();\n-        if (!partitionValues.isEmpty() && table != null) {\n-            partition = pageSinkMetadataProvider.getPartition(partitionValues);\n+        if (partitionName.isPresent() && table != null) {\n+            partition = pageSinkMetadataProvider.getPartition(toPartitionValues(partitionName.get()));\n         }\n \n         UpdateMode updateMode;\n         Properties schema;\n         WriteInfo writeInfo;\n         StorageFormat outputStorageFormat;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzMxNDUxNQ=="}, "originalCommit": {"oid": "522248f158838926751ef340aa72bf154961cd59"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwNDk4NTQ4OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNVQxNjozMToxMFrOGBBb5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQyMDo1ODoxM1rOGBpoNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzcyNTI4Ng==", "bodyText": "If they are are enabled by default then isn't what the test needs to do the inverse? It needs test with the optimized writers disabled?", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r403725286", "createdAt": "2020-04-05T16:31:10Z", "author": {"login": "aweisberg"}, "path": "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java", "diffHunk": "@@ -4704,59 +4703,8 @@ private static ConnectorSession getConnectorSession(Session session)\n \n     private void testWithAllStorageFormats(BiConsumer<Session, HiveStorageFormat> test)\n     {\n-        for (TestingHiveStorageFormat storageFormat : getAllTestingHiveStorageFormat()) {\n-            testWithStorageFormat(storageFormat, test);\n-        }\n-    }\n-\n-    private static void testWithStorageFormat(TestingHiveStorageFormat storageFormat, BiConsumer<Session, HiveStorageFormat> test)\n-    {\n-        requireNonNull(storageFormat, \"storageFormat is null\");\n-        requireNonNull(test, \"test is null\");\n-        Session session = storageFormat.getSession();\n-        try {\n-            test.accept(session, storageFormat.getFormat());\n-        }\n-        catch (Exception | AssertionError e) {\n-            fail(format(\"Failure for format %s with properties %s\", storageFormat.getFormat(), session.getConnectorProperties()), e);\n-        }\n-    }\n-\n-    private List<TestingHiveStorageFormat> getAllTestingHiveStorageFormat()\n-    {\n-        Session session = getSession();\n-        ImmutableList.Builder<TestingHiveStorageFormat> formats = ImmutableList.builder();\n-        for (HiveStorageFormat hiveStorageFormat : HiveStorageFormat.values()) {\n-            formats.add(new TestingHiveStorageFormat(session, hiveStorageFormat));\n-        }\n-        formats.add(new TestingHiveStorageFormat(\n-                Session.builder(session).setCatalogSessionProperty(session.getCatalog().get(), \"orc_optimized_writer_enabled\", \"true\").build(),\n-                HiveStorageFormat.ORC));\n-        formats.add(new TestingHiveStorageFormat(\n-                Session.builder(session).setCatalogSessionProperty(session.getCatalog().get(), \"orc_optimized_writer_enabled\", \"true\").build(),\n-                HiveStorageFormat.DWRF));\n-        return formats.build();\n-    }\n-\n-    private static class TestingHiveStorageFormat\n-    {\n-        private final Session session;\n-        private final HiveStorageFormat format;\n-\n-        TestingHiveStorageFormat(Session session, HiveStorageFormat format)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.format = requireNonNull(format, \"format is null\");\n-        }\n-\n-        public Session getSession()\n-        {\n-            return session;\n-        }\n-\n-        public HiveStorageFormat getFormat()\n-        {\n-            return format;\n+        for (HiveStorageFormat storageFormat : HiveStorageFormat.values()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cf2809cfd33ed87a1940df0814e897255cb62e4"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDM4Mzc5Ng==", "bodyText": "The old writers are provided by the Hadoop library. The old writers have been also disabled for a quite some time, and it doesn't look like we support them anymore.", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r404383796", "createdAt": "2020-04-06T20:58:13Z", "author": {"login": "arhimondr"}, "path": "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java", "diffHunk": "@@ -4704,59 +4703,8 @@ private static ConnectorSession getConnectorSession(Session session)\n \n     private void testWithAllStorageFormats(BiConsumer<Session, HiveStorageFormat> test)\n     {\n-        for (TestingHiveStorageFormat storageFormat : getAllTestingHiveStorageFormat()) {\n-            testWithStorageFormat(storageFormat, test);\n-        }\n-    }\n-\n-    private static void testWithStorageFormat(TestingHiveStorageFormat storageFormat, BiConsumer<Session, HiveStorageFormat> test)\n-    {\n-        requireNonNull(storageFormat, \"storageFormat is null\");\n-        requireNonNull(test, \"test is null\");\n-        Session session = storageFormat.getSession();\n-        try {\n-            test.accept(session, storageFormat.getFormat());\n-        }\n-        catch (Exception | AssertionError e) {\n-            fail(format(\"Failure for format %s with properties %s\", storageFormat.getFormat(), session.getConnectorProperties()), e);\n-        }\n-    }\n-\n-    private List<TestingHiveStorageFormat> getAllTestingHiveStorageFormat()\n-    {\n-        Session session = getSession();\n-        ImmutableList.Builder<TestingHiveStorageFormat> formats = ImmutableList.builder();\n-        for (HiveStorageFormat hiveStorageFormat : HiveStorageFormat.values()) {\n-            formats.add(new TestingHiveStorageFormat(session, hiveStorageFormat));\n-        }\n-        formats.add(new TestingHiveStorageFormat(\n-                Session.builder(session).setCatalogSessionProperty(session.getCatalog().get(), \"orc_optimized_writer_enabled\", \"true\").build(),\n-                HiveStorageFormat.ORC));\n-        formats.add(new TestingHiveStorageFormat(\n-                Session.builder(session).setCatalogSessionProperty(session.getCatalog().get(), \"orc_optimized_writer_enabled\", \"true\").build(),\n-                HiveStorageFormat.DWRF));\n-        return formats.build();\n-    }\n-\n-    private static class TestingHiveStorageFormat\n-    {\n-        private final Session session;\n-        private final HiveStorageFormat format;\n-\n-        TestingHiveStorageFormat(Session session, HiveStorageFormat format)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.format = requireNonNull(format, \"format is null\");\n-        }\n-\n-        public Session getSession()\n-        {\n-            return session;\n-        }\n-\n-        public HiveStorageFormat getFormat()\n-        {\n-            return format;\n+        for (HiveStorageFormat storageFormat : HiveStorageFormat.values()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzcyNTI4Ng=="}, "originalCommit": {"oid": "7cf2809cfd33ed87a1940df0814e897255cb62e4"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwNTAwNDM2OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNVQxNjo0ODo1OVrOGBBlHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQyMDo1OToxOFrOGBpq-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzcyNzY0Ng==", "bodyText": "Why does this have to include partitions being appended to? Seems like we don't use this in the append path.\nAre there cases where we will attempt to append and also overwrite/new?", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r403727646", "createdAt": "2020-04-05T16:48:59Z", "author": {"login": "aweisberg"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java", "diffHunk": "@@ -1548,6 +1553,8 @@ public HiveInsertTableHandle beginInsert(ConnectorSession session, ConnectorTabl\n                 .collect(toImmutableMap(HiveColumnHandle::getName, column -> column.getHiveType().getType(typeManager)));\n         Map<List<String>, ComputedStatistics> partitionComputedStatistics = createComputedStatisticsToPartitionMap(computedStatistics, partitionedBy, columnTypes);\n \n+        Set<String> existingPartitions = getExistingPartitionNames(handle.getSchemaName(), handle.getTableName(), partitionUpdates);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d2a9267d95e0ada0ece9861643b3d8111ced84a"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDM4NDUwNg==", "bodyText": "The write mode APPEND can only be set if the partition exist. The existence of partition is checked explicitly on the writer, so i thought that checking it here again can be avoided.", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r404384506", "createdAt": "2020-04-06T20:59:18Z", "author": {"login": "arhimondr"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java", "diffHunk": "@@ -1548,6 +1553,8 @@ public HiveInsertTableHandle beginInsert(ConnectorSession session, ConnectorTabl\n                 .collect(toImmutableMap(HiveColumnHandle::getName, column -> column.getHiveType().getType(typeManager)));\n         Map<List<String>, ComputedStatistics> partitionComputedStatistics = createComputedStatisticsToPartitionMap(computedStatistics, partitionedBy, columnTypes);\n \n+        Set<String> existingPartitions = getExistingPartitionNames(handle.getSchemaName(), handle.getTableName(), partitionUpdates);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzcyNzY0Ng=="}, "originalCommit": {"oid": "2d2a9267d95e0ada0ece9861643b3d8111ced84a"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMjk3ODg3OnYy", "diffSide": "RIGHT", "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNzowMjo1OVrOGCNYyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozOTowNFrOGCYq7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk2OTY3NQ==", "bodyText": "The only concern I have is that this seems to change the default behavior and I'm not sure if we want to start with setting the default value to true.", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r404969675", "createdAt": "2020-04-07T17:02:59Z", "author": {"login": "shixuan-fan"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java", "diffHunk": "@@ -88,6 +88,7 @@\n     private boolean respectTableFormat = true;\n     private boolean immutablePartitions;\n     private boolean insertOverwriteImmutablePartitions;\n+    private boolean failFastOnInsertIntoImmutablePartitionsEnabled;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d2a9267d95e0ada0ece9861643b3d8111ced84a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NDU0MQ==", "bodyText": "I think you are right. Changing it to true by default.", "url": "https://github.com/prestodb/presto/pull/14321#discussion_r405154541", "createdAt": "2020-04-07T22:39:04Z", "author": {"login": "arhimondr"}, "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java", "diffHunk": "@@ -88,6 +88,7 @@\n     private boolean respectTableFormat = true;\n     private boolean immutablePartitions;\n     private boolean insertOverwriteImmutablePartitions;\n+    private boolean failFastOnInsertIntoImmutablePartitionsEnabled;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk2OTY3NQ=="}, "originalCommit": {"oid": "2d2a9267d95e0ada0ece9861643b3d8111ced84a"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2966, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}