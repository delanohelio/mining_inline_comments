{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ1NjQ2OTky", "number": 1142, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQxNjo0NjozOFrOENnSyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNzo0MTo0MVrOEQBNmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzEwNzMxOnYy", "diffSide": "RIGHT", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQxNjo0NjozOFrOGwVlrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjoxNzoyNFrOGyTOLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU0Mw==", "bodyText": "Would it be cleaner to define a set of enums (states) where we should mark the tasks aborted? Then we could do a simple exists() check.\nOr, it seems that if we check for != condition, we won't have to list out so many states here?", "url": "https://github.com/apache/helix/pull/1142#discussion_r453338543", "createdAt": "2020-07-12T16:46:38Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -185,17 +185,13 @@ public void updatePreviousAssignedTasksStatus(\n         switch (currState) {\n         case RUNNING: {\n           TaskPartitionState nextState = TaskPartitionState.RUNNING;\n-          if (jobState == TaskState.TIMING_OUT) {\n+          if (jobState == TaskState.TIMING_OUT || jobState == TaskState.TIMED_OUT\n+              || jobState == TaskState.FAILING || jobState == TaskState.FAILED\n+              || jobState == TaskState.ABORTED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTU4MQ==", "bodyText": "Also just to confirm, this is to make sure that task states that are stuck in RUNNING get updated to ABORTED?", "url": "https://github.com/apache/helix/pull/1142#discussion_r453339581", "createdAt": "2020-07-12T16:57:57Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -185,17 +185,13 @@ public void updatePreviousAssignedTasksStatus(\n         switch (currState) {\n         case RUNNING: {\n           TaskPartitionState nextState = TaskPartitionState.RUNNING;\n-          if (jobState == TaskState.TIMING_OUT) {\n+          if (jobState == TaskState.TIMING_OUT || jobState == TaskState.TIMED_OUT\n+              || jobState == TaskState.FAILING || jobState == TaskState.FAILED\n+              || jobState == TaskState.ABORTED) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU0Mw=="}, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM5NjkxMA==", "bodyText": "Done. I create a list for these states. Yes and also make sure task goes to ABORTED state and we release quota.", "url": "https://github.com/apache/helix/pull/1142#discussion_r455396910", "createdAt": "2020-07-15T22:17:24Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -185,17 +185,13 @@ public void updatePreviousAssignedTasksStatus(\n         switch (currState) {\n         case RUNNING: {\n           TaskPartitionState nextState = TaskPartitionState.RUNNING;\n-          if (jobState == TaskState.TIMING_OUT) {\n+          if (jobState == TaskState.TIMING_OUT || jobState == TaskState.TIMED_OUT\n+              || jobState == TaskState.FAILING || jobState == TaskState.FAILED\n+              || jobState == TaskState.ABORTED) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU0Mw=="}, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzEwNzYwOnYy", "diffSide": "RIGHT", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQxNjo0NzowNFrOGwVl0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjo0NTo0OFrOGyT4PA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU3OA==", "bodyText": "Thanks for renaming!", "url": "https://github.com/apache/helix/pull/1142#discussion_r453338578", "createdAt": "2020-07-12T16:47:04Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -560,7 +556,7 @@ protected void handleAdditionalTaskAssignment(\n       excludeSet.addAll(assignedSet);\n     }\n     addCompletedTasks(excludeSet, jobCtx, allPartitions);\n-    addGiveupPartitions(excludeSet, jobCtx, allPartitions, jobCfg);\n+    addPartitionsReachedMaximumRetries(excludeSet, jobCtx, allPartitions, jobCfg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQwNzY3Ng==", "bodyText": ";-)", "url": "https://github.com/apache/helix/pull/1142#discussion_r455407676", "createdAt": "2020-07-15T22:45:48Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -560,7 +556,7 @@ protected void handleAdditionalTaskAssignment(\n       excludeSet.addAll(assignedSet);\n     }\n     addCompletedTasks(excludeSet, jobCtx, allPartitions);\n-    addGiveupPartitions(excludeSet, jobCtx, allPartitions, jobCfg);\n+    addPartitionsReachedMaximumRetries(excludeSet, jobCtx, allPartitions, jobCfg);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU3OA=="}, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzEwODIwOnYy", "diffSide": "RIGHT", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQxNjo0Nzo1MVrOGwVmGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjoyMjo1NFrOGyTW8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODY1MA==", "bodyText": "What is the distinction between \"given up\" and partitions that exceeded the # of maximum attempts? Do you think we could clarify that here?\nI have a feeling that \"given up\" might be too broad or abstract.", "url": "https://github.com/apache/helix/pull/1142#discussion_r453338650", "createdAt": "2020-07-12T16:47:51Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -745,7 +743,7 @@ protected void scheduleForNextTask(String job, JobContext jobCtx, long now) {\n     }\n   }\n \n-  // add all partitions that have been tried maxNumberAttempts\n+  // add all partitions that are given up", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM5OTE1Mg==", "bodyText": "Thanks for the comment. I added comments for more explanation.\nAlso I noticed the given up tasks has been already added in job dispatcher. So we do not need to add the tasks again. However, we want to have a sanity check to filter all of the tasks, regarding of their states, if they reached their maximum number of attempts. Because it has been observed that sometimes, the state of the task might be INIT or DROPPED or other states (if we have pending messages) and we miss max number of attempts check for those tasks.", "url": "https://github.com/apache/helix/pull/1142#discussion_r455399152", "createdAt": "2020-07-15T22:22:54Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -745,7 +743,7 @@ protected void scheduleForNextTask(String job, JobContext jobCtx, long now) {\n     }\n   }\n \n-  // add all partitions that have been tried maxNumberAttempts\n+  // add all partitions that are given up", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODY1MA=="}, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzExMjUwOnYy", "diffSide": "RIGHT", "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQxNjo1Mzo1MFrOGwVoLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjo0MjoxN1rOGyTzSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTE4MA==", "bodyText": "Could you explain further why this change is needed? If we add more nodes, the db partitions will get shifted. Are we adding nodes while the workflow is in progress and before its tasks complete? In that case, the shuffling of tasks (getting dropped and rescheduled onto another instance) makes sense and we should increment # of attempts.\nSince the focus of this test doesn't seem to be # of attempts, do you think we should add a new test that tests for this? We want to make sure that for targeted tasks, if we add more nodes and if that causes db partitions to be moved, then we should observe the tasks get dropped and retried with an increase in the # of attempts. One thing to watch out for here is \"rebalanceRunningTask\" - what role should that parameter play?", "url": "https://github.com/apache/helix/pull/1142#discussion_r453339180", "createdAt": "2020-07-12T16:53:50Z", "author": {"login": "narendly"}, "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java", "diffHunk": "@@ -44,10 +44,10 @@ public void testForceDeleteJobFromJobQueue() throws InterruptedException {\n     // Create two jobs: job1 will complete fast, and job2 will be stuck in progress (taking a long\n     // time to finish). The idea is to force-delete a stuck job (job2).\n     JobConfig.Builder jobBuilder = JobConfig.Builder.fromMap(WorkflowGenerator.DEFAULT_JOB_CONFIG)\n-        .setMaxAttemptsPerTask(1).setWorkflow(jobQueueName)\n+        .setWorkflow(jobQueueName)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTM0Mg==", "bodyText": "I just realized that you did add a new test below, TestMaxNumberOfAttemptsMasterSwitch. Great job!\nNow, if we could reason through how the \"rebalanceRunningTask\" config should (or should not) apply here, we're set :)", "url": "https://github.com/apache/helix/pull/1142#discussion_r453339342", "createdAt": "2020-07-12T16:55:28Z", "author": {"login": "narendly"}, "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java", "diffHunk": "@@ -44,10 +44,10 @@ public void testForceDeleteJobFromJobQueue() throws InterruptedException {\n     // Create two jobs: job1 will complete fast, and job2 will be stuck in progress (taking a long\n     // time to finish). The idea is to force-delete a stuck job (job2).\n     JobConfig.Builder jobBuilder = JobConfig.Builder.fromMap(WorkflowGenerator.DEFAULT_JOB_CONFIG)\n-        .setMaxAttemptsPerTask(1).setWorkflow(jobQueueName)\n+        .setWorkflow(jobQueueName)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTE4MA=="}, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQwNjQxMA==", "bodyText": "I reverted this change for this test. However, we still need to increase number of attempts for some test that we stop the participant and start them again.\nFor example, in the TestForceDeleteWorkflow.testDeleteCompletedWorkflowForcefully:\nIn super.beforeClass() we start the participant, however, once the test started, we stop those participants and start them again. I have observed that if we start the workflow very close to the time that we start the participant one by one (kind of like rolling upgrade), Master might jump several times between the nodes. Hence if we set maxNumberOfAttempts to 1, then the test will fail. Since we just started to respect maxNumberOfAttempts in this PR.\nAbout your second comment: Yeah the newly added test checks maxNumberOfAttempts for targetJobChange. For \"rebalanceRunningTask\", I checked other tests and I saw we this logic is already covered in \"TestRebalanceRunningTask\" extensively for different scenarios. So if rebalance running task is set and we need to drop the task in one instance and run in other one (Send INIT->RUNNING), then this is new scheduling and we are respecting max number of attempts. So rebalanceRunningTask is happening until we reach maxNumberOfAttempts. In other word, once a task reaches maxNumberOfAttempts, it won't be scheduled again. So no new INIT->RUNNING once we reach maxNumberOfAttempts :)", "url": "https://github.com/apache/helix/pull/1142#discussion_r455406410", "createdAt": "2020-07-15T22:42:17Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java", "diffHunk": "@@ -44,10 +44,10 @@ public void testForceDeleteJobFromJobQueue() throws InterruptedException {\n     // Create two jobs: job1 will complete fast, and job2 will be stuck in progress (taking a long\n     // time to finish). The idea is to force-delete a stuck job (job2).\n     JobConfig.Builder jobBuilder = JobConfig.Builder.fromMap(WorkflowGenerator.DEFAULT_JOB_CONFIG)\n-        .setMaxAttemptsPerTask(1).setWorkflow(jobQueueName)\n+        .setWorkflow(jobQueueName)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTE4MA=="}, "originalCommit": {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MjMxMDQyOnYy", "diffSide": "RIGHT", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNzozODo0MVrOGz9ZOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxODoxNTowMlrOG0Zx7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjQ0MQ==", "bodyText": "Nit:\nIf a job is in one of the following states and its tasks are in RUNNING states, the tasks will be aborted.", "url": "https://github.com/apache/helix/pull/1142#discussion_r457136441", "createdAt": "2020-07-20T07:38:41Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -74,6 +75,12 @@ public void updatePreviousAssignedTasksStatus(\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n+    // If a job is in the following states and contains running tasks, all of the running tasks\n+    // should go to the ABORTED states.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e153c6722c52f472447c3db922050f25e79b237"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzYwMTUxNg==", "bodyText": "Fixed. Thanks", "url": "https://github.com/apache/helix/pull/1142#discussion_r457601516", "createdAt": "2020-07-20T18:15:02Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -74,6 +75,12 @@ public void updatePreviousAssignedTasksStatus(\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n+    // If a job is in the following states and contains running tasks, all of the running tasks\n+    // should go to the ABORTED states.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjQ0MQ=="}, "originalCommit": {"oid": "7e153c6722c52f472447c3db922050f25e79b237"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MjMxMzMxOnYy", "diffSide": "RIGHT", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNzozOToxNlrOGz9avQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxODoxNTo0OVrOG0ZzWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjgyOQ==", "bodyText": "I think calling it jobStatesForAbortingTasks does the trick and is shorter. What do you think?", "url": "https://github.com/apache/helix/pull/1142#discussion_r457136829", "createdAt": "2020-07-20T07:39:16Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -74,6 +75,12 @@ public void updatePreviousAssignedTasksStatus(\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n+    // If a job is in the following states and contains running tasks, all of the running tasks\n+    // should go to the ABORTED states.\n+    Set<TaskState> jobStatesForRunningTaskToAbortedState =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e153c6722c52f472447c3db922050f25e79b237"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzYwMTg4MA==", "bodyText": "Yeah I wasn't happy with previous name as well. Change it as you suggested. Thanks.", "url": "https://github.com/apache/helix/pull/1142#discussion_r457601880", "createdAt": "2020-07-20T18:15:49Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -74,6 +75,12 @@ public void updatePreviousAssignedTasksStatus(\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n+    // If a job is in the following states and contains running tasks, all of the running tasks\n+    // should go to the ABORTED states.\n+    Set<TaskState> jobStatesForRunningTaskToAbortedState =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjgyOQ=="}, "originalCommit": {"oid": "7e153c6722c52f472447c3db922050f25e79b237"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MjMyNTM2OnYy", "diffSide": "RIGHT", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwNzo0MTo0MVrOGz9hHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxODoxNToxNVrOG0ZySg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzODQ2MQ==", "bodyText": "Typo: addGivenUpPartitions", "url": "https://github.com/apache/helix/pull/1142#discussion_r457138461", "createdAt": "2020-07-20T07:41:41Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -745,8 +748,12 @@ protected void scheduleForNextTask(String job, JobContext jobCtx, long now) {\n     }\n   }\n \n-  // add all partitions that have been tried maxNumberAttempts\n-  protected static void addGiveupPartitions(Set<Integer> set, JobContext ctx,\n+  // Add all partitions/tasks that are cannot be retried. These tasks are:\n+  // 1- Task is in ABORTED or ERROR state.\n+  // 2- Task has just gone to TIMED_OUT, ERROR or DROPPED states and has reached to its\n+  // maxNumberAttempts\n+  // These tasks determine whether the job needs to FAILED or not.\n+  protected static void addGiveUpPartitions(Set<Integer> set, JobContext ctx,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e153c6722c52f472447c3db922050f25e79b237"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzYwMTYxMA==", "bodyText": "Done :-)", "url": "https://github.com/apache/helix/pull/1142#discussion_r457601610", "createdAt": "2020-07-20T18:15:15Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -745,8 +748,12 @@ protected void scheduleForNextTask(String job, JobContext jobCtx, long now) {\n     }\n   }\n \n-  // add all partitions that have been tried maxNumberAttempts\n-  protected static void addGiveupPartitions(Set<Integer> set, JobContext ctx,\n+  // Add all partitions/tasks that are cannot be retried. These tasks are:\n+  // 1- Task is in ABORTED or ERROR state.\n+  // 2- Task has just gone to TIMED_OUT, ERROR or DROPPED states and has reached to its\n+  // maxNumberAttempts\n+  // These tasks determine whether the job needs to FAILED or not.\n+  protected static void addGiveUpPartitions(Set<Integer> set, JobContext ctx,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzODQ2MQ=="}, "originalCommit": {"oid": "7e153c6722c52f472447c3db922050f25e79b237"}, "originalPosition": 91}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1228, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}