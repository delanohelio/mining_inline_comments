{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxNTM4ODY2", "number": 1076, "title": "Recover Workflow Garbage Collection Logic", "bodyText": "Issues\n\n My PR addresses the following Helix issues and references them in the PR description:\n\nFixes #1075\nDescription\n\n Here are some details about my PR, including screenshots of any UI changes:\n\nPreviously, the workflow garbage collection logic was removed in fear of race conditions (PR: #803 ). As we see value in bringing back workflow garbage collection, we are getting rid of the race conditions and bringing back the logic.\nSpecifically, two items need to be completed:\n\nAvoid direct dependency of garbage collection stage on the cache. We can instead deliver the relevant content of the cache into the stage during sync time, and use it during async time. This way the race condition with cache is no longer an issue.\nWe are getting rid of an unnecessary JobConfig write that serves no purpose to avoid possible race conditions.\n\nTests\n\n The following tests are written for this issue:\n\ntestGetExpiredJobsFromCache, testWorkflowContextGarbageCollection(recovered from old PR), testWorkflowGarbageCollection\n\n The following is the result of the \"mvn test\" command on the appropriate module:\n\n[ERROR] Tests run: 1153, Failures: 3, Errors: 0, Skipped: 1, Time elapsed: 4,609.951 s <<< FAILURE! - in TestSuite\n[ERROR] testEnableCompressionResource(org.apache.helix.integration.TestEnableCompression)  Time elapsed: 218.35 s  <<< FAILURE!\njava.lang.AssertionError: expected:<true> but was:<false>\n        at org.apache.helix.integration.TestEnableCompression.testEnableCompressionResource(TestEnableCompression.java:117)\n\n[ERROR] testStateTransitionTimeOut(org.apache.helix.integration.paticipant.TestStateTransitionTimeoutWithResource)  Time elapsed: 36.446 s  <<< FAILURE!\njava.lang.AssertionError: expected:<true> but was:<false>\n        at org.apache.helix.integration.paticipant.TestStateTransitionTimeoutWithResource.testStateTransitionTimeOut(TestStateTransitionTimeoutWithResource.java:171)\n\n[ERROR] testHappyPathExtOpZkCacheBaseDataAccessor(org.apache.helix.manager.zk.TestZkCacheAsyncOpSingleThread)  Time elapsed: 1.191 s  <<< FAILURE!\njava.lang.AssertionError: zkCache doesn't match data on Zk expected:<true> but was:<false>\n        at org.apache.helix.manager.zk.TestZkCacheAsyncOpSingleThread.testHappyPathExtOpZkCacheBaseDataAccessor(TestZkCacheAsyncOpSingleThread.java:214)\n\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Failures: \n[ERROR]   TestEnableCompression.testEnableCompressionResource:117 expected:<true> but was:<false>\n[ERROR]   TestStateTransitionTimeoutWithResource.testStateTransitionTimeOut:171 expected:<true> but was:<false>\n[ERROR]   TestZkCacheAsyncOpSingleThread.testHappyPathExtOpZkCacheBaseDataAccessor:214 zkCache doesn't match data on Zk expected:<true> but was:<false>\n[INFO] \n[ERROR] Tests run: 1153, Failures: 3, Errors: 0, Skipped: 1\n[INFO] \n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  01:16 h\n[INFO] Finished at: 2020-07-14T11:10:05-07:00\n[INFO] ------------------------------------------------------------------------\n\nRerun\n[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 51.087 s - in TestSuite\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0\n[INFO] \n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  56.554 s\n[INFO] Finished at: 2020-07-14T11:14:48-07:00\n[INFO] ------------------------------------------------------------------------\n\nCommits\n\n My commits all reference appropriate Apache Helix GitHub issues in their subject lines. In addition, my commits follow the guidelines from \"How to write a good git commit message\":\n\nSubject is separated from body by a blank line\nSubject is limited to 50 characters (not including Jira issue reference)\nSubject does not end with a period\nSubject uses the imperative mood (\"add\", not \"adding\")\nBody wraps at 72 characters\nBody explains \"what\" and \"why\", not \"how\"\n\n\n\nCode Quality\n\n My diff has been formatted using helix-style.xml\n(helix-style-intellij.xml if IntelliJ IDE is used)", "createdAt": "2020-06-09T05:06:35Z", "url": "https://github.com/apache/helix/pull/1076", "merged": true, "mergeCommit": {"oid": "c7a97bdce66b21dab5522a4f100d08054ada99c2"}, "closed": true, "closedAt": "2020-07-15T20:01:24Z", "author": {"login": "NealSun96"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcpeihPAFqTQyNjgwNjkyNw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc14k2SAFqTQ1MDg5MDE2OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA2OTI3", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426806927", "createdAt": "2020-06-09T05:51:50Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1MTo1MFrOGg5sPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1MTo1MFrOGg5sPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MjgzMA==", "bodyText": "Why are we starting with a space?", "url": "https://github.com/apache/helix/pull/1076#discussion_r437152830", "createdAt": "2020-06-09T05:51:50Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA3NDgy", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426807482", "createdAt": "2020-06-09T05:53:11Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1MzoxMlrOGg5uCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1MzoxMlrOGg5uCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ==", "bodyText": "How would this affect the memory footprint of each event?\nAlso, is a soft copy okay here? I guess if soft copy is okay, then it won't be too much overhead because this would just be two more references.", "url": "https://github.com/apache/helix/pull/1076#discussion_r437153289", "createdAt": "2020-06-09T05:53:12Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA4NDEz", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426808413", "createdAt": "2020-06-09T05:55:34Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NTozNFrOGg5xEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NTozNFrOGg5xEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDA2Nw==", "bodyText": "If we assumed soft copy was okay from above, then why are we creating a new set? Also this is not a hard copy either is it?", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154067", "createdAt": "2020-06-09T05:55:34Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA4ODg0", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426808884", "createdAt": "2020-06-09T05:56:39Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NjozOVrOGg5yyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NjozOVrOGg5yyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDUwNA==", "bodyText": "Aren't these two checks same checks? Why are we doing the same thing twice?", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154504", "createdAt": "2020-06-09T05:56:39Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());\n     for (WorkflowConfig workflowConfig : existingWorkflows) {\n       // clean up the expired jobs if it is a queue.\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        String workflowId = workflowConfig.getWorkflowId();\n+        if (resourceContextMap.containsKey(workflowId)\n+            && resourceContextMap.get(workflowId) != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA5MDcx", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426809071", "createdAt": "2020-06-09T05:57:09Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NzowOVrOGg5zWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NzowOVrOGg5zWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDY0OQ==", "bodyText": "Use parameterized logging.", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154649", "createdAt": "2020-06-09T05:57:09Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());\n     for (WorkflowConfig workflowConfig : existingWorkflows) {\n       // clean up the expired jobs if it is a queue.\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        String workflowId = workflowConfig.getWorkflowId();\n+        if (resourceContextMap.containsKey(workflowId)\n+            && resourceContextMap.get(workflowId) != null) {\n+          try {\n+            TaskUtil.purgeExpiredJobs(workflowId, workflowConfig,\n+                new WorkflowContext(resourceContextMap.get(workflowId)), manager,\n+                _rebalanceScheduler);\n+          } catch (Exception e) {\n+            LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\", workflowId,\n+                e.toString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA5MTI3", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426809127", "createdAt": "2020-06-09T05:57:20Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NzoyMFrOGg5zgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1NzoyMFrOGg5zgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDY5MQ==", "bodyText": "Use parameterized logging.", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154691", "createdAt": "2020-06-09T05:57:20Z", "author": {"login": "narendly"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());\n     for (WorkflowConfig workflowConfig : existingWorkflows) {\n       // clean up the expired jobs if it is a queue.\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        String workflowId = workflowConfig.getWorkflowId();\n+        if (resourceContextMap.containsKey(workflowId)\n+            && resourceContextMap.get(workflowId) != null) {\n+          try {\n+            TaskUtil.purgeExpiredJobs(workflowId, workflowConfig,\n+                new WorkflowContext(resourceContextMap.get(workflowId)), manager,\n+                _rebalanceScheduler);\n+          } catch (Exception e) {\n+            LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\", workflowId,\n+                e.toString()));\n+          }\n+        } else {\n+          LOG.warn(String.format(\"Workflow %s context does not exist!\", workflowId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2ODA5NjE5", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-426809619", "createdAt": "2020-06-09T05:58:29Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1ODoyOVrOGg51Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwNTo1ODoyOVrOGg51Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NTA5NQ==", "bodyText": "Redundant line", "url": "https://github.com/apache/helix/pull/1076#discussion_r437155095", "createdAt": "2020-06-09T05:58:29Z", "author": {"login": "narendly"}, "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestWorkflowContextWithoutConfig.java", "diffHunk": "@@ -103,6 +103,73 @@ public void testWorkflowContextWithoutConfig() throws Exception {\n     Assert.assertTrue(workflowContextNotCreated);\n   }\n \n+  @Test\n+  public void testWorkflowContextGarbageCollection() throws Exception {\n+    String workflowName = TestHelper.getTestMethodName();\n+    Workflow.Builder builder1 = createSimpleWorkflowBuilder(workflowName);\n+    _driver.start(builder1.build());\n+\n+    // Wait until workflow is created and IN_PROGRESS state.\n+    _driver.pollForWorkflowState(workflowName, TaskState.IN_PROGRESS);\n+\n+    // Check that WorkflowConfig, WorkflowContext, and IdealState are indeed created for this\n+    // workflow\n+    Assert.assertNotNull(_driver.getWorkflowConfig(workflowName));\n+    Assert.assertNotNull(_driver.getWorkflowContext(workflowName));\n+    Assert.assertNotNull(_admin.getResourceIdealState(CLUSTER_NAME, workflowName));\n+\n+    String workflowContextPath =\n+        \"/\" + CLUSTER_NAME + \"/PROPERTYSTORE/TaskRebalancer/\" + workflowName + \"/Context\";\n+\n+    ZNRecord record = _manager.getHelixDataAccessor().getBaseDataAccessor().get(workflowContextPath,\n+        null, AccessOption.PERSISTENT);\n+    Assert.assertNotNull(record);\n+\n+    // Wait until workflow is completed.\n+    _driver.pollForWorkflowState(workflowName, TaskState.COMPLETED);\n+\n+    // Verify that WorkflowConfig, WorkflowContext, and IdealState are removed after workflow got\n+    // expired.\n+    boolean workflowExpired = TestHelper.verify(() -> {\n+      WorkflowContext wCtx = _driver.getWorkflowContext(workflowName);\n+      WorkflowConfig wCfg = _driver.getWorkflowConfig(workflowName);\n+      IdealState idealState = _admin.getResourceIdealState(CLUSTER_NAME, workflowName);\n+      return (wCtx == null && wCfg == null && idealState == null);\n+    }, TestHelper.WAIT_DURATION);\n+    Assert.assertTrue(workflowExpired);\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NDI4ODk1", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-427428895", "createdAt": "2020-06-09T18:24:45Z", "commit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxODoyNDo0NVrOGhW8Qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxOToyNTowM1rOGhY9og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYzMjA2Nw==", "bodyText": "Instead of having 2 additional maps that duplicate the fields of ControllerDataProvider, can we have a specific class presenting the to be GCed objects?", "url": "https://github.com/apache/helix/pull/1076#discussion_r437632067", "createdAt": "2020-06-09T18:24:45Z", "author": {"login": "jiajunwang"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/AttributeName.java", "diffHunk": "@@ -40,5 +40,7 @@\n   PipelineType,\n   LastRebalanceFinishTimeStamp,\n   ControllerDataProvider,\n-  STATEFUL_REBALANCER\n+  STATEFUL_REBALANCER,\n+  WORKFLOW_CONFIG_MAP,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MTMyMA==", "bodyText": "Is this tool related to the TaskGarbageCollectionStage?\nIt would be better to split the PR if they are not strictly related.", "url": "https://github.com/apache/helix/pull/1076#discussion_r437661320", "createdAt": "2020-06-09T19:17:42Z", "author": {"login": "jiajunwang"}, "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -1037,6 +1037,53 @@ public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConf\n     setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);\n   }\n \n+  /**\n+   * The function that loops through the all existing workflow contexts and removes IdealState and\n+   * workflow context of the workflow whose workflow config does not exist.\n+   * @param workflowConfigMap\n+   * @param resourceContextMap\n+   * @param manager\n+   */\n+  public static void workflowGarbageCollection(final Map<String, WorkflowConfig> workflowConfigMap,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "890aa1fbc90e852e6a2beea5e9ca89cdc7639483"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NDA2Nw==", "bodyText": "I don't think a reference to the map object itself here is OK. My understanding is that you want to persist in the cached map before it is modified in the later stages. In this case, you need to copy the map, not just pass the reference.\nAlso, if this change passes our current test, then it may indicate the test does not cover the race condition case. To justify that this PR really fixes the race condition, please add the corresponding test case which fails because of the race condition without the fix.", "url": "https://github.com/apache/helix/pull/1076#discussion_r437664067", "createdAt": "2020-06-09T19:22:57Z", "author": {"login": "jiajunwang"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ=="}, "originalCommit": {"oid": "a91b75b9d415ebbc56062ba4c36f17fc160428ca"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng==", "bodyText": "Just curious, why are we adding all the workflow/resource contexts here?\nIMHO, a better design would be after all workflows, jobs being processed, add the rest unprocessed workflows, jobs to the async GC thread. Will that be better?", "url": "https://github.com/apache/helix/pull/1076#discussion_r437665186", "createdAt": "2020-06-09T19:25:03Z", "author": {"login": "jiajunwang"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "890aa1fbc90e852e6a2beea5e9ca89cdc7639483"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNzE4NDUw", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-433718450", "createdAt": "2020-06-18T23:50:50Z", "commit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMzo1MDo1MFrOGmDuLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOTo1OToyNFrOGmf3Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2MDA0NQ==", "bodyText": "We can name it consistently.", "url": "https://github.com/apache/helix/pull/1076#discussion_r442560045", "createdAt": "2020-06-18T23:50:50Z", "author": {"login": "junkaixue"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/AttributeName.java", "diffHunk": "@@ -40,5 +40,9 @@\n   PipelineType,\n   LastRebalanceFinishTimeStamp,\n   ControllerDataProvider,\n-  STATEFUL_REBALANCER\n+  STATEFUL_REBALANCER,\n+  // This attribute should only be used in TaskGarbageCollectionStage, misuse could cause race conditions.\n+  WORKFLOWS_TO_BE_DELETED,\n+  // This attribute should only be used in TaskGarbageCollectionStage, misuse could cause race conditions.\n+  EXPIRED_JOBS_MAP", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyMTExNA==", "bodyText": "If you do garbage collection. This triggering is not necessary.", "url": "https://github.com/apache/helix/pull/1076#discussion_r443021114", "createdAt": "2020-06-19T19:59:24Z", "author": {"login": "junkaixue"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +29,89 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n-    WorkflowControllerDataProvider dataProvider =\n-        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+  public void process(ClusterEvent event) throws Exception {\n+    // Use main thread to compute what jobs need to be purged, and what workflows need to be gc'ed.\n+    // This is to avoid race conditions since the cache will be modified. After this work, then the\n+    // async work will happen.\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n-\n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \"HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n-    for (WorkflowConfig workflowConfig : existingWorkflows) {\n-      // clean up the expired jobs if it is a queue.\n+    Map<String, Set<String>> expiredJobsMap = new HashMap<>();\n+    Set<String> workflowsToBeDeleted = new HashSet<>();\n+    WorkflowControllerDataProvider dataProvider =\n+        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    for (Map.Entry<String, ZNRecord> entry : dataProvider.getContexts().entrySet()) {\n+      WorkflowConfig workflowConfig = dataProvider.getWorkflowConfig(entry.getKey());\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        WorkflowContext workflowContext = dataProvider.getWorkflowContext(entry.getKey());\n+        long purgeInterval = workflowConfig.getJobPurgeInterval();\n+        long currentTime = System.currentTimeMillis();\n+        if (purgeInterval > 0\n+            && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n+          // Find jobs that are ready to be purged\n+          Set<String> expiredJobs =\n+              TaskUtil.getExpiredJobsFromCache(dataProvider, workflowConfig, workflowContext);\n+          if (!expiredJobs.isEmpty()) {\n+            expiredJobsMap.put(workflowConfig.getWorkflowId(), expiredJobs);\n+          }\n+          scheduleNextJobPurge(workflowConfig.getWorkflowId(), currentTime, purgeInterval,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MzcyMDE0", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-434372014", "createdAt": "2020-06-19T22:49:40Z", "commit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMjo0OTo0MFrOGmi2jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMjo0OTo0MFrOGmi2jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDA5NQ==", "bodyText": "This function is moved from here to TaskGarbageCollectionStage's main thread. The reason for the move is that purgeInterval no longer needs to be passed to the async thread. I believe the trade-off is worth it because the frequency isn't critical. FYI: @dasahcc", "url": "https://github.com/apache/helix/pull/1076#discussion_r443070095", "createdAt": "2020-06-19T22:49:40Z", "author": {"login": "NealSun96"}, "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -977,72 +1011,69 @@ public static boolean isJobStarted(String job, WorkflowContext workflowContext)\n   }\n \n   /**\n-   * Clean up all jobs that are COMPLETED and passes its expiry time.\n-   * @param workflowConfig\n-   * @param workflowContext\n+   * Clean up all jobs that are marked as expired.\n    */\n-  public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConfig,\n-      WorkflowContext workflowContext, HelixManager manager,\n-      RebalanceScheduler rebalanceScheduler) {\n-    if (workflowContext == null) {\n-      LOG.warn(String.format(\"Workflow %s context does not exist!\", workflow));\n-      return;\n+  public static void purgeExpiredJobs(String workflow, Set<String> expiredJobs,\n+      HelixManager manager, RebalanceScheduler rebalanceScheduler) {\n+    Set<String> failedJobRemovals = new HashSet<>();\n+    for (String job : expiredJobs) {\n+      if (!TaskUtil\n+          .removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(), job)) {\n+        failedJobRemovals.add(job);\n+        LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n+      }\n+      rebalanceScheduler.removeScheduledRebalance(job);\n     }\n-    long purgeInterval = workflowConfig.getJobPurgeInterval();\n-    long currentTime = System.currentTimeMillis();\n-    final Set<String> expiredJobs = Sets.newHashSet();\n-    if (purgeInterval > 0 && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n-      expiredJobs.addAll(TaskUtil.getExpiredJobs(manager.getHelixDataAccessor(),\n-          manager.getHelixPropertyStore(), workflowConfig, workflowContext));\n-      if (expiredJobs.isEmpty()) {\n-        LOG.info(\"No job to purge for the queue \" + workflow);\n-      } else {\n-        LOG.info(\"Purge jobs \" + expiredJobs + \" from queue \" + workflow);\n-        Set<String> failedJobRemovals = new HashSet<>();\n-        for (String job : expiredJobs) {\n-          if (!TaskUtil.removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(),\n-              job)) {\n-            failedJobRemovals.add(job);\n-            LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n-          }\n-          rebalanceScheduler.removeScheduledRebalance(job);\n-        }\n \n-        // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n-        // removal will be tried again at next purge\n-        expiredJobs.removeAll(failedJobRemovals);\n+    // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n+    // removal will be tried again at next purge\n+    expiredJobs.removeAll(failedJobRemovals);\n \n-        if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs,\n-            true)) {\n-          LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs\n-              + \" from the workflow \" + workflow);\n-        }\n+    if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs, true)) {\n+      LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs + \" from the workflow \"\n+          + workflow);\n+    }\n \n-        if (expiredJobs.size() > 0) {\n-          // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n-          // concurrent write issue. It is possible that jobs got purged but there is no event to\n-          // trigger the pipeline to clean context.\n-          HelixDataAccessor accessor = manager.getHelixDataAccessor();\n-          List<String> resourceConfigs =\n-              accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n-          if (resourceConfigs.size() > 0) {\n-            RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n-          } else {\n-            LOG.warn(\n-                \"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n-          }\n-        }\n+    if (expiredJobs.size() > 0) {\n+      // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n+      // concurrent write issue. It is possible that jobs got purged but there is no event to\n+      // trigger the pipeline to clean context.\n+      HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+      List<String> resourceConfigs =\n+          accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n+      if (resourceConfigs.size() > 0) {\n+        RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n+      } else {\n+        LOG.warn(\"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n       }\n     }\n-    setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "originalPosition": 164}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MTc3MTkw", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-436177190", "createdAt": "2020-06-23T21:38:35Z", "commit": {"oid": "42afe9ded7b7cafdab5eaaaa864965dc1efda414"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMTozODozNVrOGn7ksA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMTo0MTo1MFrOGn7p5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyMzY5Ng==", "bodyText": "You can make it:\nWORKFLOW_TO_BE_DELETED with JOB_TO_BE_DELETED. Or EXPIRED_WORKFLOWS_MAP with EXPIRED_JOBS_MAP. Logically, they belong to same group.", "url": "https://github.com/apache/helix/pull/1076#discussion_r444523696", "createdAt": "2020-06-23T21:38:35Z", "author": {"login": "junkaixue"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/AttributeName.java", "diffHunk": "@@ -40,5 +40,9 @@\n   PipelineType,\n   LastRebalanceFinishTimeStamp,\n   ControllerDataProvider,\n-  STATEFUL_REBALANCER\n+  STATEFUL_REBALANCER,\n+  // This attribute should only be used in TaskGarbageCollectionStage, misuse could cause race conditions.\n+  WORKFLOWS_TO_BE_DELETED,\n+  // This attribute should only be used in TaskGarbageCollectionStage, misuse could cause race conditions.\n+  EXPIRED_JOBS_MAP", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2MDA0NQ=="}, "originalCommit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyNDc1MA==", "bodyText": "One concern here is that if we move the logic from function to the stage. We make the clean up logic dedicated to pipeline. Say some one they just want to call the purge from REST and do a job purge. You will not be able to do it.", "url": "https://github.com/apache/helix/pull/1076#discussion_r444524750", "createdAt": "2020-06-23T21:41:09Z", "author": {"login": "junkaixue"}, "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -977,72 +1011,69 @@ public static boolean isJobStarted(String job, WorkflowContext workflowContext)\n   }\n \n   /**\n-   * Clean up all jobs that are COMPLETED and passes its expiry time.\n-   * @param workflowConfig\n-   * @param workflowContext\n+   * Clean up all jobs that are marked as expired.\n    */\n-  public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConfig,\n-      WorkflowContext workflowContext, HelixManager manager,\n-      RebalanceScheduler rebalanceScheduler) {\n-    if (workflowContext == null) {\n-      LOG.warn(String.format(\"Workflow %s context does not exist!\", workflow));\n-      return;\n+  public static void purgeExpiredJobs(String workflow, Set<String> expiredJobs,\n+      HelixManager manager, RebalanceScheduler rebalanceScheduler) {\n+    Set<String> failedJobRemovals = new HashSet<>();\n+    for (String job : expiredJobs) {\n+      if (!TaskUtil\n+          .removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(), job)) {\n+        failedJobRemovals.add(job);\n+        LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n+      }\n+      rebalanceScheduler.removeScheduledRebalance(job);\n     }\n-    long purgeInterval = workflowConfig.getJobPurgeInterval();\n-    long currentTime = System.currentTimeMillis();\n-    final Set<String> expiredJobs = Sets.newHashSet();\n-    if (purgeInterval > 0 && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n-      expiredJobs.addAll(TaskUtil.getExpiredJobs(manager.getHelixDataAccessor(),\n-          manager.getHelixPropertyStore(), workflowConfig, workflowContext));\n-      if (expiredJobs.isEmpty()) {\n-        LOG.info(\"No job to purge for the queue \" + workflow);\n-      } else {\n-        LOG.info(\"Purge jobs \" + expiredJobs + \" from queue \" + workflow);\n-        Set<String> failedJobRemovals = new HashSet<>();\n-        for (String job : expiredJobs) {\n-          if (!TaskUtil.removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(),\n-              job)) {\n-            failedJobRemovals.add(job);\n-            LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n-          }\n-          rebalanceScheduler.removeScheduledRebalance(job);\n-        }\n \n-        // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n-        // removal will be tried again at next purge\n-        expiredJobs.removeAll(failedJobRemovals);\n+    // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n+    // removal will be tried again at next purge\n+    expiredJobs.removeAll(failedJobRemovals);\n \n-        if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs,\n-            true)) {\n-          LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs\n-              + \" from the workflow \" + workflow);\n-        }\n+    if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs, true)) {\n+      LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs + \" from the workflow \"\n+          + workflow);\n+    }\n \n-        if (expiredJobs.size() > 0) {\n-          // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n-          // concurrent write issue. It is possible that jobs got purged but there is no event to\n-          // trigger the pipeline to clean context.\n-          HelixDataAccessor accessor = manager.getHelixDataAccessor();\n-          List<String> resourceConfigs =\n-              accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n-          if (resourceConfigs.size() > 0) {\n-            RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n-          } else {\n-            LOG.warn(\n-                \"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n-          }\n-        }\n+    if (expiredJobs.size() > 0) {\n+      // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n+      // concurrent write issue. It is possible that jobs got purged but there is no event to\n+      // trigger the pipeline to clean context.\n+      HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+      List<String> resourceConfigs =\n+          accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n+      if (resourceConfigs.size() > 0) {\n+        RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n+      } else {\n+        LOG.warn(\"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n       }\n     }\n-    setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDA5NQ=="}, "originalCommit": {"oid": "c089b286ffab0a320c7d997b88cdd77614c5aa65"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyNTAyOA==", "bodyText": "You can mark a TODO here. We dont need it in the future since TF is not relying on IS/EV anymore.", "url": "https://github.com/apache/helix/pull/1076#discussion_r444525028", "createdAt": "2020-06-23T21:41:50Z", "author": {"login": "junkaixue"}, "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -977,72 +1011,70 @@ public static boolean isJobStarted(String job, WorkflowContext workflowContext)\n   }\n \n   /**\n-   * Clean up all jobs that are COMPLETED and passes its expiry time.\n-   * @param workflowConfig\n-   * @param workflowContext\n+   * Clean up all jobs that are marked as expired.\n    */\n-  public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConfig,\n-      WorkflowContext workflowContext, HelixManager manager,\n-      RebalanceScheduler rebalanceScheduler) {\n-    if (workflowContext == null) {\n-      LOG.warn(String.format(\"Workflow %s context does not exist!\", workflow));\n-      return;\n+  public static void purgeExpiredJobs(String workflow, Set<String> expiredJobs,\n+      HelixManager manager, RebalanceScheduler rebalanceScheduler) {\n+    Set<String> failedJobRemovals = new HashSet<>();\n+    for (String job : expiredJobs) {\n+      if (!TaskUtil\n+          .removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(), job)) {\n+        failedJobRemovals.add(job);\n+        LOG.warn(\"Failed to clean up expired and completed jobs from workflow {}!\", workflow);\n+      }\n+      rebalanceScheduler.removeScheduledRebalance(job);\n     }\n-    long purgeInterval = workflowConfig.getJobPurgeInterval();\n-    long currentTime = System.currentTimeMillis();\n-    final Set<String> expiredJobs = Sets.newHashSet();\n-    if (purgeInterval > 0 && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n-      expiredJobs.addAll(TaskUtil.getExpiredJobs(manager.getHelixDataAccessor(),\n-          manager.getHelixPropertyStore(), workflowConfig, workflowContext));\n-      if (expiredJobs.isEmpty()) {\n-        LOG.info(\"No job to purge for the queue \" + workflow);\n-      } else {\n-        LOG.info(\"Purge jobs \" + expiredJobs + \" from queue \" + workflow);\n-        Set<String> failedJobRemovals = new HashSet<>();\n-        for (String job : expiredJobs) {\n-          if (!TaskUtil.removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(),\n-              job)) {\n-            failedJobRemovals.add(job);\n-            LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n-          }\n-          rebalanceScheduler.removeScheduledRebalance(job);\n-        }\n \n-        // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n-        // removal will be tried again at next purge\n-        expiredJobs.removeAll(failedJobRemovals);\n+    // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n+    // removal will be tried again at next purge\n+    expiredJobs.removeAll(failedJobRemovals);\n \n-        if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs,\n-            true)) {\n-          LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs\n-              + \" from the workflow \" + workflow);\n-        }\n+    if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs, true)) {\n+      LOG.warn(\"Error occurred while trying to remove jobs {} from the workflow {}!\", expiredJobs,\n+          workflow);\n+    }\n \n-        if (expiredJobs.size() > 0) {\n-          // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n-          // concurrent write issue. It is possible that jobs got purged but there is no event to\n-          // trigger the pipeline to clean context.\n-          HelixDataAccessor accessor = manager.getHelixDataAccessor();\n-          List<String> resourceConfigs =\n-              accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n-          if (resourceConfigs.size() > 0) {\n-            RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n-          } else {\n-            LOG.warn(\n-                \"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n-          }\n-        }\n+    if (expiredJobs.size() > 0) {\n+      // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n+      // concurrent write issue. It is possible that jobs got purged but there is no event to\n+      // trigger the pipeline to clean context.\n+      HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+      List<String> resourceConfigs =\n+          accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n+      if (resourceConfigs.size() > 0) {\n+        RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n+      } else {\n+        LOG.warn(\"No resource config to trigger rebalance for clean up contexts for {}!\",\n+            expiredJobs);\n       }\n     }\n-    setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);\n   }\n \n-  private static void setNextJobPurgeTime(String workflow, long currentTime, long purgeInterval,\n-      RebalanceScheduler rebalanceScheduler, HelixManager manager) {\n-    long nextPurgeTime = currentTime + purgeInterval;\n-    long currentScheduledTime = rebalanceScheduler.getRebalanceTime(workflow);\n-    if (currentScheduledTime == -1 || currentScheduledTime > nextPurgeTime) {\n-      rebalanceScheduler.scheduleRebalance(manager, workflow, nextPurgeTime);\n+  /**\n+   * The function that removes IdealStates and workflow contexts of the workflows that need to be\n+   * deleted.\n+   * @param toBeDeletedWorkflows\n+   * @param manager\n+   */\n+  public static void workflowGarbageCollection(final Set<String> toBeDeletedWorkflows,\n+      final HelixManager manager) {\n+    HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+    HelixPropertyStore<ZNRecord> propertyStore = manager.getHelixPropertyStore();\n+\n+    for (String workflowName : toBeDeletedWorkflows) {\n+      LOG.warn(\n+          \"WorkflowContext exists for workflow {}. However, Workflow Config is missing! Deleting the WorkflowConfig and IdealState!!\",\n+          workflowName);\n+\n+      if (!cleanupWorkflowIdealStateExtView(accessor, workflowName)) {\n+        LOG.warn(\"Error occurred while trying to remove workflow idealstate/externalview for {}.\",\n+            workflowName);\n+        continue;\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "42afe9ded7b7cafdab5eaaaa864965dc1efda414"}, "originalPosition": 194}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNDQxNTk0", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-440441594", "createdAt": "2020-06-30T22:59:08Z", "commit": {"oid": "986a9ee0aaf27f629129d37433eea7b67096bbcd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMjo1OTowOVrOGrRRaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMjo1OTowOVrOGrRRaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDkzNg==", "bodyText": "Can we have workflow config without having context? Do we need to do null check?", "url": "https://github.com/apache/helix/pull/1076#discussion_r448024936", "createdAt": "2020-06-30T22:59:09Z", "author": {"login": "alirezazamani"}, "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +29,87 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n-    WorkflowControllerDataProvider dataProvider =\n-        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+  public void process(ClusterEvent event) throws Exception {\n+    // Use main thread to compute what jobs need to be purged, and what workflows need to be gc'ed.\n+    // This is to avoid race conditions since the cache will be modified. After this work, then the\n+    // async work will happen.\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n-\n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \"HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n-    for (WorkflowConfig workflowConfig : existingWorkflows) {\n-      // clean up the expired jobs if it is a queue.\n+    Map<String, Set<String>> expiredJobsMap = new HashMap<>();\n+    Set<String> workflowsToBeDeleted = new HashSet<>();\n+    WorkflowControllerDataProvider dataProvider =\n+        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    for (Map.Entry<String, ZNRecord> entry : dataProvider.getContexts().entrySet()) {\n+      WorkflowConfig workflowConfig = dataProvider.getWorkflowConfig(entry.getKey());\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        WorkflowContext workflowContext = dataProvider.getWorkflowContext(entry.getKey());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "986a9ee0aaf27f629129d37433eea7b67096bbcd"}, "originalPosition": 64}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a882840ce44096c5a6e099ceca86d326de69b8d5", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/a882840ce44096c5a6e099ceca86d326de69b8d5", "committedDate": "2020-07-01T17:55:27Z", "message": "Recover logics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e11a983683cc00f4642d8809ec41fe284863aa49", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/e11a983683cc00f4642d8809ec41fe284863aa49", "committedDate": "2020-07-01T17:55:27Z", "message": "Fix broken test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6e0f441d8f532e00d0c82d06e00fab134ac0886", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/a6e0f441d8f532e00d0c82d06e00fab134ac0886", "committedDate": "2020-07-01T17:55:27Z", "message": "Address some comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dcf29923e9669d25bc62a3e2b242de8efd91955", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/5dcf29923e9669d25bc62a3e2b242de8efd91955", "committedDate": "2020-07-01T17:55:27Z", "message": "Temporary changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cca49ab1f69e38b448b16ee49fe3e22a1ebeca0", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/7cca49ab1f69e38b448b16ee49fe3e22a1ebeca0", "committedDate": "2020-07-01T17:55:28Z", "message": "New direction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87f86bcb2427ec83e4ff66ecc5639571d31fe4d7", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/87f86bcb2427ec83e4ff66ecc5639571d31fe4d7", "committedDate": "2020-07-01T17:55:28Z", "message": "Add comments etc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ec883ed1204dec8309e07303ea3750843281b36", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/3ec883ed1204dec8309e07303ea3750843281b36", "committedDate": "2020-07-01T17:55:28Z", "message": "New test and nits"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d327f92bc676d5609cf8cab8dbfd530b954ca30", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/8d327f92bc676d5609cf8cab8dbfd530b954ca30", "committedDate": "2020-07-01T17:55:28Z", "message": "address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "986a9ee0aaf27f629129d37433eea7b67096bbcd", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/986a9ee0aaf27f629129d37433eea7b67096bbcd", "committedDate": "2020-06-26T22:32:05Z", "message": "address comments"}, "afterCommit": {"oid": "8d327f92bc676d5609cf8cab8dbfd530b954ca30", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/8d327f92bc676d5609cf8cab8dbfd530b954ca30", "committedDate": "2020-07-01T17:55:28Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMTgyNjAz", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-441182603", "createdAt": "2020-07-01T20:41:27Z", "commit": {"oid": "8d327f92bc676d5609cf8cab8dbfd530b954ca30"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7df8665602af2368a2224dc85b89af1fda3d2a7", "author": {"user": null}, "url": "https://github.com/apache/helix/commit/b7df8665602af2368a2224dc85b89af1fda3d2a7", "committedDate": "2020-07-14T00:14:21Z", "message": "Renaming"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwODkwMTY4", "url": "https://github.com/apache/helix/pull/1076#pullrequestreview-450890168", "createdAt": "2020-07-17T18:59:00Z", "commit": {"oid": "b7df8665602af2368a2224dc85b89af1fda3d2a7"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4469, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}