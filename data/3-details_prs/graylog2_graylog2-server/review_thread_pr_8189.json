{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxODg2ODkx", "number": 8189, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzoxODozN1rOECjjZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxNDowMlrOECm8vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTE1MTExOnYy", "diffSide": "RIGHT", "path": "graylog-storage-elasticsearch6/src/main/java/org/graylog/storage/elasticsearch6/MessagesAdapterES6.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzoxODozN1rOGfFSfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxMzoxODozN1rOGfFSfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NTY5Mg==", "bodyText": "This is probably a stray comment that slipped in when moving the code around. Let's remove it.", "url": "https://github.com/Graylog2/graylog2-server/pull/8189#discussion_r435245692", "createdAt": "2020-06-04T13:18:37Z", "author": {"login": "alex-konn"}, "path": "graylog-storage-elasticsearch6/src/main/java/org/graylog/storage/elasticsearch6/MessagesAdapterES6.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.graylog.storage.elasticsearch6;\n+\n+import com.codahale.metrics.Meter;\n+import com.codahale.metrics.MetricRegistry;\n+import com.github.joschi.jadconfig.util.Duration;\n+import com.github.rholder.retry.Attempt;\n+import com.github.rholder.retry.RetryException;\n+import com.github.rholder.retry.RetryListener;\n+import com.github.rholder.retry.Retryer;\n+import com.github.rholder.retry.RetryerBuilder;\n+import com.github.rholder.retry.WaitStrategies;\n+import com.github.rholder.retry.WaitStrategy;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Sets;\n+import io.searchbox.client.JestClient;\n+import io.searchbox.client.JestResult;\n+import io.searchbox.core.Bulk;\n+import io.searchbox.core.BulkResult;\n+import io.searchbox.core.DocumentResult;\n+import io.searchbox.core.Get;\n+import io.searchbox.core.Index;\n+import io.searchbox.indices.Analyze;\n+import org.apache.http.client.config.RequestConfig;\n+import org.graylog2.indexer.ElasticsearchException;\n+import org.graylog2.indexer.IndexFailure;\n+import org.graylog2.indexer.IndexFailureImpl;\n+import org.graylog2.indexer.IndexMapping;\n+import org.graylog2.indexer.cluster.jest.JestUtils;\n+import org.graylog2.indexer.messages.DocumentNotFoundException;\n+import org.graylog2.indexer.messages.IndexBlockRetryAttempt;\n+import org.graylog2.indexer.messages.IndexingRequest;\n+import org.graylog2.indexer.messages.Messages;\n+import org.graylog2.indexer.messages.MessagesAdapter;\n+import org.graylog2.indexer.results.ResultMessage;\n+import org.graylog2.plugin.Message;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.inject.Inject;\n+import javax.inject.Named;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.codahale.metrics.MetricRegistry.name;\n+\n+public class MessagesAdapterES6 implements MessagesAdapter {\n+    private static final Duration MAX_WAIT_TIME = Duration.seconds(30L);\n+    private static final Logger LOG = LoggerFactory.getLogger(MessagesAdapterES6.class);\n+\n+    @VisibleForTesting\n+    static final WaitStrategy exponentialWaitMilliseconds = WaitStrategies.exponentialWait(MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit());\n+\n+    // the wait strategy uses powers of 2 to compute wait times.\n+    // see https://github.com/rholder/guava-retrying/blob/177b6c9b9f3e7957f404f0bdb8e23374cb1de43f/src/main/java/com/github/rholder/retry/WaitStrategies.java#L304\n+    // using 500 leads to the expected exponential pattern of 1000, 2000, 4000, 8000, ...\n+    private static final int retrySecondsMultiplier = 500;\n+\n+    @VisibleForTesting\n+    static final WaitStrategy exponentialWaitSeconds = WaitStrategies.exponentialWait(retrySecondsMultiplier, MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit());\n+\n+    static final String INDEX_BLOCK_ERROR = \"cluster_block_exception\";\n+    static final String INDEX_BLOCK_REASON = \"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\";\n+\n+    private final JestClient client;\n+    private final boolean useExpectContinue;\n+\n+    private static final Retryer<BulkResult> BULK_REQUEST_RETRYER = RetryerBuilder.<BulkResult>newBuilder()\n+            .retryIfException(t -> t instanceof IOException)\n+            .withWaitStrategy(WaitStrategies.exponentialWait(MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit()))\n+            .withRetryListener(new RetryListener() {\n+                @Override\n+                public <V> void onRetry(Attempt<V> attempt) {\n+                    if (attempt.hasException()) {\n+                        LOG.error(\"Caught exception during bulk indexing: {}, retrying (attempt #{}).\", attempt.getExceptionCause(), attempt.getAttemptNumber());\n+                    } else if (attempt.getAttemptNumber() > 1) {\n+                        LOG.info(\"Bulk indexing finally successful (attempt #{}).\", attempt.getAttemptNumber());\n+                    }\n+                }\n+            })\n+            .build();\n+\n+    private final Meter invalidTimestampMeter;\n+\n+    @Inject\n+    public MessagesAdapterES6(JestClient client,\n+                              @Named(\"elasticsearch_use_expect_continue\") boolean useExpectContinue,\n+                              MetricRegistry metricRegistry) {\n+        this.client = client;\n+        this.useExpectContinue = useExpectContinue;\n+        invalidTimestampMeter = metricRegistry.meter(name(Messages.class, \"invalid-timestamps\"));\n+    }\n+\n+    @Override\n+    public ResultMessage get(String messageId, String index) throws IOException, DocumentNotFoundException {\n+        final Get get = new Get.Builder(index, messageId).type(IndexMapping.TYPE_MESSAGE).build();\n+        final DocumentResult result = client.execute(get);\n+\n+        if (!result.isSucceeded()) {\n+            throw new DocumentNotFoundException(index, messageId);\n+        }\n+\n+        @SuppressWarnings(\"unchecked\") final Map<String, Object> message = (Map<String, Object>) result.getSourceAsObject(Map.class, false);\n+\n+        return ResultMessage.parseFromSource(result.getId(), result.getIndex(), message);\n+    }\n+\n+    @Override\n+    public List<String> analyze(String toAnalyze, String index, String analyzer) throws IOException {\n+        final Analyze analyze = new Analyze.Builder().index(index).analyzer(analyzer).text(toAnalyze).build();\n+        final JestResult result = client.execute(analyze);\n+\n+        @SuppressWarnings(\"unchecked\") final List<Map<String, Object>> tokens = (List<Map<String, Object>>) result.getValue(\"tokens\");\n+        final List<String> terms = new ArrayList<>(tokens.size());\n+        tokens.forEach(token -> terms.add((String) token.get(\"token\")));\n+\n+        return terms;\n+    }\n+\n+    @Override\n+    public List<IndexFailure> bulkIndex(List<IndexingRequest> messageList) {\n+        if (messageList.isEmpty()) {\n+            return Collections.emptyList();\n+        }\n+\n+        int chunkSize = messageList.size();\n+        int offset = 0;\n+        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n+        for (;;) {\n+            try {\n+                final List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, offset, chunkSize);\n+                failedItems.addAll(failures);\n+                break; // on success\n+            } catch (EntityTooLargeException e) {\n+                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n+                if (chunkSize == messageList.size()) {\n+                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n+                }\n+                failedItems.addAll(e.failedItems);\n+                offset += e.indexedSuccessfully;\n+                chunkSize /= 2;\n+            }\n+            if (chunkSize == 0) {\n+                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n+            }\n+        }\n+\n+        return indexFailuresFromMessages(failedItems, messageList);\n+    }\n+\n+    private List<IndexFailure> indexFailuresFromMessages(List<BulkResult.BulkResultItem> failedItems, List<IndexingRequest> messageList) {\n+        if (failedItems.isEmpty()) {\n+            return Collections.emptyList();\n+        }\n+\n+        final Map<String, Message> messageMap = messageList.stream()\n+                .map(IndexingRequest::message)\n+                .distinct()\n+                .collect(Collectors.toMap(Message::getId, Function.identity()));\n+        final List<IndexFailure> indexFailures = new ArrayList<>(failedItems.size());\n+        for (BulkResult.BulkResultItem item : failedItems) {\n+            LOG.warn(\"Failed to index message: index=<{}> id=<{}> error=<{}>\", item.index, item.id, item.error);\n+\n+            // Write failure to index_failures.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3ed13253edc8610310ed1bd2d7a623869344fd2"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTcwMzQzOnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxMzowOFrOGfK0aQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxMzowOFrOGfK0aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjI5Nw==", "bodyText": "IntelliJ says this is a typo and should be renamed successfulRequests with one l. I believe it blindly!", "url": "https://github.com/Graylog2/graylog2-server/pull/8189#discussion_r435336297", "createdAt": "2020-06-04T15:13:08Z", "author": {"login": "alex-konn"}, "path": "graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java", "diffHunk": "@@ -164,210 +75,47 @@ public ResultMessage get(String messageId, String index) throws DocumentNotFound\n             return Collections.emptyList();\n         }\n \n-        int chunkSize = messageList.size();\n-        int offset = 0;\n-        List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        for (;;) {\n-            try {\n-                List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, isSystemTraffic, offset, chunkSize);\n-                failedItems.addAll(failures);\n-                break; // on success\n-            } catch (EntityTooLargeException e) {\n-                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n-                if (chunkSize == messageList.size()) {\n-                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n-                }\n-                failedItems.addAll(e.failedItems);\n-                offset += e.indexedSuccessfully;\n-                chunkSize /= 2;\n-            }\n-            if (chunkSize == 0) {\n-                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n-            }\n-        }\n-\n-        if (!failedItems.isEmpty()) {\n-            final Set<String> failedIds = failedItems.stream().map(item -> item.id).collect(Collectors.toSet());\n-            recordTimestamp(messageList, failedIds);\n-            return propagateFailure(failedItems, messageList);\n-        } else {\n-            recordTimestamp(messageList, Collections.emptySet());\n-            return Collections.emptyList();\n-        }\n-    }\n-\n-    private List<BulkResult.BulkResultItem> bulkIndexChunked(final List<Map.Entry<IndexSet, Message>> messageList, boolean isSystemTraffic, int offset, int chunkSize) throws EntityTooLargeException {\n-        chunkSize = Math.min(messageList.size(), chunkSize);\n-\n-        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        final Iterable<List<Map.Entry<IndexSet, Message>>> chunks = Iterables.partition(messageList.subList(offset, messageList.size()), chunkSize);\n-        int chunkCount = 1;\n-        int indexedSuccessfully = 0;\n-        for (List<Map.Entry<IndexSet, Message>> chunk : chunks) {\n-\n-            long messageSizes = chunk.stream().mapToLong(m -> m.getValue().getSize()).sum();\n-\n-            final BulkResult result = bulkIndexChunk(chunk);\n-\n-            if (result.getResponseCode() == 413) {\n-                throw new EntityTooLargeException(indexedSuccessfully, failedItems);\n-            }\n+        final List<IndexingRequest> indexingRequestList = messageList.stream()\n+                .map(entry -> IndexingRequest.create(entry.getKey(), entry.getValue()))\n+                .collect(Collectors.toList());\n \n-            // TODO should we check result.isSucceeded()?\n+        final List<IndexFailure> indexFailures = messagesAdapter.bulkIndex(indexingRequestList);\n \n-            indexedSuccessfully += chunk.size();\n+        final Set<String> failedIds = indexFailures.stream().map(IndexFailure::letterId).collect(Collectors.toSet());\n+        final List<IndexingRequest> successfullRequests = indexingRequestList.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "152b4efec5c6581b36771a05fd88e87b792e5b24"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTcwNzUwOnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxNDowMlrOGfK3CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxNDowMlrOGfK3CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjk2OA==", "bodyText": "Now that we reproduced the old behavior the parameter should just be named request.", "url": "https://github.com/Graylog2/graylog2-server/pull/8189#discussion_r435336968", "createdAt": "2020-06-04T15:14:02Z", "author": {"login": "alex-konn"}, "path": "graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java", "diffHunk": "@@ -164,210 +75,47 @@ public ResultMessage get(String messageId, String index) throws DocumentNotFound\n             return Collections.emptyList();\n         }\n \n-        int chunkSize = messageList.size();\n-        int offset = 0;\n-        List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        for (;;) {\n-            try {\n-                List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, isSystemTraffic, offset, chunkSize);\n-                failedItems.addAll(failures);\n-                break; // on success\n-            } catch (EntityTooLargeException e) {\n-                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n-                if (chunkSize == messageList.size()) {\n-                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n-                }\n-                failedItems.addAll(e.failedItems);\n-                offset += e.indexedSuccessfully;\n-                chunkSize /= 2;\n-            }\n-            if (chunkSize == 0) {\n-                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n-            }\n-        }\n-\n-        if (!failedItems.isEmpty()) {\n-            final Set<String> failedIds = failedItems.stream().map(item -> item.id).collect(Collectors.toSet());\n-            recordTimestamp(messageList, failedIds);\n-            return propagateFailure(failedItems, messageList);\n-        } else {\n-            recordTimestamp(messageList, Collections.emptySet());\n-            return Collections.emptyList();\n-        }\n-    }\n-\n-    private List<BulkResult.BulkResultItem> bulkIndexChunked(final List<Map.Entry<IndexSet, Message>> messageList, boolean isSystemTraffic, int offset, int chunkSize) throws EntityTooLargeException {\n-        chunkSize = Math.min(messageList.size(), chunkSize);\n-\n-        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        final Iterable<List<Map.Entry<IndexSet, Message>>> chunks = Iterables.partition(messageList.subList(offset, messageList.size()), chunkSize);\n-        int chunkCount = 1;\n-        int indexedSuccessfully = 0;\n-        for (List<Map.Entry<IndexSet, Message>> chunk : chunks) {\n-\n-            long messageSizes = chunk.stream().mapToLong(m -> m.getValue().getSize()).sum();\n-\n-            final BulkResult result = bulkIndexChunk(chunk);\n-\n-            if (result.getResponseCode() == 413) {\n-                throw new EntityTooLargeException(indexedSuccessfully, failedItems);\n-            }\n+        final List<IndexingRequest> indexingRequestList = messageList.stream()\n+                .map(entry -> IndexingRequest.create(entry.getKey(), entry.getValue()))\n+                .collect(Collectors.toList());\n \n-            // TODO should we check result.isSucceeded()?\n+        final List<IndexFailure> indexFailures = messagesAdapter.bulkIndex(indexingRequestList);\n \n-            indexedSuccessfully += chunk.size();\n+        final Set<String> failedIds = indexFailures.stream().map(IndexFailure::letterId).collect(Collectors.toSet());\n+        final List<IndexingRequest> successfullRequests = indexingRequestList.stream()\n+                .filter(indexingRequest -> !failedIds.contains(indexingRequest.message().getId()))\n+                .collect(Collectors.toList());\n \n-            Set<BulkResult.BulkResultItem> remainingFailures = retryOnlyIndexBlockItemsForever(chunk, result.getFailedItems());\n+        recordTimestamp(successfullRequests);\n+        accountTotalMessageSizes(indexingRequestList, isSystemTraffic);\n \n-            failedItems.addAll(remainingFailures);\n-            if (isSystemTraffic) {\n-                systemTrafficCounter.inc(messageSizes);\n-            } else {\n-                outputByteCounter.inc(messageSizes);\n-            }\n-            if (LOG.isDebugEnabled()) {\n-                String chunkInfo = \"\";\n-                if (chunkSize != messageList.size()) {\n-                    chunkInfo = String.format(Locale.ROOT, \" (chunk %d/%d offset %d)\", chunkCount,\n-                            (int) Math.ceil((double) messageList.size() / chunkSize), offset);\n-                }\n-                LOG.debug(\"Index: Bulk indexed {} messages{}, failures: {}\",\n-                        result.getItems().size(), chunkInfo, failedItems.size());\n-            }\n-            if (!remainingFailures.isEmpty()) {\n-                LOG.error(\"Failed to index [{}] messages. Please check the index error log in your web interface for the reason. Error: {}\",\n-                        remainingFailures.size(), result.getErrorMessage());\n-            }\n-            chunkCount++;\n-        }\n-        return failedItems;\n-    }\n-\n-    private BulkResult bulkIndexChunk(List<Map.Entry<IndexSet, Message>> chunk) {\n-        Bulk.Builder bulk = new Bulk.Builder();\n-\n-        for (Map.Entry<IndexSet, Message> entry : chunk) {\n-            final Message message = entry.getValue();\n-\n-            bulk.addAction(new Index.Builder(message.toElasticSearchObject(invalidTimestampMeter))\n-                    .index(entry.getKey().getWriteIndexAlias())\n-                    .type(IndexMapping.TYPE_MESSAGE)\n-                    .id(message.getId())\n-                    .build());\n-        }\n-\n-        return runBulkRequest(bulk.build(), chunk.size());\n+        return propagateFailure(indexFailures);\n     }\n \n-    private Set<BulkResult.BulkResultItem> retryOnlyIndexBlockItemsForever(List<Map.Entry<IndexSet, Message>> chunk, List<BulkResult.BulkResultItem> allFailedItems) {\n-        Set<BulkResult.BulkResultItem> indexBlocks = indexBlocksFrom(allFailedItems);\n-        final Set<BulkResult.BulkResultItem> otherFailures = new HashSet<>(Sets.difference(new HashSet<>(allFailedItems), indexBlocks));\n-        List<Map.Entry<IndexSet, Message>> blockedMessages = messagesForResultItems(chunk, indexBlocks);\n-\n-        if (!indexBlocks.isEmpty()) {\n-            LOG.warn(\"Retrying {} messages, because their indices are blocked with status [read-only / allow delete]\", indexBlocks.size());\n-        }\n-\n-        long attempt = 1;\n-\n-        while (!indexBlocks.isEmpty()) {\n-            waitBeforeRetrying(attempt++);\n-\n-            final BulkResult bulkResult = bulkIndexChunk(blockedMessages);\n-\n-            final List<BulkResult.BulkResultItem> failedItems = bulkResult.getFailedItems();\n-\n-            indexBlocks = indexBlocksFrom(failedItems);\n-            blockedMessages = messagesForResultItems(blockedMessages, indexBlocks);\n-\n-            final Set<BulkResult.BulkResultItem> newOtherFailures = Sets.difference(new HashSet<>(failedItems), indexBlocks);\n-            otherFailures.addAll(newOtherFailures);\n-\n-            if (indexBlocks.isEmpty()) {\n-                LOG.info(\"Retries were successful after {} attempts. Ingestion will continue now.\", attempt);\n-            }\n-        }\n+    private void accountTotalMessageSizes(List<IndexingRequest> successfullRequests, boolean isSystemTraffic) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "152b4efec5c6581b36771a05fd88e87b792e5b24"}, "originalPosition": 292}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4224, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}