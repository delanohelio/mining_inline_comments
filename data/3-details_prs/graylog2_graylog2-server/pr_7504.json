{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc3MjgzNDE3", "number": 7504, "title": "Add support for high level Kafka consumer mode", "bodyText": "Newer Kafka versions are supported by using the new high level consumer API.\nLucky for us, support for the new API is already included in our currently used Kafka\nclient library. This means we can avoid updating it, and still be\ncompatible with our on-disk journal.\nInstead of creating a new Transport class, use a legacy switch to keep\nsupporting older Kafka versions.\nMost of this was spearheaded by Muralidhar Basani in\n#4770\nThank you very much! :-)\nTo configure the new Kafka client, a list of one or more Kafka brokers needs to be provided.\nThe legacy one needs to connect to Zookeeper.\nIntroduce a custom properties field, so users can tweak advanced settings or setup TLS.\nRefs #5088\nRefs #3960\nFixes #5819\nFixes #5778\nFixes #5001\nFixes #4481\nFixes #3730\nFixes #2780", "createdAt": "2020-02-19T17:11:34Z", "url": "https://github.com/Graylog2/graylog2-server/pull/7504", "merged": true, "mergeCommit": {"oid": "37ec5dbe25c891664e68782d36ec482fbcc15d88"}, "closed": true, "closedAt": "2020-03-11T14:02:16Z", "author": {"login": "mpfz0r"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcGdJNYAH2gAyMzc3MjgzNDE3OjNhM2JhYzIwMmZlZjc2ZWFmNTFmMzdhMDY3YzY4ZWExMmY4YTM2YzA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcMnlNlgFqTM3Mjc4MzI0Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3a3bac202fef76eaf51f37a067c68ea12f8a36c0", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/3a3bac202fef76eaf51f37a067c68ea12f8a36c0", "committedDate": "2020-02-21T10:26:56Z", "message": "Add support for newer Kafka versions\n\nNewer Kafka versions are supported by using the new high level consumer API.\nLucky for us, support for the new API is already included in our currently used Kafka\nclient library. This means we can avoid updating it, and still be\ncompatible with our on-disk journal.\n\nInstead of creating a new Transport class, use a legacy switch to keep\nsupporting older Kafka versions.\n\nMost of this was spearheaded by Muralidhar Basani in\n https://github.com/Graylog2/graylog2-server/pull/4770\nThank you very much! :-)\n\nCo-authored-by: Muralidhar Basani <murali.basani@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15e931acc94ed413661f5abadd8e764db89357a3", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/15e931acc94ed413661f5abadd8e764db89357a3", "committedDate": "2020-02-21T10:26:56Z", "message": "Drop stale comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "153ecabd2623e9313f4460fe78fa11158418e23a", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/153ecabd2623e9313f4460fe78fa11158418e23a", "committedDate": "2020-02-21T10:26:56Z", "message": "Allow ordering of configuration fields\n\nFields in generated configuration forms are not placed in any specific\norder. (Apart from the fact that optional fields are ordered at the end)\n\nWith this change it is possible to specify an explicit position\nattribute on a config field.\nThe default positioning value is 100.\nIf you want your field higher up in the form, choose a value below 100.\nIf you want it at the end, set it higher than 100."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea9249d0d3bb26706b7895f877da2a8e73b7ee86", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/ea9249d0d3bb26706b7895f877da2a8e73b7ee86", "committedDate": "2020-02-21T10:26:56Z", "message": "Fix validation, adjust comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3fead00be15b3f683d2f313ec7683da458b8f1b5", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/3fead00be15b3f683d2f313ec7683da458b8f1b5", "committedDate": "2020-02-21T10:26:56Z", "message": "Add support for custom properties\n\nThis allows users to configure things like TLS."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "committedDate": "2020-02-21T10:26:56Z", "message": "Set system wide default for TLS protocols"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dd026426fe95a0541b9b190745b9bc272b0b0b31", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/dd026426fe95a0541b9b190745b9bc272b0b0b31", "committedDate": "2020-02-19T17:03:35Z", "message": "Fix validation, adjust comments"}, "afterCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "committedDate": "2020-02-21T10:26:56Z", "message": "Set system wide default for TLS protocols"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "85beb41a344e52093bd411f9cc8b236bdee6299d", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/85beb41a344e52093bd411f9cc8b236bdee6299d", "committedDate": "2020-02-25T17:43:20Z", "message": "Revert \"Set system wide default for TLS protocols\"\n\nBetter leave this entirely up to the user\n\nThis reverts commit dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "125bde86aa64d068f90b8b1f8a7b08dc92489b39", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/125bde86aa64d068f90b8b1f8a7b08dc92489b39", "committedDate": "2020-02-25T17:44:08Z", "message": "Avoid logging ConsumerConfig\n\nIt might contain sensitive credentials"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76283dde3c04124e9a39ad5eace7829f7538a7f2", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/76283dde3c04124e9a39ad5eace7829f7538a7f2", "committedDate": "2020-02-25T17:58:23Z", "message": "Adjust comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641", "author": {"user": {"login": "bernd", "name": "Bernd Ahlers"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/a4681b9505cab2f5794cccb60f0c6575edf36641", "committedDate": "2020-02-25T18:08:56Z", "message": "Merge branch 'master' into kafka-consumer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY0MjMzNzQ5", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#pullrequestreview-364233749", "createdAt": "2020-02-25T15:29:30Z", "commit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNToyOTozMFrOFuKlDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxODo1NjoxNVrOFuRJ6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk1MjE0MA==", "bodyText": "Since we are already using Kafka 0.9, we cannot connect to 0.8 brokers anymore. This comment needs to be adjusted.\nWe only need the legacy flag to support existing Kafka inputs.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383952140", "createdAt": "2020-02-25T15:29:30Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -340,13 +473,27 @@ public MetricSet getMetricSet() {\n         public ConfigurationRequest getRequestedConfiguration() {\n             final ConfigurationRequest cr = super.getRequestedConfiguration();\n \n+            cr.addField(new BooleanField(CK_LEGACY,\n+                    \"Legacy mode\",\n+                    true,\n+                    \"Use old ZooKeeper-based consumer API. Needed for Kafka clusters < 0.9\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5MTEzMA==", "bodyText": "Is this comment still valid? And can the commented code be removed?", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383991130", "createdAt": "2020-02-25T16:44:27Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5NTUyNA==", "bodyText": "The javadoc for KafkaConsumer#close() says:\n\nClose the consumer, waiting indefinitely for any needed cleanup.\n\nNewer versions of the API support a close(long timeout, TimeUnit timeUnit).\nCan you please add a TODO to consumer.close() as a reminder to update this call once we use a newer client library?", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383995524", "createdAt": "2020-02-25T16:50:58Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();\n+\n+                            final byte[] bytes = consumerIterator.next().value();\n+\n+                            // it is possible that the message is null\n+                            if (bytes == null) {\n+                                continue;\n+                            }\n+\n+                            totalBytesRead.addAndGet(bytes.length);\n+                            lastSecBytesReadTmp.addAndGet(bytes.length);\n+\n+                            final RawMessage rawMessage = new RawMessage(bytes);\n+\n+                            input.processRawMessage(rawMessage);\n+                        }\n+                    } catch (Exception e) {\n+                        LOG.error(\"Kafka error in consumer thread.\", e);\n+                    }\n+                }\n+                // explicitly commit our offsets when stopping.\n+                // this might trigger a couple of times, but it won't hurt\n+                consumer.commitAsync();\n+                stopLatch.countDown();\n+                consumer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA==", "bodyText": "Is that comment still valid? I checked the code briefly and I cannot see where next() would mark the message as processed.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384058508", "createdAt": "2020-02-25T18:53:42Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1OTg4Mw==", "bodyText": "The consumer.poll() method can throw a few exceptions which we never catch anywhere. This would basically abort the thread without closing any resources. (no consumer.close() call)\nI think we should handle these errors and check what we can do to avoid having a dead input.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384059883", "createdAt": "2020-02-25T18:56:15Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 151}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51665cec51fffbe0e92d7470452852658a202e87", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/51665cec51fffbe0e92d7470452852658a202e87", "committedDate": "2020-02-26T13:36:39Z", "message": "Improve Exception handling and refactor code\n\nAvoid resource leaks by handling more exceptions.\nShutdown the input on unrecoverable exceptions.\nRetry on others.\n\nAdd workaround for hanging poll if kafka is down:\n https://issues.apache.org/jira/browse/KAFKA-4189"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d6cd47444dee3a119a13db2f7d8d60c8be54d3b", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/7d6cd47444dee3a119a13db2f7d8d60c8be54d3b", "committedDate": "2020-02-27T16:32:32Z", "message": "Restart Input after unrecoverable Kafka error\n\nAlso fix thread leak. We need to shutdown the ExecutorService."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "582d538d3dd5d6157a52f08b83ca3d511b697017", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/582d538d3dd5d6157a52f08b83ca3d511b697017", "committedDate": "2020-02-28T11:01:55Z", "message": "Merge remote-tracking branch 'origin/master' into kafka-consumer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwMTY2Mjc3", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#pullrequestreview-370166277", "createdAt": "2020-03-06T08:57:06Z", "commit": {"oid": "7d6cd47444dee3a119a13db2f7d8d60c8be54d3b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwODo1NzowNlrOFyxT5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwODo1NzowNlrOFyxT5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODc4MTAyOQ==", "bodyText": "This is ugly. Maybe we should just not do this at all.\nI might be misinterpreting the KafkaException being not retryable.\nFurthermore, I don't know how to trigger one, so this isn't a \"real\" problem.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r388781029", "createdAt": "2020-03-06T08:57:06Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +197,158 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> executor.submit(new ConsumerRunnable(props, input, i)));\n+    }\n+\n+    private class ConsumerRunnable implements Runnable {\n+        private final Properties props;\n+        private final MessageInput input;\n+        private final KafkaConsumer<byte[], byte[]> consumer;\n+\n+        public ConsumerRunnable(Properties props, MessageInput input, int threadId) {\n+            this.input = input;\n+            final Properties nprops = (Properties) props.clone();\n+            nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + threadId);\n+            this.props = nprops;\n+            consumer = new KafkaConsumer<>(props);\n+            //noinspection ConstantConditions\n+            consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+        }\n+\n+        private void consumeRecords(Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator) {\n+            // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+            // noinspection WhileLoopReplaceableByForEach\n+            while (consumerIterator.hasNext()) {\n+                if (paused) {\n+                    // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                    LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                    Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                }\n+                // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                if (stopped) {\n+                    break;\n+                }\n+                if (isThrottled()) {\n+                    blockUntilUnthrottled();\n+                }\n+\n+                // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                // if we get an exception about processing it down below.\n+                final byte[] bytes = consumerIterator.next().value();\n+\n+                // it is possible that the message is null\n+                if (bytes == null) {\n+                    continue;\n+                }\n+                totalBytesRead.addAndGet(bytes.length);\n+                lastSecBytesReadTmp.addAndGet(bytes.length);\n+\n+                final RawMessage rawMessage = new RawMessage(bytes);\n+                input.processRawMessage(rawMessage);\n+            }\n+        }\n+\n+        private Optional<Iterator<ConsumerRecord<byte[], byte[]>>> tryPoll() {\n+            try {\n+                // Workaround https://issues.apache.org/jira/browse/KAFKA-4189 by calling wakeup()\n+                final ScheduledFuture<?> future = scheduler.schedule(consumer::wakeup, 2000, TimeUnit.MILLISECONDS);\n+                final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                future.cancel(true);\n+\n+                return Optional.of(consumerRecords.iterator());\n+            } catch (WakeupException e) {\n+                LOG.error(\"WakeupException in poll. Kafka server is not responding.\");\n+            } catch (InvalidOffsetException | AuthorizationException e) {\n+                LOG.error(\"Exception in poll.\", e);\n+            }\n+            return Optional.empty();\n+        }\n+\n+        @Override\n+        public void run() {\n+            while (!stopped) {\n+                final Optional<Iterator<ConsumerRecord<byte[], byte[]>>> consumerIterator;\n+                try {\n+                    consumerIterator = tryPoll();\n+                    if (! consumerIterator.isPresent()) {\n+                        LOG.error(\"Caught recoverable exception. Retrying\");\n+                        Thread.sleep(2000);\n+                        continue;\n+                    }\n+                } catch (KafkaException | InterruptedException e) {\n+                    LOG.error(\"Caught unrecoverable exception in poll. Restarting input\", e);\n+                    // (Ab)use serverEventBus to properly restart the entire input.\n+                    serverEventBus.post(InputCreated.create(input.getId()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d6cd47444dee3a119a13db2f7d8d60c8be54d3b"}, "originalPosition": 245}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38de62e01a48a9ec36aacca5ce2f5bbd9cc7b3c9", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/38de62e01a48a9ec36aacca5ce2f5bbd9cc7b3c9", "committedDate": "2020-03-06T09:50:58Z", "message": "Don't restart the Input after a KafkaException\n\nThis reacharound via the eventbus is too ugly\nfor a problem that I can't trigger in practice.\n\nSimply log the exception and stop the transport."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e10a64f3fd5fb0dac3dbae306f35c7fc9e61fbf1", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/e10a64f3fd5fb0dac3dbae306f35c7fc9e61fbf1", "committedDate": "2020-03-06T09:52:45Z", "message": "Merge remote-tracking branch 'origin/master' into kafka-consumer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "279cd81cdfc0c079a0c711ec7283f8eb1154d2fa", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/279cd81cdfc0c079a0c711ec7283f8eb1154d2fa", "committedDate": "2020-03-06T10:00:54Z", "message": "Merge remote-tracking branch 'origin/master' into kafka-consumer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b423109e43bae50235f1e9b6026f9d7b4a14b032", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/b423109e43bae50235f1e9b6026f9d7b4a14b032", "committedDate": "2020-03-09T12:23:03Z", "message": "Stop transport if we can't process the records\n\nIf our processing throws an exception while\nconsuming the records into our journal,\nit's better to stop the transport early so we\ndon't lose too many messages.\n(enable.auto.commit is our default)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/ee609ddf8cdb97e052b4e39186c708c4c6d6d280", "committedDate": "2020-03-09T12:53:20Z", "message": "Simplify consumeRecords()\n\nWith the old kafka client, every next() call would mark the offset\nas commited. With the new client, this isn't the case anymore,\nthus we can simplify the code."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxMjMwMjAx", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#pullrequestreview-371230201", "createdAt": "2020-03-09T14:28:36Z", "commit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDoyODozNlrOFzq8ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDoyOTozMFrOFzq_AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTMwNw==", "bodyText": "I think we should return DEFAULT_POSITION here instead.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389725307", "createdAt": "2020-03-09T14:28:36Z", "author": {"login": "dennisoelkers"}, "path": "graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java", "diffHunk": "@@ -42,4 +42,8 @@\n     List<String> getAttributes();\n \n     Map<String, Map<String, String>> getAdditionalInformation();\n+\n+    default int getPosition() {\n+        return 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTk1Mg==", "bodyText": "Instead of a magic number we should use a constant introduced in ConfigurationField here, maybe something like BELOW_DEFAULT?", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389725952", "createdAt": "2020-03-09T14:29:30Z", "author": {"login": "dennisoelkers"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -389,8 +575,18 @@ public ConfigurationRequest getRequestedConfiguration() {\n                     DEFAULT_GROUP_ID,\n                     \"Name of the consumer group the Kafka input belongs to\",\n                     ConfigurationField.Optional.OPTIONAL));\n+            cr.addField(new TextField(\n+                    CK_CUSTOM_PROPERTIES,\n+                    \"Custom Kafka properties\",\n+                    \"\",\n+                    \"A newline separated list of Kafka properties. (e.g.: \\\"ssl.keystore.location=/etc/graylog/server/kafka.keystore.jks\\\").\",\n+                    ConfigurationField.Optional.OPTIONAL,\n+                    110,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "originalPosition": 373}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "298a5930989fa6a987f816ff49ed89f9cd1f5782", "author": {"user": {"login": "mpfz0r", "name": "Marco Pfatschbacher"}}, "url": "https://github.com/Graylog2/graylog2-server/commit/298a5930989fa6a987f816ff49ed89f9cd1f5782", "committedDate": "2020-03-09T15:12:47Z", "message": "Improve position defaults"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzc5Nzc2", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#pullrequestreview-372779776", "createdAt": "2020-03-11T13:56:29Z", "commit": {"oid": "298a5930989fa6a987f816ff49ed89f9cd1f5782"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzgzMjQz", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#pullrequestreview-372783243", "createdAt": "2020-03-11T14:00:07Z", "commit": {"oid": "298a5930989fa6a987f816ff49ed89f9cd1f5782"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2637, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}