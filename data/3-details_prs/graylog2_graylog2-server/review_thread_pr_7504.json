{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc3MjgzNDE3", "number": 7504, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNToyOTozMFrODivxpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDoyOTozMFrODmUjwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3NzYwOTM1OnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNToyOTozMFrOFuKlDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNToyOTozMFrOFuKlDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk1MjE0MA==", "bodyText": "Since we are already using Kafka 0.9, we cannot connect to 0.8 brokers anymore. This comment needs to be adjusted.\nWe only need the legacy flag to support existing Kafka inputs.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383952140", "createdAt": "2020-02-25T15:29:30Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -340,13 +473,27 @@ public MetricSet getMetricSet() {\n         public ConfigurationRequest getRequestedConfiguration() {\n             final ConfigurationRequest cr = super.getRequestedConfiguration();\n \n+            cr.addField(new BooleanField(CK_LEGACY,\n+                    \"Legacy mode\",\n+                    true,\n+                    \"Use old ZooKeeper-based consumer API. Needed for Kafka clusters < 0.9\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 280}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3Nzg1MTUxOnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNjo0NDoyN1rOFuM9Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQwODo0NToyN1rOFuidnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5MTEzMA==", "bodyText": "Is this comment still valid? And can the commented code be removed?", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383991130", "createdAt": "2020-02-25T16:44:27Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM0MzQ1Mg==", "bodyText": "the first one, i think yes. but the code is a leftover, deleting it", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384343452", "createdAt": "2020-02-26T08:45:27Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5MTEzMA=="}, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3Nzg3ODk1OnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNjo1MDo1OFrOFuNOhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxNjo1MDo1OFrOFuNOhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5NTUyNA==", "bodyText": "The javadoc for KafkaConsumer#close() says:\n\nClose the consumer, waiting indefinitely for any needed cleanup.\n\nNewer versions of the API support a close(long timeout, TimeUnit timeUnit).\nCan you please add a TODO to consumer.close() as a reminder to update this call once we use a newer client library?", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383995524", "createdAt": "2020-02-25T16:50:58Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();\n+\n+                            final byte[] bytes = consumerIterator.next().value();\n+\n+                            // it is possible that the message is null\n+                            if (bytes == null) {\n+                                continue;\n+                            }\n+\n+                            totalBytesRead.addAndGet(bytes.length);\n+                            lastSecBytesReadTmp.addAndGet(bytes.length);\n+\n+                            final RawMessage rawMessage = new RawMessage(bytes);\n+\n+                            input.processRawMessage(rawMessage);\n+                        }\n+                    } catch (Exception e) {\n+                        LOG.error(\"Kafka error in consumer thread.\", e);\n+                    }\n+                }\n+                // explicitly commit our offsets when stopping.\n+                // this might trigger a couple of times, but it won't hurt\n+                consumer.commitAsync();\n+                stopLatch.countDown();\n+                consumer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3ODI4ODc0OnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxODo1Mzo0MlrOFuREjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxMjo1NjoxMFrOFzl3KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA==", "bodyText": "Is that comment still valid? I checked the code briefly and I cannot see where next() would mark the message as processed.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384058508", "createdAt": "2020-02-25T18:53:42Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM0ODE2Mg==", "bodyText": "I guess that depends. The default is auto.commit.enable = true. So I'd say it does.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384348162", "createdAt": "2020-02-26T08:54:55Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA=="}, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY0MjAyNQ==", "bodyText": "Nevermind. You're right :)", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389642025", "createdAt": "2020-03-09T12:56:10Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA=="}, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3ODI5NzM3OnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQxODo1NjoxNVrOFuRJ6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxMzo0NTozMVrOFusBDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1OTg4Mw==", "bodyText": "The consumer.poll() method can throw a few exceptions which we never catch anywhere. This would basically abort the thread without closing any resources. (no consumer.close() call)\nI think we should handle these errors and check what we can do to avoid having a dead input.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384059883", "createdAt": "2020-02-25T18:56:15Z", "author": {"login": "bernd"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQ5OTk4MQ==", "bodyText": "good catch. I found a couple of issues while fixing this.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384499981", "createdAt": "2020-02-26T13:45:31Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1OTg4Mw=="}, "originalCommit": {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwODgwNTQ4OnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwODo1NzowNlrOFyxT5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwODo1NzowNlrOFyxT5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODc4MTAyOQ==", "bodyText": "This is ugly. Maybe we should just not do this at all.\nI might be misinterpreting the KafkaException being not retryable.\nFurthermore, I don't know how to trigger one, so this isn't a \"real\" problem.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r388781029", "createdAt": "2020-03-06T08:57:06Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +197,158 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> executor.submit(new ConsumerRunnable(props, input, i)));\n+    }\n+\n+    private class ConsumerRunnable implements Runnable {\n+        private final Properties props;\n+        private final MessageInput input;\n+        private final KafkaConsumer<byte[], byte[]> consumer;\n+\n+        public ConsumerRunnable(Properties props, MessageInput input, int threadId) {\n+            this.input = input;\n+            final Properties nprops = (Properties) props.clone();\n+            nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + threadId);\n+            this.props = nprops;\n+            consumer = new KafkaConsumer<>(props);\n+            //noinspection ConstantConditions\n+            consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+        }\n+\n+        private void consumeRecords(Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator) {\n+            // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+            // noinspection WhileLoopReplaceableByForEach\n+            while (consumerIterator.hasNext()) {\n+                if (paused) {\n+                    // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                    LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                    Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                }\n+                // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                if (stopped) {\n+                    break;\n+                }\n+                if (isThrottled()) {\n+                    blockUntilUnthrottled();\n+                }\n+\n+                // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                // if we get an exception about processing it down below.\n+                final byte[] bytes = consumerIterator.next().value();\n+\n+                // it is possible that the message is null\n+                if (bytes == null) {\n+                    continue;\n+                }\n+                totalBytesRead.addAndGet(bytes.length);\n+                lastSecBytesReadTmp.addAndGet(bytes.length);\n+\n+                final RawMessage rawMessage = new RawMessage(bytes);\n+                input.processRawMessage(rawMessage);\n+            }\n+        }\n+\n+        private Optional<Iterator<ConsumerRecord<byte[], byte[]>>> tryPoll() {\n+            try {\n+                // Workaround https://issues.apache.org/jira/browse/KAFKA-4189 by calling wakeup()\n+                final ScheduledFuture<?> future = scheduler.schedule(consumer::wakeup, 2000, TimeUnit.MILLISECONDS);\n+                final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                future.cancel(true);\n+\n+                return Optional.of(consumerRecords.iterator());\n+            } catch (WakeupException e) {\n+                LOG.error(\"WakeupException in poll. Kafka server is not responding.\");\n+            } catch (InvalidOffsetException | AuthorizationException e) {\n+                LOG.error(\"Exception in poll.\", e);\n+            }\n+            return Optional.empty();\n+        }\n+\n+        @Override\n+        public void run() {\n+            while (!stopped) {\n+                final Optional<Iterator<ConsumerRecord<byte[], byte[]>>> consumerIterator;\n+                try {\n+                    consumerIterator = tryPoll();\n+                    if (! consumerIterator.isPresent()) {\n+                        LOG.error(\"Caught recoverable exception. Retrying\");\n+                        Thread.sleep(2000);\n+                        continue;\n+                    }\n+                } catch (KafkaException | InterruptedException e) {\n+                    LOG.error(\"Caught unrecoverable exception in poll. Restarting input\", e);\n+                    // (Ab)use serverEventBus to properly restart the entire input.\n+                    serverEventBus.post(InputCreated.create(input.getId()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d6cd47444dee3a119a13db2f7d8d60c8be54d3b"}, "originalPosition": 245}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTA4OTEwOnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDoyODozNlrOFzq8ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNTowOToxOVrOFzsqaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTMwNw==", "bodyText": "I think we should return DEFAULT_POSITION here instead.", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389725307", "createdAt": "2020-03-09T14:28:36Z", "author": {"login": "dennisoelkers"}, "path": "graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java", "diffHunk": "@@ -42,4 +42,8 @@\n     List<String> getAttributes();\n \n     Map<String, Map<String, String>> getAdditionalInformation();\n+\n+    default int getPosition() {\n+        return 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1MzQ0OA==", "bodyText": "Thanks! that was a sloppy oversight :/", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389753448", "createdAt": "2020-03-09T15:09:19Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java", "diffHunk": "@@ -42,4 +42,8 @@\n     List<String> getAttributes();\n \n     Map<String, Map<String, String>> getAdditionalInformation();\n+\n+    default int getPosition() {\n+        return 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTMwNw=="}, "originalCommit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNTA5MzEzOnYy", "diffSide": "RIGHT", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNDoyOTozMFrOFzq_AA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNTowOToyOVrOFzsqyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTk1Mg==", "bodyText": "Instead of a magic number we should use a constant introduced in ConfigurationField here, maybe something like BELOW_DEFAULT?", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389725952", "createdAt": "2020-03-09T14:29:30Z", "author": {"login": "dennisoelkers"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -389,8 +575,18 @@ public ConfigurationRequest getRequestedConfiguration() {\n                     DEFAULT_GROUP_ID,\n                     \"Name of the consumer group the Kafka input belongs to\",\n                     ConfigurationField.Optional.OPTIONAL));\n+            cr.addField(new TextField(\n+                    CK_CUSTOM_PROPERTIES,\n+                    \"Custom Kafka properties\",\n+                    \"\",\n+                    \"A newline separated list of Kafka properties. (e.g.: \\\"ssl.keystore.location=/etc/graylog/server/kafka.keystore.jks\\\").\",\n+                    ConfigurationField.Optional.OPTIONAL,\n+                    110,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "originalPosition": 373}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1MzU0Nw==", "bodyText": "ack!", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389753547", "createdAt": "2020-03-09T15:09:29Z", "author": {"login": "mpfz0r"}, "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -389,8 +575,18 @@ public ConfigurationRequest getRequestedConfiguration() {\n                     DEFAULT_GROUP_ID,\n                     \"Name of the consumer group the Kafka input belongs to\",\n                     ConfigurationField.Optional.OPTIONAL));\n+            cr.addField(new TextField(\n+                    CK_CUSTOM_PROPERTIES,\n+                    \"Custom Kafka properties\",\n+                    \"\",\n+                    \"A newline separated list of Kafka properties. (e.g.: \\\"ssl.keystore.location=/etc/graylog/server/kafka.keystore.jks\\\").\",\n+                    ConfigurationField.Optional.OPTIONAL,\n+                    110,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTk1Mg=="}, "originalCommit": {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280"}, "originalPosition": 373}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3872, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}