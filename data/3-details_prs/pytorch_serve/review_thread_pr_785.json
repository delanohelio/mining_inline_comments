{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIyMzY1OTc0", "number": 785, "reviewThreads": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0MzowNVrOFYnIAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yOVQxODoxODozMFrOFrv-nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzUxMTY4OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0MzowNVrOIitMjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0MzowNVrOIitMjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2Mjk4OQ==", "bodyText": "Large method. Can be split in to small pieces.\n\nParsing manifest file.\nCreating / validating DAG", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573262989", "createdAt": "2021-02-09T21:43:05Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "diffHunk": "@@ -0,0 +1,204 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.Reader;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.pytorch.serve.archive.workflow.InvalidWorkflowException;\n+import org.pytorch.serve.archive.workflow.WorkflowArchive;\n+import org.yaml.snakeyaml.Yaml;\n+import org.yaml.snakeyaml.error.YAMLException;\n+\n+public class WorkFlow {\n+\n+    private Map<String, Object> workflowSpec;\n+\n+    private WorkflowArchive workflowArchive;\n+    private int minWorkers = 1;\n+    private int maxWorkers = 1;\n+    private int batchSize = 1;\n+    private int maxBatchDelay = 50;\n+\n+    private Dag dag = new Dag();\n+\n+    public WorkFlow(WorkflowArchive workflowArchive)\n+            throws IOException, InvalidDAGException, InvalidWorkflowException {\n+        this.workflowArchive = workflowArchive;\n+        File specFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getSpecFile());\n+        File handlerFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getHandler());\n+\n+        String workFlowName = this.workflowArchive.getWorkflowName();\n+        Map<String, WorkflowModel> models = new HashMap<String, WorkflowModel>();\n+\n+        @SuppressWarnings(\"unchecked\")\n+        LinkedHashMap<String, Object> spec =\n+                (LinkedHashMap<String, Object>) this.readSpecFile(specFile);\n+        this.workflowSpec = spec;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> modelsInfo = (Map<String, Object>) this.workflowSpec.get(\"models\");\n+        for (Map.Entry<String, Object> entry : modelsInfo.entrySet()) {\n+            String keyName = entry.getKey();\n+\n+            switch (keyName) {\n+                case \"min-workers\":\n+                    minWorkers = (int) entry.getValue();\n+                    break;\n+                case \"max-workers\":\n+                    maxWorkers = (int) entry.getValue();\n+                    break;\n+                case \"batch-size\":\n+                    batchSize = (int) entry.getValue();\n+                    break;\n+                case \"max-batch-delay\":\n+                    maxBatchDelay = (int) entry.getValue();\n+                    break;\n+                default:\n+                    // entry.getValue().getClass() check object type.\n+                    // assuming Map containing model info\n+                    @SuppressWarnings(\"unchecked\")\n+                    LinkedHashMap<String, Object> model =\n+                            (LinkedHashMap<String, Object>) entry.getValue();\n+                    String modelName = workFlowName + \"__\" + keyName;\n+\n+                    WorkflowModel wfm =\n+                            new WorkflowModel(\n+                                    modelName,\n+                                    (String) model.get(\"url\"),\n+                                    (int) model.getOrDefault(\"min-workers\", minWorkers),\n+                                    (int) model.getOrDefault(\"max-workers\", maxWorkers),\n+                                    (int) model.getOrDefault(\"batch-size\", batchSize),\n+                                    (int) model.getOrDefault(\"max-batch-delay\", maxBatchDelay),\n+                                    null);\n+\n+                    models.put(modelName, wfm);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> dagInfo = (Map<String, Object>) this.workflowSpec.get(\"dag\");\n+\n+        for (Map.Entry<String, Object> entry : dagInfo.entrySet()) {\n+            String nodeName = entry.getKey();\n+            String modelName = workFlowName + \"__\" + nodeName;\n+            WorkflowModel wfm;\n+            if (!models.containsKey(modelName)) {\n+                wfm =\n+                        new WorkflowModel(\n+                                modelName,\n+                                null,\n+                                1,\n+                                1,\n+                                1,\n+                                0,\n+                                handlerFile.getPath() + \":\" + nodeName);\n+            } else {\n+                wfm = models.get(modelName);\n+            }\n+            Node fromNode = new Node(nodeName, wfm);\n+            dag.addNode(fromNode);\n+\n+            @SuppressWarnings(\"unchecked\")\n+            ArrayList<String> values = (ArrayList<String>) entry.getValue();\n+            for (String toNodeName : values) {\n+\n+                if (toNodeName == null || (\"\").equals(toNodeName.strip())) {\n+                    continue;\n+                }\n+                String toModelName = workFlowName + \"__\" + toNodeName;\n+                WorkflowModel toWfm;\n+                if (!models.containsKey(toModelName)) {\n+                    toWfm =\n+                            new WorkflowModel(\n+                                    toModelName,\n+                                    null,\n+                                    1,\n+                                    1,\n+                                    1,\n+                                    0,\n+                                    handlerFile.getPath() + \":\" + toNodeName);\n+                } else {\n+                    toWfm = models.get(toModelName);\n+                }\n+                Node toNode = new Node(toNodeName, toWfm);\n+                dag.addNode(toNode);\n+                dag.addEdge(fromNode, toNode);\n+            }\n+        }\n+        dag.validate();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzUxNDk3OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0NDowMVrOIitOjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0NzoyNVrOIitWyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2MzUwMg==", "bodyText": "min-workers / max-workers / others are Java constants", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573263502", "createdAt": "2021-02-09T21:44:01Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "diffHunk": "@@ -0,0 +1,204 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.Reader;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.pytorch.serve.archive.workflow.InvalidWorkflowException;\n+import org.pytorch.serve.archive.workflow.WorkflowArchive;\n+import org.yaml.snakeyaml.Yaml;\n+import org.yaml.snakeyaml.error.YAMLException;\n+\n+public class WorkFlow {\n+\n+    private Map<String, Object> workflowSpec;\n+\n+    private WorkflowArchive workflowArchive;\n+    private int minWorkers = 1;\n+    private int maxWorkers = 1;\n+    private int batchSize = 1;\n+    private int maxBatchDelay = 50;\n+\n+    private Dag dag = new Dag();\n+\n+    public WorkFlow(WorkflowArchive workflowArchive)\n+            throws IOException, InvalidDAGException, InvalidWorkflowException {\n+        this.workflowArchive = workflowArchive;\n+        File specFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getSpecFile());\n+        File handlerFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getHandler());\n+\n+        String workFlowName = this.workflowArchive.getWorkflowName();\n+        Map<String, WorkflowModel> models = new HashMap<String, WorkflowModel>();\n+\n+        @SuppressWarnings(\"unchecked\")\n+        LinkedHashMap<String, Object> spec =\n+                (LinkedHashMap<String, Object>) this.readSpecFile(specFile);\n+        this.workflowSpec = spec;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> modelsInfo = (Map<String, Object>) this.workflowSpec.get(\"models\");\n+        for (Map.Entry<String, Object> entry : modelsInfo.entrySet()) {\n+            String keyName = entry.getKey();\n+\n+            switch (keyName) {\n+                case \"min-workers\":\n+                    minWorkers = (int) entry.getValue();\n+                    break;\n+                case \"max-workers\":\n+                    maxWorkers = (int) entry.getValue();\n+                    break;\n+                case \"batch-size\":\n+                    batchSize = (int) entry.getValue();\n+                    break;\n+                case \"max-batch-delay\":\n+                    maxBatchDelay = (int) entry.getValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2NTYwOA==", "bodyText": "Also SnakeYML will convert a Java YML file to a object. Much better to use than parsing manually. https://bitbucket.org/asomov/snakeyaml/wiki/Documentation", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573265608", "createdAt": "2021-02-09T21:47:25Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "diffHunk": "@@ -0,0 +1,204 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.Reader;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.pytorch.serve.archive.workflow.InvalidWorkflowException;\n+import org.pytorch.serve.archive.workflow.WorkflowArchive;\n+import org.yaml.snakeyaml.Yaml;\n+import org.yaml.snakeyaml.error.YAMLException;\n+\n+public class WorkFlow {\n+\n+    private Map<String, Object> workflowSpec;\n+\n+    private WorkflowArchive workflowArchive;\n+    private int minWorkers = 1;\n+    private int maxWorkers = 1;\n+    private int batchSize = 1;\n+    private int maxBatchDelay = 50;\n+\n+    private Dag dag = new Dag();\n+\n+    public WorkFlow(WorkflowArchive workflowArchive)\n+            throws IOException, InvalidDAGException, InvalidWorkflowException {\n+        this.workflowArchive = workflowArchive;\n+        File specFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getSpecFile());\n+        File handlerFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getHandler());\n+\n+        String workFlowName = this.workflowArchive.getWorkflowName();\n+        Map<String, WorkflowModel> models = new HashMap<String, WorkflowModel>();\n+\n+        @SuppressWarnings(\"unchecked\")\n+        LinkedHashMap<String, Object> spec =\n+                (LinkedHashMap<String, Object>) this.readSpecFile(specFile);\n+        this.workflowSpec = spec;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> modelsInfo = (Map<String, Object>) this.workflowSpec.get(\"models\");\n+        for (Map.Entry<String, Object> entry : modelsInfo.entrySet()) {\n+            String keyName = entry.getKey();\n+\n+            switch (keyName) {\n+                case \"min-workers\":\n+                    minWorkers = (int) entry.getValue();\n+                    break;\n+                case \"max-workers\":\n+                    maxWorkers = (int) entry.getValue();\n+                    break;\n+                case \"batch-size\":\n+                    batchSize = (int) entry.getValue();\n+                    break;\n+                case \"max-batch-delay\":\n+                    maxBatchDelay = (int) entry.getValue();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2MzUwMg=="}, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzUxNjIyOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0NDoxOVrOIitPTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0NDoxOVrOIitPTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2MzY5Mg==", "bodyText": "final", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573263692", "createdAt": "2021-02-09T21:44:19Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "diffHunk": "@@ -0,0 +1,204 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.Reader;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.pytorch.serve.archive.workflow.InvalidWorkflowException;\n+import org.pytorch.serve.archive.workflow.WorkflowArchive;\n+import org.yaml.snakeyaml.Yaml;\n+import org.yaml.snakeyaml.error.YAMLException;\n+\n+public class WorkFlow {\n+\n+    private Map<String, Object> workflowSpec;\n+\n+    private WorkflowArchive workflowArchive;\n+    private int minWorkers = 1;\n+    private int maxWorkers = 1;\n+    private int batchSize = 1;\n+    private int maxBatchDelay = 50;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzUxOTY4OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0NToyMVrOIitRiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0NToyMVrOIitRiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2NDI2NQ==", "bodyText": "Lombok ?\nThis is refactoring opportunity for larger code base.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573264265", "createdAt": "2021-02-09T21:45:21Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "diffHunk": "@@ -0,0 +1,204 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.Reader;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.pytorch.serve.archive.workflow.InvalidWorkflowException;\n+import org.pytorch.serve.archive.workflow.WorkflowArchive;\n+import org.yaml.snakeyaml.Yaml;\n+import org.yaml.snakeyaml.error.YAMLException;\n+\n+public class WorkFlow {\n+\n+    private Map<String, Object> workflowSpec;\n+\n+    private WorkflowArchive workflowArchive;\n+    private int minWorkers = 1;\n+    private int maxWorkers = 1;\n+    private int batchSize = 1;\n+    private int maxBatchDelay = 50;\n+\n+    private Dag dag = new Dag();\n+\n+    public WorkFlow(WorkflowArchive workflowArchive)\n+            throws IOException, InvalidDAGException, InvalidWorkflowException {\n+        this.workflowArchive = workflowArchive;\n+        File specFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getSpecFile());\n+        File handlerFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getHandler());\n+\n+        String workFlowName = this.workflowArchive.getWorkflowName();\n+        Map<String, WorkflowModel> models = new HashMap<String, WorkflowModel>();\n+\n+        @SuppressWarnings(\"unchecked\")\n+        LinkedHashMap<String, Object> spec =\n+                (LinkedHashMap<String, Object>) this.readSpecFile(specFile);\n+        this.workflowSpec = spec;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> modelsInfo = (Map<String, Object>) this.workflowSpec.get(\"models\");\n+        for (Map.Entry<String, Object> entry : modelsInfo.entrySet()) {\n+            String keyName = entry.getKey();\n+\n+            switch (keyName) {\n+                case \"min-workers\":\n+                    minWorkers = (int) entry.getValue();\n+                    break;\n+                case \"max-workers\":\n+                    maxWorkers = (int) entry.getValue();\n+                    break;\n+                case \"batch-size\":\n+                    batchSize = (int) entry.getValue();\n+                    break;\n+                case \"max-batch-delay\":\n+                    maxBatchDelay = (int) entry.getValue();\n+                    break;\n+                default:\n+                    // entry.getValue().getClass() check object type.\n+                    // assuming Map containing model info\n+                    @SuppressWarnings(\"unchecked\")\n+                    LinkedHashMap<String, Object> model =\n+                            (LinkedHashMap<String, Object>) entry.getValue();\n+                    String modelName = workFlowName + \"__\" + keyName;\n+\n+                    WorkflowModel wfm =\n+                            new WorkflowModel(\n+                                    modelName,\n+                                    (String) model.get(\"url\"),\n+                                    (int) model.getOrDefault(\"min-workers\", minWorkers),\n+                                    (int) model.getOrDefault(\"max-workers\", maxWorkers),\n+                                    (int) model.getOrDefault(\"batch-size\", batchSize),\n+                                    (int) model.getOrDefault(\"max-batch-delay\", maxBatchDelay),\n+                                    null);\n+\n+                    models.put(modelName, wfm);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> dagInfo = (Map<String, Object>) this.workflowSpec.get(\"dag\");\n+\n+        for (Map.Entry<String, Object> entry : dagInfo.entrySet()) {\n+            String nodeName = entry.getKey();\n+            String modelName = workFlowName + \"__\" + nodeName;\n+            WorkflowModel wfm;\n+            if (!models.containsKey(modelName)) {\n+                wfm =\n+                        new WorkflowModel(\n+                                modelName,\n+                                null,\n+                                1,\n+                                1,\n+                                1,\n+                                0,\n+                                handlerFile.getPath() + \":\" + nodeName);\n+            } else {\n+                wfm = models.get(modelName);\n+            }\n+            Node fromNode = new Node(nodeName, wfm);\n+            dag.addNode(fromNode);\n+\n+            @SuppressWarnings(\"unchecked\")\n+            ArrayList<String> values = (ArrayList<String>) entry.getValue();\n+            for (String toNodeName : values) {\n+\n+                if (toNodeName == null || (\"\").equals(toNodeName.strip())) {\n+                    continue;\n+                }\n+                String toModelName = workFlowName + \"__\" + toNodeName;\n+                WorkflowModel toWfm;\n+                if (!models.containsKey(toModelName)) {\n+                    toWfm =\n+                            new WorkflowModel(\n+                                    toModelName,\n+                                    null,\n+                                    1,\n+                                    1,\n+                                    1,\n+                                    0,\n+                                    handlerFile.getPath() + \":\" + toNodeName);\n+                } else {\n+                    toWfm = models.get(toModelName);\n+                }\n+                Node toNode = new Node(toNodeName, toWfm);\n+                dag.addNode(toNode);\n+                dag.addEdge(fromNode, toNode);\n+            }\n+        }\n+        dag.validate();\n+    }\n+\n+    private static Map<String, Object> readSpecFile(File file)\n+            throws IOException, InvalidWorkflowException {\n+        Yaml yaml = new Yaml();\n+        try (Reader r =\n+                new InputStreamReader(\n+                        Files.newInputStream(file.toPath()), StandardCharsets.UTF_8)) {\n+            @SuppressWarnings(\"unchecked\")\n+            Map<String, Object> loadedYaml = (Map<String, Object>) yaml.load(r);\n+            return loadedYaml;\n+        } catch (YAMLException e) {\n+            throw new InvalidWorkflowException(\"Failed to parse yaml.\", e);\n+        }\n+    }\n+\n+    public Object getWorkflowSpec() {\n+        return workflowSpec;\n+    }\n+\n+    public Dag getDag() {\n+        return this.dag;\n+    }\n+\n+    public WorkflowArchive getWorkflowArchive() {\n+        return workflowArchive;\n+    }\n+\n+    public int getMinWorkers() {\n+        return minWorkers;\n+    }\n+\n+    public void setMinWorkers(int minWorkers) {\n+        this.minWorkers = minWorkers;\n+    }\n+\n+    public int getMaxWorkers() {\n+        return maxWorkers;\n+    }\n+\n+    public void setMaxWorkers(int maxWorkers) {\n+        this.maxWorkers = maxWorkers;\n+    }\n+\n+    public int getBatchSize() {\n+        return batchSize;\n+    }\n+\n+    public void setBatchSize(int batchSize) {\n+        this.batchSize = batchSize;\n+    }\n+\n+    public int getMaxBatchDelay() {\n+        return maxBatchDelay;\n+    }\n+\n+    public void setMaxBatchDelay(int maxBatchDelay) {\n+        this.maxBatchDelay = maxBatchDelay;\n+    }\n+\n+    public String getWorkflowDag() {\n+        return this.workflowSpec.get(\"dag\").toString();\n+    }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzUzMTQ5OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0ODowN1rOIitYlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMTo0ODowN1rOIitYlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI2NjA2OA==", "bodyText": "String builder", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573266068", "createdAt": "2021-02-09T21:48:07Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java", "diffHunk": "@@ -0,0 +1,204 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.Reader;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.pytorch.serve.archive.workflow.InvalidWorkflowException;\n+import org.pytorch.serve.archive.workflow.WorkflowArchive;\n+import org.yaml.snakeyaml.Yaml;\n+import org.yaml.snakeyaml.error.YAMLException;\n+\n+public class WorkFlow {\n+\n+    private Map<String, Object> workflowSpec;\n+\n+    private WorkflowArchive workflowArchive;\n+    private int minWorkers = 1;\n+    private int maxWorkers = 1;\n+    private int batchSize = 1;\n+    private int maxBatchDelay = 50;\n+\n+    private Dag dag = new Dag();\n+\n+    public WorkFlow(WorkflowArchive workflowArchive)\n+            throws IOException, InvalidDAGException, InvalidWorkflowException {\n+        this.workflowArchive = workflowArchive;\n+        File specFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getSpecFile());\n+        File handlerFile =\n+                new File(\n+                        this.workflowArchive.getWorkflowDir(),\n+                        this.workflowArchive.getManifest().getWorkflow().getHandler());\n+\n+        String workFlowName = this.workflowArchive.getWorkflowName();\n+        Map<String, WorkflowModel> models = new HashMap<String, WorkflowModel>();\n+\n+        @SuppressWarnings(\"unchecked\")\n+        LinkedHashMap<String, Object> spec =\n+                (LinkedHashMap<String, Object>) this.readSpecFile(specFile);\n+        this.workflowSpec = spec;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> modelsInfo = (Map<String, Object>) this.workflowSpec.get(\"models\");\n+        for (Map.Entry<String, Object> entry : modelsInfo.entrySet()) {\n+            String keyName = entry.getKey();\n+\n+            switch (keyName) {\n+                case \"min-workers\":\n+                    minWorkers = (int) entry.getValue();\n+                    break;\n+                case \"max-workers\":\n+                    maxWorkers = (int) entry.getValue();\n+                    break;\n+                case \"batch-size\":\n+                    batchSize = (int) entry.getValue();\n+                    break;\n+                case \"max-batch-delay\":\n+                    maxBatchDelay = (int) entry.getValue();\n+                    break;\n+                default:\n+                    // entry.getValue().getClass() check object type.\n+                    // assuming Map containing model info\n+                    @SuppressWarnings(\"unchecked\")\n+                    LinkedHashMap<String, Object> model =\n+                            (LinkedHashMap<String, Object>) entry.getValue();\n+                    String modelName = workFlowName + \"__\" + keyName;\n+\n+                    WorkflowModel wfm =\n+                            new WorkflowModel(\n+                                    modelName,\n+                                    (String) model.get(\"url\"),\n+                                    (int) model.getOrDefault(\"min-workers\", minWorkers),\n+                                    (int) model.getOrDefault(\"max-workers\", maxWorkers),\n+                                    (int) model.getOrDefault(\"batch-size\", batchSize),\n+                                    (int) model.getOrDefault(\"max-batch-delay\", maxBatchDelay),\n+                                    null);\n+\n+                    models.put(modelName, wfm);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Map<String, Object> dagInfo = (Map<String, Object>) this.workflowSpec.get(\"dag\");\n+\n+        for (Map.Entry<String, Object> entry : dagInfo.entrySet()) {\n+            String nodeName = entry.getKey();\n+            String modelName = workFlowName + \"__\" + nodeName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc0Njg3OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/Node.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0MDozMlrOIivbfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0MDozMlrOIivbfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI5OTU4MA==", "bodyText": "final / Lombok", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573299580", "createdAt": "2021-02-09T22:40:32Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/Node.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package org.pytorch.serve.ensemble;\n+\n+public class Node {\n+\n+    private String name;\n+    private String parentName;\n+    private WorkflowModel workflowModel;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc0OTE3OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0MToyMFrOIivc9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0MToyMFrOIivc9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzI5OTk1Ng==", "bodyText": "Refactor to use Guava Graph lib - https://guava.dev/releases/23.0/api/docs/com/google/common/graph/Graph.html", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573299956", "createdAt": "2021-02-09T22:41:20Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java", "diffHunk": "@@ -0,0 +1,112 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/** Direct acyclic graph for ensemble */\n+public class Dag {\n+\n+    private Map<String, Node> nodes = new HashMap<>();\n+\n+    private Map<String, Map<String, Set<String>>> dagMap = new HashMap<>();\n+\n+    public void addNode(Node node) {\n+        if (!checkNodeExist(node)) {\n+            nodes.put(node.getName(), node);\n+            Map<String, Set<String>> degreeMap = new HashMap<>();\n+            degreeMap.put(\"inDegree\", new HashSet<String>());\n+            degreeMap.put(\"outDegree\", new HashSet<String>());\n+            dagMap.put(node.getName(), degreeMap);\n+        }\n+    }\n+\n+    public boolean checkNodeExist(Node node) {\n+        return nodes.containsKey(node.getName());\n+    }\n+\n+    public boolean hasEdgeTo(Node from, Node to) {\n+        return dagMap.get(from.getName()).get(\"inDegree\").contains(to.getName());\n+    }\n+\n+    public void addEdge(Node from, Node to) throws InvalidDAGException {\n+        if (!checkNodeExist(from)) {\n+            addNode(from);\n+        }\n+        if (!checkNodeExist(to)) {\n+            addNode(to);\n+        }\n+\n+        if (from.getName().equals(to.getName())) {\n+            throw new InvalidDAGException(\"Self loop exception\");\n+        }\n+\n+        if (hasEdgeTo(to, from)) {\n+            throw new InvalidDAGException(\"loop exception\");\n+        }\n+\n+        dagMap.get(from.getName()).get(\"outDegree\").add(to.getName());\n+        dagMap.get(to.getName()).get(\"inDegree\").add(from.getName());\n+    }\n+\n+    public Set<String> getEndNodeNames(String degree) {\n+        Set<String> startNodes = new HashSet<>();\n+        for (Map.Entry<String, Map<String, Set<String>>> entry : dagMap.entrySet()) {\n+            Set<String> value = entry.getValue().get(degree);\n+            if (value.isEmpty()) {\n+                startNodes.add(entry.getKey());\n+            }\n+        }\n+        return startNodes;\n+    }\n+\n+    public Set<String> getStartNodeNames() {\n+        return getEndNodeNames(\"inDegree\");\n+    }\n+\n+    public Set<String> getLeafNodeNames() {\n+        return getEndNodeNames(\"outDegree\");\n+    }\n+\n+    public Map<String, Integer> getDegreeMap(String degree) {\n+        Map<String, Integer> inDegreeMap = new HashMap<>();\n+        for (Map.Entry<String, Map<String, Set<String>>> entry : dagMap.entrySet()) {\n+            inDegreeMap.put(entry.getKey(), entry.getValue().get(degree).size());\n+        }\n+        return inDegreeMap;\n+    }\n+\n+    public Map<String, Integer> getInDegreeMap() {\n+        return getDegreeMap(\"inDegree\");\n+    }\n+\n+    public Map<String, Integer> getOutDegreeMap() {\n+        return getDegreeMap(\"outDegree\");\n+    }\n+\n+    public Map<String, Node> getNodes() {\n+        return nodes;\n+    }\n+\n+    public Map<String, Map<String, Set<String>>> getDagMap() {\n+        return dagMap;\n+    }\n+\n+    public ArrayList<String> validate() throws InvalidDAGException {\n+        Set<String> startNodes = getStartNodeNames();\n+\n+        if (startNodes.size() != 1) {\n+            throw new InvalidDAGException(\"DAG should have only one start node\");\n+        }\n+\n+        ArrayList<String> topoSortedList = new ArrayList<>();\n+        DagExecutor de = new DagExecutor(this);\n+        de.execute(null, topoSortedList);\n+        if (topoSortedList.size() != nodes.size()) {\n+            throw new InvalidDAGException(\"Not a valid DAG\");\n+        }\n+        return topoSortedList;\n+    }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc2MDQzOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/DagExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NDo1M1rOIivjyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NDo1M1rOIivjyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMwMTcwNQ==", "bodyText": "ListenableFuture may be a more appropriate\nhttps://guava.dev/releases/21.0/api/docs/com/google/common/util/concurrent/ListenableFuture.html\nhttps://stackoverflow.com/questions/19138212/how-to-implement-a-dag-like-scheduler-in-java has a good example.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573301705", "createdAt": "2021-02-09T22:44:53Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/DagExecutor.java", "diffHunk": "@@ -0,0 +1,161 @@\n+package org.pytorch.serve.ensemble;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionService;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorCompletionService;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.pytorch.serve.archive.model.ModelNotFoundException;\n+import org.pytorch.serve.archive.model.ModelVersionNotFoundException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.job.RestJob;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class DagExecutor {\n+\n+    private static final Logger logger = LoggerFactory.getLogger(DagExecutor.class);\n+\n+    private Dag dag;\n+    private Map<String, RequestInput> inputRequestMap;\n+\n+    public DagExecutor(Dag dag) {\n+        this.dag = dag;\n+        inputRequestMap = new ConcurrentHashMap<>();\n+    }\n+\n+    public ArrayList<NodeOutput> execute(RequestInput input, ArrayList<String> topoSortedList) {\n+\n+        CompletionService<NodeOutput> executorCompletionService = null;\n+        if (topoSortedList == null) {\n+            ExecutorService executorService = Executors.newFixedThreadPool(4);\n+            executorCompletionService = new ExecutorCompletionService<>(executorService);\n+        }\n+\n+        Map<String, Integer> inDegreeMap = this.dag.getInDegreeMap();\n+        Set<String> zeroInDegree = dag.getStartNodeNames();\n+        Set<String> executing = new HashSet<>();\n+\n+        if (topoSortedList == null) {\n+            for (String s : zeroInDegree) {\n+                RequestInput newInput = new RequestInput(UUID.randomUUID().toString());\n+                newInput.setHeaders(input.getHeaders());\n+                newInput.setParameters(input.getParameters());\n+                inputRequestMap.put(s, newInput);\n+            }\n+        }\n+\n+        ArrayList<NodeOutput> leafOutputs = new ArrayList<>();\n+\n+        while (!zeroInDegree.isEmpty()) {\n+            Set<String> readyToExecute = new HashSet<>(zeroInDegree);\n+            readyToExecute.removeAll(executing);\n+            executing.addAll(readyToExecute);\n+\n+            ArrayList<NodeOutput> outputs = new ArrayList<>();\n+            if (topoSortedList == null) {\n+                for (String name : readyToExecute) {\n+                    executorCompletionService.submit(\n+                            () ->\n+                                    invokeModel(\n+                                            name,\n+                                            this.dag.getNodes().get(name).getWorkflowModel(),\n+                                            inputRequestMap.get(name)));\n+                }\n+\n+                try {\n+                    outputs.add(executorCompletionService.take().get());\n+                } catch (InterruptedException | ExecutionException e) {\n+                    String[] error = e.getMessage().split(\":\");\n+                    throw new InternalServerException(error[error.length - 1]); // NOPMD\n+                }\n+            } else {\n+                for (String name : readyToExecute) {\n+                    outputs.add(new NodeOutput(name, null));\n+                }\n+            }\n+\n+            for (NodeOutput output : outputs) {\n+                String nodeName = output.getNodeName();\n+                executing.remove(nodeName);\n+                zeroInDegree.remove(nodeName);\n+\n+                if (topoSortedList != null) {\n+                    topoSortedList.add(nodeName);\n+                }\n+\n+                Set<String> childNodes = this.dag.getDagMap().get(nodeName).get(\"outDegree\");\n+                if (childNodes.isEmpty()) {\n+                    leafOutputs.add(output);\n+                } else {\n+                    for (String newNodeName : childNodes) {\n+\n+                        if (topoSortedList == null) {\n+                            byte[] response = (byte[]) output.getData();\n+\n+                            RequestInput newInput = this.inputRequestMap.get(newNodeName);\n+                            if (newInput == null) {\n+                                List<InputParameter> params = new ArrayList<>();\n+                                newInput = new RequestInput(UUID.randomUUID().toString());\n+                                if (inDegreeMap.get(newNodeName) == 1) {\n+                                    params.add(new InputParameter(\"body\", response));\n+                                } else {\n+                                    params.add(new InputParameter(nodeName, response));\n+                                }\n+                                newInput.setParameters(params);\n+                                newInput.setHeaders(input.getHeaders());\n+                            } else {\n+                                newInput.addParameter(new InputParameter(nodeName, response));\n+                            }\n+                            this.inputRequestMap.put(newNodeName, newInput);\n+                        }\n+\n+                        inDegreeMap.replace(newNodeName, inDegreeMap.get(newNodeName) - 1);\n+                        if (inDegreeMap.get(newNodeName) == 0) {\n+                            zeroInDegree.add(newNodeName);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return leafOutputs;\n+    }\n+\n+    private NodeOutput invokeModel(String nodeName, WorkflowModel workflowModel, RequestInput input)\n+            throws ModelNotFoundException, ModelVersionNotFoundException, ExecutionException,\n+                    InterruptedException {\n+        try {\n+            CompletableFuture<byte[]> respFuture = new CompletableFuture<>();\n+\n+            RestJob job = ApiUtils.addRESTInferenceJob(null, workflowModel.getName(), null, input);\n+            job.setResponsePromise(respFuture);\n+            byte[] resp = respFuture.get();\n+            return new NodeOutput(nodeName, resp);\n+        } catch (InterruptedException | ExecutionException e) {\n+            logger.error(\"Failed to execute workflow Node.\");\n+            logger.error(nodeName + \" : \" + e.getMessage());\n+            String[] error = e.getMessage().split(\":\");\n+            throw new InternalServerException(nodeName + \" - \" + error[error.length - 1]); // NOPMD\n+        } catch (ModelNotFoundException e) {\n+            logger.error(\"Model not found.\");\n+            logger.error(e.getMessage());\n+            throw e;\n+        } catch (ModelVersionNotFoundException e) {\n+            logger.error(\"Model version not found.\");\n+            logger.error(e.getMessage());\n+            throw e;\n+        }\n+    }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc2MjY2OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/NodeOutput.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NTozMFrOIivlEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NTozMFrOIivlEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMwMjAzMg==", "bodyText": "Lombok", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573302032", "createdAt": "2021-02-09T22:45:30Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/NodeOutput.java", "diffHunk": "@@ -0,0 +1,27 @@\n+package org.pytorch.serve.ensemble;\n+\n+public class NodeOutput {\n+    private String nodeName;\n+    private Object data;\n+\n+    public NodeOutput(String nodeName, Object data) {\n+        this.nodeName = nodeName;\n+        this.data = data;\n+    }\n+\n+    public String getNodeName() {\n+        return nodeName;\n+    }\n+\n+    public void setNodeName(String nodeName) {\n+        this.nodeName = nodeName;\n+    }\n+\n+    public Object getData() {\n+        return data;\n+    }\n+\n+    public void setData(Object data) {\n+        this.data = data;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc2NDk0OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowManifest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NjoxMlrOIivmgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NjoxMlrOIivmgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMwMjQwMg==", "bodyText": "final / lombok", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573302402", "createdAt": "2021-02-09T22:46:12Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowManifest.java", "diffHunk": "@@ -0,0 +1,145 @@\n+package org.pytorch.serve.ensemble;\n+\n+import com.google.gson.annotations.SerializedName;\n+\n+public class WorkflowManifest {\n+\n+    private String createdOn;\n+    private String description;\n+    private String archiverVersion;\n+    private RuntimeType runtime;\n+    private Workflow workflow;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc2NTkyOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowModel.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NjozM1rOIivnJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo0NjozM1rOIivnJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMwMjU2NQ==", "bodyText": "final / lombok", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573302565", "createdAt": "2021-02-09T22:46:33Z", "author": {"login": "dhanainme"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowModel.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package org.pytorch.serve.ensemble;\n+\n+public class WorkflowModel {\n+\n+    private String name;\n+    private String url;\n+    private int minWorkers;\n+    private int maxWorkers;\n+    private int batchSize;\n+    private int maxBatchDelay;\n+    private String handler;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzc5OTU3OnYy", "diffSide": "RIGHT", "path": "workflow-archiver/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo1Njo1MFrOIiv7Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo1Njo1MFrOIiv7Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMwNzcxMQ==", "bodyText": "Nit : Fix narrative style.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573307711", "createdAt": "2021-02-09T22:56:50Z", "author": {"login": "dhanainme"}, "path": "workflow-archiver/README.md", "diffHunk": "@@ -0,0 +1,97 @@\n+# Torch Workflow archiver for TorchServe\n+\n+## Contents of this Document\n+* [Overview](#overview)\n+* [Installation](#installation)\n+* [Torch Workflow Archiver CLI](#torch-workflow-archiver-command-line-interface)\n+* [Artifact Details](#artifact-details)\n+    * [WAR-INFO](#war-inf)\n+    * [Workflow name](#workflow-name)\n+    * [Spec File](#spec-file)\n+    * [handler](#handler)\n+\n+## Overview\n+\n+A key feature of TorchServe is the ability to package workflow specification (.yaml) and other workflow dependency files into a single workflow archive file (.war). This file can then be redistributed and served by anyone using TorchServe.\n+ \n+The CLI creates a `.war` file that TorchServe CLI uses to serve the workflows.\n+\n+The following information is required to create a standalone workflow archive:\n+1. [Workflow name](#workflow-name)\n+2. [Spec file](#spec-file)\n+\n+## Installation\n+\n+Install torch-workflow-archiver as follows:\n+\n+```bash\n+pip install torch-workflow-archiver\n+```\n+\n+## Installation from source\n+\n+Install torch-workflow-archiver from source as follows:\n+\n+```bash\n+git clone https://github.com/pytorch/serve.git\n+cd serve/workflow-archiver\n+pip install .\n+```\n+\n+## Torch Workflow Archiver Command Line Interface\n+\n+Now let's cover the details on using the CLI tool: `torch-workflow-archiver`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzgwNjUwOnYy", "diffSide": "RIGHT", "path": "workflow-archiver/workflow_archiver/arg_parser.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo1ODo0N1rOIiv_Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMjo1ODo0N1rOIiv_Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMwODcyMw==", "bodyText": "required : False ?\nWhat happens if no handler is provided ?", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573308723", "createdAt": "2021-02-09T22:58:47Z", "author": {"login": "dhanainme"}, "path": "workflow-archiver/workflow_archiver/arg_parser.py", "diffHunk": "@@ -0,0 +1,72 @@\n+\n+\n+\"\"\"\n+This module parses the arguments given through the torch-workflow-archiver command-line.\n+\"\"\"\n+\n+import argparse\n+import os\n+\n+\n+# noinspection PyTypeChecker\n+class ArgParser(object):\n+\n+    \"\"\"\n+    Argument parser for torch-workflow-archiver commands\n+    \"\"\"\n+\n+    @staticmethod\n+    def workflow_archiver_args_parser():\n+\n+        \"\"\" Argument parser for torch-workflow-archiver\n+        \"\"\"\n+\n+        parser_workflow_archiver = argparse.ArgumentParser(prog='torch-workflow-archiver',\n+                                                           description='Torch Workflow Archiver Tool',\n+                                                           formatter_class=argparse.RawTextHelpFormatter)\n+\n+        parser_workflow_archiver.add_argument('--workflow-name',\n+                                              required=True,\n+                                              type=str,\n+                                              default=None,\n+                                              help='Exported workflow name. Exported file will be named as'\n+                                                   ' workflow-name.war and saved in current working directory '\n+                                                   'if no --export-path is specified, '\n+                                                   'else it will be saved under the export path')\n+\n+        parser_workflow_archiver.add_argument('--spec-file',\n+                                              required=True,\n+                                              type=str,\n+                                              default=None,\n+                                              help='Path to .yaml file containing workflow DAG specification.')\n+\n+        parser_workflow_archiver.add_argument('--handler',\n+                                              required=False,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzgyNDkxOnYy", "diffSide": "RIGHT", "path": "workflow-archiver/workflow_archiver/workflow_packaging_utils.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMzowNToxNVrOIiwKwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMzowNToxNVrOIiwKwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMxMTY4MA==", "bodyText": "May have to explicitly document this behavior on what files get archived.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573311680", "createdAt": "2021-02-09T23:05:15Z", "author": {"login": "dhanainme"}, "path": "workflow-archiver/workflow_archiver/workflow_packaging_utils.py", "diffHunk": "@@ -0,0 +1,198 @@\n+\n+\n+\"\"\"\n+Helper utils for Workflow Archiver tool\n+\"\"\"\n+\n+import logging\n+import os\n+import re\n+import zipfile\n+import shutil\n+import tempfile\n+from .workflow_archiver_error import WorkflowArchiverError\n+\n+from .manifest_components.manifest import Manifest\n+from .manifest_components.workflow import Workflow\n+\n+MANIFEST_FILE_NAME = 'MANIFEST.json'\n+WAR_INF = 'WAR-INF'\n+\n+\n+class WorkflowExportUtils(object):\n+    \"\"\"\n+    Helper utils for Workflow Archiver tool.\n+    This class lists out all the methods such as validations for workflow archiving.\n+    \"\"\"\n+\n+    @staticmethod\n+    def get_archive_export_path(export_file_path, workflow_name):\n+        return os.path.join(export_file_path, f'{workflow_name}.war')\n+\n+    @staticmethod\n+    def check_war_already_exists(workflow_name, export_file_path, overwrite):\n+        \"\"\"\n+        Function to check if .war already exists\n+        :param workflow_name:\n+        :param export_file_path:\n+        :param overwrite:\n+        :return:\n+        \"\"\"\n+        if export_file_path is None:\n+            export_file_path = os.getcwd()\n+\n+        export_file = WorkflowExportUtils.get_archive_export_path(export_file_path, workflow_name)\n+\n+        if os.path.exists(export_file):\n+            if overwrite:\n+                logging.warning(\"Overwriting %s ...\", export_file)\n+            else:\n+                raise WorkflowArchiverError(\"{0} already exists.\\n\"\n+                                            \"Please specify --force/-f option to overwrite the workflow archive \"\n+                                            \"output file.\\n\"\n+                                            \"See -h/--help for more details.\".format(export_file))\n+\n+        return export_file_path\n+\n+    @staticmethod\n+    def generate_workflow(workflow_args):\n+        workflow = Workflow(workflow_name=workflow_args.workflow_name, spec_file=workflow_args.spec_file,\n+                            handler=workflow_args.handler)\n+        return workflow\n+\n+    @staticmethod\n+    def generate_manifest_json(args):\n+        \"\"\"\n+        Function to generate manifest as a json string from the inputs provided by the user in the command line\n+        :param args:\n+        :return:s\n+        \"\"\"\n+\n+        workflow = WorkflowExportUtils.generate_workflow(args)\n+\n+        manifest = Manifest(workflow=workflow)\n+\n+        return str(manifest)\n+\n+    @staticmethod\n+    def clean_temp_files(temp_files):\n+        for f in temp_files:\n+            os.remove(f)\n+\n+    @staticmethod\n+    def make_dir(d):\n+        if not os.path.isdir(d):\n+            os.makedirs(d)\n+\n+    @staticmethod\n+    def copy_artifacts(workflow_name, artifact_files):\n+        \"\"\"\n+        copy workflow artifacts in a common workflow directory for archiving\n+        :param workflow_name: name of workflow being archived\n+        :param artifact_files: list of files to be copied in archive\n+        :return:\n+        \"\"\"\n+        workflow_path = os.path.join(tempfile.gettempdir(), workflow_name)\n+        if os.path.exists(workflow_path):\n+            shutil.rmtree(workflow_path)\n+        WorkflowExportUtils.make_dir(workflow_path)\n+        for path in artifact_files:\n+            if path:\n+                for file in path.split(\",\"):\n+                    shutil.copy(file, workflow_path)\n+\n+        return workflow_path\n+\n+    @staticmethod\n+    def archive(export_file, workflow_name, workflow_path, manifest):\n+        \"\"\"\n+        Create a workflow-archive\n+        :param export_file:\n+        :param workflow_name:\n+        :param workflow_path\n+        :param manifest:\n+        :return:\n+        \"\"\"\n+        war_path = WorkflowExportUtils.get_archive_export_path(export_file, workflow_name)\n+        try:\n+            with zipfile.ZipFile(war_path, 'w', zipfile.ZIP_DEFLATED) as z:\n+                WorkflowExportUtils.archive_dir(workflow_path, z)\n+                # Write the manifest here now as a json\n+                z.writestr(os.path.join(WAR_INF, MANIFEST_FILE_NAME), manifest)\n+        except IOError:\n+            logging.error(\"Failed to save the workflow-archive to workflow-path \\\"%s\\\". \"\n+                          \"Check the file permissions and retry.\", export_file)\n+            raise\n+        except Exception as e:\n+            logging.error(\"Failed to convert %s to the workflow-archive.\", workflow_name)\n+            raise e\n+\n+    @staticmethod\n+    def archive_dir(path, dst):\n+\n+        \"\"\"\n+        This method zips the dir and filters out some files based on a expression\n+        :param path:\n+        :param dst:\n+        :return:\n+        \"\"\"\n+        unwanted_dirs = {'__MACOSX', '__pycache__'}\n+\n+        for root, directories, files in os.walk(path):\n+            # Filter directories\n+            directories[:] = [d for d in directories if WorkflowExportUtils.directory_filter(d, unwanted_dirs)]\n+            for f in files:\n+                file_path = os.path.join(root, f)\n+                dst.write(file_path, os.path.relpath(file_path, path))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzgyOTI5OnYy", "diffSide": "RIGHT", "path": "workflow-archiver/workflow_archiver/workflow_packaging_utils.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMzowNjozN1rOIiwNTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMzowNjozN1rOIiwNTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMxMjMzMg==", "bodyText": "Nit : Separate methods for single method calls which can be grouped.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573312332", "createdAt": "2021-02-09T23:06:37Z", "author": {"login": "dhanainme"}, "path": "workflow-archiver/workflow_archiver/workflow_packaging_utils.py", "diffHunk": "@@ -0,0 +1,198 @@\n+\n+\n+\"\"\"\n+Helper utils for Workflow Archiver tool\n+\"\"\"\n+\n+import logging\n+import os\n+import re\n+import zipfile\n+import shutil\n+import tempfile\n+from .workflow_archiver_error import WorkflowArchiverError\n+\n+from .manifest_components.manifest import Manifest\n+from .manifest_components.workflow import Workflow\n+\n+MANIFEST_FILE_NAME = 'MANIFEST.json'\n+WAR_INF = 'WAR-INF'\n+\n+\n+class WorkflowExportUtils(object):\n+    \"\"\"\n+    Helper utils for Workflow Archiver tool.\n+    This class lists out all the methods such as validations for workflow archiving.\n+    \"\"\"\n+\n+    @staticmethod\n+    def get_archive_export_path(export_file_path, workflow_name):\n+        return os.path.join(export_file_path, f'{workflow_name}.war')\n+\n+    @staticmethod\n+    def check_war_already_exists(workflow_name, export_file_path, overwrite):\n+        \"\"\"\n+        Function to check if .war already exists\n+        :param workflow_name:\n+        :param export_file_path:\n+        :param overwrite:\n+        :return:\n+        \"\"\"\n+        if export_file_path is None:\n+            export_file_path = os.getcwd()\n+\n+        export_file = WorkflowExportUtils.get_archive_export_path(export_file_path, workflow_name)\n+\n+        if os.path.exists(export_file):\n+            if overwrite:\n+                logging.warning(\"Overwriting %s ...\", export_file)\n+            else:\n+                raise WorkflowArchiverError(\"{0} already exists.\\n\"\n+                                            \"Please specify --force/-f option to overwrite the workflow archive \"\n+                                            \"output file.\\n\"\n+                                            \"See -h/--help for more details.\".format(export_file))\n+\n+        return export_file_path\n+\n+    @staticmethod\n+    def generate_workflow(workflow_args):\n+        workflow = Workflow(workflow_name=workflow_args.workflow_name, spec_file=workflow_args.spec_file,\n+                            handler=workflow_args.handler)\n+        return workflow\n+\n+    @staticmethod\n+    def generate_manifest_json(args):\n+        \"\"\"\n+        Function to generate manifest as a json string from the inputs provided by the user in the command line\n+        :param args:\n+        :return:s\n+        \"\"\"\n+\n+        workflow = WorkflowExportUtils.generate_workflow(args)\n+\n+        manifest = Manifest(workflow=workflow)\n+\n+        return str(manifest)\n+\n+    @staticmethod\n+    def clean_temp_files(temp_files):\n+        for f in temp_files:\n+            os.remove(f)\n+\n+    @staticmethod\n+    def make_dir(d):\n+        if not os.path.isdir(d):\n+            os.makedirs(d)\n+\n+    @staticmethod\n+    def copy_artifacts(workflow_name, artifact_files):\n+        \"\"\"\n+        copy workflow artifacts in a common workflow directory for archiving\n+        :param workflow_name: name of workflow being archived\n+        :param artifact_files: list of files to be copied in archive\n+        :return:\n+        \"\"\"\n+        workflow_path = os.path.join(tempfile.gettempdir(), workflow_name)\n+        if os.path.exists(workflow_path):\n+            shutil.rmtree(workflow_path)\n+        WorkflowExportUtils.make_dir(workflow_path)\n+        for path in artifact_files:\n+            if path:\n+                for file in path.split(\",\"):\n+                    shutil.copy(file, workflow_path)\n+\n+        return workflow_path\n+\n+    @staticmethod\n+    def archive(export_file, workflow_name, workflow_path, manifest):\n+        \"\"\"\n+        Create a workflow-archive\n+        :param export_file:\n+        :param workflow_name:\n+        :param workflow_path\n+        :param manifest:\n+        :return:\n+        \"\"\"\n+        war_path = WorkflowExportUtils.get_archive_export_path(export_file, workflow_name)\n+        try:\n+            with zipfile.ZipFile(war_path, 'w', zipfile.ZIP_DEFLATED) as z:\n+                WorkflowExportUtils.archive_dir(workflow_path, z)\n+                # Write the manifest here now as a json\n+                z.writestr(os.path.join(WAR_INF, MANIFEST_FILE_NAME), manifest)\n+        except IOError:\n+            logging.error(\"Failed to save the workflow-archive to workflow-path \\\"%s\\\". \"\n+                          \"Check the file permissions and retry.\", export_file)\n+            raise\n+        except Exception as e:\n+            logging.error(\"Failed to convert %s to the workflow-archive.\", workflow_name)\n+            raise e\n+\n+    @staticmethod\n+    def archive_dir(path, dst):\n+\n+        \"\"\"\n+        This method zips the dir and filters out some files based on a expression\n+        :param path:\n+        :param dst:\n+        :return:\n+        \"\"\"\n+        unwanted_dirs = {'__MACOSX', '__pycache__'}\n+\n+        for root, directories, files in os.walk(path):\n+            # Filter directories\n+            directories[:] = [d for d in directories if WorkflowExportUtils.directory_filter(d, unwanted_dirs)]\n+            for f in files:\n+                file_path = os.path.join(root, f)\n+                dst.write(file_path, os.path.relpath(file_path, path))\n+\n+    @staticmethod\n+    def directory_filter(directory, unwanted_dirs):\n+        \"\"\"\n+        This method weeds out unwanted hidden directories from the workflow archive .war file\n+        :param directory:\n+        :param unwanted_dirs:\n+        :return:\n+        \"\"\"\n+        if directory in unwanted_dirs:\n+            return False\n+        if directory.startswith('.'):\n+            return False\n+\n+        return True\n+\n+    @staticmethod\n+    def file_filter(current_file, files_to_exclude):\n+        \"\"\"\n+        This method weeds out unwanted files\n+        :param current_file:\n+        :param files_to_exclude:\n+        :return:\n+        \"\"\"\n+        files_to_exclude.add('MANIFEST.json')\n+        if current_file in files_to_exclude:\n+            return False\n+\n+        elif current_file.endswith(('.pyc', '.DS_Store', '.war')):\n+            return False\n+\n+        return True\n+\n+    @staticmethod\n+    def check_workflow_name_regex_or_exit(workflow_name):\n+        \"\"\"\n+        Method checks whether workflow name passes regex filter.\n+        If the regex Filter fails, the method exits.\n+        :param workflow_name:\n+        :return:\n+        \"\"\"\n+        if not re.match(r'^[A-Za-z0-9][A-Za-z0-9_\\-.]*$', workflow_name):\n+            raise WorkflowArchiverError(\"Workflow name contains special characters.\\n\"\n+                                        \"The allowed regular expression filter for workflow \"\n+                                        \"name is: ^[A-Za-z0-9][A-Za-z0-9_\\\\-.]*$\")\n+\n+    @staticmethod\n+    def validate_inputs(workflow_name, export_path):\n+        WorkflowExportUtils.check_workflow_name_regex_or_exit(workflow_name)\n+        if not os.path.isdir(os.path.abspath(export_path)):\n+            raise WorkflowArchiverError(\"Given export-path {} is not a directory. \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 197}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxMzg2MzY1OnYy", "diffSide": "RIGHT", "path": "ts/arg_parser.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMzoxMzozMlrOIiwi0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOVQyMzoxMzozMlrOIiwi0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzMxNzg0Mw==", "bodyText": "Q : Why 2 stores ? Why not just 1 dir for both models & workflow.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r573317843", "createdAt": "2021-02-09T23:13:32Z", "author": {"login": "dhanainme"}, "path": "ts/arg_parser.py", "diffHunk": "@@ -33,6 +33,10 @@ def ts_parser():\n                             required=False,\n                             dest='model_store',\n                             help='Model store location from where local or default models can be loaded')\n+        parser.add_argument('--workflow-store',\n+                            required=False,\n+                            dest='workflow_store',\n+                            help='Workflow store location from where local or default workflows can be loaded')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14627541568be02cf2ba7256c53b350d7b3ee3ad"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzgwMjUwMDkyOnYy", "diffSide": "RIGHT", "path": "examples/Workflows/densenet_image_classifier_pipeline/densenet_model/densenet_handler.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQyMDozMToyNlrOI98HLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQyMDozMToyNlrOI98HLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgxODkyNw==", "bodyText": "@dhanainme We need to document/add user guide for the differences of the handler for models(nodes) in the workflow with regular handlers we have in the non-workflow use-cases.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r601818927", "createdAt": "2021-03-25T20:31:26Z", "author": {"login": "HamidShojanazeri"}, "path": "examples/Workflows/densenet_image_classifier_pipeline/densenet_model/densenet_handler.py", "diffHunk": "@@ -0,0 +1,147 @@\n+\"\"\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad264e6919f6d84536b25ae0c0bf2f92a618a307"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzgwMjUyMzI4OnYy", "diffSide": "RIGHT", "path": "docs/workflows.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQyMDozNzoyOFrOI98U6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQyMDozNzoyOFrOI98U6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyMjQ0Mg==", "bodyText": "@dhanainme Please add some clarification on how the gpu assignment would be handled here.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r601822442", "createdAt": "2021-03-25T20:37:28Z", "author": {"login": "HamidShojanazeri"}, "path": "docs/workflows.md", "diffHunk": "@@ -0,0 +1,114 @@\n+# TorchServe Workflows\n+\n+TorchServe can be used for serving ensemble of models & functions (python) through Workflow APIs. \n+\n+It utilizes [REST based APIs](rest_api.md) for workflow management and predictions.\n+\n+A Workflow is served on TorchServe using a workflow-archive(.war) which comprises of following: \n+\n+## Workflow Specification file\n+\n+A workflow specification is a YAML file which provides the details of the models to be executed and a DAG for defining data flow.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad264e6919f6d84536b25ae0c0bf2f92a618a307"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzgwMjU2OTc4OnYy", "diffSide": "RIGHT", "path": "ts/torch_handler/densenet_handler.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQyMDo0OTowNFrOI98w6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yNVQyMDo0OTowNFrOI98w6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgyOTYxMA==", "bodyText": "@dhanainme is this planned to be aded as default handler? or having it in the example would suffice.", "url": "https://github.com/pytorch/serve/pull/785#discussion_r601829610", "createdAt": "2021-03-25T20:49:04Z", "author": {"login": "HamidShojanazeri"}, "path": "ts/torch_handler/densenet_handler.py", "diffHunk": "@@ -0,0 +1,147 @@\n+\"\"\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad264e6919f6d84536b25ae0c0bf2f92a618a307"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzgxNDE5MDkxOnYy", "diffSide": "RIGHT", "path": "docs/workflows.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yOVQxODoxODoyMFrOI_jdYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yOVQxODoxODoyMFrOI_jdYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUxMjE2Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            def preprocss(data, context):\n          \n          \n            \n            def preprocess(data, context):", "url": "https://github.com/pytorch/serve/pull/785#discussion_r603512162", "createdAt": "2021-03-29T18:18:20Z", "author": {"login": "maaquib"}, "path": "docs/workflows.md", "diffHunk": "@@ -0,0 +1,114 @@\n+# TorchServe Workflows\n+\n+TorchServe can be used for serving ensemble of models & functions (python) through Workflow APIs. \n+\n+It utilizes [REST based APIs](rest_api.md) for workflow management and predictions.\n+\n+A Workflow is served on TorchServe using a workflow-archive(.war) which comprises of following: \n+\n+## Workflow Specification file\n+\n+A workflow specification is a YAML file which provides the details of the models to be executed and a DAG for defining data flow.\n+\n+E.g.\n+\n+```\n+models:\n+    #global model params\n+    min-workers: 1\n+    max-workers: 4\n+    batch-size: 3\n+    max-batch-delay : 5000\n+    m1:\n+       url : model1.mar #local or public URI\n+       min-workers: 1   #override the global params\n+       max-workers: 2\n+       batch-size: 4\n+     \n+    m2:\n+       url : model2.mar\n+\n+    m3:\n+       url : model3.mar\n+       batch-size: 3\n+\n+    m4:\n+      url : model4.mar\n+ \n+dag:\n+  pre_processing : [m1]\n+  m1 : [m2]\n+  m2 : [m3]\n+  m3 : [m4]\n+  m4 : [postprocessing]\n+```\n+\n+### Workflow Models\n+\n+The `models` section of the workflow specification defines the models used in the workflow. It uses the following syntax:\n+\n+```\n+models:\n+    <model_name>:\n+        url: <local or public url for mar file>\n+```\n+\n+### Workflow model properties\n+\n+User can define following workflow model properties:\n+\n+| Properties | Description | Default value |\n+| --- | --- | --- |\n+| min-workers | Number of minimum workers launched for every workflow model | 1 |\n+| max-workers | Number of maximum workers launched for every workflow model | 1 |\n+| batch-size | Batch size used for every workflow model | 1 |\n+| max-batch-delay | Maximum batch delay time TorchServe waits for every workflow model to receive `batch_size` number of requests.| 50 ms |\n+\n+These properties can be defined as a global value for every model and can be over-ridden at every model level in workflow specification. Refer the above example for more details.\n+\n+## Workflow DAG\n+\n+User can define the dataflow of a workflow using the `dag` section of the workflow specification. The `dag` consists of the model names defined in the `model` section and python function names which are implemented in the workflow-archive's handler file.\n+\n+Eg.\n+```\n+dag:\n+  function1 : [model1]\n+  model1 : [model2]\n+  model2 : [function2]\n+```\n+\n+In the above example the data will flow as follows:\n+\n+```\n+input -> function1 -> model1 -> model2 -> function2 -> output\n+```\n+\n+## Handler file\n+\n+A handler file (python) is supplied in the workflow archive (.war) and consists of all the functions used in the workflow dag.\n+\n+Eg.\n+```\n+\n+def preprocss(data, context):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad264e6919f6d84536b25ae0c0bf2f92a618a307"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzgxNDE5MTY1OnYy", "diffSide": "RIGHT", "path": "docs/workflows.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yOVQxODoxODozMFrOI_jd2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0yOVQxODoxODozMFrOI_jd2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMzUxMjI4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            def postprocess(data, xontext):\n          \n          \n            \n            def postprocess(data, context):", "url": "https://github.com/pytorch/serve/pull/785#discussion_r603512282", "createdAt": "2021-03-29T18:18:30Z", "author": {"login": "maaquib"}, "path": "docs/workflows.md", "diffHunk": "@@ -0,0 +1,114 @@\n+# TorchServe Workflows\n+\n+TorchServe can be used for serving ensemble of models & functions (python) through Workflow APIs. \n+\n+It utilizes [REST based APIs](rest_api.md) for workflow management and predictions.\n+\n+A Workflow is served on TorchServe using a workflow-archive(.war) which comprises of following: \n+\n+## Workflow Specification file\n+\n+A workflow specification is a YAML file which provides the details of the models to be executed and a DAG for defining data flow.\n+\n+E.g.\n+\n+```\n+models:\n+    #global model params\n+    min-workers: 1\n+    max-workers: 4\n+    batch-size: 3\n+    max-batch-delay : 5000\n+    m1:\n+       url : model1.mar #local or public URI\n+       min-workers: 1   #override the global params\n+       max-workers: 2\n+       batch-size: 4\n+     \n+    m2:\n+       url : model2.mar\n+\n+    m3:\n+       url : model3.mar\n+       batch-size: 3\n+\n+    m4:\n+      url : model4.mar\n+ \n+dag:\n+  pre_processing : [m1]\n+  m1 : [m2]\n+  m2 : [m3]\n+  m3 : [m4]\n+  m4 : [postprocessing]\n+```\n+\n+### Workflow Models\n+\n+The `models` section of the workflow specification defines the models used in the workflow. It uses the following syntax:\n+\n+```\n+models:\n+    <model_name>:\n+        url: <local or public url for mar file>\n+```\n+\n+### Workflow model properties\n+\n+User can define following workflow model properties:\n+\n+| Properties | Description | Default value |\n+| --- | --- | --- |\n+| min-workers | Number of minimum workers launched for every workflow model | 1 |\n+| max-workers | Number of maximum workers launched for every workflow model | 1 |\n+| batch-size | Batch size used for every workflow model | 1 |\n+| max-batch-delay | Maximum batch delay time TorchServe waits for every workflow model to receive `batch_size` number of requests.| 50 ms |\n+\n+These properties can be defined as a global value for every model and can be over-ridden at every model level in workflow specification. Refer the above example for more details.\n+\n+## Workflow DAG\n+\n+User can define the dataflow of a workflow using the `dag` section of the workflow specification. The `dag` consists of the model names defined in the `model` section and python function names which are implemented in the workflow-archive's handler file.\n+\n+Eg.\n+```\n+dag:\n+  function1 : [model1]\n+  model1 : [model2]\n+  model2 : [function2]\n+```\n+\n+In the above example the data will flow as follows:\n+\n+```\n+input -> function1 -> model1 -> model2 -> function2 -> output\n+```\n+\n+## Handler file\n+\n+A handler file (python) is supplied in the workflow archive (.war) and consists of all the functions used in the workflow dag.\n+\n+Eg.\n+```\n+\n+def preprocss(data, context):\n+    pass\n+\n+def postprocess(data, xontext):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad264e6919f6d84536b25ae0c0bf2f92a618a307"}, "originalPosition": 97}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1469, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}