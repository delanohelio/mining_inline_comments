{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyMTU3OTA1", "number": 388, "title": "Document the docker settings for running in production mode", "bodyText": "Document the docker settings for running in production mode #234.\nAdds documentation for TS in Production env for Docker -  Shared Memory Size / User Limits for System Resources / Exposing specific ports / volumes between the host & docker env.", "createdAt": "2020-05-22T22:15:33Z", "url": "https://github.com/pytorch/serve/pull/388", "merged": true, "mergeCommit": {"oid": "96589121cda9e073dc851861a070a2a806dd96cf"}, "closed": true, "closedAt": "2020-05-23T01:50:23Z", "author": {"login": "dhanainme"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcj5rVOgH2gAyNDIyMTU3OTA1OjY3YWYxNTM3MDlmOWNiZjg2YTI0MTRjMWQ2YWU2NzVjY2M3YTA3YjE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcj85KFAFqTQxNzI0MDQ5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "67af153709f9cbf86a2414c1d6ae675ccc7a07b1", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/67af153709f9cbf86a2414c1d6ae675ccc7a07b1", "committedDate": "2020-05-22T22:05:21Z", "message": "Docker Production Options #234"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f771e3f6bb007d5c5dd00a1df47f67cfbae89b25", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/f771e3f6bb007d5c5dd00a1df47f67cfbae89b25", "committedDate": "2020-05-22T22:11:44Z", "message": "Docker Production Options - Typo Fixes / Fit & Finish  #234"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef", "committedDate": "2020-05-22T22:12:48Z", "message": "Typos - Docker Docs #234"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjE0NDg1", "url": "https://github.com/pytorch/serve/pull/388#pullrequestreview-417214485", "createdAt": "2020-05-22T22:37:56Z", "commit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "state": "DISMISSED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjozNzo1NlrOGZladA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjo0MDo1OVrOGZlcVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDU2NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n          \n          \n            \n            You may want to consider the following aspects / docker options when deploying torchserve in Production with Docker.", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429480564", "createdAt": "2020-05-22T22:37:56Z", "author": {"login": "maaquib"}, "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDk5MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n          \n          \n            \n                *  ```-p8080:p8080 -p8081:8081``` TorchServe uses default ports 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429480991", "createdAt": "2020-05-22T22:40:42Z", "author": {"login": "maaquib"}, "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n+\n+\n+* Shared Memory Size \n+\n+    * ```shm-size``` - The shm-size parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n+\n+\n+* User Limits for System Resources\n+    \n+    * ```--ulimit memlock=-1``` : Maximum locked-in-memory address space. \n+    * ```--ulimit stack``` : Linux stack size \n+\n+    The current ulimit values can be viewed by executing ```ulimit -a```. A more exhaustive set of options for resource constraining can be found in the Docker Documentation [here](https://docs.docker.com/config/containers/resource_constraints/), [here](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit) and [here](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n+\n+\n+* Exposing specific ports / volumes between the host & docker env.\n+\n+    *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MTA0Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                * The model store is passed to torchserve with the --model-store option. You may want want to consider using a shared volume if you prefer pre populating models in model-store directory.\n          \n          \n            \n                * The model store is passed to torchserve with the --model-store option. You may want to consider using a shared volume if you prefer pre populating models in model-store directory.", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429481046", "createdAt": "2020-05-22T22:40:59Z", "author": {"login": "maaquib"}, "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n+\n+\n+* Shared Memory Size \n+\n+    * ```shm-size``` - The shm-size parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n+\n+\n+* User Limits for System Resources\n+    \n+    * ```--ulimit memlock=-1``` : Maximum locked-in-memory address space. \n+    * ```--ulimit stack``` : Linux stack size \n+\n+    The current ulimit values can be viewed by executing ```ulimit -a```. A more exhaustive set of options for resource constraining can be found in the Docker Documentation [here](https://docs.docker.com/config/containers/resource_constraints/), [here](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit) and [here](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n+\n+\n+* Exposing specific ports / volumes between the host & docker env.\n+\n+    *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n+    * The model store is passed to torchserve with the --model-store option. You may want want to consider using a shared volume if you prefer pre populating models in model-store directory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "originalPosition": 34}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "935d9d248b5042e72995a700e586ef9ab1bb56bc", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/935d9d248b5042e72995a700e586ef9ab1bb56bc", "committedDate": "2020-05-22T23:13:46Z", "message": "grammar check  - docker/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87f261c925e203e1b6908c2e2873705688be0e71", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/87f261c925e203e1b6908c2e2873705688be0e71", "committedDate": "2020-05-22T23:14:03Z", "message": "grammar check  - docker/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2be662e93e25bbf977ec567f10b80a0499e2eaf1", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/2be662e93e25bbf977ec567f10b80a0499e2eaf1", "committedDate": "2020-05-22T23:14:16Z", "message": "grammar check  - docker/README.md\n\nCo-authored-by: Aaqib <maaquib@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7a4c0a8f671705ebebd3f9754b05fb23e1c5982", "author": {"user": {"login": "dhanainme", "name": null}}, "url": "https://github.com/pytorch/serve/commit/a7a4c0a8f671705ebebd3f9754b05fb23e1c5982", "committedDate": "2020-05-22T23:14:33Z", "message": "Merge branch 'staging_0_1_1' into issue_234"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e490bd18d2b07f8d1be73f07a3a7a4292b95d006", "author": {"user": {"login": "maaquib", "name": "Aaqib"}}, "url": "https://github.com/pytorch/serve/commit/e490bd18d2b07f8d1be73f07a3a7a4292b95d006", "committedDate": "2020-05-22T23:41:00Z", "message": "Merge branch 'staging_0_1_1' into issue_234"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MjQwNDk2", "url": "https://github.com/pytorch/serve/pull/388#pullrequestreview-417240496", "createdAt": "2020-05-23T01:50:10Z", "commit": {"oid": "e490bd18d2b07f8d1be73f07a3a7a4292b95d006"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2159, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}