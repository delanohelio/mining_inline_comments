{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1MTQ2NTc0", "number": 212, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxOToyOToyMFrODzBSfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjoxNzowMFrODzD6Qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODI1MDg0OnYy", "diffSide": "RIGHT", "path": "README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxOToyOToyMFrOGHaflg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQxNjoxMzoyOFrOGHsMCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQyNzI4Ng==", "bodyText": "This is fine on a CPU machine, but does not give the Docker container access to GPU devices. Switching to @harshbafna's command line from Issue #205, doesn't seem to help, either.\nBecause of the lack of GPU instructions, this is incomplete, but I recommend leaving it so until there's a fix for the GPU Docker container.", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410427286", "createdAt": "2020-04-17T19:29:20Z", "author": {"login": "fbbradheintz"}, "path": "README.md", "diffHunk": "@@ -2,208 +2,237 @@\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).\n+**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n ## Contents of this Document\n \n * [Install TorchServe](#install-torchserve)\n-* [Quick Start with docker](#quick-start-with-docker)\n-* [Quick Start for local environment](#quick-start-guide-for-local-environment)\n * [Serve a Model](#serve-a-model)\n-* [Other Features](#other-features)\n+* [Quick start with docker](#quick-start-with-docker)\n * [Contributing](#contributing)\n \n ## Install TorchServe\n \n-## Quick Start with docker\n+Conda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n+**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n \n-### Start TorchServe using docker image\n+### Install with pip\n+To use `pip` to install TorchServe and the model archiver:\n \n-#### Prerequisites\n-\n-* docker - Refer [official docker installation guide](https://docs.docker.com/install/)\n-* git    - Refer [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n-\n-#### Building docker image\n-\n-```bash\n-git clone https://github.com/pytorch/serve.git\n-cd serve\n-./build_image.sh\n ```\n-\n-The above command builds the TorchServe image for CPU device with `master` branch\n-\n-To create image for specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name>\n+pip install torchserve torch-model-archiver\n ```\n \n-To create image for GPU device use following command :\n-```bash\n-./build_image.sh --gpu\n-```\n-\n-To create image for GPU device with specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name> --gpu\n-```\n-\n-**Running docker image and starting TorchServe inside container with pre-registered resnet-18 image classification model**\n-\n-```bash\n-./start.sh\n-```\n-\n-**For pre-trained and pre-packaged models-archives refer [TorchServe model zoo](docs/model_zoo.md)**\n-**For managing models with TorchServe refer [management api documentation](docs/management_api.md)**\n-**For running inference on registered models with TorchServe refer [inference api documentation](docs/inference_api.md)**\n-\n-## Quick Start for local environment\n-\n-### Prerequisites\n-\n-Before proceeding further with this document, make sure you have the following prerequisites.\n-\n-1. Ubuntu, CentOS, or macOS. Windows support is experimental. The following instructions will focus on Linux and macOS only.\n-1. Python     - TorchServe requires python to run the workers.\n-1. pip        - Pip is a python package management system.\n-1. Java 11    - TorchServe requires Java 11 to start. You have the following options for installing Java 11:\n-\n-    For Ubuntu:\n+### Install with Conda\n+_Ubuntu_\n \n+1. Install Java 11\n     ```bash\n     sudo apt-get install openjdk-11-jdk\n     ```\n-\n-    For CentOS:\n-\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n     ```bash\n-    openjdk-11-jdk\n-    sudo yum install java-11-openjdk\n+    source activate torchserve\n     ```\n \n-    For macOS\n+_macOS_\n \n+1. Install Java 11\n     ```bash\n     brew tap AdoptOpenJDK/openjdk\n     brew cask install adoptopenjdk11\n     ```\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n+    ```bash\n+    source activate torchserve\n+    ```\n \n-### Installing TorchServe with pip\n+Now you are ready to [package and serve models with TorchServe](#serve-a-model).\n \n-#### Setup\n+### Install TorchServe for development\n \n-**Step 1:** Setup a Virtual Environment\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n-We recommend installing and running TorchServe in a virtual environment. It's a good practice to run and install all of the Python dependencies in virtual environments. This will provide isolation of the dependencies and ease dependency management.\n+```bash\n+pip install -e .\n+```\n \n-* **Use Virtualenv** : This is used to create virtual Python environments. You may install and activate a virtualenv for Python 3.7 as follows:\n+* To develop with torch-model-archiver:\n \n ```bash\n-pip install virtualenv\n+cd serve/model-archiver\n+pip install -e .\n ```\n \n-Then create a virtual environment:\n+To upgrade TorchServe or model archiver from source code and make changes executable, run:\n \n ```bash\n-# Assuming we want to run python3.7 in /usr/local/bin/python3.7\n-virtualenv -p /usr/local/bin/python3.7 /tmp/pyenv3\n-# Enter this virtual environment as follows\n-source /tmp/pyenv3/bin/activate\n+pip install -U -e .\n ```\n \n-Refer to the [Virtualenv documentation](https://virtualenv.pypa.io/en/stable/) for further information.\n+For information about the model archiver, see [detailed documentation](model-archiver/README.md).\n+\n+## Serve a model\n \n-* **Use Anaconda** : This is package, dependency and environment manager. You may download and install Anaconda as follows :\n-[Download anaconda distribution](https://www.anaconda.com/distribution/#download-section)\n+This section shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver.\n \n-Then create a virtual environment using conda.\n+To run this example, clone the TorchServe repository and navigate to the root of the repository:\n \n ```bash\n-conda create -n myenv\n-source activate myenv\n+git clone https://github.com/pytorch/serve.git\n+cd serve\n ```\n \n-**Step 2:** Install torch\n+Then run the following steps from the root of the repository.\n+\n+### Store a Model\n \n-TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n \n-* For virtualenv\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n \n ```bash\n-#For CPU/GPU\n-pip install torch torchvision torchtext\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n+mkdir model_store\n+mv densenet161.mar model_store/\n ```\n \n-* For conda\n+For more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n \n-The `torchtext` package has a dependency on `sentencepiece`, which is not available via Anaconda. You can install it via `pip`:\n+### Start TorchServe to serve the model\n+\n+After you archive and store the model, use the `torchserve` command to serve the model.\n \n ```bash\n-pip install sentencepiece\n+torchserve --start --model-store model_store --models densenet161=densenet161.mar\n ```\n \n+After you execute the `torchserve` command above, TorchServe runs on your host, listening for inference requests.\n+\n+**Note**: If you specify model(s) when you run TorchServe, it automatically scales backend workers to the number equal to available vCPUs (if you run on a CPU instance) or to the number of available GPUs (if you run on a GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs). This start up and autoscaling process might take considerable time. If you want to minimize TorchServe start up time you avoid registering and scaling the model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model), which allows finer grain control of the resources that are allocated for any particular model).\n+\n+### Get predictions from a model\n+\n+To test the model server, send a request to the server's `predictions` API.\n+\n+Complete the following steps:\n+\n+* Open a new terminal window (other than the one running TorchServe).\n+* Use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images)\n+  and use the  `-o` flag to name it `kitten.jpg` for you.\n+* Use `curl` to send `POST` to the TorchServe `predict` endpoint with the kitten's image.\n+\n+![kitten](docs/images/kitten_small.jpg)\n+\n+The following code completes all three steps:\n+\n ```bash\n-#For CPU\n-conda install psutil pytorch torchvision torchtext -c pytorch\n+curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n+curl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n+```\n+\n+The predict endpoint returns a prediction response in JSON. It will look something like the following result:\n+\n+```json\n+[\n+  {\n+    \"tiger_cat\": 0.46933549642562866\n+  },\n+  {\n+    \"tabby\": 0.4633878469467163\n+  },\n+  {\n+    \"Egyptian_cat\": 0.06456148624420166\n+  },\n+  {\n+    \"lynx\": 0.0012828214094042778\n+  },\n+  {\n+    \"plastic_bag\": 0.00023323034110944718\n+  }\n+]\n ```\n \n+You will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n+\n+Now you've seen how easy it can be to serve a deep learning model with TorchServe! [Would you like to know more?](docs/server.md)\n+\n+### Stop the running TorchServe\n+\n+To stop the currently running TorchServe instance, run the following command:\n+\n ```bash\n-#For GPU\n-conda install future psutil pytorch torchvision cudatoolkit=10.1 torchtext -c pytorch\n+torchserve --stop\n ```\n \n-**Step 3:** Install TorchServe as follows:\n+You see output specifying that TorchServe has stopped.\n+\n+## Quick Start with Docker\n+\n+### Prerequisites\n+\n+* docker - Refer to the [official docker installation guide](https://docs.docker.com/install/)\n+* git    - Refer to the [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n+* TorchServe source code. Clone and enter the repo as follows:\n \n ```bash\n git clone https://github.com/pytorch/serve.git\n cd serve\n-pip install .\n ```\n \n-**Notes:**\n-\n-* If `pip install .`  fails, run `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n-* See the [advanced installation](docs/install.md) page for more options and troubleshooting.\n+### Build the TorchServe Docker image\n \n-### Install torch-model-archiver\n+The following are examples on how to use the `build_image.sh` script to build Docker images to support CPU or GPU inference.\n \n-* Install torch-model-archiver as follows:\n+Build the TorchServe image for a CPU device using the `master` branch:\n \n ```bash\n-cd serve/model-archiver\n-pip install .\n+./build_image.sh\n ```\n \n-For information about the model archiver, see [detailed documentation](model-archiver/README.md).\n+Create a Docker image for a specific branch, use the following command:\n \n-### Install TorchServe for development\n+```bash\n+./build_image.sh -b <branch_name>\n+```\n \n-If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n+To create a Docker image for a GPU device, use the following command:\n \n ```bash\n-pip install -e .\n+./build_image.sh --gpu\n ```\n \n-To upgrade TorchServe from source code and make changes executable, run:\n+To create a Docker image for a GPU device with a specific branch, use following command:\n \n ```bash\n-pip install -U -e .\n+./build_image.sh -b <branch_name> --gpu\n ```\n \n-## Troubleshoot Installation\n-\n-Here is an easy example for serving an object classification model (make sure to run it at the root of the repository):\n+To run your TorchServe Docker image and start TorchServe inside the container with a pre-registered resnet-18 image classification model, use the following command:\n \n ```bash\n-wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n-torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n-mkdir model_store\n-mv densenet161.mar model_store/\n-torchserve --start --model-store model_store --models densenet161=densenet161.mar\n+./start.sh", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 339}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDcxNzE5NQ==", "bodyText": "resolving this since the fix will come later and we'll update accordingly", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410717195", "createdAt": "2020-04-18T16:13:28Z", "author": {"login": "aaronmarkham"}, "path": "README.md", "diffHunk": "@@ -2,208 +2,237 @@\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).\n+**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n ## Contents of this Document\n \n * [Install TorchServe](#install-torchserve)\n-* [Quick Start with docker](#quick-start-with-docker)\n-* [Quick Start for local environment](#quick-start-guide-for-local-environment)\n * [Serve a Model](#serve-a-model)\n-* [Other Features](#other-features)\n+* [Quick start with docker](#quick-start-with-docker)\n * [Contributing](#contributing)\n \n ## Install TorchServe\n \n-## Quick Start with docker\n+Conda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n+**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n \n-### Start TorchServe using docker image\n+### Install with pip\n+To use `pip` to install TorchServe and the model archiver:\n \n-#### Prerequisites\n-\n-* docker - Refer [official docker installation guide](https://docs.docker.com/install/)\n-* git    - Refer [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n-\n-#### Building docker image\n-\n-```bash\n-git clone https://github.com/pytorch/serve.git\n-cd serve\n-./build_image.sh\n ```\n-\n-The above command builds the TorchServe image for CPU device with `master` branch\n-\n-To create image for specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name>\n+pip install torchserve torch-model-archiver\n ```\n \n-To create image for GPU device use following command :\n-```bash\n-./build_image.sh --gpu\n-```\n-\n-To create image for GPU device with specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name> --gpu\n-```\n-\n-**Running docker image and starting TorchServe inside container with pre-registered resnet-18 image classification model**\n-\n-```bash\n-./start.sh\n-```\n-\n-**For pre-trained and pre-packaged models-archives refer [TorchServe model zoo](docs/model_zoo.md)**\n-**For managing models with TorchServe refer [management api documentation](docs/management_api.md)**\n-**For running inference on registered models with TorchServe refer [inference api documentation](docs/inference_api.md)**\n-\n-## Quick Start for local environment\n-\n-### Prerequisites\n-\n-Before proceeding further with this document, make sure you have the following prerequisites.\n-\n-1. Ubuntu, CentOS, or macOS. Windows support is experimental. The following instructions will focus on Linux and macOS only.\n-1. Python     - TorchServe requires python to run the workers.\n-1. pip        - Pip is a python package management system.\n-1. Java 11    - TorchServe requires Java 11 to start. You have the following options for installing Java 11:\n-\n-    For Ubuntu:\n+### Install with Conda\n+_Ubuntu_\n \n+1. Install Java 11\n     ```bash\n     sudo apt-get install openjdk-11-jdk\n     ```\n-\n-    For CentOS:\n-\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n     ```bash\n-    openjdk-11-jdk\n-    sudo yum install java-11-openjdk\n+    source activate torchserve\n     ```\n \n-    For macOS\n+_macOS_\n \n+1. Install Java 11\n     ```bash\n     brew tap AdoptOpenJDK/openjdk\n     brew cask install adoptopenjdk11\n     ```\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n+    ```bash\n+    source activate torchserve\n+    ```\n \n-### Installing TorchServe with pip\n+Now you are ready to [package and serve models with TorchServe](#serve-a-model).\n \n-#### Setup\n+### Install TorchServe for development\n \n-**Step 1:** Setup a Virtual Environment\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n-We recommend installing and running TorchServe in a virtual environment. It's a good practice to run and install all of the Python dependencies in virtual environments. This will provide isolation of the dependencies and ease dependency management.\n+```bash\n+pip install -e .\n+```\n \n-* **Use Virtualenv** : This is used to create virtual Python environments. You may install and activate a virtualenv for Python 3.7 as follows:\n+* To develop with torch-model-archiver:\n \n ```bash\n-pip install virtualenv\n+cd serve/model-archiver\n+pip install -e .\n ```\n \n-Then create a virtual environment:\n+To upgrade TorchServe or model archiver from source code and make changes executable, run:\n \n ```bash\n-# Assuming we want to run python3.7 in /usr/local/bin/python3.7\n-virtualenv -p /usr/local/bin/python3.7 /tmp/pyenv3\n-# Enter this virtual environment as follows\n-source /tmp/pyenv3/bin/activate\n+pip install -U -e .\n ```\n \n-Refer to the [Virtualenv documentation](https://virtualenv.pypa.io/en/stable/) for further information.\n+For information about the model archiver, see [detailed documentation](model-archiver/README.md).\n+\n+## Serve a model\n \n-* **Use Anaconda** : This is package, dependency and environment manager. You may download and install Anaconda as follows :\n-[Download anaconda distribution](https://www.anaconda.com/distribution/#download-section)\n+This section shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver.\n \n-Then create a virtual environment using conda.\n+To run this example, clone the TorchServe repository and navigate to the root of the repository:\n \n ```bash\n-conda create -n myenv\n-source activate myenv\n+git clone https://github.com/pytorch/serve.git\n+cd serve\n ```\n \n-**Step 2:** Install torch\n+Then run the following steps from the root of the repository.\n+\n+### Store a Model\n \n-TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n \n-* For virtualenv\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n \n ```bash\n-#For CPU/GPU\n-pip install torch torchvision torchtext\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n+mkdir model_store\n+mv densenet161.mar model_store/\n ```\n \n-* For conda\n+For more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n \n-The `torchtext` package has a dependency on `sentencepiece`, which is not available via Anaconda. You can install it via `pip`:\n+### Start TorchServe to serve the model\n+\n+After you archive and store the model, use the `torchserve` command to serve the model.\n \n ```bash\n-pip install sentencepiece\n+torchserve --start --model-store model_store --models densenet161=densenet161.mar\n ```\n \n+After you execute the `torchserve` command above, TorchServe runs on your host, listening for inference requests.\n+\n+**Note**: If you specify model(s) when you run TorchServe, it automatically scales backend workers to the number equal to available vCPUs (if you run on a CPU instance) or to the number of available GPUs (if you run on a GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs). This start up and autoscaling process might take considerable time. If you want to minimize TorchServe start up time you avoid registering and scaling the model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model), which allows finer grain control of the resources that are allocated for any particular model).\n+\n+### Get predictions from a model\n+\n+To test the model server, send a request to the server's `predictions` API.\n+\n+Complete the following steps:\n+\n+* Open a new terminal window (other than the one running TorchServe).\n+* Use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images)\n+  and use the  `-o` flag to name it `kitten.jpg` for you.\n+* Use `curl` to send `POST` to the TorchServe `predict` endpoint with the kitten's image.\n+\n+![kitten](docs/images/kitten_small.jpg)\n+\n+The following code completes all three steps:\n+\n ```bash\n-#For CPU\n-conda install psutil pytorch torchvision torchtext -c pytorch\n+curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n+curl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n+```\n+\n+The predict endpoint returns a prediction response in JSON. It will look something like the following result:\n+\n+```json\n+[\n+  {\n+    \"tiger_cat\": 0.46933549642562866\n+  },\n+  {\n+    \"tabby\": 0.4633878469467163\n+  },\n+  {\n+    \"Egyptian_cat\": 0.06456148624420166\n+  },\n+  {\n+    \"lynx\": 0.0012828214094042778\n+  },\n+  {\n+    \"plastic_bag\": 0.00023323034110944718\n+  }\n+]\n ```\n \n+You will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n+\n+Now you've seen how easy it can be to serve a deep learning model with TorchServe! [Would you like to know more?](docs/server.md)\n+\n+### Stop the running TorchServe\n+\n+To stop the currently running TorchServe instance, run the following command:\n+\n ```bash\n-#For GPU\n-conda install future psutil pytorch torchvision cudatoolkit=10.1 torchtext -c pytorch\n+torchserve --stop\n ```\n \n-**Step 3:** Install TorchServe as follows:\n+You see output specifying that TorchServe has stopped.\n+\n+## Quick Start with Docker\n+\n+### Prerequisites\n+\n+* docker - Refer to the [official docker installation guide](https://docs.docker.com/install/)\n+* git    - Refer to the [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n+* TorchServe source code. Clone and enter the repo as follows:\n \n ```bash\n git clone https://github.com/pytorch/serve.git\n cd serve\n-pip install .\n ```\n \n-**Notes:**\n-\n-* If `pip install .`  fails, run `python setup.py install` and install the following python packages using `pip install` : Pillow, psutil, future\n-* See the [advanced installation](docs/install.md) page for more options and troubleshooting.\n+### Build the TorchServe Docker image\n \n-### Install torch-model-archiver\n+The following are examples on how to use the `build_image.sh` script to build Docker images to support CPU or GPU inference.\n \n-* Install torch-model-archiver as follows:\n+Build the TorchServe image for a CPU device using the `master` branch:\n \n ```bash\n-cd serve/model-archiver\n-pip install .\n+./build_image.sh\n ```\n \n-For information about the model archiver, see [detailed documentation](model-archiver/README.md).\n+Create a Docker image for a specific branch, use the following command:\n \n-### Install TorchServe for development\n+```bash\n+./build_image.sh -b <branch_name>\n+```\n \n-If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n+To create a Docker image for a GPU device, use the following command:\n \n ```bash\n-pip install -e .\n+./build_image.sh --gpu\n ```\n \n-To upgrade TorchServe from source code and make changes executable, run:\n+To create a Docker image for a GPU device with a specific branch, use following command:\n \n ```bash\n-pip install -U -e .\n+./build_image.sh -b <branch_name> --gpu\n ```\n \n-## Troubleshoot Installation\n-\n-Here is an easy example for serving an object classification model (make sure to run it at the root of the repository):\n+To run your TorchServe Docker image and start TorchServe inside the container with a pre-registered resnet-18 image classification model, use the following command:\n \n ```bash\n-wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n-torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier\n-mkdir model_store\n-mv densenet161.mar model_store/\n-torchserve --start --model-store model_store --models densenet161=densenet161.mar\n+./start.sh", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQyNzI4Ng=="}, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 339}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODYyODcxOnYy", "diffSide": "RIGHT", "path": "README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTo1MzozNFrOGHeNBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMTo1MzozNFrOGHeNBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ4ODA3MQ==", "bodyText": "This should also include the step of checking out the source repository and moving into that folder:\ngit clone https://github.com/pytorch/serve\ncd serve", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410488071", "createdAt": "2020-04-17T21:53:34Z", "author": {"login": "fbbradheintz"}, "path": "README.md", "diffHunk": "@@ -2,208 +2,237 @@\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).\n+**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n ## Contents of this Document\n \n * [Install TorchServe](#install-torchserve)\n-* [Quick Start with docker](#quick-start-with-docker)\n-* [Quick Start for local environment](#quick-start-guide-for-local-environment)\n * [Serve a Model](#serve-a-model)\n-* [Other Features](#other-features)\n+* [Quick start with docker](#quick-start-with-docker)\n * [Contributing](#contributing)\n \n ## Install TorchServe\n \n-## Quick Start with docker\n+Conda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n+**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n \n-### Start TorchServe using docker image\n+### Install with pip\n+To use `pip` to install TorchServe and the model archiver:\n \n-#### Prerequisites\n-\n-* docker - Refer [official docker installation guide](https://docs.docker.com/install/)\n-* git    - Refer [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n-\n-#### Building docker image\n-\n-```bash\n-git clone https://github.com/pytorch/serve.git\n-cd serve\n-./build_image.sh\n ```\n-\n-The above command builds the TorchServe image for CPU device with `master` branch\n-\n-To create image for specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name>\n+pip install torchserve torch-model-archiver\n ```\n \n-To create image for GPU device use following command :\n-```bash\n-./build_image.sh --gpu\n-```\n-\n-To create image for GPU device with specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name> --gpu\n-```\n-\n-**Running docker image and starting TorchServe inside container with pre-registered resnet-18 image classification model**\n-\n-```bash\n-./start.sh\n-```\n-\n-**For pre-trained and pre-packaged models-archives refer [TorchServe model zoo](docs/model_zoo.md)**\n-**For managing models with TorchServe refer [management api documentation](docs/management_api.md)**\n-**For running inference on registered models with TorchServe refer [inference api documentation](docs/inference_api.md)**\n-\n-## Quick Start for local environment\n-\n-### Prerequisites\n-\n-Before proceeding further with this document, make sure you have the following prerequisites.\n-\n-1. Ubuntu, CentOS, or macOS. Windows support is experimental. The following instructions will focus on Linux and macOS only.\n-1. Python     - TorchServe requires python to run the workers.\n-1. pip        - Pip is a python package management system.\n-1. Java 11    - TorchServe requires Java 11 to start. You have the following options for installing Java 11:\n-\n-    For Ubuntu:\n+### Install with Conda\n+_Ubuntu_\n \n+1. Install Java 11\n     ```bash\n     sudo apt-get install openjdk-11-jdk\n     ```\n-\n-    For CentOS:\n-\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n     ```bash\n-    openjdk-11-jdk\n-    sudo yum install java-11-openjdk\n+    source activate torchserve\n     ```\n \n-    For macOS\n+_macOS_\n \n+1. Install Java 11\n     ```bash\n     brew tap AdoptOpenJDK/openjdk\n     brew cask install adoptopenjdk11\n     ```\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n+    ```bash\n+    source activate torchserve\n+    ```\n \n-### Installing TorchServe with pip\n+Now you are ready to [package and serve models with TorchServe](#serve-a-model).\n \n-#### Setup\n+### Install TorchServe for development\n \n-**Step 1:** Setup a Virtual Environment\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n-We recommend installing and running TorchServe in a virtual environment. It's a good practice to run and install all of the Python dependencies in virtual environments. This will provide isolation of the dependencies and ease dependency management.\n+```bash", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODY0NzUzOnYy", "diffSide": "RIGHT", "path": "README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjowMDo1OFrOGHeX5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMzoxNTo0NFrOGHkCWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5MDg1NA==", "bodyText": "This is a minor usability nitpick: If someone has installed with conda or pip, then this has them creating the model store (and, if they start TorchServe, their logs & metrics folder) inside the cloned repository, rather than in the folder where they'd rather have these things.\nThese instructions could benefit from two tweaks:\n\nSkipping the cd serve step - it's extraneous here\nChanging the file paths in the torch-model-archiver command to be serve/examples/blah/blah rather than examples/blah/blah", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410490854", "createdAt": "2020-04-17T22:00:58Z", "author": {"login": "fbbradheintz"}, "path": "README.md", "diffHunk": "@@ -2,208 +2,237 @@\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).\n+**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n ## Contents of this Document\n \n * [Install TorchServe](#install-torchserve)\n-* [Quick Start with docker](#quick-start-with-docker)\n-* [Quick Start for local environment](#quick-start-guide-for-local-environment)\n * [Serve a Model](#serve-a-model)\n-* [Other Features](#other-features)\n+* [Quick start with docker](#quick-start-with-docker)\n * [Contributing](#contributing)\n \n ## Install TorchServe\n \n-## Quick Start with docker\n+Conda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n+**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n \n-### Start TorchServe using docker image\n+### Install with pip\n+To use `pip` to install TorchServe and the model archiver:\n \n-#### Prerequisites\n-\n-* docker - Refer [official docker installation guide](https://docs.docker.com/install/)\n-* git    - Refer [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n-\n-#### Building docker image\n-\n-```bash\n-git clone https://github.com/pytorch/serve.git\n-cd serve\n-./build_image.sh\n ```\n-\n-The above command builds the TorchServe image for CPU device with `master` branch\n-\n-To create image for specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name>\n+pip install torchserve torch-model-archiver\n ```\n \n-To create image for GPU device use following command :\n-```bash\n-./build_image.sh --gpu\n-```\n-\n-To create image for GPU device with specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name> --gpu\n-```\n-\n-**Running docker image and starting TorchServe inside container with pre-registered resnet-18 image classification model**\n-\n-```bash\n-./start.sh\n-```\n-\n-**For pre-trained and pre-packaged models-archives refer [TorchServe model zoo](docs/model_zoo.md)**\n-**For managing models with TorchServe refer [management api documentation](docs/management_api.md)**\n-**For running inference on registered models with TorchServe refer [inference api documentation](docs/inference_api.md)**\n-\n-## Quick Start for local environment\n-\n-### Prerequisites\n-\n-Before proceeding further with this document, make sure you have the following prerequisites.\n-\n-1. Ubuntu, CentOS, or macOS. Windows support is experimental. The following instructions will focus on Linux and macOS only.\n-1. Python     - TorchServe requires python to run the workers.\n-1. pip        - Pip is a python package management system.\n-1. Java 11    - TorchServe requires Java 11 to start. You have the following options for installing Java 11:\n-\n-    For Ubuntu:\n+### Install with Conda\n+_Ubuntu_\n \n+1. Install Java 11\n     ```bash\n     sudo apt-get install openjdk-11-jdk\n     ```\n-\n-    For CentOS:\n-\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n     ```bash\n-    openjdk-11-jdk\n-    sudo yum install java-11-openjdk\n+    source activate torchserve\n     ```\n \n-    For macOS\n+_macOS_\n \n+1. Install Java 11\n     ```bash\n     brew tap AdoptOpenJDK/openjdk\n     brew cask install adoptopenjdk11\n     ```\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n+    ```bash\n+    source activate torchserve\n+    ```\n \n-### Installing TorchServe with pip\n+Now you are ready to [package and serve models with TorchServe](#serve-a-model).\n \n-#### Setup\n+### Install TorchServe for development\n \n-**Step 1:** Setup a Virtual Environment\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n-We recommend installing and running TorchServe in a virtual environment. It's a good practice to run and install all of the Python dependencies in virtual environments. This will provide isolation of the dependencies and ease dependency management.\n+```bash\n+pip install -e .\n+```\n \n-* **Use Virtualenv** : This is used to create virtual Python environments. You may install and activate a virtualenv for Python 3.7 as follows:\n+* To develop with torch-model-archiver:\n \n ```bash\n-pip install virtualenv\n+cd serve/model-archiver\n+pip install -e .\n ```\n \n-Then create a virtual environment:\n+To upgrade TorchServe or model archiver from source code and make changes executable, run:\n \n ```bash\n-# Assuming we want to run python3.7 in /usr/local/bin/python3.7\n-virtualenv -p /usr/local/bin/python3.7 /tmp/pyenv3\n-# Enter this virtual environment as follows\n-source /tmp/pyenv3/bin/activate\n+pip install -U -e .\n ```\n \n-Refer to the [Virtualenv documentation](https://virtualenv.pypa.io/en/stable/) for further information.\n+For information about the model archiver, see [detailed documentation](model-archiver/README.md).\n+\n+## Serve a model\n \n-* **Use Anaconda** : This is package, dependency and environment manager. You may download and install Anaconda as follows :\n-[Download anaconda distribution](https://www.anaconda.com/distribution/#download-section)\n+This section shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver.\n \n-Then create a virtual environment using conda.\n+To run this example, clone the TorchServe repository and navigate to the root of the repository:\n \n ```bash\n-conda create -n myenv\n-source activate myenv\n+git clone https://github.com/pytorch/serve.git\n+cd serve\n ```\n \n-**Step 2:** Install torch\n+Then run the following steps from the root of the repository.\n+\n+### Store a Model\n \n-TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n \n-* For virtualenv\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n \n ```bash\n-#For CPU/GPU\n-pip install torch torchvision torchtext\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU4MzY0Mw==", "bodyText": "Ya, not a fan of this flow really. Will update.", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410583643", "createdAt": "2020-04-18T03:15:44Z", "author": {"login": "aaronmarkham"}, "path": "README.md", "diffHunk": "@@ -2,208 +2,237 @@\n \n TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n-For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).\n+**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n ## Contents of this Document\n \n * [Install TorchServe](#install-torchserve)\n-* [Quick Start with docker](#quick-start-with-docker)\n-* [Quick Start for local environment](#quick-start-guide-for-local-environment)\n * [Serve a Model](#serve-a-model)\n-* [Other Features](#other-features)\n+* [Quick start with docker](#quick-start-with-docker)\n * [Contributing](#contributing)\n \n ## Install TorchServe\n \n-## Quick Start with docker\n+Conda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n+**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n \n-### Start TorchServe using docker image\n+### Install with pip\n+To use `pip` to install TorchServe and the model archiver:\n \n-#### Prerequisites\n-\n-* docker - Refer [official docker installation guide](https://docs.docker.com/install/)\n-* git    - Refer [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n-\n-#### Building docker image\n-\n-```bash\n-git clone https://github.com/pytorch/serve.git\n-cd serve\n-./build_image.sh\n ```\n-\n-The above command builds the TorchServe image for CPU device with `master` branch\n-\n-To create image for specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name>\n+pip install torchserve torch-model-archiver\n ```\n \n-To create image for GPU device use following command :\n-```bash\n-./build_image.sh --gpu\n-```\n-\n-To create image for GPU device with specific branch use following command :\n-```bash\n-./build_image.sh -b <branch_name> --gpu\n-```\n-\n-**Running docker image and starting TorchServe inside container with pre-registered resnet-18 image classification model**\n-\n-```bash\n-./start.sh\n-```\n-\n-**For pre-trained and pre-packaged models-archives refer [TorchServe model zoo](docs/model_zoo.md)**\n-**For managing models with TorchServe refer [management api documentation](docs/management_api.md)**\n-**For running inference on registered models with TorchServe refer [inference api documentation](docs/inference_api.md)**\n-\n-## Quick Start for local environment\n-\n-### Prerequisites\n-\n-Before proceeding further with this document, make sure you have the following prerequisites.\n-\n-1. Ubuntu, CentOS, or macOS. Windows support is experimental. The following instructions will focus on Linux and macOS only.\n-1. Python     - TorchServe requires python to run the workers.\n-1. pip        - Pip is a python package management system.\n-1. Java 11    - TorchServe requires Java 11 to start. You have the following options for installing Java 11:\n-\n-    For Ubuntu:\n+### Install with Conda\n+_Ubuntu_\n \n+1. Install Java 11\n     ```bash\n     sudo apt-get install openjdk-11-jdk\n     ```\n-\n-    For CentOS:\n-\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n     ```bash\n-    openjdk-11-jdk\n-    sudo yum install java-11-openjdk\n+    source activate torchserve\n     ```\n \n-    For macOS\n+_macOS_\n \n+1. Install Java 11\n     ```bash\n     brew tap AdoptOpenJDK/openjdk\n     brew cask install adoptopenjdk11\n     ```\n+1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n+1. Create an environment and install torchserve and torch-model-archiver\n+    ```bash\n+    conda create --name torchserve torchserve torch-model-archiver -c pytorch\n+    ```\n+1. Activate the environment\n+    ```bash\n+    source activate torchserve\n+    ```\n \n-### Installing TorchServe with pip\n+Now you are ready to [package and serve models with TorchServe](#serve-a-model).\n \n-#### Setup\n+### Install TorchServe for development\n \n-**Step 1:** Setup a Virtual Environment\n+If you plan to develop with TorchServe and change some of the source code, install it from source code and make your changes executable with this command:\n \n-We recommend installing and running TorchServe in a virtual environment. It's a good practice to run and install all of the Python dependencies in virtual environments. This will provide isolation of the dependencies and ease dependency management.\n+```bash\n+pip install -e .\n+```\n \n-* **Use Virtualenv** : This is used to create virtual Python environments. You may install and activate a virtualenv for Python 3.7 as follows:\n+* To develop with torch-model-archiver:\n \n ```bash\n-pip install virtualenv\n+cd serve/model-archiver\n+pip install -e .\n ```\n \n-Then create a virtual environment:\n+To upgrade TorchServe or model archiver from source code and make changes executable, run:\n \n ```bash\n-# Assuming we want to run python3.7 in /usr/local/bin/python3.7\n-virtualenv -p /usr/local/bin/python3.7 /tmp/pyenv3\n-# Enter this virtual environment as follows\n-source /tmp/pyenv3/bin/activate\n+pip install -U -e .\n ```\n \n-Refer to the [Virtualenv documentation](https://virtualenv.pypa.io/en/stable/) for further information.\n+For information about the model archiver, see [detailed documentation](model-archiver/README.md).\n+\n+## Serve a model\n \n-* **Use Anaconda** : This is package, dependency and environment manager. You may download and install Anaconda as follows :\n-[Download anaconda distribution](https://www.anaconda.com/distribution/#download-section)\n+This section shows a simple example of serving a model with TorchServe. To complete this example, you must have already installed TorchServe and the model archiver.\n \n-Then create a virtual environment using conda.\n+To run this example, clone the TorchServe repository and navigate to the root of the repository:\n \n ```bash\n-conda create -n myenv\n-source activate myenv\n+git clone https://github.com/pytorch/serve.git\n+cd serve\n ```\n \n-**Step 2:** Install torch\n+Then run the following steps from the root of the repository.\n+\n+### Store a Model\n \n-TorchServe won't install the PyTorch engine by default. If it isn't already installed in your virtual environment, you must install the PyTorch pip packages.\n+To serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\n+You can also create model stores to store your archived models.\n \n-* For virtualenv\n+The following code gets a trained model, archives the model by using the model archiver, and then stores the model in a model store.\n \n ```bash\n-#For CPU/GPU\n-pip install torch torchvision torchtext\n+wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n+torch-model-archiver --model-name densenet161 --version 1.0 --model-file examples/image_classifier/densenet_161/model.py --serialized-file densenet161-8d451a50.pth --extra-files examples/image_classifier/index_to_name.json --handler image_classifier", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5MDg1NA=="}, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODY3MDk4OnYy", "diffSide": "RIGHT", "path": "docs/custom_service.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjoxMjoyMFrOGHemcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQxNjo0OTo1MFrOGHsdFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5NDU3Ng==", "bodyText": "Two things:\n\nThere's a weird comma in there\nEveryplace else, we talk about making \"custom handlers\" - the model handler code is the custom part that interfaces with TorchServe. Do we want to change the \"custom service\" verbiage to \"custom handler\"?", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410494576", "createdAt": "2020-04-17T22:12:20Z", "author": {"login": "fbbradheintz"}, "path": "docs/custom_service.md", "diffHunk": "@@ -9,16 +9,16 @@\n \n ## Introduction\n \n-A custom service , is the code that is packaged into model archive, that is executed by Model Server for PyTorch (TorchServe). \n+A custom service , is the code that is packaged into model archive, that is executed by Model Server for PyTorch (TorchServe).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDcyMTU1Nw==", "bodyText": "I fixed the comma, but swapping custom service out is too big of a change. If someone wants to go for changing all the py files where custom service is used, then it should be a separate PR, and probably not just before launch.", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410721557", "createdAt": "2020-04-18T16:49:50Z", "author": {"login": "aaronmarkham"}, "path": "docs/custom_service.md", "diffHunk": "@@ -9,16 +9,16 @@\n \n ## Introduction\n \n-A custom service , is the code that is packaged into model archive, that is executed by Model Server for PyTorch (TorchServe). \n+A custom service , is the code that is packaged into model archive, that is executed by Model Server for PyTorch (TorchServe).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5NDU3Ng=="}, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODY3NzAzOnYy", "diffSide": "RIGHT", "path": "examples/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjoxNToyOVrOGHeqIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjoxNToyOVrOGHeqIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5NTUyMw==", "bodyText": "Was there meant to be a link associated with \"[mar]\"?", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410495523", "createdAt": "2020-04-17T22:15:29Z", "author": {"login": "fbbradheintz"}, "path": "examples/README.md", "diffHunk": "@@ -1,18 +1,15 @@\n # Contents of this Document\n * [Creating mar file for an eager mode model](#creating-mar-file-for-eager-mode-model)\n-* [Creating mar file for an eager mode model](#creating-mar-file-for-torchscript-mode-model)\n-* [Serving torchvision image classification models in TorchServe](#serving-torchvision-image-classification-models-in-torchserve)\n-  * [Serving densenet161 model](#serving-torchvision-image-classification-models)\n-  * [Serving resnet18 model](#example-to-serve-resnet18-image-classification-model)\n+* [Creating mar file for torchscript mode model](#creating-mar-file-for-torchscript-mode-model)\n+* [Serving torchvision image classification models in TorchServe](#examples-torchvision-image-classification-models-in-torchserve)\n * [Serving custom model with custom service handler](#example-to-serve-a-custom-model-with-custom-service-handler)\n * [Serving text classification model](#example-to-serve-text-classification-model)\n * [Serving object detection model](#example-to-serve-object-detection-model)\n * [Serving image segmentation model](#example-to-serve-image-segmentation-model)\n-* [Serving speech synthesis model](#example-to-serve-speech-synthesis-model)\n \n # TorchServe Examples\n \n-The following are examples on how to create and serve model archives with TorchServe.\n+The following are examples on how to create and serve model archives [mar] with TorchServe.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODY4MDM0OnYy", "diffSide": "RIGHT", "path": "examples/image_classifier/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjoxNzowMFrOGHesFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjoxNzowMFrOGHesFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5NjAyMg==", "bodyText": "This should be a \"model archive file\".", "url": "https://github.com/pytorch/serve/pull/212#discussion_r410496022", "createdAt": "2020-04-17T22:17:00Z", "author": {"login": "fbbradheintz"}, "path": "examples/image_classifier/README.md", "diffHunk": "@@ -2,15 +2,15 @@\n \n * TorchVision Image Classification Models : Download a pre-trained model state_dict for computer vision model that classifies images from the following :\n \n-  * [Image Classification with AlexNet](image_classifier/alexnet) - https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\n-  * [Image Classification with DenseNet161](image_classifier/densenet_161) - https://download.pytorch.org/models/densenet161-8d451a50.pth\n-  * [Image Classification with ResNet18](image_classifier/resnet_18) - https://download.pytorch.org/models/resnet18-5c106cde.pth\n-  * [Image Classification with SqueezeNet 1_1](image_classifier/squeezenet) - https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\n-  * [Image Classification with VGG11](image_classifier/vgg_11) - https://download.pytorch.org/models/vgg11-bbd30ac9.pth\n+  * [Image Classification with AlexNet](alexnet) - https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\n+  * [Image Classification with DenseNet161](densenet_161) - https://download.pytorch.org/models/densenet161-8d451a50.pth\n+  * [Image Classification with ResNet18](resnet_18) - https://download.pytorch.org/models/resnet18-5c106cde.pth\n+  * [Image Classification with SqueezeNet 1_1](squeezenet) - https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\n+  * [Image Classification with VGG11](vgg_11) - https://download.pytorch.org/models/vgg11-bbd30ac9.pth\n \n * Create a model architecture file (model-file) based on selected model or use the sample provided with above examples.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa074d3c61c64d8a23c83cafb9a5cfeba7f6709b"}, "originalPosition": 15}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1596, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}