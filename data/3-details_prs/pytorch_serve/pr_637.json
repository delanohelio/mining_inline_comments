{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcyMDM4OTAz", "number": 637, "title": " FAQ and Troubleshooting Guide", "bodyText": "Description\n\nAdds FAQ doc\nAdds Troubleshooting guide.\n\nFixes #384 #215\nType of change\nPlease delete options that are not relevant.\n\n Bug fix (non-breaking change which fixes an issue)\n New feature (non-breaking change which adds functionality)\n Breaking change (fix or feature that would cause existing functionality to not work as expected)\n This change requires a documentation update\n\nFeature/Issue validation/testing\nPlease describe the tests [UT/IT] that you ran to verify your changes and relevent result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n\n Test A\n\n\n Test B\n\n\nUT/IT execution results\n\n\nLogs\n\n\nChecklist:\n\n Have you added tests that prove your fix is effective or that this feature works?\n New and existing unit tests pass locally with these changes?\n Has code been commented, particularly in hard-to-understand areas?\n Have you made corresponding changes to the documentation?", "createdAt": "2020-08-22T20:57:32Z", "url": "https://github.com/pytorch/serve/pull/637", "merged": true, "mergeCommit": {"oid": "7474e4707a91a490ce873e45fffac5ab1f1bd8b1"}, "closed": true, "closedAt": "2020-12-10T16:50:32Z", "author": {"login": "shivamshriwas"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdBfyy_AH2gAyNDcyMDM4OTAzOjgyMjVjZmZlNGUxMGRjMTZkZDBiYzc5YWZmMzBiNzIyOTRjMjg3YzI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdk2PkJgFqTU0OTQwMDgzNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8225cffe4e10dc16dd0bc79aff30b72294c287c2", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/8225cffe4e10dc16dd0bc79aff30b72294c287c2", "committedDate": "2020-08-22T20:53:42Z", "message": "Added FAQ and Troubleshooting docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee845c53712f3b2a811f6b7036af99aad50c5c09", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/ee845c53712f3b2a811f6b7036af99aad50c5c09", "committedDate": "2020-08-22T20:58:20Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bcfbd79433bb9c88e07fd5624217fb54d554ffd", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/4bcfbd79433bb9c88e07fd5624217fb54d554ffd", "committedDate": "2020-08-24T08:41:41Z", "message": "added FAQ's for docker images"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4589f75759a67aa5eae31105a42a99b70384631", "author": {"user": {"login": "dhaniram-kshirsagar", "name": null}}, "url": "https://github.com/pytorch/serve/commit/a4589f75759a67aa5eae31105a42a99b70384631", "committedDate": "2020-08-27T12:28:47Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMjQyODgx", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-481242881", "createdAt": "2020-09-02T19:41:01Z", "commit": {"oid": "a4589f75759a67aa5eae31105a42a99b70384631"}, "state": "DISMISSED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTo0MTowMVrOHMApEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTo0MzoxN1rOHMA1JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM1NTQ3Mw==", "bodyText": "This question can be better worded to say how to use TS in Production", "url": "https://github.com/pytorch/serve/pull/637#discussion_r482355473", "createdAt": "2020-09-02T19:41:01Z", "author": {"login": "dhanainme"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,146 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### What is not Torchserve?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4589f75759a67aa5eae31105a42a99b70384631"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM1NjkyOA==", "bodyText": "Can you please add comments to issues / specific customer ask for this question. Seems a bit too general", "url": "https://github.com/pytorch/serve/pull/637#discussion_r482356928", "createdAt": "2020-09-02T19:42:03Z", "author": {"login": "dhanainme"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,146 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### What is not Torchserve?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4589f75759a67aa5eae31105a42a99b70384631"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM1ODAzNQ==", "bodyText": "Lets not make specific statements about roadmap as this could change often", "url": "https://github.com/pytorch/serve/pull/637#discussion_r482358035", "createdAt": "2020-09-02T19:42:54Z", "author": {"login": "dhanainme"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,146 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### What is not Torchserve?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+We don't have anything particular in our roadmap - [pytorch/pytorch#27610](https://github.com/pytorch/pytorch/issues/27610)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4589f75759a67aa5eae31105a42a99b70384631"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM1ODU2NA==", "bodyText": "Can you please add comments to issues / specific customer ask for this question. Seems a bit too general", "url": "https://github.com/pytorch/serve/pull/637#discussion_r482358564", "createdAt": "2020-09-02T19:43:17Z", "author": {"login": "dhanainme"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,146 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### What is not Torchserve?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+We don't have anything particular in our roadmap - [pytorch/pytorch#27610](https://github.com/pytorch/pytorch/issues/27610)\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4589f75759a67aa5eae31105a42a99b70384631"}, "originalPosition": 34}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40c55e235a7e1be9be34b1f58eaff26f3b00e7c7", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/40c55e235a7e1be9be34b1f58eaff26f3b00e7c7", "committedDate": "2020-09-17T09:41:58Z", "message": "Merge branch 'master' of https://github.com/pytorch/serve into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eca0b38162e4c38d9c1be780cf9070d33b58d1cb", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/eca0b38162e4c38d9c1be780cf9070d33b58d1cb", "committedDate": "2020-09-18T06:02:33Z", "message": "Added more FAQ's\n- updated bugs template"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/38f306c267c2b3d963d72763f0c53f4302b63f80", "committedDate": "2020-09-18T06:04:00Z", "message": "Merge branch 'issue_384' of https://github.com/pytorch/serve into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abdf1cb5e177d22857ffedc027858751e1848e53", "author": {"user": {"login": "dhaniram-kshirsagar", "name": null}}, "url": "https://github.com/pytorch/serve/commit/abdf1cb5e177d22857ffedc027858751e1848e53", "committedDate": "2020-09-23T11:25:07Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81b02e0015218856553bb85ced34c29405e7e341", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/81b02e0015218856553bb85ced34c29405e7e341", "committedDate": "2020-09-23T13:02:51Z", "message": "Merge branch 'master' of https://github.com/pytorch/serve into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73c37898502a40dcafa3c85e4ac5e1ea0232f2cd", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/73c37898502a40dcafa3c85e4ac5e1ea0232f2cd", "committedDate": "2020-09-23T13:43:46Z", "message": "addressed  review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxNzY3MTI1", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-491767125", "createdAt": "2020-09-18T20:38:32Z", "commit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQyMDozODozMlrOHUbNNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxNzoxNjo0M1rOHanspQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE3OTMxOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n          \n          \n            \n            Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models. You can also check out the [examples](https://github.com/pytorch/serve/tree/master/examples) folder.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491179318", "createdAt": "2020-09-18T20:38:32Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MDAxNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### What benefits Torchserve has over AWS Multi-Model-server?\n          \n          \n            \n            ### What benefits does Torchserve have over AWS Multi-Model-Server?", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491180017", "createdAt": "2020-09-18T20:40:16Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MDI2Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n          \n          \n            \n            Torchserve is derived from Multi-Model-Server. However, Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491180267", "createdAt": "2020-09-18T20:40:50Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MDQ3Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n          \n          \n            \n            Yes, Torchserve API ports are configurable using a properties file or environment variable. Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491180476", "createdAt": "2020-09-18T20:41:19Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MDY3OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n          \n          \n            \n            ### Can I run Torchserve APIs on ports other than the default 8080 & 8081?", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491180679", "createdAt": "2020-09-18T20:41:51Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MDg5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n          \n          \n            \n            You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491180898", "createdAt": "2020-09-18T20:42:30Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MTA2NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n          \n          \n            \n            [Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n          \n          \n            \n            Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer [Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491181064", "createdAt": "2020-09-18T20:42:53Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MTMyMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n          \n          \n            \n            [ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n          \n          \n            \n            Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for such deployments. Refer [Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491181322", "createdAt": "2020-09-18T20:43:34Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MjgyNw==", "bodyText": "Not sure if this is the right way to frame this.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491182827", "createdAt": "2020-09-18T20:46:44Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4MzEwMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n          \n          \n            \n            The native Windows support will be added in upcoming releases.\n          \n          \n            \n            Currently, Torchserve supports Windows only via WSL (Windows Subsystem for Linux). The native Windows support will be added in upcoming releases.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491183102", "createdAt": "2020-09-18T20:47:12Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NDAzOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            All these docker images can be created using `build_image.sh` with Valid options are given.\n          \n          \n            \n            All these docker images can be created using `build_image.sh` with appropriate options.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491184039", "createdAt": "2020-09-18T20:49:25Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NDE2OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n          \n          \n            \n            Refer [Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491184168", "createdAt": "2020-09-18T20:49:41Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NDMyMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ###  How to build Torchserve image for a specific branch or commit id?\n          \n          \n            \n            ###  How to build a Torchserve image for a specific branch or commit id?", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491184321", "createdAt": "2020-09-18T20:50:03Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NDUxOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n          \n          \n            \n            The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pypi distribution.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491184518", "createdAt": "2020-09-18T20:50:31Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NTE5NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n          \n          \n            \n            - [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491185195", "createdAt": "2020-09-18T20:52:12Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NTM2Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### How can add a custom API to an existing framework?\n          \n          \n            \n            ### How can I add a custom API to an existing framework?", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491185366", "createdAt": "2020-09-18T20:52:41Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTE4NTQ5NA==", "bodyText": "Add link", "url": "https://github.com/pytorch/serve/pull/637#discussion_r491185494", "createdAt": "2020-09-18T20:53:00Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+The Flask app and Torchserve are completely different except the fact that both support handling HTTP requests however using different engines [netty or python apis].\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes, it is possible to deploy(with Python inference logic) however Torchserve has been certified with any other framework hence there can be unknowns.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?\n+You can a custom API using **plugins SDK** available in Torchserve.\n+The [Health check API ](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#health-check-api) is an example of a custom API integrated using plugins SDK.\n+Refer to Plugins Documentation for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38f306c267c2b3d963d72763f0c53f4302b63f80"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3MjQ0Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Refer  [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n          \n          \n            \n            Refer [custom service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r497672447", "createdAt": "2020-09-30T17:12:16Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,156 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues:  [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.\n+NOTE : Torchserve has not been certified with any other framework apart from Pytorch.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?\n+You can a custom API using **plugins SDK** available in Torchserve.\n+The [Health check API ](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#health-check-api) is an example of a custom API integrated using plugins SDK.\n+Refer to Plugins Documentation for more details.\n+\n+### How can pass multiple images in Inference request call to my model?\n+You can provide multiple data in a single inference request to your custom handler as a key-value pair in the `data` object.\n+\n+Refer [this](https://github.com/pytorch/serve/issues/529#issuecomment-658012913) for more details.\n+\n+## Handler\n+Relevant documents\n+- [Default handlers](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Custom Handlers](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers)\n+\n+###  How do I return an image output for a model?\n+You would have to write a custom handler with the post processing to return image.\n+Refer [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to enhance the default handlers?\n+Write a custom handler that extends the default handler and just override the methods to be tuned.\n+Refer  [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c37898502a40dcafa3c85e4ac5e1ea0232f2cd"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NDA0Nw==", "bodyText": "This seems a little misleading\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### How to serve a model with no-code/zero code?\n          \n          \n            \n            ### Do I always have to write a custom handler or are there default ones that I can use?", "url": "https://github.com/pytorch/serve/pull/637#discussion_r497674047", "createdAt": "2020-09-30T17:14:18Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,156 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues:  [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.\n+NOTE : Torchserve has not been certified with any other framework apart from Pytorch.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?\n+You can a custom API using **plugins SDK** available in Torchserve.\n+The [Health check API ](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#health-check-api) is an example of a custom API integrated using plugins SDK.\n+Refer to Plugins Documentation for more details.\n+\n+### How can pass multiple images in Inference request call to my model?\n+You can provide multiple data in a single inference request to your custom handler as a key-value pair in the `data` object.\n+\n+Refer [this](https://github.com/pytorch/serve/issues/529#issuecomment-658012913) for more details.\n+\n+## Handler\n+Relevant documents\n+- [Default handlers](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Custom Handlers](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers)\n+\n+###  How do I return an image output for a model?\n+You would have to write a custom handler with the post processing to return image.\n+Refer [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to enhance the default handlers?\n+Write a custom handler that extends the default handler and just override the methods to be tuned.\n+Refer  [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to serve a model with no-code/zero code?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c37898502a40dcafa3c85e4ac5e1ea0232f2cd"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NDIyMQ==", "bodyText": "Same as above", "url": "https://github.com/pytorch/serve/pull/637#discussion_r497674221", "createdAt": "2020-09-30T17:14:37Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,156 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues:  [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.\n+NOTE : Torchserve has not been certified with any other framework apart from Pytorch.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?\n+You can a custom API using **plugins SDK** available in Torchserve.\n+The [Health check API ](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#health-check-api) is an example of a custom API integrated using plugins SDK.\n+Refer to Plugins Documentation for more details.\n+\n+### How can pass multiple images in Inference request call to my model?\n+You can provide multiple data in a single inference request to your custom handler as a key-value pair in the `data` object.\n+\n+Refer [this](https://github.com/pytorch/serve/issues/529#issuecomment-658012913) for more details.\n+\n+## Handler\n+Relevant documents\n+- [Default handlers](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Custom Handlers](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers)\n+\n+###  How do I return an image output for a model?\n+You would have to write a custom handler with the post processing to return image.\n+Refer [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to enhance the default handlers?\n+Write a custom handler that extends the default handler and just override the methods to be tuned.\n+Refer  [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to serve a model with no-code/zero code?\n+Yes, you can deploy your model with no-code/ zero code by using builtin default handlers. Refer [default handlers](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c37898502a40dcafa3c85e4ac5e1ea0232f2cd"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NDg0Nw==", "bodyText": "Please be consistent. In some FAQs I see the \"Refer...\" sentence on a new line and on others it is not.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r497674847", "createdAt": "2020-09-30T17:15:40Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,156 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues:  [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.\n+NOTE : Torchserve has not been certified with any other framework apart from Pytorch.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?\n+You can a custom API using **plugins SDK** available in Torchserve.\n+The [Health check API ](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#health-check-api) is an example of a custom API integrated using plugins SDK.\n+Refer to Plugins Documentation for more details.\n+\n+### How can pass multiple images in Inference request call to my model?\n+You can provide multiple data in a single inference request to your custom handler as a key-value pair in the `data` object.\n+\n+Refer [this](https://github.com/pytorch/serve/issues/529#issuecomment-658012913) for more details.\n+\n+## Handler\n+Relevant documents\n+- [Default handlers](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Custom Handlers](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers)\n+\n+###  How do I return an image output for a model?\n+You would have to write a custom handler with the post processing to return image.\n+Refer [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to enhance the default handlers?\n+Write a custom handler that extends the default handler and just override the methods to be tuned.\n+Refer  [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to serve a model with no-code/zero code?\n+Yes, you can deploy your model with no-code/ zero code by using builtin default handlers. Refer [default handlers](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers) for more details.\n+\n+### Is it possible to deploy Hugging Face models?\n+Yes, you can deploy Hugging Face models using a custom handler.\n+Refer [Huggingface_Transformers](https://github.com/pytorch/serve/blob/master/examples/Huggingface_Transformers/README.md) for example. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c37898502a40dcafa3c85e4ac5e1ea0232f2cd"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzY3NTQyOQ==", "bodyText": "Please remove any unnecessary line breaks and spaces throughout this doc\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A mar file is a zip file consisting of all model artifacts with the \".mar\" extension. The cmd-line utility *torch-model-archiver*  is used to create a mar file.\n          \n          \n            \n            A mar file is a zip file consisting of all model artifacts with the \".mar\" extension. The cmd-line utility *torch-model-archiver* is used to create a mar file.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r497675429", "createdAt": "2020-09-30T17:16:43Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,156 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues:  [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.\n+NOTE : Torchserve has not been certified with any other framework apart from Pytorch.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?\n+Yes, you can deploy Torchserve on a multinode ASG EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer\n+[ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I  build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with Valid options are given.\n+\n+Run `./build_image.sh --help'`  for all availble options.\n+\n+Refer [ Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.\n+\n+###  How to build Torchserve image for a specific branch or commit id?\n+To create a Docker image for a specific branch, use the following command:\n+\n+`./build_image.sh -b <branch_name>/<commit_id>`\n+\n+To create a Docker image for a specific branch and specific tag, use the following command:\n+\n+`./build_image.sh -b <branch_name> -t <tagname:latest>`\n+\n+\n+### What is the difference between image created using Dockerfile and image created using Dockerfile.dev?\n+The image created using Dockerfile.dev has Torchserve installed from source where as image created using Dockerfile has Torchserve installed from pyi distribution.  \n+\n+## API\n+Relevant documents\n+- [Torchserve Rest API](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+###  What can I use other than *curl* to make requests to Torchserve?\n+You can use any tool like Postman, Insomnia or even use a python script to do so. Find sample python script [here](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers).\n+\n+### How can add a custom API to an existing framework?\n+You can a custom API using **plugins SDK** available in Torchserve.\n+The [Health check API ](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#health-check-api) is an example of a custom API integrated using plugins SDK.\n+Refer to Plugins Documentation for more details.\n+\n+### How can pass multiple images in Inference request call to my model?\n+You can provide multiple data in a single inference request to your custom handler as a key-value pair in the `data` object.\n+\n+Refer [this](https://github.com/pytorch/serve/issues/529#issuecomment-658012913) for more details.\n+\n+## Handler\n+Relevant documents\n+- [Default handlers](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Custom Handlers](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers)\n+\n+###  How do I return an image output for a model?\n+You would have to write a custom handler with the post processing to return image.\n+Refer [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to enhance the default handlers?\n+Write a custom handler that extends the default handler and just override the methods to be tuned.\n+Refer  [custom  service documentation](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers) for more details.\n+\n+### How to serve a model with no-code/zero code?\n+Yes, you can deploy your model with no-code/ zero code by using builtin default handlers. Refer [default handlers](https://github.com/pytorch/serve/blob/master/docs/default_handlers.md#torchserve-default-inference-handlers) for more details.\n+\n+### Is it possible to deploy Hugging Face models?\n+Yes, you can deploy Hugging Face models using a custom handler.\n+Refer [Huggingface_Transformers](https://github.com/pytorch/serve/blob/master/examples/Huggingface_Transformers/README.md) for example. \n+\n+## Model-archiver\n+ Relevant documents\n+ - [Model-archiver ](https://github.com/pytorch/serve/tree/master/model-archiver#torch-model-archiver-for-torchserve)\n+ - [Docker Readme](https://github.com/pytorch/serve/blob/master/docker/README.md)\n+\n+### What is a mar file?\n+A mar file is a zip file consisting of all model artifacts with the \".mar\" extension. The cmd-line utility *torch-model-archiver*  is used to create a mar file.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73c37898502a40dcafa3c85e4ac5e1ea0232f2cd"}, "originalPosition": 135}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6411f52ed7fb077047aae8ffeeb13c3c95c1ed12", "author": {"user": {"login": "maaquib", "name": "Aaqib"}}, "url": "https://github.com/pytorch/serve/commit/6411f52ed7fb077047aae8ffeeb13c3c95c1ed12", "committedDate": "2020-09-30T17:17:38Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzMTM4MTY0", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-503138164", "createdAt": "2020-10-06T16:15:22Z", "commit": {"oid": "6411f52ed7fb077047aae8ffeeb13c3c95c1ed12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNjoxNToyMlrOHdPm0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxNjoxNToyMlrOHdPm0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDQyNjQ0OQ==", "bodyText": "@shivamshriwas Please note that TorchServe is cloud agnostic sol. When adding items related to AWS services, please prefix with AWS so that it is clear to the users what ELB/ASG and other services refer to", "url": "https://github.com/pytorch/serve/pull/637#discussion_r500426449", "createdAt": "2020-10-06T16:15:22Z", "author": {"login": "chauhang"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,156 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's  follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Torchserve is not a complete web application to serve end to end business use cases. Hence a lot of security aspects should be made available via 3rd party wrapper components in front of Torchserve.\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues:  [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+###  Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models in model zoo. Also check out examples folder all available [examples](https://github.com/pytorch/serve/tree/master/examples).\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux).\n+The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.\n+NOTE : Torchserve has not been certified with any other framework apart from Pytorch.\n+\n+###  Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+\n+### What benefits Torchserve has over AWS Multi-Model-server?\n+Torchserve is derived from Multi-Model-server, But Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on different ports other than 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag. Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts. Refer\n+[Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can deploy Torchserve with ELB and ASG?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6411f52ed7fb077047aae8ffeeb13c3c95c1ed12"}, "originalPosition": 63}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f568681f3d48550021f0a8184ad12cc3526ca039", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/f568681f3d48550021f0a8184ad12cc3526ca039", "committedDate": "2020-10-07T19:53:42Z", "message": "addressed review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa5765e8b01c11fed29af7beeaa2c541a392c89f", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/aa5765e8b01c11fed29af7beeaa2c541a392c89f", "committedDate": "2020-10-07T19:54:48Z", "message": "Merge branch 'issue_384' of https://github.com/pytorch/serve into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1952db1a4e4a68e9692587acf773b8d1fe619aa9", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/1952db1a4e4a68e9692587acf773b8d1fe619aa9", "committedDate": "2020-10-07T19:55:22Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cbcdf2c0753ad64dae152d606127da35359dc10f", "author": {"user": {"login": "dhaniram-kshirsagar", "name": null}}, "url": "https://github.com/pytorch/serve/commit/cbcdf2c0753ad64dae152d606127da35359dc10f", "committedDate": "2020-10-08T02:53:21Z", "message": "Updated few questions based on review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7a4170772fa62db18c1bd0dfd60860e6f373173", "author": {"user": {"login": "harshbafna", "name": "Harsh Bafna"}}, "url": "https://github.com/pytorch/serve/commit/e7a4170772fa62db18c1bd0dfd60860e6f373173", "committedDate": "2020-10-14T09:27:52Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96bf41a24185d8ac1a95a3e715860a33779911c7", "author": {"user": {"login": "shivamshriwas", "name": "shivamshriwas"}}, "url": "https://github.com/pytorch/serve/commit/96bf41a24185d8ac1a95a3e715860a33779911c7", "committedDate": "2020-10-22T13:45:36Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2541f1676f456a8b67b4ac288a0562d7b095f613", "author": {"user": {"login": "dhaniram-kshirsagar", "name": null}}, "url": "https://github.com/pytorch/serve/commit/2541f1676f456a8b67b4ac288a0562d7b095f613", "committedDate": "2020-10-29T10:19:54Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2MjE3NDUy", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-536217452", "createdAt": "2020-11-23T07:21:32Z", "commit": {"oid": "2541f1676f456a8b67b4ac288a0562d7b095f613"}, "state": "DISMISSED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwNzoyMTozMlrOH4BcSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwNzoyMTozMlrOH4BcSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUwNTkyOA==", "bodyText": "@shivamshriwas Please change this. TorchServe is dedicated for PyTorch", "url": "https://github.com/pytorch/serve/pull/637#discussion_r528505928", "createdAt": "2020-11-23T07:21:32Z", "author": {"login": "chauhang"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,158 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Depending on your use case, you will be able to deploy torchserve in production using following mechanisms.\n+> Standalone deployment. Refer https://github.com/pytorch/serve/docker or https://github.com/pytorch/serve/docs/README.md\n+> Cloud based deployment. Refer https://github.com/pytorch/serve/kubernetes https://github.com/pytorch/serve/cloudformation\n+\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues: [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+### Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models. You can also check out the [examples](https://github.com/pytorch/serve/tree/master/examples) folder.\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux). The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Is it possible to deploy model other than Pytorch framework?\n+Yes it is possible to deploy model with custom python handler or you can use [AWS Multi Model Server](https://github.com/awslabs/multi-model-server) a similar model serving framework.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2541f1676f456a8b67b4ac288a0562d7b095f613"}, "originalPosition": 37}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb7a14848ff1bd8ec5860468f0ea10976aaa0ad4", "author": {"user": {"login": "chauhang", "name": "Geeta Chauhan"}}, "url": "https://github.com/pytorch/serve/commit/bb7a14848ff1bd8ec5860468f0ea10976aaa0ad4", "committedDate": "2020-11-25T20:43:21Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "434ef74ce18a1219a6e200cc7713d8466e14cc1b", "author": {"user": {"login": "chauhang", "name": "Geeta Chauhan"}}, "url": "https://github.com/pytorch/serve/commit/434ef74ce18a1219a6e200cc7713d8466e14cc1b", "committedDate": "2020-12-08T02:37:17Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e11b63cebc8389a1393746efa70276ca7c86568", "author": {"user": {"login": "harshbafna", "name": "Harsh Bafna"}}, "url": "https://github.com/pytorch/serve/commit/2e11b63cebc8389a1393746efa70276ca7c86568", "committedDate": "2020-12-09T03:59:41Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1", "author": {"user": {"login": "harshbafna", "name": "Harsh Bafna"}}, "url": "https://github.com/pytorch/serve/commit/5433ed6607d329db5e8f5f9c2c351cde2fcd87c1", "committedDate": "2020-12-09T04:01:07Z", "message": "Update FAQs.md"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NTA0MjU0", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-548504254", "createdAt": "2020-12-09T18:52:31Z", "commit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxODo1MjozMVrOICkPqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxOTowMTo1NlrOICknsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2MTg5Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run `./build_image.sh --help'` for all availble options.\n          \n          \n            \n            Run `./build_image.sh --help` for all availble options.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539561897", "createdAt": "2020-12-09T18:52:31Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Depending on your use case, you will be able to deploy torchserve in production using following mechanisms.\n+> Standalone deployment. Refer https://github.com/pytorch/serve/docker or https://github.com/pytorch/serve/docs/README.md\n+> Cloud based deployment. Refer https://github.com/pytorch/serve/kubernetes https://github.com/pytorch/serve/cloudformation\n+\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues: [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+### Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models. You can also check out the [examples](https://github.com/pytorch/serve/tree/master/examples) folder.\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux). The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+### What benefits does Torchserve have over AWS Multi-Model-Server?\n+Torchserve is derived from Multi-Model-Server. However, Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on ports other than the default 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.\n+Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag.\n+Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts.\n+Refer [Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can I deploy Torchserve with AWS ELB and AWS ASG?\n+Yes, you can deploy Torchserve on a multinode ASG AWS EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer [ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with appropriate options.\n+\n+Run `./build_image.sh --help'` for all availble options.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2MjM1NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n          \n          \n            \n            Torchserve's main purpose is to serve models via http REST APIs, Torchserve is not a Flask app and it uses netty engine for serving http requests.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539562354", "createdAt": "2020-12-09T18:53:15Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Depending on your use case, you will be able to deploy torchserve in production using following mechanisms.\n+> Standalone deployment. Refer https://github.com/pytorch/serve/docker or https://github.com/pytorch/serve/docs/README.md\n+> Cloud based deployment. Refer https://github.com/pytorch/serve/kubernetes https://github.com/pytorch/serve/cloudformation\n+\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2MjU5OA==", "bodyText": "This is not true anymore", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539562598", "createdAt": "2020-12-09T18:53:39Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Depending on your use case, you will be able to deploy torchserve in production using following mechanisms.\n+> Standalone deployment. Refer https://github.com/pytorch/serve/docker or https://github.com/pytorch/serve/docs/README.md\n+> Cloud based deployment. Refer https://github.com/pytorch/serve/kubernetes https://github.com/pytorch/serve/cloudformation\n+\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues: [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+### Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models. You can also check out the [examples](https://github.com/pytorch/serve/tree/master/examples) folder.\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux). The native Windows support will be added in upcoming releases.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2Mjk1MQ==", "bodyText": "Remove this preemptively since gRPC PR will be merged in this release", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539562951", "createdAt": "2020-12-09T18:54:16Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Depending on your use case, you will be able to deploy torchserve in production using following mechanisms.\n+> Standalone deployment. Refer https://github.com/pytorch/serve/docker or https://github.com/pytorch/serve/docs/README.md\n+> Cloud based deployment. Refer https://github.com/pytorch/serve/kubernetes https://github.com/pytorch/serve/cloudformation\n+\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues: [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+### Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models. You can also check out the [examples](https://github.com/pytorch/serve/tree/master/examples) folder.\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux). The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NDE2MA==", "bodyText": "Broken link", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539564160", "createdAt": "2020-12-09T18:56:07Z", "author": {"login": "maaquib"}, "path": "docs/FAQs.md", "diffHunk": "@@ -0,0 +1,153 @@\n+# FAQ'S\n+Contents of this document.\n+* [General](#general)\n+* [Deployment and config](#deployment-and-config)\n+* [API](#api)\n+* [Handler](#handler)\n+* [Model-archiver](#model-archiver)\n+\n+## General\n+Relevant documents.\n+- [Torchserve readme](https://github.com/pytorch/serve#torchserve)\n+\n+### Does Torchserve API's follow some REST API standard?\n+Torchserve API's are compliant with the [OpenAPI specification 3.0](https://swagger.io/specification/).\n+\n+### How to use Torchserve in production?\n+Depending on your use case, you will be able to deploy torchserve in production using following mechanisms.\n+> Standalone deployment. Refer https://github.com/pytorch/serve/docker or https://github.com/pytorch/serve/docs/README.md\n+> Cloud based deployment. Refer https://github.com/pytorch/serve/kubernetes https://github.com/pytorch/serve/cloudformation\n+\n+\n+### What's difference between Torchserve and a python web app using web frameworks like Flask, Django?\n+Torchserve's main purpose is to serve models via http REST APIs , Torchserve is not a Flask app and it uses netty engine for serving http requests.\n+\n+Relevant issues: [[581](https://github.com/pytorch/serve/issues/581),[569](https://github.com/pytorch/serve/issues/569)]\n+\n+### Are there any sample Models available?\n+Various models are provided in Torchserve out of the box. Checkout out Torchserve [Model Zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md) for list of all available models. You can also check out the [examples](https://github.com/pytorch/serve/tree/master/examples) folder.\n+\n+### Does Torchserve has Windows Support?\n+Currently, Torchserve supports Windows only via WSL(Windows Subsystem for Linux). The native Windows support will be added in upcoming releases.\n+\n+### Can I do streaming service with Torchserve like streaming speech recognition?\n+Torchserve currently supports only inference through HTTP 1.0 - Request / Response style.\n+\n+### Does Torchserve support other models based on programming languages other than python?\n+No, As of now only python based models are supported.\n+\n+### What benefits does Torchserve have over AWS Multi-Model-Server?\n+Torchserve is derived from Multi-Model-Server. However, Torchserve is specifically tuned for Pytorch models. It also has new features like Snapshot and model versioning.\n+\n+## Deployment and config\n+Relevant documents.\n+- [Torchserve configuration](https://github.com/pytorch/serve/blob/master/docs/configuration.md)\n+- [Model zoo](https://github.com/pytorch/serve/blob/master/docs/model_zoo.md#model-zoo)\n+- [Snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md)\n+- [Docker]([https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md))\n+\n+### Can I run Torchserve APIs on ports other than the default 8080 & 8081?\n+Yes, Torchserve API ports are configurable using a properties file or environment variable.\n+Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+\n+### How can I resolve model specific python dependency?\n+You can provide a requirements.txt while creating a mar file using \"--requirements-file/ -r\" flag. Also, you can add dependency files using \"--extra-files\" flag.\n+Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+### Can I deploy Torchserve in Kubernetes?\n+Yes, you can deploy Torchserve in Kubernetes using Helm charts.\n+Refer [Kubernetes deployment ](https://github.com/pytorch/serve/blob/master/kubernetes/README.md) for more details.\n+\n+### Can I deploy Torchserve with AWS ELB and AWS ASG?\n+Yes, you can deploy Torchserve on a multinode ASG AWS EC2 cluster. There is a cloud formation template available [here](https://github.com/pytorch/serve/blob/master/cloudformation/ec2-asg.yaml) for this type of deployment. Refer [ Multi-node EC2 deployment behind Elastic LoadBalancer (ELB)](https://github.com/pytorch/serve/tree/master/cloudformation#multi-node-ec2-deployment-behind-elastic-loadbalancer-elb) more details.\n+\n+### How can I backup and restore Torchserve state?\n+TorchServe preserves server runtime configuration across sessions such that a TorchServe instance experiencing either a planned or unplanned service stop can restore its state upon restart. These saved runtime configuration files can be used for backup and restore.\n+Refer [TorchServe model snapshot](https://github.com/pytorch/serve/blob/master/docs/snapshot.md#torchserve-model-snapshot) for more details.\n+\n+### How can I build a Torchserve image from source?\n+Torchserve has a utility [script]([https://github.com/pytorch/serve/blob/master/docker/build_image.sh](https://github.com/pytorch/serve/blob/master/docker/build_image.sh)) for creating docker images, the docker image can be hardware-based CPU or GPU compatible. A Torchserve docker image could be CUDA version specific as well.\n+\n+All these docker images can be created using `build_image.sh` with appropriate options.\n+\n+Run `./build_image.sh --help'` for all availble options.\n+\n+Refer [Create Torchserve docker image from source](https://github.com/pytorch/serve/tree/master/docker#create-torchserve-docker-image-from-source) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NTk3NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n          \n          \n            \n            either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n          \n          \n            \n            Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`. There are two ways to troubleshoot this issue either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539565974", "createdAt": "2020-12-09T18:58:45Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NjMzNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n          \n          \n            \n            Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539566336", "createdAt": "2020-12-09T18:59:19Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NjUyNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]\n          \n          \n            \n            Relevant issues: [[542](https://github.com/pytorch/serve/issues/542)]", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539566526", "createdAt": "2020-12-09T18:59:37Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2Njc5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and java versions before it is not supported.\n          \n          \n            \n            This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and older java versions are not supported.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539566798", "createdAt": "2020-12-09T19:00:01Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]\n+\n+#### \"java.lang.NoSuchMethodError\" when starting Torchserve.[[473](https://github.com/pytorch/serve/issues/473)]\n+This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and java versions before it is not supported.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NzAyMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n          \n          \n            \n            Refer [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539567023", "createdAt": "2020-12-09T19:00:21Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]\n+\n+#### \"java.lang.NoSuchMethodError\" when starting Torchserve.[[473](https://github.com/pytorch/serve/issues/473)]\n+This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and java versions before it is not supported.\n+\n+Relevant issues: [[#473](https://github.com/pytorch/serve/issues/473)]\n+\n+####  Unable to send big files for inference request?\n+The default max request size and response size is roughly 6.5 Mb. Hence any file size greater than 6.5mb cannot be uploaded.\n+To resolve this update `max_request_size` and `max_response_size` in a config.properties file and start the torchserve with this config file.\n+```\n+$ cat config.properties\n+max_request_size=<request size in bytes>\n+max_response_size=<response size in bytes>\n+$ torchserve --start --model-store model_store --ts-config /path/to/config.properties\n+```\n+You can also use environment variables to set these values.\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NzMyNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            By default, the snapshot feature is enabled. To disable snapshot feature start torchserve using --ncs flag or  specify config file using --ts-config path/to/config \n          \n          \n            \n            By default, the snapshot feature is enabled. To disable snapshot feature start torchserve using --ncs flag or specify config file using --ts-config path/to/config", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539567324", "createdAt": "2020-12-09T19:00:49Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]\n+\n+#### \"java.lang.NoSuchMethodError\" when starting Torchserve.[[473](https://github.com/pytorch/serve/issues/473)]\n+This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and java versions before it is not supported.\n+\n+Relevant issues: [[#473](https://github.com/pytorch/serve/issues/473)]\n+\n+####  Unable to send big files for inference request?\n+The default max request size and response size is roughly 6.5 Mb. Hence any file size greater than 6.5mb cannot be uploaded.\n+To resolve this update `max_request_size` and `max_response_size` in a config.properties file and start the torchserve with this config file.\n+```\n+$ cat config.properties\n+max_request_size=<request size in bytes>\n+max_response_size=<response size in bytes>\n+$ torchserve --start --model-store model_store --ts-config /path/to/config.properties\n+```\n+You can also use environment variables to set these values.\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+Relevant issues: [[#335](https://github.com/pytorch/serve/issues/335)]\n+\n+###  Snapshot related issues\n+#### How to disable Snapshot feature?\n+By default, the snapshot feature is enabled. To disable snapshot feature start torchserve using --ncs flag or  specify config file using --ts-config path/to/config ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NzY0Mw==", "bodyText": "Unnecessary line breaks here and in the following sections", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539567643", "createdAt": "2020-12-09T19:01:20Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]\n+\n+#### \"java.lang.NoSuchMethodError\" when starting Torchserve.[[473](https://github.com/pytorch/serve/issues/473)]\n+This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and java versions before it is not supported.\n+\n+Relevant issues: [[#473](https://github.com/pytorch/serve/issues/473)]\n+\n+####  Unable to send big files for inference request?\n+The default max request size and response size is roughly 6.5 Mb. Hence any file size greater than 6.5mb cannot be uploaded.\n+To resolve this update `max_request_size` and `max_response_size` in a config.properties file and start the torchserve with this config file.\n+```\n+$ cat config.properties\n+max_request_size=<request size in bytes>\n+max_response_size=<response size in bytes>\n+$ torchserve --start --model-store model_store --ts-config /path/to/config.properties\n+```\n+You can also use environment variables to set these values.\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+Relevant issues: [[#335](https://github.com/pytorch/serve/issues/335)]\n+\n+###  Snapshot related issues\n+#### How to disable Snapshot feature?\n+By default, the snapshot feature is enabled. To disable snapshot feature start torchserve using --ncs flag or  specify config file using --ts-config path/to/config \n+Relevant issues:[[#383](https://github.com/pytorch/serve/issues/383), [#512](https://github.com/pytorch/serve/issues/512)]\n+\n+#### Torchserve stopped after restart with \"InvalidSnapshotException\" exception.\n+Torchserve when restarted uses the last snapshot config file to restore its state of models and their number of workers.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2ODA1MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Refer  [snapshot.md](https://github.com/pytorch/serve/blob/master/docs/snapshot.md) for more details.\n          \n          \n            \n            Refer [snapshot.md](https://github.com/pytorch/serve/blob/master/docs/snapshot.md) for more details.", "url": "https://github.com/pytorch/serve/pull/637#discussion_r539568051", "createdAt": "2020-12-09T19:01:56Z", "author": {"login": "maaquib"}, "path": "docs/Troubleshooting.md", "diffHunk": "@@ -0,0 +1,113 @@\n+## Troubleshooting guide.\n+Refer to this section for common issues faced while deploying your Pytorch models using Torchserve and their corresponding troubleshooting steps.\n+\n+* [Deployment and config issues](#deployment-and-config-issues)\n+* [Snapshot related issues](#snapshot-related-issues)\n+* [API related issues](#api-relate-issues)\n+* [Model-archiver](#model-archiver)\n+\n+\n+### Deployment and config issues\n+#### \"Failed to bind to address: http://127.0.0.1:8080\", port 8080/8081 already in use.\n+Usually, the port number 8080/8081 is already used by some other application or service, it can be verified by using cmd `ss -ntl | grep 8080`.  There are two ways to troubleshoot this issue\n+either kill the process which is using port 8080/8081 or run Torchserve on different ports other than 8080 & 8081.\n+\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+\n+Relevant issues:  [[542](https://github.com/pytorch/serve/issues/542)]\n+\n+#### \"java.lang.NoSuchMethodError\" when starting Torchserve.[[473](https://github.com/pytorch/serve/issues/473)]\n+This error usually occurs when Java 11 is not installed or used. Java 11 is required by Torchserve and java versions before it is not supported.\n+\n+Relevant issues: [[#473](https://github.com/pytorch/serve/issues/473)]\n+\n+####  Unable to send big files for inference request?\n+The default max request size and response size is roughly 6.5 Mb. Hence any file size greater than 6.5mb cannot be uploaded.\n+To resolve this update `max_request_size` and `max_response_size` in a config.properties file and start the torchserve with this config file.\n+```\n+$ cat config.properties\n+max_request_size=<request size in bytes>\n+max_response_size=<response size in bytes>\n+$ torchserve --start --model-store model_store --ts-config /path/to/config.properties\n+```\n+You can also use environment variables to set these values.\n+Refer  [configuration.md](https://github.com/pytorch/serve/blob/master/docs/configuration.md) for more details.\n+Relevant issues: [[#335](https://github.com/pytorch/serve/issues/335)]\n+\n+###  Snapshot related issues\n+#### How to disable Snapshot feature?\n+By default, the snapshot feature is enabled. To disable snapshot feature start torchserve using --ncs flag or  specify config file using --ts-config path/to/config \n+Relevant issues:[[#383](https://github.com/pytorch/serve/issues/383), [#512](https://github.com/pytorch/serve/issues/512)]\n+\n+#### Torchserve stopped after restart with \"InvalidSnapshotException\" exception.\n+Torchserve when restarted uses the last snapshot config file to restore its state of models and their number of workers.\n+\n+When \"InvalidSnapshotException\" is thrown then the model store is in an inconsistent state as compared with the snapshot.\n+\n+To resolve this the snapshot config files can be removed or torchserve can be started with specific config file using --ts-config path/to/config.\n+\n+Refer  [snapshot.md](https://github.com/pytorch/serve/blob/master/docs/snapshot.md) for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5433ed6607d329db5e8f5f9c2c351cde2fcd87c1"}, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "19682a4fd6225161e66202debf4dcf685c294e3d", "author": {"user": {"login": "harshbafna", "name": "Harsh Bafna"}}, "url": "https://github.com/pytorch/serve/commit/19682a4fd6225161e66202debf4dcf685c294e3d", "committedDate": "2020-12-10T04:23:19Z", "message": "incorporated code review comments\n\nCo-authored-by: Aaqib <maaquib@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28d754e20efb5f0f95bce016b29290ba29966d3a", "author": {"user": {"login": "harshbafna", "name": "Harsh Bafna"}}, "url": "https://github.com/pytorch/serve/commit/28d754e20efb5f0f95bce016b29290ba29966d3a", "committedDate": "2020-12-10T04:28:47Z", "message": "incorporated code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a35e6517c0c67a075a8b83d0efcc5fbbdf2f4dc7", "author": {"user": {"login": "harshbafna", "name": "Harsh Bafna"}}, "url": "https://github.com/pytorch/serve/commit/a35e6517c0c67a075a8b83d0efcc5fbbdf2f4dc7", "committedDate": "2020-12-10T04:30:13Z", "message": "Merge branch 'master' into issue_384"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Mzk1MDIy", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-549395022", "createdAt": "2020-12-10T16:44:07Z", "commit": {"oid": "a35e6517c0c67a075a8b83d0efcc5fbbdf2f4dc7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NDAwODM2", "url": "https://github.com/pytorch/serve/pull/637#pullrequestreview-549400836", "createdAt": "2020-12-10T16:50:23Z", "commit": {"oid": "a35e6517c0c67a075a8b83d0efcc5fbbdf2f4dc7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2031, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}