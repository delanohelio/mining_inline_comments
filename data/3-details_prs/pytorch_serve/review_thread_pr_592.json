{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MDA5OTMy", "number": 592, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToxNjoxM1rOEYqywA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNTozMzowN1rOEfQM4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAyNDAwOnYy", "diffSide": "RIGHT", "path": "examples/Language_Translation/English_to_French_Translation/requirements.txt", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToxNjoxM1rOHBH4Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNDo0NToxN1rOHBr2xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTczOA==", "bodyText": "@AshwinChafale Is there any reason for not using the public pypi version of fairseq?", "url": "https://github.com/pytorch/serve/pull/592#discussion_r470939738", "createdAt": "2020-08-15T05:16:13Z", "author": {"login": "chauhang"}, "path": "examples/Language_Translation/English_to_French_Translation/requirements.txt", "diffHunk": "@@ -0,0 +1,6 @@\n+fastBPE\n+regex \n+requests \n+sacremoses \n+subword_nmt\n+fairseq-0.9.0.tar.gz", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fec5757a3bb55fd07a912549c2a64f8f224b7c9"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyOTE1Nw==", "bodyText": "PyPI version of the fairseq hasn't been updated since Dec 4, 2019. And our model is using the updated fairseq classes and modules that are not present in the PyPi version of fairseq.", "url": "https://github.com/pytorch/serve/pull/592#discussion_r471529157", "createdAt": "2020-08-17T14:45:17Z", "author": {"login": "AshwinChafale"}, "path": "examples/Language_Translation/English_to_French_Translation/requirements.txt", "diffHunk": "@@ -0,0 +1,6 @@\n+fastBPE\n+regex \n+requests \n+sacremoses \n+subword_nmt\n+fairseq-0.9.0.tar.gz", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTczOA=="}, "originalCommit": {"oid": "0fec5757a3bb55fd07a912549c2a64f8f224b7c9"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAyNTY5OnYy", "diffSide": "RIGHT", "path": "examples/Language_Translation/English_to_French_Translation/model_handler.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToxNzozNVrOHBH5EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNzo0MjozOVrOHCfSjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTkyMA==", "bodyText": "@AshwinChafale It will be good if you can add support for Batch processing as well. Otherwise please add clarification notes that batch processing is not supported", "url": "https://github.com/pytorch/serve/pull/592#discussion_r470939920", "createdAt": "2020-08-15T05:17:35Z", "author": {"login": "chauhang"}, "path": "examples/Language_Translation/English_to_French_Translation/model_handler.py", "diffHunk": "@@ -0,0 +1,61 @@\n+from ts.torch_handler.base_handler import BaseHandler\n+from fairseq.models.transformer import TransformerModel\n+import torch\n+from pathlib import Path\n+import os\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LanguageTranslationHandler(BaseHandler):\n+\n+    def __init__(self):\n+        self._context = None\n+        self.initialized = False\n+        self.model = None\n+        self.device = None\n+\n+    def initialize(self, context):\n+        self._context = context\n+        self.initialized = True\n+        self.manifest = context.manifest\n+\n+        properties = context.system_properties\n+        model_dir = properties.get(\"model_dir\")\n+\n+        self.device = torch.device(\n+            \"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n+\n+        #  load the model\n+        self.model = TransformerModel.from_pretrained(\n+            model_dir,\n+            checkpoint_file='model.pt',\n+            data_name_or_path=model_dir,\n+            tokenizer='moses',\n+            bpe='subword_nmt'\n+        )\n+        self.model.to(self.device)\n+        self.model.eval()\n+\n+        self.initialized = True\n+\n+    def preprocess(self, data):\n+        text = data[0].get(\"data\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fec5757a3bb55fd07a912549c2a64f8f224b7c9"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM3MTg1Mw==", "bodyText": "Added support for batch processing. I have also updated README.md file content also", "url": "https://github.com/pytorch/serve/pull/592#discussion_r472371853", "createdAt": "2020-08-18T17:42:39Z", "author": {"login": "AshwinChafale"}, "path": "examples/Language_Translation/English_to_French_Translation/model_handler.py", "diffHunk": "@@ -0,0 +1,61 @@\n+from ts.torch_handler.base_handler import BaseHandler\n+from fairseq.models.transformer import TransformerModel\n+import torch\n+from pathlib import Path\n+import os\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LanguageTranslationHandler(BaseHandler):\n+\n+    def __init__(self):\n+        self._context = None\n+        self.initialized = False\n+        self.model = None\n+        self.device = None\n+\n+    def initialize(self, context):\n+        self._context = context\n+        self.initialized = True\n+        self.manifest = context.manifest\n+\n+        properties = context.system_properties\n+        model_dir = properties.get(\"model_dir\")\n+\n+        self.device = torch.device(\n+            \"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n+\n+        #  load the model\n+        self.model = TransformerModel.from_pretrained(\n+            model_dir,\n+            checkpoint_file='model.pt',\n+            data_name_or_path=model_dir,\n+            tokenizer='moses',\n+            bpe='subword_nmt'\n+        )\n+        self.model.to(self.device)\n+        self.model.eval()\n+\n+        self.initialized = True\n+\n+    def preprocess(self, data):\n+        text = data[0].get(\"data\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTkyMA=="}, "originalCommit": {"oid": "0fec5757a3bb55fd07a912549c2a64f8f224b7c9"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAyNzU2OnYy", "diffSide": "RIGHT", "path": "examples/Language_Translation/English_to_French_Translation/create_mar.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToyMDoxMFrOHBH54Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNzo0MTo0M1rOHCfQSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDk0MDEyOQ==", "bodyText": "@AshwinChafale For ease of maintainability with future releases of Fairseq, it will be better to not peg to a specific version. Is there any special reason for using the 0.9.0 version explicitly?", "url": "https://github.com/pytorch/serve/pull/592#discussion_r470940129", "createdAt": "2020-08-15T05:20:10Z", "author": {"login": "chauhang"}, "path": "examples/Language_Translation/English_to_French_Translation/create_mar.sh", "diffHunk": "@@ -0,0 +1,37 @@\n+#!/bin/bash\n+\n+rm -rf fairseq\n+rm wmt14.en-fr.joined-dict.transformer.tar.bz2\n+rm -rf fairseq-build\n+\n+#installing cython\n+pip install Cython\n+\n+#download zip file of fairseq\n+git clone https://github.com/pytorch/fairseq\n+cd fairseq/\n+python3 setup.py sdist bdist_wheel\n+cd ..\n+mkdir fairseq-build\n+cp fairseq/dist/fairseq-0.9.0.tar.gz fairseq-build/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fec5757a3bb55fd07a912549c2a64f8f224b7c9"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM3MTI3NQ==", "bodyText": "\"python3 setup.py sdist bdist_wheel\" command has auto-generated fairseq-0.9.0.tar.gz in \"/dist/\" folder. There is no specific reason to use 0.9.0 version.\nFurthermore, I have also updated the \"create_mar.sh\" to use the latest build file rather than using a specific version", "url": "https://github.com/pytorch/serve/pull/592#discussion_r472371275", "createdAt": "2020-08-18T17:41:43Z", "author": {"login": "AshwinChafale"}, "path": "examples/Language_Translation/English_to_French_Translation/create_mar.sh", "diffHunk": "@@ -0,0 +1,37 @@\n+#!/bin/bash\n+\n+rm -rf fairseq\n+rm wmt14.en-fr.joined-dict.transformer.tar.bz2\n+rm -rf fairseq-build\n+\n+#installing cython\n+pip install Cython\n+\n+#download zip file of fairseq\n+git clone https://github.com/pytorch/fairseq\n+cd fairseq/\n+python3 setup.py sdist bdist_wheel\n+cd ..\n+mkdir fairseq-build\n+cp fairseq/dist/fairseq-0.9.0.tar.gz fairseq-build/", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDk0MDEyOQ=="}, "originalCommit": {"oid": "0fec5757a3bb55fd07a912549c2a64f8f224b7c9"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMjAwMDg3OnYy", "diffSide": "RIGHT", "path": "examples/nmt_transformer/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyMToxNlrOHLZmdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyMToxNlrOHLZmdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcxNTgyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n          \n          \n            \n                The above command will create a \"model_store\" directory in the current working directory and generate TransformerEn2De.mar file.", "url": "https://github.com/pytorch/serve/pull/592#discussion_r481715828", "createdAt": "2020-09-02T05:21:16Z", "author": {"login": "harshbafna"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n+\n+## Objectives\n+1. Demonstrate how to package a pre-trained Transformer (NMT) models for English-French and English-German translation with generalized custom handler into torch model archive (.mar) file\n+2. Demonstrate how to load model archive (.mar) file into TorchServe and run inference.\n+\n+## Serve the Transformer (NMT) models for English-French/English-German on TorchServe\n+\n+* To generate the model archive (.mar) file for English-to-French translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2fr_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\t\n+* To generate the model archive (.mar) file for English-to-German translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2de_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMjAyMDIyOnYy", "diffSide": "RIGHT", "path": "examples/nmt_transformer/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyNDo0N1rOHLZzrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyNDo0N1rOHLZzrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcxOTIxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n          \n          \n            \n                The above command will create a \"model_store\" directory in the current working directory and generate TransformerEn2Fr.mar file.", "url": "https://github.com/pytorch/serve/pull/592#discussion_r481719214", "createdAt": "2020-09-02T05:24:47Z", "author": {"login": "harshbafna"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n+\n+## Objectives\n+1. Demonstrate how to package a pre-trained Transformer (NMT) models for English-French and English-German translation with generalized custom handler into torch model archive (.mar) file\n+2. Demonstrate how to load model archive (.mar) file into TorchServe and run inference.\n+\n+## Serve the Transformer (NMT) models for English-French/English-German on TorchServe\n+\n+* To generate the model archive (.mar) file for English-to-French translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2fr_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMjAyOTM5OnYy", "diffSide": "RIGHT", "path": "examples/nmt_transformer/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyNjoyNFrOHLZ5qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyNjoyNFrOHLZ5qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyMDc0Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n          \n          \n            \n            In this example, we have shown how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized [custom handler](model_handler_generalized.py) which enables us to translate English-to-French and English-to-German simultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq).", "url": "https://github.com/pytorch/serve/pull/592#discussion_r481720747", "createdAt": "2020-09-02T05:26:24Z", "author": {"login": "harshbafna"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMjA0NjQ0OnYy", "diffSide": "RIGHT", "path": "examples/nmt_transformer/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNToyOTozMlrOHLaFAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTo0NToyM1rOHL1n0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyMzY0OA==", "bodyText": "Provide the actual output.", "url": "https://github.com/pytorch/serve/pull/592#discussion_r481723648", "createdAt": "2020-09-02T05:29:32Z", "author": {"login": "harshbafna"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n+\n+## Objectives\n+1. Demonstrate how to package a pre-trained Transformer (NMT) models for English-French and English-German translation with generalized custom handler into torch model archive (.mar) file\n+2. Demonstrate how to load model archive (.mar) file into TorchServe and run inference.\n+\n+## Serve the Transformer (NMT) models for English-French/English-German on TorchServe\n+\n+* To generate the model archive (.mar) file for English-to-French translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2fr_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\t\n+* To generate the model archive (.mar) file for English-to-German translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2de_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\n+\n+* Start the TorchServe using the model archive (.mar) file created in above step\n+\n+    ```bash\n+    torchserve --start --model-store model_store --ts-config config.properties\n+    ```\n+\n+* Use [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api) to register the model with one initial worker   \n+\tFor English-to-French model\n+    ```bash\n+    curl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2Fr.mar\"\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2De.mar\"\n+\t```\n+    Note:- Our model works only for one worker.\n+\n+* To get the inference use the following curl command  \n+\tFor English-to-French model\n+    ```bash\n+    curl http://127.0.0.1:8080/predictions/TransformerEn2Fr -T model_input/sample.txt\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl http://127.0.0.1:8080/predictions/TransformerEn2De -T model_input/sample.txt\n+\t```\n+    Here sample.txt contains simple english sentences which are given as input to [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#predictions-api). The output of above curl command will be the french translation of sentences present in the sample.txt file.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3NDkyOA==", "bodyText": "Added the output", "url": "https://github.com/pytorch/serve/pull/592#discussion_r482174928", "createdAt": "2020-09-02T15:45:23Z", "author": {"login": "AshwinChafale"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n+\n+## Objectives\n+1. Demonstrate how to package a pre-trained Transformer (NMT) models for English-French and English-German translation with generalized custom handler into torch model archive (.mar) file\n+2. Demonstrate how to load model archive (.mar) file into TorchServe and run inference.\n+\n+## Serve the Transformer (NMT) models for English-French/English-German on TorchServe\n+\n+* To generate the model archive (.mar) file for English-to-French translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2fr_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\t\n+* To generate the model archive (.mar) file for English-to-German translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2de_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\n+\n+* Start the TorchServe using the model archive (.mar) file created in above step\n+\n+    ```bash\n+    torchserve --start --model-store model_store --ts-config config.properties\n+    ```\n+\n+* Use [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api) to register the model with one initial worker   \n+\tFor English-to-French model\n+    ```bash\n+    curl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2Fr.mar\"\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2De.mar\"\n+\t```\n+    Note:- Our model works only for one worker.\n+\n+* To get the inference use the following curl command  \n+\tFor English-to-French model\n+    ```bash\n+    curl http://127.0.0.1:8080/predictions/TransformerEn2Fr -T model_input/sample.txt\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl http://127.0.0.1:8080/predictions/TransformerEn2De -T model_input/sample.txt\n+\t```\n+    Here sample.txt contains simple english sentences which are given as input to [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#predictions-api). The output of above curl command will be the french translation of sentences present in the sample.txt file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyMzY0OA=="}, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMjA1NjkxOnYy", "diffSide": "RIGHT", "path": "examples/nmt_transformer/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNTozMToyM1rOHLaL1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxMzoyMjoyMFrOHLu2CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyNTM5Ng==", "bodyText": "Point to docs/batch_inference_with_ts.md file instead.", "url": "https://github.com/pytorch/serve/pull/592#discussion_r481725396", "createdAt": "2020-09-02T05:31:23Z", "author": {"login": "harshbafna"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n+\n+## Objectives\n+1. Demonstrate how to package a pre-trained Transformer (NMT) models for English-French and English-German translation with generalized custom handler into torch model archive (.mar) file\n+2. Demonstrate how to load model archive (.mar) file into TorchServe and run inference.\n+\n+## Serve the Transformer (NMT) models for English-French/English-German on TorchServe\n+\n+* To generate the model archive (.mar) file for English-to-French translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2fr_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\t\n+* To generate the model archive (.mar) file for English-to-German translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2de_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\n+\n+* Start the TorchServe using the model archive (.mar) file created in above step\n+\n+    ```bash\n+    torchserve --start --model-store model_store --ts-config config.properties\n+    ```\n+\n+* Use [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api) to register the model with one initial worker   \n+\tFor English-to-French model\n+    ```bash\n+    curl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2Fr.mar\"\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2De.mar\"\n+\t```\n+    Note:- Our model works only for one worker.\n+\n+* To get the inference use the following curl command  \n+\tFor English-to-French model\n+    ```bash\n+    curl http://127.0.0.1:8080/predictions/TransformerEn2Fr -T model_input/sample.txt\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl http://127.0.0.1:8080/predictions/TransformerEn2De -T model_input/sample.txt\n+\t```\n+    Here sample.txt contains simple english sentences which are given as input to [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#predictions-api). The output of above curl command will be the french translation of sentences present in the sample.txt file.\n+\n+## Batch Inference with TorchServe using Translation (NMT) model\n+\n+### TorchServe Model Configuration\n+\n+To configure TorchServe to use the batching feature, provide the batch configuration information through \"POST /models\" API.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA2Mzg4MA==", "bodyText": "Done", "url": "https://github.com/pytorch/serve/pull/592#discussion_r482063880", "createdAt": "2020-09-02T13:22:20Z", "author": {"login": "AshwinChafale"}, "path": "examples/nmt_transformer/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Transformer (NMT) models for English-French and English-German translation.\n+\n+The Transformer, introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), is a powerful sequence-to-sequence modeling architecture capable of producing state-of-the-art neural machine translation (NMT) systems.\n+\n+Recently, the [fairseq](https://github.com/pytorch/fairseq#join-the-fairseq-community) team has explored large-scale semi-supervised training of Transformers using back-translated data, further improving translation quality over the original model. More details can be found in [this blog post](https://engineering.fb.com/ai-research/scaling-neural-machine-translation-to-bigger-data-sets-with-faster-training-and-inference/).\n+\n+In this example, we have show how to serve a [English-to-French/English-German Translation](https://pytorch.org/hub/pytorch_fairseq_translation/#english-to-french-translation) model using TorchServe. We have used a generalized custom handler, model_handler_generalized.py which enables us to translate English-to-French and English-to-German symultaneously. The generalized custom handler uses pre-trained [Transformer_WMT14_En-Fr / Transformer_WMT19_En-De](https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md ) models from [fairseq](https://github.com/pytorch/fairseq). \n+\n+## Objectives\n+1. Demonstrate how to package a pre-trained Transformer (NMT) models for English-French and English-German translation with generalized custom handler into torch model archive (.mar) file\n+2. Demonstrate how to load model archive (.mar) file into TorchServe and run inference.\n+\n+## Serve the Transformer (NMT) models for English-French/English-German on TorchServe\n+\n+* To generate the model archive (.mar) file for English-to-French translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2fr_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\t\n+* To generate the model archive (.mar) file for English-to-German translation model using following command\n+\n+    ```bash\n+    ./create_mar.sh en2de_model\n+    ```\n+    By execuing the above script file, the the model archive (.mar) file will be auto generated in \"model_store\" folder in the same working directory.\n+\n+\n+* Start the TorchServe using the model archive (.mar) file created in above step\n+\n+    ```bash\n+    torchserve --start --model-store model_store --ts-config config.properties\n+    ```\n+\n+* Use [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api) to register the model with one initial worker   \n+\tFor English-to-French model\n+    ```bash\n+    curl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2Fr.mar\"\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl -X POST \"http://localhost:8081/models?initial_workers=1&synchronous=true&url=TransformerEn2De.mar\"\n+\t```\n+    Note:- Our model works only for one worker.\n+\n+* To get the inference use the following curl command  \n+\tFor English-to-French model\n+    ```bash\n+    curl http://127.0.0.1:8080/predictions/TransformerEn2Fr -T model_input/sample.txt\n+    ```\n+\tFor English-to-German model\n+\t```bash\n+\tcurl http://127.0.0.1:8080/predictions/TransformerEn2De -T model_input/sample.txt\n+\t```\n+    Here sample.txt contains simple english sentences which are given as input to [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md#predictions-api). The output of above curl command will be the french translation of sentences present in the sample.txt file.\n+\n+## Batch Inference with TorchServe using Translation (NMT) model\n+\n+### TorchServe Model Configuration\n+\n+To configure TorchServe to use the batching feature, provide the batch configuration information through \"POST /models\" API.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyNTM5Ng=="}, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMjA2NzUzOnYy", "diffSide": "RIGHT", "path": "examples/nmt_transformer/french_translation/README.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwNTozMzowN1rOHLaSqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTo0NTo0OFrOHL1o4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyNzE0NA==", "bodyText": "This is already covered in the main README of this example?", "url": "https://github.com/pytorch/serve/pull/592#discussion_r481727144", "createdAt": "2020-09-02T05:33:07Z", "author": {"login": "harshbafna"}, "path": "examples/nmt_transformer/french_translation/README.md", "diffHunk": "@@ -0,0 +1,58 @@\n+# English-to-French translation using Fairseq Transformer model", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA2MTk4Mw==", "bodyText": "Removed french_translation and german_translation folders.", "url": "https://github.com/pytorch/serve/pull/592#discussion_r482061983", "createdAt": "2020-09-02T13:19:35Z", "author": {"login": "AshwinChafale"}, "path": "examples/nmt_transformer/french_translation/README.md", "diffHunk": "@@ -0,0 +1,58 @@\n+# English-to-French translation using Fairseq Transformer model", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyNzE0NA=="}, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3NTIwMA==", "bodyText": "Done", "url": "https://github.com/pytorch/serve/pull/592#discussion_r482175200", "createdAt": "2020-09-02T15:45:48Z", "author": {"login": "AshwinChafale"}, "path": "examples/nmt_transformer/french_translation/README.md", "diffHunk": "@@ -0,0 +1,58 @@\n+# English-to-French translation using Fairseq Transformer model", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTcyNzE0NA=="}, "originalCommit": {"oid": "f2b147098de84bc55cedf4c50ad7706a1caa28ed"}, "originalPosition": 1}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1382, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}