{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1Mzg1MDIw", "number": 551, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQwNDoxODowOFrOESG9lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOToxNzo0MFrOETZE0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NDIzODk1OnYy", "diffSide": "RIGHT", "path": "kubernetes/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQwNDoxODowOFrOG3JOOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQwNDoxODowOFrOG3JOOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NTk2Mg==", "bodyText": "@dhanainme Please add install steps for jq", "url": "https://github.com/pytorch/serve/pull/551#discussion_r460475962", "createdAt": "2020-07-26T04:18:08Z", "author": {"login": "chauhang"}, "path": "kubernetes/README.md", "diffHunk": "@@ -0,0 +1,255 @@\n+\n+## Torchserve on Kubernetes\n+\n+## Overview\n+ \n+This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+![EKS Overview](overview.png)\n+\n+In the following sections we would \n+* Create a EKS Cluster for deploying Torchserve\n+* Create a PersistentVolume backed by EFS to store models and config\n+* Use Helm charts to deploy Torchserve\n+\n+## Prerequisites\n+\n+We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+* AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+* eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+* kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+* helm - [Installation](https://helm.sh/docs/intro/install/)\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NDI0MDA1OnYy", "diffSide": "RIGHT", "path": "kubernetes/setup_efs.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQwNDoxOTo1MlrOG3JOrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNjozMzozMlrOG3-hkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NjA3Nw==", "bodyText": "Duplicate of line 4", "url": "https://github.com/pytorch/serve/pull/551#discussion_r460476077", "createdAt": "2020-07-26T04:19:52Z", "author": {"login": "chauhang"}, "path": "kubernetes/setup_efs.sh", "diffHunk": "@@ -0,0 +1,39 @@\n+#!/bin/bash\n+set -x\n+\n+CLUSTER_NAME=TorchserveCluster\n+MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+\n+# Fetch VPC ID / CIDR Block for the EKS Cluster\n+CLUSTER_NAME=TorchserveCluster", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTM0OTI2Nw==", "bodyText": "Fixed.", "url": "https://github.com/pytorch/serve/pull/551#discussion_r461349267", "createdAt": "2020-07-28T06:33:32Z", "author": {"login": "dhanainme"}, "path": "kubernetes/setup_efs.sh", "diffHunk": "@@ -0,0 +1,39 @@\n+#!/bin/bash\n+set -x\n+\n+CLUSTER_NAME=TorchserveCluster\n+MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+\n+# Fetch VPC ID / CIDR Block for the EKS Cluster\n+CLUSTER_NAME=TorchserveCluster", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NjA3Nw=="}, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NDI0MjAyOnYy", "diffSide": "RIGHT", "path": "kubernetes/templates/eks_cluster.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQwNDoyMzo1MVrOG3JPhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMDoyODoyM1rOG4yZhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NjI5Mw==", "bodyText": "@dhanainme  Please add instructions for what user needs to change to customize these -- choose diff instance type, aws region etc. To make the template general pick values from env variable or add option for generating the templates", "url": "https://github.com/pytorch/serve/pull/551#discussion_r460476293", "createdAt": "2020-07-26T04:23:51Z", "author": {"login": "chauhang"}, "path": "kubernetes/templates/eks_cluster.yaml", "diffHunk": "@@ -0,0 +1,15 @@\n+apiVersion: eksctl.io/v1alpha5\n+kind: ClusterConfig\n+\n+metadata:\n+  name: \"TorchserveCluster\"\n+  region: \"us-west-2\"\n+  \n+nodeGroups:\n+  - name: ng-1\n+    instanceType: g4dn.xlarge", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE5OTE3NA==", "bodyText": "Fixed", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462199174", "createdAt": "2020-07-29T10:28:23Z", "author": {"login": "dhanainme"}, "path": "kubernetes/templates/eks_cluster.yaml", "diffHunk": "@@ -0,0 +1,15 @@\n+apiVersion: eksctl.io/v1alpha5\n+kind: ClusterConfig\n+\n+metadata:\n+  name: \"TorchserveCluster\"\n+  region: \"us-west-2\"\n+  \n+nodeGroups:\n+  - name: ng-1\n+    instanceType: g4dn.xlarge", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NjI5Mw=="}, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NDI0MzUyOnYy", "diffSide": "RIGHT", "path": "kubernetes/values.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQwNDoyNTo1NVrOG3JQLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMDoyODozMVrOG4yaAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NjQ2MA==", "bodyText": "file system dns and file system id will be very specific to user's env and we can't have defaults for these", "url": "https://github.com/pytorch/serve/pull/551#discussion_r460476460", "createdAt": "2020-07-26T04:25:55Z", "author": {"login": "chauhang"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,25 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1\n+  n_cpu: 1\n+  memory_limit: 4Gi\n+  memory_request: 1Gi\n+\n+deployment:\n+  replicas: 3\n+\n+persitant_volume:\n+  size: 1Gi\n+\n+efs:\n+  region: us-west-2\n+  file_system_dns_name: fs-4d0ed148.efs.us-west-2.amazonaws.com", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE5OTI5OQ==", "bodyText": "Fixed", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462199299", "createdAt": "2020-07-29T10:28:31Z", "author": {"login": "dhanainme"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,25 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1\n+  n_cpu: 1\n+  memory_limit: 4Gi\n+  memory_request: 1Gi\n+\n+deployment:\n+  replicas: 3\n+\n+persitant_volume:\n+  size: 1Gi\n+\n+efs:\n+  region: us-west-2\n+  file_system_dns_name: fs-4d0ed148.efs.us-west-2.amazonaws.com", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDQ3NjQ2MA=="}, "originalCommit": {"oid": "49902408a5bf87fa47c7f5ccf50e447da3a01a1f"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYwNTM3OnYy", "diffSide": "RIGHT", "path": "kubernetes/setup_efs.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODo1MzozM1rOG5FvzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOToyMzo0NFrOG5Gytw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxNjE3Mw==", "bodyText": "@dhanainme Please verify this only pulls the details for the current region and not from a cluster with the same name in a different region", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462516173", "createdAt": "2020-07-29T18:53:33Z", "author": {"login": "chauhang"}, "path": "kubernetes/setup_efs.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+CLUSTER_NAME=TorchserveCluster\n+MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+\n+# Fetch VPC ID / CIDR Block for the EKS Cluster\n+\n+echo \"Obtaining VPC ID for $CLUSTER_NAME\"\n+VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \"cluster.resourcesVpcConfig.vpcId\" --output text)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUzMzMwMw==", "bodyText": "This is picked up from AWS_DEFAULT_REGION env. Have confirmed this", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462533303", "createdAt": "2020-07-29T19:23:44Z", "author": {"login": "dhanainme"}, "path": "kubernetes/setup_efs.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+CLUSTER_NAME=TorchserveCluster\n+MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+\n+# Fetch VPC ID / CIDR Block for the EKS Cluster\n+\n+echo \"Obtaining VPC ID for $CLUSTER_NAME\"\n+VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query \"cluster.resourcesVpcConfig.vpcId\" --output text)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxNjE3Mw=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYwODk3OnYy", "diffSide": "RIGHT", "path": "kubernetes/setup_efs.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODo1NDozNFrOG5FyIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTo0MjozOVrOG5HbDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxNjc2OA==", "bodyText": "@dhanainme It might be better to ask user's to define this Environment variable at the beginning of the install steps and then use the same variable in other scripts / instructions", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462516768", "createdAt": "2020-07-29T18:54:34Z", "author": {"login": "chauhang"}, "path": "kubernetes/setup_efs.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+CLUSTER_NAME=TorchserveCluster\n+MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+\n+# Fetch VPC ID / CIDR Block for the EKS Cluster\n+\n+echo \"Obtaining VPC ID for $CLUSTER_NAME\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU0MzYyOQ==", "bodyText": "Have updated the instruction to source ./setup_efs.sh . This would set the env variables so that we can refer at a later time.", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462543629", "createdAt": "2020-07-29T19:42:39Z", "author": {"login": "dhanainme"}, "path": "kubernetes/setup_efs.sh", "diffHunk": "@@ -0,0 +1,56 @@\n+#!/bin/bash\n+\n+CLUSTER_NAME=TorchserveCluster\n+MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+\n+# Fetch VPC ID / CIDR Block for the EKS Cluster\n+\n+echo \"Obtaining VPC ID for $CLUSTER_NAME\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxNjc2OA=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYxOTUwOnYy", "diffSide": "RIGHT", "path": "kubernetes/values.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODo1Nzo0NFrOG5F4wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOToyNTo0N1rOG5G2_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxODQ2NQ==", "bodyText": "@dhanainme  Please add in the readme of description some place -- for CPU only nodes user the docker image corresponding to CPUs", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462518465", "createdAt": "2020-07-29T18:57:44Z", "author": {"login": "chauhang"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUzNDM5Ng==", "bodyText": "Done", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462534396", "createdAt": "2020-07-29T19:25:47Z", "author": {"login": "dhanainme"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxODQ2NQ=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYyNDEwOnYy", "diffSide": "RIGHT", "path": "kubernetes/values.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxODo1OTowNFrOG5F7qA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTo0MDoxM1rOG5HWMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxOTIwOA==", "bodyText": "@dhanainme  It will be good to clarify the n_gpu, n_cpu settings here apply for per worker node on TorchServe and not an aggregation across the entire cluster", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462519208", "createdAt": "2020-07-29T18:59:04Z", "author": {"login": "chauhang"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU0MjM4NA==", "bodyText": "Have added a note.", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462542384", "createdAt": "2020-07-29T19:40:13Z", "author": {"login": "dhanainme"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxOTIwOA=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYyODQ0OnYy", "diffSide": "RIGHT", "path": "kubernetes/values.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTowMDoxOFrOG5F-Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOToyOToxMVrOG5G-Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxOTkxMA==", "bodyText": "@dhanainme Where is this size getting used? If user's have lots of models they will need to increase. Add some description in readme for this", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462519910", "createdAt": "2020-07-29T19:00:18Z", "author": {"login": "chauhang"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1\n+  n_cpu: 1\n+  memory_limit: 4Gi\n+  memory_request: 1Gi\n+\n+deployment:\n+  replicas: 1\n+\n+persitant_volume:\n+  size: 1Gi", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUzNjIyMw==", "bodyText": "Have added a note for templates/efs_pv_claim.yaml & this.", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462536223", "createdAt": "2020-07-29T19:29:11Z", "author": {"login": "dhanainme"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1\n+  n_cpu: 1\n+  memory_limit: 4Gi\n+  memory_request: 1Gi\n+\n+deployment:\n+  replicas: 1\n+\n+persitant_volume:\n+  size: 1Gi", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUxOTkxMA=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYzMjk2OnYy", "diffSide": "RIGHT", "path": "kubernetes/config.properties", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTowMToyNVrOG5GBGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTozODoyMlrOG5HR4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMDYwMQ==", "bodyText": "@dhanainme Does the number_of_gpu need to match with the n_gpu value in the TS helm chart?", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462520601", "createdAt": "2020-07-29T19:01:25Z", "author": {"login": "chauhang"}, "path": "kubernetes/config.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+inference_address=http://0.0.0.0:8080\n+management_address=http://0.0.0.0:8081\n+NUM_WORKERS=1\n+number_of_gpu=1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU0MTI4MQ==", "bodyText": "n_gpu would be exposed to TS container by docker. This should be set to number_of_gpu in config.properties above.\nHave added a note", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462541281", "createdAt": "2020-07-29T19:38:22Z", "author": {"login": "dhanainme"}, "path": "kubernetes/config.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+inference_address=http://0.0.0.0:8080\n+management_address=http://0.0.0.0:8081\n+NUM_WORKERS=1\n+number_of_gpu=1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMDYwMQ=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzYzNTI1OnYy", "diffSide": "RIGHT", "path": "kubernetes/config.properties", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTowMjowMVrOG5GCfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTozMTowMVrOG5HCTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMDk1Nw==", "bodyText": "@dhanainme We will probably need to describe this is a worker insider TorchServe and not the worker nodes on Kubernetes", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462520957", "createdAt": "2020-07-29T19:02:01Z", "author": {"login": "chauhang"}, "path": "kubernetes/config.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+inference_address=http://0.0.0.0:8080\n+management_address=http://0.0.0.0:8081\n+NUM_WORKERS=1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUzNzI5NA==", "bodyText": "Done", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462537294", "createdAt": "2020-07-29T19:31:01Z", "author": {"login": "dhanainme"}, "path": "kubernetes/config.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+inference_address=http://0.0.0.0:8080\n+management_address=http://0.0.0.0:8081\n+NUM_WORKERS=1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMDk1Nw=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzY0NjU2OnYy", "diffSide": "RIGHT", "path": "kubernetes/values.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTowNTowMVrOG5GJYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTozNjozMFrOG5HOGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMjcyMg==", "bodyText": "@dhanainme Mention that the replicas value needs to be <= the total number of worker nodes in the Kubernetes cluster", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462522722", "createdAt": "2020-07-29T19:05:01Z", "author": {"login": "chauhang"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1\n+  n_cpu: 1\n+  memory_limit: 4Gi\n+  memory_request: 1Gi\n+\n+deployment:\n+  replicas: 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU0MDMxNQ==", "bodyText": "Done", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462540315", "createdAt": "2020-07-29T19:36:30Z", "author": {"login": "dhanainme"}, "path": "kubernetes/values.yaml", "diffHunk": "@@ -0,0 +1,20 @@\n+# Default values for torchserve helm chart.\n+\n+torchserve_image: pytorch/torchserve:latest-gpu\n+\n+namespace: torchserve\n+\n+torchserve:\n+  management_port: 8081\n+  inference_port: 8080\n+  pvd_mount: /home/model-server/shared/\n+  n_gpu: 1\n+  n_cpu: 1\n+  memory_limit: 4Gi\n+  memory_request: 1Gi\n+\n+deployment:\n+  replicas: 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyMjcyMg=="}, "originalCommit": {"oid": "6ac1cf2b84a8e80d893dba84387c7b94e59e5056"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzY4NzM2OnYy", "diffSide": "RIGHT", "path": "kubernetes/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOToxNjowN1rOG5GimQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTozMToyNlrOG5HDRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyOTE3Nw==", "bodyText": "@dhanainme It should with --name with two hyphens", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462529177", "createdAt": "2020-07-29T19:16:07Z", "author": {"login": "chauhang"}, "path": "kubernetes/README.md", "diffHunk": "@@ -0,0 +1,815 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/\n+  ```\n+\n+  \n+\n+  ## EKS Cluster setup\n+\n+  In this section we decribe creating a EKS Kubernetes cluster with GPU nodes. If you have an existing EKS / Kubernetes cluster you may skip this section and skip ahead to PersistentVolume preparation. \n+\n+  Ensure you have your installed all required dependices & configured AWS CLI from the previous steps  appropriate permissions. The following steps would,\n+\n+  * Create a EKS cluster\n+  * Install all the required driver for NVIDIA GPU.\n+\n+\n+  ### Creating a EKS cluster\n+\n+  **EKS Optimized AMI Subscription**\n+\n+  First subscribe to EKS-optimized AMI with GPU Support in the AWS Marketplace. Subscribe [here](https://aws.amazon.com/marketplace/pp/B07GRHFXGM). These hosts would be used for the EKS Node Group. \n+\n+  More details about these AMIs and configuring can be found [here](https://github.com/awslabs/amazon-eks-ami) and [here](https://eksctl.io/usage/custom-ami-support/)\n+\n+  **Create a EKS Cluster**\n+\n+\n+  To create a cluster run the following command. \n+\n+  First update the `templates/eks_cluster.yaml` with \n+\n+  ```yaml\n+  apiVersion: eksctl.io/v1alpha5\n+  kind: ClusterConfig\n+  \n+  metadata:\n+    name: \"TorchserveCluster\"\n+    region: \"us-west-2\" # Update AWS Region\n+  \n+  nodeGroups:\n+    - name: ng-1\n+      instanceType: g4dn.xlarge # Update Node Type\n+      desiredCapacity: 3 # Update Node count\n+  ```\n+\n+  \n+\n+  Then run the following command\n+\n+  ```eksctl create cluster -f templates/eks_cluster.yaml```\n+\n+  \n+\n+  Your output should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ eksctl create cluster -f templates/eks_cluster.yaml\n+  [\u2139]  eksctl version 0.24.0\n+  [\u2139]  using region us-west-2\n+  [\u2139]  setting availability zones to [us-west-2c us-west-2b us-west-2a]\n+  [\u2139]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19\n+  [\u2139]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n+  [\u2139]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19\n+  [\u2139]  nodegroup \"ng-1\" will use \"ami-0b6e3586ae536bd40\" [AmazonLinux2/1.16]\n+  [\u2139]  using Kubernetes version 1.16\n+  [\u2139]  creating EKS cluster \"TorchserveCluster\" in \"us-west-2\" region with un-managed nodes\n+  [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\n+  [\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=TorchserveCluster'\n+  [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"TorchserveCluster\" in \"us-west-2\"\n+  [\u2139]  2 sequential tasks: { create cluster control plane \"TorchserveCluster\", 2 sequential sub-tasks: { update CloudWatch logging configuration, create nodegroup \"ng-1\" } }\n+  [\u2139]  building cluster stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2714]  configured CloudWatch logging for cluster \"TorchserveCluster\" in \"us-west-2\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)\n+  [\u2139]  building nodegroup stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  --nodes-min=1 was set automatically for nodegroup ng-1\n+  [\u2139]  --nodes-max=1 was set automatically for nodegroup ng-1\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  waiting for the control plane availability...\n+  [\u2714]  saved kubeconfig as \"/home/ubuntu/.kube/config\"\n+  [\u2139]  no tasks\n+  [\u2714]  all EKS cluster resources for \"TorchserveCluster\" have been created\n+  [\u2139]  adding identity \"arn:aws:iam::ACCOUNT_ID:role/eksctl-TorchserveCluster-nodegrou-NodeInstanceRole\" to auth ConfigMap\n+  [\u2139]  nodegroup \"ng-1\" has 0 node(s)\n+  [\u2139]  waiting for at least 1 node(s) to become ready in \"ng-1\"\n+  [\u2139]  nodegroup \"ng-1\" has 1 node(s)\n+  [\u2139]  node \"ip-instance_id.us-west-2.compute.internal\" is ready\n+  [\u2139]  as you are using a GPU optimized instance type you will need to install NVIDIA Kubernetes device plugin.\n+  [\u2139]  \t see the following page for instructions: https://github.com/NVIDIA/k8s-device-plugin\n+  [\u2139]  kubectl command should work with \"/home/ubuntu/.kube/config\", try 'kubectl get nodes'\n+  [\u2714]  EKS cluster \"TorchserveCluster\" in \"us-west-2\" region is ready\n+  ```\n+\n+  \n+\n+  This would create a EKS cluster named **TorchserveCluster**. This step would takes a considetable amount time to create EKS clusters. You would be able to track the progress in your cloudformation console. If you run in to any error inspect the events tab of the Cloud Formation UI.\n+\n+  \n+\n+  ![EKS Overview](images/eks_cfn.png)\n+\n+  \n+\n+  Verify that the cluster has been created with the following commands \n+\n+  ```bash\n+  eksctl get  clusters\n+  kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  ```\n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ eksctl get  clusters\n+  NAME\t\t\tREGION\n+  TorchserveCluster\tus-west-2\n+  \n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         27m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   27m\n+  \n+  NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE\n+  kube-system   pod/aws-node-2flf5             1/1     Running   0          19m\n+  kube-system   pod/coredns-55c5fcd78f-2h7s4   1/1     Running   0          27m\n+  kube-system   pod/coredns-55c5fcd78f-pm6n5   1/1     Running   0          27m\n+  kube-system   pod/kube-proxy-pp8t2           1/1     Running   0          19m\n+  \n+  NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          27m\n+  kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          27m\n+  \n+  ```\n+\n+  \n+\n+  **NVIDIA Plugin**\n+\n+  The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to run GPU enabled containers. The instructions for installing the plugin can be found [here](https://github.com/NVIDIA/k8s-device-plugin#installing-via-helm-installfrom-the-nvidia-device-plugin-helm-repository)\n+\n+  ```bash\n+  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n+  helm repo update\n+  helm install \\\n+      --version=0.6.0 \\\n+      --generate-name \\\n+      nvdp/nvidia-device-plugin\n+  ```\n+\n+  To verify that the plugin has been installed execute the following command \n+\n+  ```bash\n+  helm list\n+  ```\n+\n+  Your output should look similar to\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ helm list\n+  NAME                           \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART                     \tAPP VERSION\n+  nvidia-device-plugin-1595917413\tdefault  \t1       \t2020-07-28 06:23:34.522975795 +0000 UTC\tdeployed\tnvidia-device-plugin-0.6.0\t0.6.0\n+  ```\n+\n+  \n+\n+  ## Setup PersistentVolume backed by EFS\n+\n+  Torchserve Helm Chart needs a PersistentVolume with a PVC label `model-store-claim` prepared with a specific folder structure shown below. This PersistentVolume contains the snapshot & model files which are shared between multiple pods of the torchserve deployment.\n+\n+      model-server/\n+      \u251c\u2500\u2500 config\n+      \u2502   \u2514\u2500\u2500 config.properties\n+      \u2514\u2500\u2500 model-store\n+          \u251c\u2500\u2500 mnist.mar\n+          \u2514\u2500\u2500 squeezenet1_1.mar\n+\n+\n+  **Create EFS Volume for the EKS Cluster**\n+\n+  This section describes steps to prepare a EFS backed PersistentVolume that would be used by the TS Helm Chart. To prepare a EFS volume as a shareifjccgiced model / config store we have to create a EFS file system, Security Group, Ingress rule, Mount Targets to enable EFS communicate across NAT of the EKS cluster. \n+\n+  The heavy lifting for these steps is performed by ``setup_efs.sh`` script. To run the script, Update the following variables in `setup_efs.sh`\n+\n+  ```bash\n+  CLUSTER_NAME=TorchserveCluster # EKS TS Cluser Name\n+  MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+  ```\n+\n+  Then run `./setup_efs.sh`\n+\n+  \n+\n+  The output of the script should look similar to,\n+\n+  \n+\n+  ```bash\n+  Configuring TorchserveCluster\n+  Obtaining VPC ID for TorchserveCluster\n+  Obtained VPC ID - vpc-fff\n+  Obtaining CIDR BLOCK for vpc-fff\n+  Obtained CIDR BLOCK - 192.168.0.0/16\n+  Creating Security Group\n+  Created Security Group - sg-fff\n+  Configuring Security Group Ingress\n+  Creating EFS Fils System\n+  Created EFS - fs-ff\n+  {\n+      \"FileSystems\": [\n+          {\n+              \"OwnerId\": \"XXXX\",\n+              \"CreationToken\": \"4ae307b6-62aa-44dd-909e-eebe0d0b19f3\",\n+              \"FileSystemId\": \"fs-88983c8d\",\n+              \"FileSystemArn\": \"arn:aws:elasticfilesystem:us-west-2:ff:file-system/fs-ff\",\n+              \"CreationTime\": \"2020-07-29T08:03:33+00:00\",\n+              \"LifeCycleState\": \"creating\",\n+              \"NumberOfMountTargets\": 0,\n+              \"SizeInBytes\": {\n+                  \"Value\": 0,\n+                  \"ValueInIA\": 0,\n+                  \"ValueInStandard\": 0\n+              },\n+              \"PerformanceMode\": \"generalPurpose\",\n+              \"Encrypted\": false,\n+              \"ThroughputMode\": \"bursting\",\n+              \"Tags\": []\n+          }\n+      ]\n+  }\n+  Waiting 30s for before procedding\n+  Obtaining Subnets\n+  Obtained Subnets - subnet-ff\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.58.19\",\n+      \"NetworkInterfaceId\": \"eni-01ce1fd11df545226\",\n+      \"AvailabilityZoneId\": \"usw2-az1\",\n+      \"AvailabilityZoneName\": \"us-west-2b\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.5.7\",\n+      \"NetworkInterfaceId\": \"eni-03db930b204de6ab2\",\n+      \"AvailabilityZoneId\": \"usw2-az3\",\n+      \"AvailabilityZoneName\": \"us-west-2c\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.73.152\",\n+      \"NetworkInterfaceId\": \"eni-0a31830833bf6b030\",\n+      \"AvailabilityZoneId\": \"usw2-az2\",\n+      \"AvailabilityZoneName\": \"us-west-2a\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  EFS File System ID - YOUR-EFS-ID\n+  EFS File System DNS Name - YOUR-EFS-ID.efs..amazonaws.com\n+  Succesfully created EFS & Mountpoints\n+  ```\n+\n+  \n+\n+  Upon completion of the script it would emit a EFS volume DNS Name similar to `fs-ab1cd.efs.us-west-2.amazonaws.com` where `fs-ab1cd` is the EFS filesystem id.\n+\n+  \n+\n+  You should be able to a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)\n+\n+  \n+\n+  ![security_group](images/security_group.png)\n+\n+  \n+\n+  You should also find Mount Points in your EFS console for every region where there is a Node in the Node Group.\n+\n+  \n+\n+  ![](images/efs_mount.png)\n+\n+  \n+\n+  \n+\n+  **Prepare PersistentVolume for Deployment**\n+\n+  We use the [ELF Provisioner Helm Chart](https://github.com/helm/charts/tree/master/stable/efs-provisioner) to create a PersistentVolume backed by EFS. Run the following command to set this up.\n+\n+  ```bash\n+  helm repo add stable https://kubernetes-charts.storage.googleapis.com\n+  helm install stable/efs-provisioner --set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name\n+  ```\n+\n+  \n+\n+  you should get an output similar to \n+\n+  \n+\n+  ```bash\n+  NAME: efs-provisioner-1596010253\n+  LAST DEPLOYED: Wed Jul 29 08:10:56 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  NOTES:\n+  You can provision an EFS-backed persistent volume with a persistent volume claim like below:\n+  \n+  kind: PersistentVolumeClaim\n+  apiVersion: v1\n+  metadata:\n+    name: my-efs-vol-1\n+    annotations:\n+      volume.beta.kubernetes.io/storage-class: aws-efs\n+  spec:\n+    storageClassName: aws-efs\n+    accessModes:\n+      - ReadWriteMany\n+    resources:\n+      requests:\n+        storage: 1Mi\n+  \n+  ```\n+\n+  \n+\n+  Verify that your EFS Provisioner installation is succesfull by invoking ```kubectl get pods```. Your output should look similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get pods\n+  NAME                                          READY   STATUS    RESTARTS   AGE\n+  efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          109s\n+  ```\n+\n+  \n+\n+  Now run, ```kubectl apply -f templates/efs_pv_claim.yaml```. This would also create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files in the same folder structure described above. \n+\n+  \n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl apply -f templates/efs_pv_claim.yaml\n+  persistentvolumeclaim/model-store-claim created\n+  pod/model-store-pod created\n+  ```\n+\n+  \n+\n+  Verify that the PVC / Pod is created  by excuting.   ```kubectl get service,po,daemonset,pv,pvc --all-namespaces``` \n+\n+  You should see\n+\n+  * ```Running``` status for ```pod/model-store-pod```  \n+  * ```Bound``` status for ```default/model-store-claim``` and ```persistentvolumeclaim/model-store-claim```\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         107m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   107m\n+  \n+  NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE\n+  default       pod/efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          4m49s\n+  default       pod/model-store-pod                               1/1     Running   0          8s\n+  kube-system   pod/aws-node-xx8kp                                1/1     Running   0          99m\n+  kube-system   pod/coredns-5c97f79574-tchfg                      1/1     Running   0          107m\n+  kube-system   pod/coredns-5c97f79574-thzqw                      1/1     Running   0          106m\n+  kube-system   pod/kube-proxy-4l8mw                              1/1     Running   0          99m\n+  kube-system   pod/nvidia-device-plugin-daemonset-dbhgq          1/1     Running   0          94m\n+  \n+  NAMESPACE     NAME                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node                         1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/kube-proxy                       1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/nvidia-device-plugin-daemonset   1         1         1       1            1           <none>          94m\n+  \n+  NAMESPACE   NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE\n+              persistentvolume/pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            Delete           Bound    default/model-store-claim   aws-efs                 8s\n+  \n+  NAMESPACE   NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+  default     persistentvolumeclaim/model-store-claim   Bound    pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            aws-efs        8s\n+  ```\n+\n+  \n+\n+  Now edit the TS config file `config.properties` that would be used for the deployment. Any changes to this config should also have corresponding changes in Torchserve Helm Chart that we install in the next section.\n+\n+  \n+\n+  The default config starts **squeezenet1_1** and **mnist** from the model zoo with 3, 5 workers.\n+\n+  \n+\n+  ```yaml\n+  inference_address=http://0.0.0.0:8080\n+  management_address=http://0.0.0.0:8081\n+  NUM_WORKERS=1\n+  number_of_gpu=1\n+  number_of_netty_threads=32\n+  job_queue_size=1000\n+  model_store=/home/model-server/shared/model-store\n+  model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}\n+  ```\n+\n+  \n+\n+  Now copy the files over to PersistentVolume using the following commands.\n+\n+  \n+\n+  ```bash\n+  wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+  wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\n+  kubectl cp squeezenet1_1.mar model-store-pod:/pv/model-store/squeezenet1_1.mar\n+  kubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n+  \n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\n+  kubectl cp config.properties model-store-pod:/pv/config/config.properties\n+  ```\n+\n+  \n+\n+  Verify that the files have been copied by executing ```kubectl exec --tty pod/model-store-pod -- find /pv/``` . You should get an output similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec --tty pod/model-store-pod -- find /pv/\n+  /pv/\n+  /pv/config\n+  /pv/config/config.properties\n+  /pv/model-store\n+  /pv/model-store/squeezenet1_1.mar\n+  /pv/model-store/mnist.mar\n+  ```\n+\n+  \n+\n+  Finally terminate the pod - `kubectl delete pod/model-store-pod`.\n+\n+  \n+\n+  ## Deploy TorchServe using Helm Charts\n+\n+  \n+  The following table describes all the parameters for the Helm Chart.\n+\n+  | Parameter          | Description              | Default                         |\n+  | ------------------ | ------------------------ | ------------------------------- |\n+  | `image`            | Torchserve Serving image | `pytorch/torchserve:latest-gpu` |\n+  | `management-port`  | TS Inference port        | `8080`                          |\n+  | `inference-port`   | TS Management port       | `8081`                          |\n+  | `replicas`         | K8S deployment replicas  | `1`                             |\n+  | `model-store`      | EFS mountpath            | `/home/model-server/shared/`    |\n+  | `persistence.size` | Storage size to request  | `1Gi`                           |\n+  | `n_gpu`            | Number of GPU            | `1`                             |\n+  | `n_cpu`            | Number of CPU            | `1`                             |\n+  | `memory_limit`     | TS Pod memory limit      | `4Gi`                           |\n+  | `memory_request`   | TS Pod memory request    | `1Gi`                           |\n+\n+\n+  Edit the values in `values.yaml` with the right parameters. \n+  \n+\n+  ```yaml\n+  # Default values for torchserve helm chart.\n+  \n+  torchserve_image: pytorch/torchserve:latest-gpu\n+  \n+  namespace: torchserve\n+  \n+  torchserve:\n+    management_port: 8081\n+    inference_port: 8080\n+    pvd_mount: /home/model-server/shared/\n+    n_gpu: 1\n+    n_cpu: 1\n+    memory_limit: 4Gi\n+    memory_request: 1Gi\n+  \n+  deployment:\n+    replicas: 1 # Changes this to number of node in Node Group\n+  \n+  persitant_volume:\n+    size: 1Gi\n+  ```\n+\n+\n+  To install Torchserve run ```helm install ts .```  \n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ helm install ts .\n+  NAME: ts\n+  LAST DEPLOYED: Wed Jul 29 08:29:04 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  ```\n+  \n+\n+  Verify that torchserve has succesfully started by executing ```kubectl exec pod/torchserve-fff -- cat logs/ts_log.log``` on your torchserve pod. You can get this id by lookingup `kubectl get po --all-namespaces`\n+\n+  \n+\n+  Your output should should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec pod/torchserve-fff -- cat logs/ts_log.log\n+  2020-07-29 08:29:08,295 [INFO ] main org.pytorch.serve.ModelServer -\n+  Torchserve version: 0.1.1\n+  TS Home: /home/venv/lib/python3.6/site-packages\n+  Current directory: /home/model-server\n+  ......\n+  ```\n+\n+\n+  ## Test Torchserve Installation\n+\n+  Fetch the Load Balancer Extenal IP by executing \n+\n+  ```bash\n+  kubectl get svc\n+  ```\n+\n+  You should see an entry similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-65-0:~/ts/rel/serve$ kubectl get svc\n+  NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)                         AGE\n+  torchserve   LoadBalancer   10.100.142.22   your_elb.us-west-2.elb.amazonaws.com   8080:31115/TCP,8081:31751/TCP   14m\n+  ```\n+\n+  Now execute the following commands to test Management / Prediction APIs\n+  ```bash\n+  curl http://your_elb.us-west-2.elb.amazonaws.com:8081/models\n+  \n+  # You should something similar to the following\n+  {\n+    \"models\": [\n+      {\n+        \"modelName\": \"mnist\",\n+        \"modelUrl\": \"mnist.mar\"\n+      },\n+      {\n+        \"modelName\": \"squeezenet1_1\",\n+        \"modelUrl\": \"squeezenet1_1.mar\"\n+      }\n+    ]\n+  }\n+  \n+  \n+  curl http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8081/models/squeezenet1_1\n+  \n+  # You should see something similar to the following\n+  [\n+    {\n+      \"modelName\": \"squeezenet1_1\",\n+      \"modelVersion\": \"1.0\",\n+      \"modelUrl\": \"squeezenet1_1.mar\",\n+      \"runtime\": \"python\",\n+      \"minWorkers\": 3,\n+      \"maxWorkers\": 3,\n+      \"batchSize\": 1,\n+      \"maxBatchDelay\": 100,\n+      \"loadedAtStartup\": false,\n+      \"workers\": [\n+        {\n+          \"id\": \"9000\",\n+          \"startTime\": \"2020-07-23T18:34:33.201Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177491968\n+        },\n+        {\n+          \"id\": \"9001\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177569792\n+        },\n+        {\n+          \"id\": \"9002\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177872896\n+        }\n+      ]\n+    }\n+  ]\n+  \n+  \n+  wget https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\n+  curl -X POST  http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8080/predictions/squeezenet1_1 -T kitten_small.jpg\n+  \n+  # You should something similar to the following\n+  [\n+    {\n+      \"lynx\": 0.5370921492576599\n+    },\n+    {\n+      \"tabby\": 0.28355881571769714\n+    },\n+    {\n+      \"Egyptian_cat\": 0.10669822245836258\n+    },\n+    {\n+      \"tiger_cat\": 0.06301568448543549\n+    },\n+    {\n+      \"leopard\": 0.006023923866450787\n+    }\n+  ]\n+  ```\n+  \n+\n+  ## Troubleshooting\n+  \n+\n+  **Troubleshooting EKCTL Cluster Creation**\n+\n+  Possible errors in this step may be a result of \n+\n+  * AWS Account limits. \n+  * IAM Policy of the role used during cluster creation - [Minimum IAM Policy](https://eksctl.io/usage/minimum-iam-policies/)\n+\n+  Inspect your Cloudformation console' events tab, to diagonize any possible issues. You should able be able to find the following resources at the end of this step in the respective AWS consoles\n+\n+  * EKS Cluser in the EKS UI\n+  * AutoScaling Group of the Node Groups\n+  * EC2 Node correponding to the node groups.\n+\n+  \n+\n+  Also, take a look at [eksctl](https://eksctl.io/introduction/) website / [github repo](https://github.com/weaveworks/eksctl/issues) for any issues. \n+\n+  \n+\n+  **Troubleshooting EFS Persitant Volume Creation** \n+\n+  \n+\n+  Possible error in this step may be a result of one of the following. Your pod my be struck in *Init / Creating* forever / persitant volume claim may be in *Pending* forever.\n+\n+  * Incorrect CLUSTER_NAME or Duplicate MOUNT_TARGET_GROUP_NAME in `setup_efs.sh` \n+\n+    * Rerun the script with a different `MOUNT_TARGET_GROUP_NAME` to avoid conflict with a previous run\n+\n+  * Faulty execution of ``setup_efs.sh`` \n+\n+    * Look up the screenshots above for the expected AWS Console UI for Security group & EFS. If you dont see the Ingress permissions / Mount points created, Execute steps from ```setup_efs.sh``` to make sure that they complete as expected.  We need 1 Mount point for every region where Nodes would be deployed. *This step is very critical to the setup* . If you run to any errors the `aws-efs-csi` driver might throw errors which might be hard to diagonize.\n+\n+  * EFS CSI Driver installation\n+\n+    * Ensure that ```--set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name``` is set correctly \n+\n+    * You may inspect the values by running ``helm list`` and ```helm get all YOUR_RELEASE_ID``` to verify if the values used for the installation\n+\n+    * You can execute the following commands to inspect the pods / events to debug EFS / CSI Issues\n+\n+      ```bash\n+      kubectl get events --sort-by='.metadata.creationTimestamp'\n+      \n+      kubectl get pod --all-namespaces # Get the Pod ID\n+      \n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl describe pod/efs-provisioner-YOUR_POD\n+      ```\n+\n+    * A more involved debugging step would involve installing a simple example app to verify EFS / EKS setup as described [here](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html) (Section : *To deploy a sample application and verify that the CSI driver is working*)\n+\n+    * More info about the driver can be found at \n+\n+      * [Github Page](https://github.com/kubernetes-sigs/aws-efs-csi-driver/) / [Helm Chart](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs) / [EKS Workshop](https://www.eksworkshop.com/beginner/190_efs/efs-provisioner/) / [AWS Docs](https://aws.amazon.com/premiumsupport/knowledge-center/eks-persistent-storage/)\n+\n+  \n+\n+  \n+\n+  **Troubleshooting Torchserve Helm Chart**\n+\n+  \n+\n+  Possible errors in this step may be a result of \n+\n+  * Incorrect values in ``values.yaml``\n+    * Changing values in `torchserve.pvd_mount`  would need corresponding change in `config.properties`\n+  * Invalid `config.properties`\n+    * You can verify these values by running this for local TS installation\n+  * TS Pods in *Pending* state\n+    * Ensure you have available Nodes in Node Group\n+\n+  * Helm Installation\n+    * You may inspect the values by running ``helm list`` and `helm get all ts` to verify if the values used for the installation\n+    * You can uninstall / reinstall the helm chart by executing  `helm uninstall ts` and `helm install ts .`\n+    * If you get an error `invalid: data: Too long: must have at most 1048576 characters`, ensure that you dont have any stale files in your kubernetes dir. Else add them to .helmignore file.\n+\n+  \n+\n+  ## Deleting Resources\n+\n+  \n+\n+  * Delete EKS cluster `eksctl delete cluster -name YOUR_CLUSTER_NAME`", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf00e1a72ef37589a175439141591a4ac6c1cd41"}, "originalPosition": 802}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUzNzU0MA==", "bodyText": "Done", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462537540", "createdAt": "2020-07-29T19:31:26Z", "author": {"login": "dhanainme"}, "path": "kubernetes/README.md", "diffHunk": "@@ -0,0 +1,815 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/\n+  ```\n+\n+  \n+\n+  ## EKS Cluster setup\n+\n+  In this section we decribe creating a EKS Kubernetes cluster with GPU nodes. If you have an existing EKS / Kubernetes cluster you may skip this section and skip ahead to PersistentVolume preparation. \n+\n+  Ensure you have your installed all required dependices & configured AWS CLI from the previous steps  appropriate permissions. The following steps would,\n+\n+  * Create a EKS cluster\n+  * Install all the required driver for NVIDIA GPU.\n+\n+\n+  ### Creating a EKS cluster\n+\n+  **EKS Optimized AMI Subscription**\n+\n+  First subscribe to EKS-optimized AMI with GPU Support in the AWS Marketplace. Subscribe [here](https://aws.amazon.com/marketplace/pp/B07GRHFXGM). These hosts would be used for the EKS Node Group. \n+\n+  More details about these AMIs and configuring can be found [here](https://github.com/awslabs/amazon-eks-ami) and [here](https://eksctl.io/usage/custom-ami-support/)\n+\n+  **Create a EKS Cluster**\n+\n+\n+  To create a cluster run the following command. \n+\n+  First update the `templates/eks_cluster.yaml` with \n+\n+  ```yaml\n+  apiVersion: eksctl.io/v1alpha5\n+  kind: ClusterConfig\n+  \n+  metadata:\n+    name: \"TorchserveCluster\"\n+    region: \"us-west-2\" # Update AWS Region\n+  \n+  nodeGroups:\n+    - name: ng-1\n+      instanceType: g4dn.xlarge # Update Node Type\n+      desiredCapacity: 3 # Update Node count\n+  ```\n+\n+  \n+\n+  Then run the following command\n+\n+  ```eksctl create cluster -f templates/eks_cluster.yaml```\n+\n+  \n+\n+  Your output should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ eksctl create cluster -f templates/eks_cluster.yaml\n+  [\u2139]  eksctl version 0.24.0\n+  [\u2139]  using region us-west-2\n+  [\u2139]  setting availability zones to [us-west-2c us-west-2b us-west-2a]\n+  [\u2139]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19\n+  [\u2139]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n+  [\u2139]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19\n+  [\u2139]  nodegroup \"ng-1\" will use \"ami-0b6e3586ae536bd40\" [AmazonLinux2/1.16]\n+  [\u2139]  using Kubernetes version 1.16\n+  [\u2139]  creating EKS cluster \"TorchserveCluster\" in \"us-west-2\" region with un-managed nodes\n+  [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\n+  [\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=TorchserveCluster'\n+  [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"TorchserveCluster\" in \"us-west-2\"\n+  [\u2139]  2 sequential tasks: { create cluster control plane \"TorchserveCluster\", 2 sequential sub-tasks: { update CloudWatch logging configuration, create nodegroup \"ng-1\" } }\n+  [\u2139]  building cluster stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2714]  configured CloudWatch logging for cluster \"TorchserveCluster\" in \"us-west-2\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)\n+  [\u2139]  building nodegroup stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  --nodes-min=1 was set automatically for nodegroup ng-1\n+  [\u2139]  --nodes-max=1 was set automatically for nodegroup ng-1\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  waiting for the control plane availability...\n+  [\u2714]  saved kubeconfig as \"/home/ubuntu/.kube/config\"\n+  [\u2139]  no tasks\n+  [\u2714]  all EKS cluster resources for \"TorchserveCluster\" have been created\n+  [\u2139]  adding identity \"arn:aws:iam::ACCOUNT_ID:role/eksctl-TorchserveCluster-nodegrou-NodeInstanceRole\" to auth ConfigMap\n+  [\u2139]  nodegroup \"ng-1\" has 0 node(s)\n+  [\u2139]  waiting for at least 1 node(s) to become ready in \"ng-1\"\n+  [\u2139]  nodegroup \"ng-1\" has 1 node(s)\n+  [\u2139]  node \"ip-instance_id.us-west-2.compute.internal\" is ready\n+  [\u2139]  as you are using a GPU optimized instance type you will need to install NVIDIA Kubernetes device plugin.\n+  [\u2139]  \t see the following page for instructions: https://github.com/NVIDIA/k8s-device-plugin\n+  [\u2139]  kubectl command should work with \"/home/ubuntu/.kube/config\", try 'kubectl get nodes'\n+  [\u2714]  EKS cluster \"TorchserveCluster\" in \"us-west-2\" region is ready\n+  ```\n+\n+  \n+\n+  This would create a EKS cluster named **TorchserveCluster**. This step would takes a considetable amount time to create EKS clusters. You would be able to track the progress in your cloudformation console. If you run in to any error inspect the events tab of the Cloud Formation UI.\n+\n+  \n+\n+  ![EKS Overview](images/eks_cfn.png)\n+\n+  \n+\n+  Verify that the cluster has been created with the following commands \n+\n+  ```bash\n+  eksctl get  clusters\n+  kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  ```\n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ eksctl get  clusters\n+  NAME\t\t\tREGION\n+  TorchserveCluster\tus-west-2\n+  \n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         27m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   27m\n+  \n+  NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE\n+  kube-system   pod/aws-node-2flf5             1/1     Running   0          19m\n+  kube-system   pod/coredns-55c5fcd78f-2h7s4   1/1     Running   0          27m\n+  kube-system   pod/coredns-55c5fcd78f-pm6n5   1/1     Running   0          27m\n+  kube-system   pod/kube-proxy-pp8t2           1/1     Running   0          19m\n+  \n+  NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          27m\n+  kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          27m\n+  \n+  ```\n+\n+  \n+\n+  **NVIDIA Plugin**\n+\n+  The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to run GPU enabled containers. The instructions for installing the plugin can be found [here](https://github.com/NVIDIA/k8s-device-plugin#installing-via-helm-installfrom-the-nvidia-device-plugin-helm-repository)\n+\n+  ```bash\n+  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n+  helm repo update\n+  helm install \\\n+      --version=0.6.0 \\\n+      --generate-name \\\n+      nvdp/nvidia-device-plugin\n+  ```\n+\n+  To verify that the plugin has been installed execute the following command \n+\n+  ```bash\n+  helm list\n+  ```\n+\n+  Your output should look similar to\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ helm list\n+  NAME                           \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART                     \tAPP VERSION\n+  nvidia-device-plugin-1595917413\tdefault  \t1       \t2020-07-28 06:23:34.522975795 +0000 UTC\tdeployed\tnvidia-device-plugin-0.6.0\t0.6.0\n+  ```\n+\n+  \n+\n+  ## Setup PersistentVolume backed by EFS\n+\n+  Torchserve Helm Chart needs a PersistentVolume with a PVC label `model-store-claim` prepared with a specific folder structure shown below. This PersistentVolume contains the snapshot & model files which are shared between multiple pods of the torchserve deployment.\n+\n+      model-server/\n+      \u251c\u2500\u2500 config\n+      \u2502   \u2514\u2500\u2500 config.properties\n+      \u2514\u2500\u2500 model-store\n+          \u251c\u2500\u2500 mnist.mar\n+          \u2514\u2500\u2500 squeezenet1_1.mar\n+\n+\n+  **Create EFS Volume for the EKS Cluster**\n+\n+  This section describes steps to prepare a EFS backed PersistentVolume that would be used by the TS Helm Chart. To prepare a EFS volume as a shareifjccgiced model / config store we have to create a EFS file system, Security Group, Ingress rule, Mount Targets to enable EFS communicate across NAT of the EKS cluster. \n+\n+  The heavy lifting for these steps is performed by ``setup_efs.sh`` script. To run the script, Update the following variables in `setup_efs.sh`\n+\n+  ```bash\n+  CLUSTER_NAME=TorchserveCluster # EKS TS Cluser Name\n+  MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+  ```\n+\n+  Then run `./setup_efs.sh`\n+\n+  \n+\n+  The output of the script should look similar to,\n+\n+  \n+\n+  ```bash\n+  Configuring TorchserveCluster\n+  Obtaining VPC ID for TorchserveCluster\n+  Obtained VPC ID - vpc-fff\n+  Obtaining CIDR BLOCK for vpc-fff\n+  Obtained CIDR BLOCK - 192.168.0.0/16\n+  Creating Security Group\n+  Created Security Group - sg-fff\n+  Configuring Security Group Ingress\n+  Creating EFS Fils System\n+  Created EFS - fs-ff\n+  {\n+      \"FileSystems\": [\n+          {\n+              \"OwnerId\": \"XXXX\",\n+              \"CreationToken\": \"4ae307b6-62aa-44dd-909e-eebe0d0b19f3\",\n+              \"FileSystemId\": \"fs-88983c8d\",\n+              \"FileSystemArn\": \"arn:aws:elasticfilesystem:us-west-2:ff:file-system/fs-ff\",\n+              \"CreationTime\": \"2020-07-29T08:03:33+00:00\",\n+              \"LifeCycleState\": \"creating\",\n+              \"NumberOfMountTargets\": 0,\n+              \"SizeInBytes\": {\n+                  \"Value\": 0,\n+                  \"ValueInIA\": 0,\n+                  \"ValueInStandard\": 0\n+              },\n+              \"PerformanceMode\": \"generalPurpose\",\n+              \"Encrypted\": false,\n+              \"ThroughputMode\": \"bursting\",\n+              \"Tags\": []\n+          }\n+      ]\n+  }\n+  Waiting 30s for before procedding\n+  Obtaining Subnets\n+  Obtained Subnets - subnet-ff\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.58.19\",\n+      \"NetworkInterfaceId\": \"eni-01ce1fd11df545226\",\n+      \"AvailabilityZoneId\": \"usw2-az1\",\n+      \"AvailabilityZoneName\": \"us-west-2b\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.5.7\",\n+      \"NetworkInterfaceId\": \"eni-03db930b204de6ab2\",\n+      \"AvailabilityZoneId\": \"usw2-az3\",\n+      \"AvailabilityZoneName\": \"us-west-2c\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.73.152\",\n+      \"NetworkInterfaceId\": \"eni-0a31830833bf6b030\",\n+      \"AvailabilityZoneId\": \"usw2-az2\",\n+      \"AvailabilityZoneName\": \"us-west-2a\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  EFS File System ID - YOUR-EFS-ID\n+  EFS File System DNS Name - YOUR-EFS-ID.efs..amazonaws.com\n+  Succesfully created EFS & Mountpoints\n+  ```\n+\n+  \n+\n+  Upon completion of the script it would emit a EFS volume DNS Name similar to `fs-ab1cd.efs.us-west-2.amazonaws.com` where `fs-ab1cd` is the EFS filesystem id.\n+\n+  \n+\n+  You should be able to a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)\n+\n+  \n+\n+  ![security_group](images/security_group.png)\n+\n+  \n+\n+  You should also find Mount Points in your EFS console for every region where there is a Node in the Node Group.\n+\n+  \n+\n+  ![](images/efs_mount.png)\n+\n+  \n+\n+  \n+\n+  **Prepare PersistentVolume for Deployment**\n+\n+  We use the [ELF Provisioner Helm Chart](https://github.com/helm/charts/tree/master/stable/efs-provisioner) to create a PersistentVolume backed by EFS. Run the following command to set this up.\n+\n+  ```bash\n+  helm repo add stable https://kubernetes-charts.storage.googleapis.com\n+  helm install stable/efs-provisioner --set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name\n+  ```\n+\n+  \n+\n+  you should get an output similar to \n+\n+  \n+\n+  ```bash\n+  NAME: efs-provisioner-1596010253\n+  LAST DEPLOYED: Wed Jul 29 08:10:56 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  NOTES:\n+  You can provision an EFS-backed persistent volume with a persistent volume claim like below:\n+  \n+  kind: PersistentVolumeClaim\n+  apiVersion: v1\n+  metadata:\n+    name: my-efs-vol-1\n+    annotations:\n+      volume.beta.kubernetes.io/storage-class: aws-efs\n+  spec:\n+    storageClassName: aws-efs\n+    accessModes:\n+      - ReadWriteMany\n+    resources:\n+      requests:\n+        storage: 1Mi\n+  \n+  ```\n+\n+  \n+\n+  Verify that your EFS Provisioner installation is succesfull by invoking ```kubectl get pods```. Your output should look similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get pods\n+  NAME                                          READY   STATUS    RESTARTS   AGE\n+  efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          109s\n+  ```\n+\n+  \n+\n+  Now run, ```kubectl apply -f templates/efs_pv_claim.yaml```. This would also create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files in the same folder structure described above. \n+\n+  \n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl apply -f templates/efs_pv_claim.yaml\n+  persistentvolumeclaim/model-store-claim created\n+  pod/model-store-pod created\n+  ```\n+\n+  \n+\n+  Verify that the PVC / Pod is created  by excuting.   ```kubectl get service,po,daemonset,pv,pvc --all-namespaces``` \n+\n+  You should see\n+\n+  * ```Running``` status for ```pod/model-store-pod```  \n+  * ```Bound``` status for ```default/model-store-claim``` and ```persistentvolumeclaim/model-store-claim```\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         107m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   107m\n+  \n+  NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE\n+  default       pod/efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          4m49s\n+  default       pod/model-store-pod                               1/1     Running   0          8s\n+  kube-system   pod/aws-node-xx8kp                                1/1     Running   0          99m\n+  kube-system   pod/coredns-5c97f79574-tchfg                      1/1     Running   0          107m\n+  kube-system   pod/coredns-5c97f79574-thzqw                      1/1     Running   0          106m\n+  kube-system   pod/kube-proxy-4l8mw                              1/1     Running   0          99m\n+  kube-system   pod/nvidia-device-plugin-daemonset-dbhgq          1/1     Running   0          94m\n+  \n+  NAMESPACE     NAME                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node                         1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/kube-proxy                       1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/nvidia-device-plugin-daemonset   1         1         1       1            1           <none>          94m\n+  \n+  NAMESPACE   NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE\n+              persistentvolume/pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            Delete           Bound    default/model-store-claim   aws-efs                 8s\n+  \n+  NAMESPACE   NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+  default     persistentvolumeclaim/model-store-claim   Bound    pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            aws-efs        8s\n+  ```\n+\n+  \n+\n+  Now edit the TS config file `config.properties` that would be used for the deployment. Any changes to this config should also have corresponding changes in Torchserve Helm Chart that we install in the next section.\n+\n+  \n+\n+  The default config starts **squeezenet1_1** and **mnist** from the model zoo with 3, 5 workers.\n+\n+  \n+\n+  ```yaml\n+  inference_address=http://0.0.0.0:8080\n+  management_address=http://0.0.0.0:8081\n+  NUM_WORKERS=1\n+  number_of_gpu=1\n+  number_of_netty_threads=32\n+  job_queue_size=1000\n+  model_store=/home/model-server/shared/model-store\n+  model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}\n+  ```\n+\n+  \n+\n+  Now copy the files over to PersistentVolume using the following commands.\n+\n+  \n+\n+  ```bash\n+  wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+  wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\n+  kubectl cp squeezenet1_1.mar model-store-pod:/pv/model-store/squeezenet1_1.mar\n+  kubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n+  \n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\n+  kubectl cp config.properties model-store-pod:/pv/config/config.properties\n+  ```\n+\n+  \n+\n+  Verify that the files have been copied by executing ```kubectl exec --tty pod/model-store-pod -- find /pv/``` . You should get an output similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec --tty pod/model-store-pod -- find /pv/\n+  /pv/\n+  /pv/config\n+  /pv/config/config.properties\n+  /pv/model-store\n+  /pv/model-store/squeezenet1_1.mar\n+  /pv/model-store/mnist.mar\n+  ```\n+\n+  \n+\n+  Finally terminate the pod - `kubectl delete pod/model-store-pod`.\n+\n+  \n+\n+  ## Deploy TorchServe using Helm Charts\n+\n+  \n+  The following table describes all the parameters for the Helm Chart.\n+\n+  | Parameter          | Description              | Default                         |\n+  | ------------------ | ------------------------ | ------------------------------- |\n+  | `image`            | Torchserve Serving image | `pytorch/torchserve:latest-gpu` |\n+  | `management-port`  | TS Inference port        | `8080`                          |\n+  | `inference-port`   | TS Management port       | `8081`                          |\n+  | `replicas`         | K8S deployment replicas  | `1`                             |\n+  | `model-store`      | EFS mountpath            | `/home/model-server/shared/`    |\n+  | `persistence.size` | Storage size to request  | `1Gi`                           |\n+  | `n_gpu`            | Number of GPU            | `1`                             |\n+  | `n_cpu`            | Number of CPU            | `1`                             |\n+  | `memory_limit`     | TS Pod memory limit      | `4Gi`                           |\n+  | `memory_request`   | TS Pod memory request    | `1Gi`                           |\n+\n+\n+  Edit the values in `values.yaml` with the right parameters. \n+  \n+\n+  ```yaml\n+  # Default values for torchserve helm chart.\n+  \n+  torchserve_image: pytorch/torchserve:latest-gpu\n+  \n+  namespace: torchserve\n+  \n+  torchserve:\n+    management_port: 8081\n+    inference_port: 8080\n+    pvd_mount: /home/model-server/shared/\n+    n_gpu: 1\n+    n_cpu: 1\n+    memory_limit: 4Gi\n+    memory_request: 1Gi\n+  \n+  deployment:\n+    replicas: 1 # Changes this to number of node in Node Group\n+  \n+  persitant_volume:\n+    size: 1Gi\n+  ```\n+\n+\n+  To install Torchserve run ```helm install ts .```  \n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ helm install ts .\n+  NAME: ts\n+  LAST DEPLOYED: Wed Jul 29 08:29:04 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  ```\n+  \n+\n+  Verify that torchserve has succesfully started by executing ```kubectl exec pod/torchserve-fff -- cat logs/ts_log.log``` on your torchserve pod. You can get this id by lookingup `kubectl get po --all-namespaces`\n+\n+  \n+\n+  Your output should should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec pod/torchserve-fff -- cat logs/ts_log.log\n+  2020-07-29 08:29:08,295 [INFO ] main org.pytorch.serve.ModelServer -\n+  Torchserve version: 0.1.1\n+  TS Home: /home/venv/lib/python3.6/site-packages\n+  Current directory: /home/model-server\n+  ......\n+  ```\n+\n+\n+  ## Test Torchserve Installation\n+\n+  Fetch the Load Balancer Extenal IP by executing \n+\n+  ```bash\n+  kubectl get svc\n+  ```\n+\n+  You should see an entry similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-65-0:~/ts/rel/serve$ kubectl get svc\n+  NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)                         AGE\n+  torchserve   LoadBalancer   10.100.142.22   your_elb.us-west-2.elb.amazonaws.com   8080:31115/TCP,8081:31751/TCP   14m\n+  ```\n+\n+  Now execute the following commands to test Management / Prediction APIs\n+  ```bash\n+  curl http://your_elb.us-west-2.elb.amazonaws.com:8081/models\n+  \n+  # You should something similar to the following\n+  {\n+    \"models\": [\n+      {\n+        \"modelName\": \"mnist\",\n+        \"modelUrl\": \"mnist.mar\"\n+      },\n+      {\n+        \"modelName\": \"squeezenet1_1\",\n+        \"modelUrl\": \"squeezenet1_1.mar\"\n+      }\n+    ]\n+  }\n+  \n+  \n+  curl http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8081/models/squeezenet1_1\n+  \n+  # You should see something similar to the following\n+  [\n+    {\n+      \"modelName\": \"squeezenet1_1\",\n+      \"modelVersion\": \"1.0\",\n+      \"modelUrl\": \"squeezenet1_1.mar\",\n+      \"runtime\": \"python\",\n+      \"minWorkers\": 3,\n+      \"maxWorkers\": 3,\n+      \"batchSize\": 1,\n+      \"maxBatchDelay\": 100,\n+      \"loadedAtStartup\": false,\n+      \"workers\": [\n+        {\n+          \"id\": \"9000\",\n+          \"startTime\": \"2020-07-23T18:34:33.201Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177491968\n+        },\n+        {\n+          \"id\": \"9001\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177569792\n+        },\n+        {\n+          \"id\": \"9002\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177872896\n+        }\n+      ]\n+    }\n+  ]\n+  \n+  \n+  wget https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\n+  curl -X POST  http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8080/predictions/squeezenet1_1 -T kitten_small.jpg\n+  \n+  # You should something similar to the following\n+  [\n+    {\n+      \"lynx\": 0.5370921492576599\n+    },\n+    {\n+      \"tabby\": 0.28355881571769714\n+    },\n+    {\n+      \"Egyptian_cat\": 0.10669822245836258\n+    },\n+    {\n+      \"tiger_cat\": 0.06301568448543549\n+    },\n+    {\n+      \"leopard\": 0.006023923866450787\n+    }\n+  ]\n+  ```\n+  \n+\n+  ## Troubleshooting\n+  \n+\n+  **Troubleshooting EKCTL Cluster Creation**\n+\n+  Possible errors in this step may be a result of \n+\n+  * AWS Account limits. \n+  * IAM Policy of the role used during cluster creation - [Minimum IAM Policy](https://eksctl.io/usage/minimum-iam-policies/)\n+\n+  Inspect your Cloudformation console' events tab, to diagonize any possible issues. You should able be able to find the following resources at the end of this step in the respective AWS consoles\n+\n+  * EKS Cluser in the EKS UI\n+  * AutoScaling Group of the Node Groups\n+  * EC2 Node correponding to the node groups.\n+\n+  \n+\n+  Also, take a look at [eksctl](https://eksctl.io/introduction/) website / [github repo](https://github.com/weaveworks/eksctl/issues) for any issues. \n+\n+  \n+\n+  **Troubleshooting EFS Persitant Volume Creation** \n+\n+  \n+\n+  Possible error in this step may be a result of one of the following. Your pod my be struck in *Init / Creating* forever / persitant volume claim may be in *Pending* forever.\n+\n+  * Incorrect CLUSTER_NAME or Duplicate MOUNT_TARGET_GROUP_NAME in `setup_efs.sh` \n+\n+    * Rerun the script with a different `MOUNT_TARGET_GROUP_NAME` to avoid conflict with a previous run\n+\n+  * Faulty execution of ``setup_efs.sh`` \n+\n+    * Look up the screenshots above for the expected AWS Console UI for Security group & EFS. If you dont see the Ingress permissions / Mount points created, Execute steps from ```setup_efs.sh``` to make sure that they complete as expected.  We need 1 Mount point for every region where Nodes would be deployed. *This step is very critical to the setup* . If you run to any errors the `aws-efs-csi` driver might throw errors which might be hard to diagonize.\n+\n+  * EFS CSI Driver installation\n+\n+    * Ensure that ```--set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name``` is set correctly \n+\n+    * You may inspect the values by running ``helm list`` and ```helm get all YOUR_RELEASE_ID``` to verify if the values used for the installation\n+\n+    * You can execute the following commands to inspect the pods / events to debug EFS / CSI Issues\n+\n+      ```bash\n+      kubectl get events --sort-by='.metadata.creationTimestamp'\n+      \n+      kubectl get pod --all-namespaces # Get the Pod ID\n+      \n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl describe pod/efs-provisioner-YOUR_POD\n+      ```\n+\n+    * A more involved debugging step would involve installing a simple example app to verify EFS / EKS setup as described [here](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html) (Section : *To deploy a sample application and verify that the CSI driver is working*)\n+\n+    * More info about the driver can be found at \n+\n+      * [Github Page](https://github.com/kubernetes-sigs/aws-efs-csi-driver/) / [Helm Chart](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs) / [EKS Workshop](https://www.eksworkshop.com/beginner/190_efs/efs-provisioner/) / [AWS Docs](https://aws.amazon.com/premiumsupport/knowledge-center/eks-persistent-storage/)\n+\n+  \n+\n+  \n+\n+  **Troubleshooting Torchserve Helm Chart**\n+\n+  \n+\n+  Possible errors in this step may be a result of \n+\n+  * Incorrect values in ``values.yaml``\n+    * Changing values in `torchserve.pvd_mount`  would need corresponding change in `config.properties`\n+  * Invalid `config.properties`\n+    * You can verify these values by running this for local TS installation\n+  * TS Pods in *Pending* state\n+    * Ensure you have available Nodes in Node Group\n+\n+  * Helm Installation\n+    * You may inspect the values by running ``helm list`` and `helm get all ts` to verify if the values used for the installation\n+    * You can uninstall / reinstall the helm chart by executing  `helm uninstall ts` and `helm install ts .`\n+    * If you get an error `invalid: data: Too long: must have at most 1048576 characters`, ensure that you dont have any stale files in your kubernetes dir. Else add them to .helmignore file.\n+\n+  \n+\n+  ## Deleting Resources\n+\n+  \n+\n+  * Delete EKS cluster `eksctl delete cluster -name YOUR_CLUSTER_NAME`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyOTE3Nw=="}, "originalCommit": {"oid": "bf00e1a72ef37589a175439141591a4ac6c1cd41"}, "originalPosition": 802}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzY5MjM0OnYy", "diffSide": "RIGHT", "path": "kubernetes/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOToxNzo0MFrOG5GlwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxOTozMjozM1rOG5HFlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyOTk4NQ==", "bodyText": "@dhanainme What is the Security group for? There was no explicit config done during install for the security group. So how will user's get the value of 'YOUR_SECURITY_GRP_ID' for deleting.", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462529985", "createdAt": "2020-07-29T19:17:40Z", "author": {"login": "chauhang"}, "path": "kubernetes/README.md", "diffHunk": "@@ -0,0 +1,815 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/\n+  ```\n+\n+  \n+\n+  ## EKS Cluster setup\n+\n+  In this section we decribe creating a EKS Kubernetes cluster with GPU nodes. If you have an existing EKS / Kubernetes cluster you may skip this section and skip ahead to PersistentVolume preparation. \n+\n+  Ensure you have your installed all required dependices & configured AWS CLI from the previous steps  appropriate permissions. The following steps would,\n+\n+  * Create a EKS cluster\n+  * Install all the required driver for NVIDIA GPU.\n+\n+\n+  ### Creating a EKS cluster\n+\n+  **EKS Optimized AMI Subscription**\n+\n+  First subscribe to EKS-optimized AMI with GPU Support in the AWS Marketplace. Subscribe [here](https://aws.amazon.com/marketplace/pp/B07GRHFXGM). These hosts would be used for the EKS Node Group. \n+\n+  More details about these AMIs and configuring can be found [here](https://github.com/awslabs/amazon-eks-ami) and [here](https://eksctl.io/usage/custom-ami-support/)\n+\n+  **Create a EKS Cluster**\n+\n+\n+  To create a cluster run the following command. \n+\n+  First update the `templates/eks_cluster.yaml` with \n+\n+  ```yaml\n+  apiVersion: eksctl.io/v1alpha5\n+  kind: ClusterConfig\n+  \n+  metadata:\n+    name: \"TorchserveCluster\"\n+    region: \"us-west-2\" # Update AWS Region\n+  \n+  nodeGroups:\n+    - name: ng-1\n+      instanceType: g4dn.xlarge # Update Node Type\n+      desiredCapacity: 3 # Update Node count\n+  ```\n+\n+  \n+\n+  Then run the following command\n+\n+  ```eksctl create cluster -f templates/eks_cluster.yaml```\n+\n+  \n+\n+  Your output should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ eksctl create cluster -f templates/eks_cluster.yaml\n+  [\u2139]  eksctl version 0.24.0\n+  [\u2139]  using region us-west-2\n+  [\u2139]  setting availability zones to [us-west-2c us-west-2b us-west-2a]\n+  [\u2139]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19\n+  [\u2139]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n+  [\u2139]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19\n+  [\u2139]  nodegroup \"ng-1\" will use \"ami-0b6e3586ae536bd40\" [AmazonLinux2/1.16]\n+  [\u2139]  using Kubernetes version 1.16\n+  [\u2139]  creating EKS cluster \"TorchserveCluster\" in \"us-west-2\" region with un-managed nodes\n+  [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\n+  [\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=TorchserveCluster'\n+  [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"TorchserveCluster\" in \"us-west-2\"\n+  [\u2139]  2 sequential tasks: { create cluster control plane \"TorchserveCluster\", 2 sequential sub-tasks: { update CloudWatch logging configuration, create nodegroup \"ng-1\" } }\n+  [\u2139]  building cluster stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2714]  configured CloudWatch logging for cluster \"TorchserveCluster\" in \"us-west-2\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)\n+  [\u2139]  building nodegroup stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  --nodes-min=1 was set automatically for nodegroup ng-1\n+  [\u2139]  --nodes-max=1 was set automatically for nodegroup ng-1\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  waiting for the control plane availability...\n+  [\u2714]  saved kubeconfig as \"/home/ubuntu/.kube/config\"\n+  [\u2139]  no tasks\n+  [\u2714]  all EKS cluster resources for \"TorchserveCluster\" have been created\n+  [\u2139]  adding identity \"arn:aws:iam::ACCOUNT_ID:role/eksctl-TorchserveCluster-nodegrou-NodeInstanceRole\" to auth ConfigMap\n+  [\u2139]  nodegroup \"ng-1\" has 0 node(s)\n+  [\u2139]  waiting for at least 1 node(s) to become ready in \"ng-1\"\n+  [\u2139]  nodegroup \"ng-1\" has 1 node(s)\n+  [\u2139]  node \"ip-instance_id.us-west-2.compute.internal\" is ready\n+  [\u2139]  as you are using a GPU optimized instance type you will need to install NVIDIA Kubernetes device plugin.\n+  [\u2139]  \t see the following page for instructions: https://github.com/NVIDIA/k8s-device-plugin\n+  [\u2139]  kubectl command should work with \"/home/ubuntu/.kube/config\", try 'kubectl get nodes'\n+  [\u2714]  EKS cluster \"TorchserveCluster\" in \"us-west-2\" region is ready\n+  ```\n+\n+  \n+\n+  This would create a EKS cluster named **TorchserveCluster**. This step would takes a considetable amount time to create EKS clusters. You would be able to track the progress in your cloudformation console. If you run in to any error inspect the events tab of the Cloud Formation UI.\n+\n+  \n+\n+  ![EKS Overview](images/eks_cfn.png)\n+\n+  \n+\n+  Verify that the cluster has been created with the following commands \n+\n+  ```bash\n+  eksctl get  clusters\n+  kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  ```\n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ eksctl get  clusters\n+  NAME\t\t\tREGION\n+  TorchserveCluster\tus-west-2\n+  \n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         27m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   27m\n+  \n+  NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE\n+  kube-system   pod/aws-node-2flf5             1/1     Running   0          19m\n+  kube-system   pod/coredns-55c5fcd78f-2h7s4   1/1     Running   0          27m\n+  kube-system   pod/coredns-55c5fcd78f-pm6n5   1/1     Running   0          27m\n+  kube-system   pod/kube-proxy-pp8t2           1/1     Running   0          19m\n+  \n+  NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          27m\n+  kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          27m\n+  \n+  ```\n+\n+  \n+\n+  **NVIDIA Plugin**\n+\n+  The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to run GPU enabled containers. The instructions for installing the plugin can be found [here](https://github.com/NVIDIA/k8s-device-plugin#installing-via-helm-installfrom-the-nvidia-device-plugin-helm-repository)\n+\n+  ```bash\n+  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n+  helm repo update\n+  helm install \\\n+      --version=0.6.0 \\\n+      --generate-name \\\n+      nvdp/nvidia-device-plugin\n+  ```\n+\n+  To verify that the plugin has been installed execute the following command \n+\n+  ```bash\n+  helm list\n+  ```\n+\n+  Your output should look similar to\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ helm list\n+  NAME                           \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART                     \tAPP VERSION\n+  nvidia-device-plugin-1595917413\tdefault  \t1       \t2020-07-28 06:23:34.522975795 +0000 UTC\tdeployed\tnvidia-device-plugin-0.6.0\t0.6.0\n+  ```\n+\n+  \n+\n+  ## Setup PersistentVolume backed by EFS\n+\n+  Torchserve Helm Chart needs a PersistentVolume with a PVC label `model-store-claim` prepared with a specific folder structure shown below. This PersistentVolume contains the snapshot & model files which are shared between multiple pods of the torchserve deployment.\n+\n+      model-server/\n+      \u251c\u2500\u2500 config\n+      \u2502   \u2514\u2500\u2500 config.properties\n+      \u2514\u2500\u2500 model-store\n+          \u251c\u2500\u2500 mnist.mar\n+          \u2514\u2500\u2500 squeezenet1_1.mar\n+\n+\n+  **Create EFS Volume for the EKS Cluster**\n+\n+  This section describes steps to prepare a EFS backed PersistentVolume that would be used by the TS Helm Chart. To prepare a EFS volume as a shareifjccgiced model / config store we have to create a EFS file system, Security Group, Ingress rule, Mount Targets to enable EFS communicate across NAT of the EKS cluster. \n+\n+  The heavy lifting for these steps is performed by ``setup_efs.sh`` script. To run the script, Update the following variables in `setup_efs.sh`\n+\n+  ```bash\n+  CLUSTER_NAME=TorchserveCluster # EKS TS Cluser Name\n+  MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+  ```\n+\n+  Then run `./setup_efs.sh`\n+\n+  \n+\n+  The output of the script should look similar to,\n+\n+  \n+\n+  ```bash\n+  Configuring TorchserveCluster\n+  Obtaining VPC ID for TorchserveCluster\n+  Obtained VPC ID - vpc-fff\n+  Obtaining CIDR BLOCK for vpc-fff\n+  Obtained CIDR BLOCK - 192.168.0.0/16\n+  Creating Security Group\n+  Created Security Group - sg-fff\n+  Configuring Security Group Ingress\n+  Creating EFS Fils System\n+  Created EFS - fs-ff\n+  {\n+      \"FileSystems\": [\n+          {\n+              \"OwnerId\": \"XXXX\",\n+              \"CreationToken\": \"4ae307b6-62aa-44dd-909e-eebe0d0b19f3\",\n+              \"FileSystemId\": \"fs-88983c8d\",\n+              \"FileSystemArn\": \"arn:aws:elasticfilesystem:us-west-2:ff:file-system/fs-ff\",\n+              \"CreationTime\": \"2020-07-29T08:03:33+00:00\",\n+              \"LifeCycleState\": \"creating\",\n+              \"NumberOfMountTargets\": 0,\n+              \"SizeInBytes\": {\n+                  \"Value\": 0,\n+                  \"ValueInIA\": 0,\n+                  \"ValueInStandard\": 0\n+              },\n+              \"PerformanceMode\": \"generalPurpose\",\n+              \"Encrypted\": false,\n+              \"ThroughputMode\": \"bursting\",\n+              \"Tags\": []\n+          }\n+      ]\n+  }\n+  Waiting 30s for before procedding\n+  Obtaining Subnets\n+  Obtained Subnets - subnet-ff\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.58.19\",\n+      \"NetworkInterfaceId\": \"eni-01ce1fd11df545226\",\n+      \"AvailabilityZoneId\": \"usw2-az1\",\n+      \"AvailabilityZoneName\": \"us-west-2b\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.5.7\",\n+      \"NetworkInterfaceId\": \"eni-03db930b204de6ab2\",\n+      \"AvailabilityZoneId\": \"usw2-az3\",\n+      \"AvailabilityZoneName\": \"us-west-2c\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.73.152\",\n+      \"NetworkInterfaceId\": \"eni-0a31830833bf6b030\",\n+      \"AvailabilityZoneId\": \"usw2-az2\",\n+      \"AvailabilityZoneName\": \"us-west-2a\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  EFS File System ID - YOUR-EFS-ID\n+  EFS File System DNS Name - YOUR-EFS-ID.efs..amazonaws.com\n+  Succesfully created EFS & Mountpoints\n+  ```\n+\n+  \n+\n+  Upon completion of the script it would emit a EFS volume DNS Name similar to `fs-ab1cd.efs.us-west-2.amazonaws.com` where `fs-ab1cd` is the EFS filesystem id.\n+\n+  \n+\n+  You should be able to a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)\n+\n+  \n+\n+  ![security_group](images/security_group.png)\n+\n+  \n+\n+  You should also find Mount Points in your EFS console for every region where there is a Node in the Node Group.\n+\n+  \n+\n+  ![](images/efs_mount.png)\n+\n+  \n+\n+  \n+\n+  **Prepare PersistentVolume for Deployment**\n+\n+  We use the [ELF Provisioner Helm Chart](https://github.com/helm/charts/tree/master/stable/efs-provisioner) to create a PersistentVolume backed by EFS. Run the following command to set this up.\n+\n+  ```bash\n+  helm repo add stable https://kubernetes-charts.storage.googleapis.com\n+  helm install stable/efs-provisioner --set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name\n+  ```\n+\n+  \n+\n+  you should get an output similar to \n+\n+  \n+\n+  ```bash\n+  NAME: efs-provisioner-1596010253\n+  LAST DEPLOYED: Wed Jul 29 08:10:56 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  NOTES:\n+  You can provision an EFS-backed persistent volume with a persistent volume claim like below:\n+  \n+  kind: PersistentVolumeClaim\n+  apiVersion: v1\n+  metadata:\n+    name: my-efs-vol-1\n+    annotations:\n+      volume.beta.kubernetes.io/storage-class: aws-efs\n+  spec:\n+    storageClassName: aws-efs\n+    accessModes:\n+      - ReadWriteMany\n+    resources:\n+      requests:\n+        storage: 1Mi\n+  \n+  ```\n+\n+  \n+\n+  Verify that your EFS Provisioner installation is succesfull by invoking ```kubectl get pods```. Your output should look similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get pods\n+  NAME                                          READY   STATUS    RESTARTS   AGE\n+  efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          109s\n+  ```\n+\n+  \n+\n+  Now run, ```kubectl apply -f templates/efs_pv_claim.yaml```. This would also create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files in the same folder structure described above. \n+\n+  \n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl apply -f templates/efs_pv_claim.yaml\n+  persistentvolumeclaim/model-store-claim created\n+  pod/model-store-pod created\n+  ```\n+\n+  \n+\n+  Verify that the PVC / Pod is created  by excuting.   ```kubectl get service,po,daemonset,pv,pvc --all-namespaces``` \n+\n+  You should see\n+\n+  * ```Running``` status for ```pod/model-store-pod```  \n+  * ```Bound``` status for ```default/model-store-claim``` and ```persistentvolumeclaim/model-store-claim```\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         107m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   107m\n+  \n+  NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE\n+  default       pod/efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          4m49s\n+  default       pod/model-store-pod                               1/1     Running   0          8s\n+  kube-system   pod/aws-node-xx8kp                                1/1     Running   0          99m\n+  kube-system   pod/coredns-5c97f79574-tchfg                      1/1     Running   0          107m\n+  kube-system   pod/coredns-5c97f79574-thzqw                      1/1     Running   0          106m\n+  kube-system   pod/kube-proxy-4l8mw                              1/1     Running   0          99m\n+  kube-system   pod/nvidia-device-plugin-daemonset-dbhgq          1/1     Running   0          94m\n+  \n+  NAMESPACE     NAME                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node                         1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/kube-proxy                       1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/nvidia-device-plugin-daemonset   1         1         1       1            1           <none>          94m\n+  \n+  NAMESPACE   NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE\n+              persistentvolume/pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            Delete           Bound    default/model-store-claim   aws-efs                 8s\n+  \n+  NAMESPACE   NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+  default     persistentvolumeclaim/model-store-claim   Bound    pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            aws-efs        8s\n+  ```\n+\n+  \n+\n+  Now edit the TS config file `config.properties` that would be used for the deployment. Any changes to this config should also have corresponding changes in Torchserve Helm Chart that we install in the next section.\n+\n+  \n+\n+  The default config starts **squeezenet1_1** and **mnist** from the model zoo with 3, 5 workers.\n+\n+  \n+\n+  ```yaml\n+  inference_address=http://0.0.0.0:8080\n+  management_address=http://0.0.0.0:8081\n+  NUM_WORKERS=1\n+  number_of_gpu=1\n+  number_of_netty_threads=32\n+  job_queue_size=1000\n+  model_store=/home/model-server/shared/model-store\n+  model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}\n+  ```\n+\n+  \n+\n+  Now copy the files over to PersistentVolume using the following commands.\n+\n+  \n+\n+  ```bash\n+  wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+  wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\n+  kubectl cp squeezenet1_1.mar model-store-pod:/pv/model-store/squeezenet1_1.mar\n+  kubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n+  \n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\n+  kubectl cp config.properties model-store-pod:/pv/config/config.properties\n+  ```\n+\n+  \n+\n+  Verify that the files have been copied by executing ```kubectl exec --tty pod/model-store-pod -- find /pv/``` . You should get an output similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec --tty pod/model-store-pod -- find /pv/\n+  /pv/\n+  /pv/config\n+  /pv/config/config.properties\n+  /pv/model-store\n+  /pv/model-store/squeezenet1_1.mar\n+  /pv/model-store/mnist.mar\n+  ```\n+\n+  \n+\n+  Finally terminate the pod - `kubectl delete pod/model-store-pod`.\n+\n+  \n+\n+  ## Deploy TorchServe using Helm Charts\n+\n+  \n+  The following table describes all the parameters for the Helm Chart.\n+\n+  | Parameter          | Description              | Default                         |\n+  | ------------------ | ------------------------ | ------------------------------- |\n+  | `image`            | Torchserve Serving image | `pytorch/torchserve:latest-gpu` |\n+  | `management-port`  | TS Inference port        | `8080`                          |\n+  | `inference-port`   | TS Management port       | `8081`                          |\n+  | `replicas`         | K8S deployment replicas  | `1`                             |\n+  | `model-store`      | EFS mountpath            | `/home/model-server/shared/`    |\n+  | `persistence.size` | Storage size to request  | `1Gi`                           |\n+  | `n_gpu`            | Number of GPU            | `1`                             |\n+  | `n_cpu`            | Number of CPU            | `1`                             |\n+  | `memory_limit`     | TS Pod memory limit      | `4Gi`                           |\n+  | `memory_request`   | TS Pod memory request    | `1Gi`                           |\n+\n+\n+  Edit the values in `values.yaml` with the right parameters. \n+  \n+\n+  ```yaml\n+  # Default values for torchserve helm chart.\n+  \n+  torchserve_image: pytorch/torchserve:latest-gpu\n+  \n+  namespace: torchserve\n+  \n+  torchserve:\n+    management_port: 8081\n+    inference_port: 8080\n+    pvd_mount: /home/model-server/shared/\n+    n_gpu: 1\n+    n_cpu: 1\n+    memory_limit: 4Gi\n+    memory_request: 1Gi\n+  \n+  deployment:\n+    replicas: 1 # Changes this to number of node in Node Group\n+  \n+  persitant_volume:\n+    size: 1Gi\n+  ```\n+\n+\n+  To install Torchserve run ```helm install ts .```  \n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ helm install ts .\n+  NAME: ts\n+  LAST DEPLOYED: Wed Jul 29 08:29:04 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  ```\n+  \n+\n+  Verify that torchserve has succesfully started by executing ```kubectl exec pod/torchserve-fff -- cat logs/ts_log.log``` on your torchserve pod. You can get this id by lookingup `kubectl get po --all-namespaces`\n+\n+  \n+\n+  Your output should should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec pod/torchserve-fff -- cat logs/ts_log.log\n+  2020-07-29 08:29:08,295 [INFO ] main org.pytorch.serve.ModelServer -\n+  Torchserve version: 0.1.1\n+  TS Home: /home/venv/lib/python3.6/site-packages\n+  Current directory: /home/model-server\n+  ......\n+  ```\n+\n+\n+  ## Test Torchserve Installation\n+\n+  Fetch the Load Balancer Extenal IP by executing \n+\n+  ```bash\n+  kubectl get svc\n+  ```\n+\n+  You should see an entry similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-65-0:~/ts/rel/serve$ kubectl get svc\n+  NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)                         AGE\n+  torchserve   LoadBalancer   10.100.142.22   your_elb.us-west-2.elb.amazonaws.com   8080:31115/TCP,8081:31751/TCP   14m\n+  ```\n+\n+  Now execute the following commands to test Management / Prediction APIs\n+  ```bash\n+  curl http://your_elb.us-west-2.elb.amazonaws.com:8081/models\n+  \n+  # You should something similar to the following\n+  {\n+    \"models\": [\n+      {\n+        \"modelName\": \"mnist\",\n+        \"modelUrl\": \"mnist.mar\"\n+      },\n+      {\n+        \"modelName\": \"squeezenet1_1\",\n+        \"modelUrl\": \"squeezenet1_1.mar\"\n+      }\n+    ]\n+  }\n+  \n+  \n+  curl http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8081/models/squeezenet1_1\n+  \n+  # You should see something similar to the following\n+  [\n+    {\n+      \"modelName\": \"squeezenet1_1\",\n+      \"modelVersion\": \"1.0\",\n+      \"modelUrl\": \"squeezenet1_1.mar\",\n+      \"runtime\": \"python\",\n+      \"minWorkers\": 3,\n+      \"maxWorkers\": 3,\n+      \"batchSize\": 1,\n+      \"maxBatchDelay\": 100,\n+      \"loadedAtStartup\": false,\n+      \"workers\": [\n+        {\n+          \"id\": \"9000\",\n+          \"startTime\": \"2020-07-23T18:34:33.201Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177491968\n+        },\n+        {\n+          \"id\": \"9001\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177569792\n+        },\n+        {\n+          \"id\": \"9002\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177872896\n+        }\n+      ]\n+    }\n+  ]\n+  \n+  \n+  wget https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\n+  curl -X POST  http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8080/predictions/squeezenet1_1 -T kitten_small.jpg\n+  \n+  # You should something similar to the following\n+  [\n+    {\n+      \"lynx\": 0.5370921492576599\n+    },\n+    {\n+      \"tabby\": 0.28355881571769714\n+    },\n+    {\n+      \"Egyptian_cat\": 0.10669822245836258\n+    },\n+    {\n+      \"tiger_cat\": 0.06301568448543549\n+    },\n+    {\n+      \"leopard\": 0.006023923866450787\n+    }\n+  ]\n+  ```\n+  \n+\n+  ## Troubleshooting\n+  \n+\n+  **Troubleshooting EKCTL Cluster Creation**\n+\n+  Possible errors in this step may be a result of \n+\n+  * AWS Account limits. \n+  * IAM Policy of the role used during cluster creation - [Minimum IAM Policy](https://eksctl.io/usage/minimum-iam-policies/)\n+\n+  Inspect your Cloudformation console' events tab, to diagonize any possible issues. You should able be able to find the following resources at the end of this step in the respective AWS consoles\n+\n+  * EKS Cluser in the EKS UI\n+  * AutoScaling Group of the Node Groups\n+  * EC2 Node correponding to the node groups.\n+\n+  \n+\n+  Also, take a look at [eksctl](https://eksctl.io/introduction/) website / [github repo](https://github.com/weaveworks/eksctl/issues) for any issues. \n+\n+  \n+\n+  **Troubleshooting EFS Persitant Volume Creation** \n+\n+  \n+\n+  Possible error in this step may be a result of one of the following. Your pod my be struck in *Init / Creating* forever / persitant volume claim may be in *Pending* forever.\n+\n+  * Incorrect CLUSTER_NAME or Duplicate MOUNT_TARGET_GROUP_NAME in `setup_efs.sh` \n+\n+    * Rerun the script with a different `MOUNT_TARGET_GROUP_NAME` to avoid conflict with a previous run\n+\n+  * Faulty execution of ``setup_efs.sh`` \n+\n+    * Look up the screenshots above for the expected AWS Console UI for Security group & EFS. If you dont see the Ingress permissions / Mount points created, Execute steps from ```setup_efs.sh``` to make sure that they complete as expected.  We need 1 Mount point for every region where Nodes would be deployed. *This step is very critical to the setup* . If you run to any errors the `aws-efs-csi` driver might throw errors which might be hard to diagonize.\n+\n+  * EFS CSI Driver installation\n+\n+    * Ensure that ```--set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name``` is set correctly \n+\n+    * You may inspect the values by running ``helm list`` and ```helm get all YOUR_RELEASE_ID``` to verify if the values used for the installation\n+\n+    * You can execute the following commands to inspect the pods / events to debug EFS / CSI Issues\n+\n+      ```bash\n+      kubectl get events --sort-by='.metadata.creationTimestamp'\n+      \n+      kubectl get pod --all-namespaces # Get the Pod ID\n+      \n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl describe pod/efs-provisioner-YOUR_POD\n+      ```\n+\n+    * A more involved debugging step would involve installing a simple example app to verify EFS / EKS setup as described [here](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html) (Section : *To deploy a sample application and verify that the CSI driver is working*)\n+\n+    * More info about the driver can be found at \n+\n+      * [Github Page](https://github.com/kubernetes-sigs/aws-efs-csi-driver/) / [Helm Chart](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs) / [EKS Workshop](https://www.eksworkshop.com/beginner/190_efs/efs-provisioner/) / [AWS Docs](https://aws.amazon.com/premiumsupport/knowledge-center/eks-persistent-storage/)\n+\n+  \n+\n+  \n+\n+  **Troubleshooting Torchserve Helm Chart**\n+\n+  \n+\n+  Possible errors in this step may be a result of \n+\n+  * Incorrect values in ``values.yaml``\n+    * Changing values in `torchserve.pvd_mount`  would need corresponding change in `config.properties`\n+  * Invalid `config.properties`\n+    * You can verify these values by running this for local TS installation\n+  * TS Pods in *Pending* state\n+    * Ensure you have available Nodes in Node Group\n+\n+  * Helm Installation\n+    * You may inspect the values by running ``helm list`` and `helm get all ts` to verify if the values used for the installation\n+    * You can uninstall / reinstall the helm chart by executing  `helm uninstall ts` and `helm install ts .`\n+    * If you get an error `invalid: data: Too long: must have at most 1048576 characters`, ensure that you dont have any stale files in your kubernetes dir. Else add them to .helmignore file.\n+\n+  \n+\n+  ## Deleting Resources\n+\n+  \n+\n+  * Delete EKS cluster `eksctl delete cluster -name YOUR_CLUSTER_NAME`\n+  * Delete EFS `aws efs delete-file-system --file-system-id YOUR_EFS_FS_ID`\n+  * Delete Security Groups ``aws ec2 delete-security-group --group-id YOUR_SECURITY_GRP_ID` ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf00e1a72ef37589a175439141591a4ac6c1cd41"}, "originalPosition": 804}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUzODEzMw==", "bodyText": "This is the output from setup_efs.sh https://github.com/pytorch/serve/blob/k8s/kubernetes/setup_efs.sh#L20", "url": "https://github.com/pytorch/serve/pull/551#discussion_r462538133", "createdAt": "2020-07-29T19:32:33Z", "author": {"login": "dhanainme"}, "path": "kubernetes/README.md", "diffHunk": "@@ -0,0 +1,815 @@\n+* ## Torchserve on Kubernetes\n+\n+  ## Overview\n+\n+  This page demonstrates a Torchserve deployment in Kubernetes using Helm Charts. It uses the DockerHub Torchserve Image for the pods and a PersistentVolume for storing config / model files.\n+\n+  ![EKS Overview](images/overview.png)\n+\n+  In the following sections we would \n+  * Create a EKS Cluster for deploying Torchserve\n+  * Create a PersistentVolume backed by EFS to store models and config\n+  * Use Helm charts to deploy Torchserve\n+\n+  All these steps scripts are written for AWS EKS with Ubuntu 18.04 for deployment, but could be easily adopted for Kubernetes offering from other vendors.\n+\n+  ## Prerequisites\n+\n+  We would need the following tools to be installed to setup the K8S Torchserve cluster.\n+\n+  * AWS CLI - [Installation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n+  * eksctl - [Installation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html)\n+  * kubectl - [Installation](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n+  * helm - [Installation](https://helm.sh/docs/intro/install/)\n+  * jq  - For JSON parsing in CLI\n+\n+  \n+\n+  ```bash\n+  sudo apt-get update\n+  \n+  # Install AWS CLI & Set Credentials\n+  curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n+  unzip awscliv2.zip\n+  sudo ./aws/install\n+  \n+  # Verify your aws cli installation\n+  aws --version\n+  \n+  # Setup your AWS credentials / region\n+  export AWS_ACCESS_KEY_ID=\n+  export AWS_SECRET_ACCESS_KEY=\n+  export AWS_DEFAULT_REGION=\n+  \n+  \n+  # Install eksctl\n+  curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n+  sudo mv /tmp/eksctl /usr/local/bin\n+  \n+  # Verify your eksctl installation\n+  eksctl version\n+  \n+  # Install kubectl\n+  curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\"\n+  chmod +x ./kubectl\n+  sudo mv ./kubectl /usr/local/bin/kubectl\n+  \n+  # Verify your kubectl installation\n+  kubectl version --client\n+  \n+  # Install helm\n+  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+  chmod 700 get_helm.sh\n+  ./get_helm.sh\n+  \n+  \n+  # Install jq\n+  sudo apt-get install jq\n+  \n+  # Clone TS\n+  git clone https://github.com/pytorch/serve/\n+  cd kubernetes/\n+  ```\n+\n+  \n+\n+  ## EKS Cluster setup\n+\n+  In this section we decribe creating a EKS Kubernetes cluster with GPU nodes. If you have an existing EKS / Kubernetes cluster you may skip this section and skip ahead to PersistentVolume preparation. \n+\n+  Ensure you have your installed all required dependices & configured AWS CLI from the previous steps  appropriate permissions. The following steps would,\n+\n+  * Create a EKS cluster\n+  * Install all the required driver for NVIDIA GPU.\n+\n+\n+  ### Creating a EKS cluster\n+\n+  **EKS Optimized AMI Subscription**\n+\n+  First subscribe to EKS-optimized AMI with GPU Support in the AWS Marketplace. Subscribe [here](https://aws.amazon.com/marketplace/pp/B07GRHFXGM). These hosts would be used for the EKS Node Group. \n+\n+  More details about these AMIs and configuring can be found [here](https://github.com/awslabs/amazon-eks-ami) and [here](https://eksctl.io/usage/custom-ami-support/)\n+\n+  **Create a EKS Cluster**\n+\n+\n+  To create a cluster run the following command. \n+\n+  First update the `templates/eks_cluster.yaml` with \n+\n+  ```yaml\n+  apiVersion: eksctl.io/v1alpha5\n+  kind: ClusterConfig\n+  \n+  metadata:\n+    name: \"TorchserveCluster\"\n+    region: \"us-west-2\" # Update AWS Region\n+  \n+  nodeGroups:\n+    - name: ng-1\n+      instanceType: g4dn.xlarge # Update Node Type\n+      desiredCapacity: 3 # Update Node count\n+  ```\n+\n+  \n+\n+  Then run the following command\n+\n+  ```eksctl create cluster -f templates/eks_cluster.yaml```\n+\n+  \n+\n+  Your output should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ eksctl create cluster -f templates/eks_cluster.yaml\n+  [\u2139]  eksctl version 0.24.0\n+  [\u2139]  using region us-west-2\n+  [\u2139]  setting availability zones to [us-west-2c us-west-2b us-west-2a]\n+  [\u2139]  subnets for us-west-2c - public:192.168.0.0/19 private:192.168.96.0/19\n+  [\u2139]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19\n+  [\u2139]  subnets for us-west-2a - public:192.168.64.0/19 private:192.168.160.0/19\n+  [\u2139]  nodegroup \"ng-1\" will use \"ami-0b6e3586ae536bd40\" [AmazonLinux2/1.16]\n+  [\u2139]  using Kubernetes version 1.16\n+  [\u2139]  creating EKS cluster \"TorchserveCluster\" in \"us-west-2\" region with un-managed nodes\n+  [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n+  [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\n+  [\u2139]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=TorchserveCluster'\n+  [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"TorchserveCluster\" in \"us-west-2\"\n+  [\u2139]  2 sequential tasks: { create cluster control plane \"TorchserveCluster\", 2 sequential sub-tasks: { update CloudWatch logging configuration, create nodegroup \"ng-1\" } }\n+  [\u2139]  building cluster stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-cluster\"\n+  [\u2714]  configured CloudWatch logging for cluster \"TorchserveCluster\" in \"us-west-2\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)\n+  [\u2139]  building nodegroup stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  --nodes-min=1 was set automatically for nodegroup ng-1\n+  [\u2139]  --nodes-max=1 was set automatically for nodegroup ng-1\n+  [\u2139]  deploying stack \"eksctl-TorchserveCluster-nodegroup-ng-1\"\n+  [\u2139]  waiting for the control plane availability...\n+  [\u2714]  saved kubeconfig as \"/home/ubuntu/.kube/config\"\n+  [\u2139]  no tasks\n+  [\u2714]  all EKS cluster resources for \"TorchserveCluster\" have been created\n+  [\u2139]  adding identity \"arn:aws:iam::ACCOUNT_ID:role/eksctl-TorchserveCluster-nodegrou-NodeInstanceRole\" to auth ConfigMap\n+  [\u2139]  nodegroup \"ng-1\" has 0 node(s)\n+  [\u2139]  waiting for at least 1 node(s) to become ready in \"ng-1\"\n+  [\u2139]  nodegroup \"ng-1\" has 1 node(s)\n+  [\u2139]  node \"ip-instance_id.us-west-2.compute.internal\" is ready\n+  [\u2139]  as you are using a GPU optimized instance type you will need to install NVIDIA Kubernetes device plugin.\n+  [\u2139]  \t see the following page for instructions: https://github.com/NVIDIA/k8s-device-plugin\n+  [\u2139]  kubectl command should work with \"/home/ubuntu/.kube/config\", try 'kubectl get nodes'\n+  [\u2714]  EKS cluster \"TorchserveCluster\" in \"us-west-2\" region is ready\n+  ```\n+\n+  \n+\n+  This would create a EKS cluster named **TorchserveCluster**. This step would takes a considetable amount time to create EKS clusters. You would be able to track the progress in your cloudformation console. If you run in to any error inspect the events tab of the Cloud Formation UI.\n+\n+  \n+\n+  ![EKS Overview](images/eks_cfn.png)\n+\n+  \n+\n+  Verify that the cluster has been created with the following commands \n+\n+  ```bash\n+  eksctl get  clusters\n+  kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  ```\n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ eksctl get  clusters\n+  NAME\t\t\tREGION\n+  TorchserveCluster\tus-west-2\n+  \n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         27m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   27m\n+  \n+  NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE\n+  kube-system   pod/aws-node-2flf5             1/1     Running   0          19m\n+  kube-system   pod/coredns-55c5fcd78f-2h7s4   1/1     Running   0          27m\n+  kube-system   pod/coredns-55c5fcd78f-pm6n5   1/1     Running   0          27m\n+  kube-system   pod/kube-proxy-pp8t2           1/1     Running   0          19m\n+  \n+  NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node     1         1         1       1            1           <none>          27m\n+  kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           <none>          27m\n+  \n+  ```\n+\n+  \n+\n+  **NVIDIA Plugin**\n+\n+  The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to run GPU enabled containers. The instructions for installing the plugin can be found [here](https://github.com/NVIDIA/k8s-device-plugin#installing-via-helm-installfrom-the-nvidia-device-plugin-helm-repository)\n+\n+  ```bash\n+  helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n+  helm repo update\n+  helm install \\\n+      --version=0.6.0 \\\n+      --generate-name \\\n+      nvdp/nvidia-device-plugin\n+  ```\n+\n+  To verify that the plugin has been installed execute the following command \n+\n+  ```bash\n+  helm list\n+  ```\n+\n+  Your output should look similar to\n+\n+  ```bash\n+  ubuntu@ip-172-31-55-101:~/serve/kubernetes$ helm list\n+  NAME                           \tNAMESPACE\tREVISION\tUPDATED                                \tSTATUS  \tCHART                     \tAPP VERSION\n+  nvidia-device-plugin-1595917413\tdefault  \t1       \t2020-07-28 06:23:34.522975795 +0000 UTC\tdeployed\tnvidia-device-plugin-0.6.0\t0.6.0\n+  ```\n+\n+  \n+\n+  ## Setup PersistentVolume backed by EFS\n+\n+  Torchserve Helm Chart needs a PersistentVolume with a PVC label `model-store-claim` prepared with a specific folder structure shown below. This PersistentVolume contains the snapshot & model files which are shared between multiple pods of the torchserve deployment.\n+\n+      model-server/\n+      \u251c\u2500\u2500 config\n+      \u2502   \u2514\u2500\u2500 config.properties\n+      \u2514\u2500\u2500 model-store\n+          \u251c\u2500\u2500 mnist.mar\n+          \u2514\u2500\u2500 squeezenet1_1.mar\n+\n+\n+  **Create EFS Volume for the EKS Cluster**\n+\n+  This section describes steps to prepare a EFS backed PersistentVolume that would be used by the TS Helm Chart. To prepare a EFS volume as a shareifjccgiced model / config store we have to create a EFS file system, Security Group, Ingress rule, Mount Targets to enable EFS communicate across NAT of the EKS cluster. \n+\n+  The heavy lifting for these steps is performed by ``setup_efs.sh`` script. To run the script, Update the following variables in `setup_efs.sh`\n+\n+  ```bash\n+  CLUSTER_NAME=TorchserveCluster # EKS TS Cluser Name\n+  MOUNT_TARGET_GROUP_NAME=\"eks-efs-group\"\n+  ```\n+\n+  Then run `./setup_efs.sh`\n+\n+  \n+\n+  The output of the script should look similar to,\n+\n+  \n+\n+  ```bash\n+  Configuring TorchserveCluster\n+  Obtaining VPC ID for TorchserveCluster\n+  Obtained VPC ID - vpc-fff\n+  Obtaining CIDR BLOCK for vpc-fff\n+  Obtained CIDR BLOCK - 192.168.0.0/16\n+  Creating Security Group\n+  Created Security Group - sg-fff\n+  Configuring Security Group Ingress\n+  Creating EFS Fils System\n+  Created EFS - fs-ff\n+  {\n+      \"FileSystems\": [\n+          {\n+              \"OwnerId\": \"XXXX\",\n+              \"CreationToken\": \"4ae307b6-62aa-44dd-909e-eebe0d0b19f3\",\n+              \"FileSystemId\": \"fs-88983c8d\",\n+              \"FileSystemArn\": \"arn:aws:elasticfilesystem:us-west-2:ff:file-system/fs-ff\",\n+              \"CreationTime\": \"2020-07-29T08:03:33+00:00\",\n+              \"LifeCycleState\": \"creating\",\n+              \"NumberOfMountTargets\": 0,\n+              \"SizeInBytes\": {\n+                  \"Value\": 0,\n+                  \"ValueInIA\": 0,\n+                  \"ValueInStandard\": 0\n+              },\n+              \"PerformanceMode\": \"generalPurpose\",\n+              \"Encrypted\": false,\n+              \"ThroughputMode\": \"bursting\",\n+              \"Tags\": []\n+          }\n+      ]\n+  }\n+  Waiting 30s for before procedding\n+  Obtaining Subnets\n+  Obtained Subnets - subnet-ff\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.58.19\",\n+      \"NetworkInterfaceId\": \"eni-01ce1fd11df545226\",\n+      \"AvailabilityZoneId\": \"usw2-az1\",\n+      \"AvailabilityZoneName\": \"us-west-2b\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.5.7\",\n+      \"NetworkInterfaceId\": \"eni-03db930b204de6ab2\",\n+      \"AvailabilityZoneId\": \"usw2-az3\",\n+      \"AvailabilityZoneName\": \"us-west-2c\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  Creating EFS Mount Target in subnet-ff\n+  {\n+      \"OwnerId\": \"XXXX\",\n+      \"MountTargetId\": \"fsmt-ff\",\n+      \"FileSystemId\": \"fs-ff\",\n+      \"SubnetId\": \"subnet-ff\",\n+      \"LifeCycleState\": \"creating\",\n+      \"IpAddress\": \"192.168.73.152\",\n+      \"NetworkInterfaceId\": \"eni-0a31830833bf6b030\",\n+      \"AvailabilityZoneId\": \"usw2-az2\",\n+      \"AvailabilityZoneName\": \"us-west-2a\",\n+      \"VpcId\": \"vpc-ff\"\n+  }\n+  EFS File System ID - YOUR-EFS-ID\n+  EFS File System DNS Name - YOUR-EFS-ID.efs..amazonaws.com\n+  Succesfully created EFS & Mountpoints\n+  ```\n+\n+  \n+\n+  Upon completion of the script it would emit a EFS volume DNS Name similar to `fs-ab1cd.efs.us-west-2.amazonaws.com` where `fs-ab1cd` is the EFS filesystem id.\n+\n+  \n+\n+  You should be able to a Security Group in your AWS Console with Inbound Rules to a NFS (Port 2049)\n+\n+  \n+\n+  ![security_group](images/security_group.png)\n+\n+  \n+\n+  You should also find Mount Points in your EFS console for every region where there is a Node in the Node Group.\n+\n+  \n+\n+  ![](images/efs_mount.png)\n+\n+  \n+\n+  \n+\n+  **Prepare PersistentVolume for Deployment**\n+\n+  We use the [ELF Provisioner Helm Chart](https://github.com/helm/charts/tree/master/stable/efs-provisioner) to create a PersistentVolume backed by EFS. Run the following command to set this up.\n+\n+  ```bash\n+  helm repo add stable https://kubernetes-charts.storage.googleapis.com\n+  helm install stable/efs-provisioner --set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name\n+  ```\n+\n+  \n+\n+  you should get an output similar to \n+\n+  \n+\n+  ```bash\n+  NAME: efs-provisioner-1596010253\n+  LAST DEPLOYED: Wed Jul 29 08:10:56 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  NOTES:\n+  You can provision an EFS-backed persistent volume with a persistent volume claim like below:\n+  \n+  kind: PersistentVolumeClaim\n+  apiVersion: v1\n+  metadata:\n+    name: my-efs-vol-1\n+    annotations:\n+      volume.beta.kubernetes.io/storage-class: aws-efs\n+  spec:\n+    storageClassName: aws-efs\n+    accessModes:\n+      - ReadWriteMany\n+    resources:\n+      requests:\n+        storage: 1Mi\n+  \n+  ```\n+\n+  \n+\n+  Verify that your EFS Provisioner installation is succesfull by invoking ```kubectl get pods```. Your output should look similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get pods\n+  NAME                                          READY   STATUS    RESTARTS   AGE\n+  efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          109s\n+  ```\n+\n+  \n+\n+  Now run, ```kubectl apply -f templates/efs_pv_claim.yaml```. This would also create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files in the same folder structure described above. \n+\n+  \n+\n+  Your output should look similar to,\n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl apply -f templates/efs_pv_claim.yaml\n+  persistentvolumeclaim/model-store-claim created\n+  pod/model-store-pod created\n+  ```\n+\n+  \n+\n+  Verify that the PVC / Pod is created  by excuting.   ```kubectl get service,po,daemonset,pv,pvc --all-namespaces``` \n+\n+  You should see\n+\n+  * ```Running``` status for ```pod/model-store-pod```  \n+  * ```Bound``` status for ```default/model-store-claim``` and ```persistentvolumeclaim/model-store-claim```\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl get service,po,daemonset,pv,pvc --all-namespaces\n+  NAMESPACE     NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE\n+  default       service/kubernetes   ClusterIP   10.100.0.1    <none>        443/TCP         107m\n+  kube-system   service/kube-dns     ClusterIP   10.100.0.10   <none>        53/UDP,53/TCP   107m\n+  \n+  NAMESPACE     NAME                                              READY   STATUS    RESTARTS   AGE\n+  default       pod/efs-provisioner-1596010253-6c459f95bb-v68bm   1/1     Running   0          4m49s\n+  default       pod/model-store-pod                               1/1     Running   0          8s\n+  kube-system   pod/aws-node-xx8kp                                1/1     Running   0          99m\n+  kube-system   pod/coredns-5c97f79574-tchfg                      1/1     Running   0          107m\n+  kube-system   pod/coredns-5c97f79574-thzqw                      1/1     Running   0          106m\n+  kube-system   pod/kube-proxy-4l8mw                              1/1     Running   0          99m\n+  kube-system   pod/nvidia-device-plugin-daemonset-dbhgq          1/1     Running   0          94m\n+  \n+  NAMESPACE     NAME                                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n+  kube-system   daemonset.apps/aws-node                         1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/kube-proxy                       1         1         1       1            1           <none>          107m\n+  kube-system   daemonset.apps/nvidia-device-plugin-daemonset   1         1         1       1            1           <none>          94m\n+  \n+  NAMESPACE   NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE\n+              persistentvolume/pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            Delete           Bound    default/model-store-claim   aws-efs                 8s\n+  \n+  NAMESPACE   NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+  default     persistentvolumeclaim/model-store-claim   Bound    pvc-baf0bd37-2084-4a08-8a3c-4f77843b4736   1Gi        RWX            aws-efs        8s\n+  ```\n+\n+  \n+\n+  Now edit the TS config file `config.properties` that would be used for the deployment. Any changes to this config should also have corresponding changes in Torchserve Helm Chart that we install in the next section.\n+\n+  \n+\n+  The default config starts **squeezenet1_1** and **mnist** from the model zoo with 3, 5 workers.\n+\n+  \n+\n+  ```yaml\n+  inference_address=http://0.0.0.0:8080\n+  management_address=http://0.0.0.0:8081\n+  NUM_WORKERS=1\n+  number_of_gpu=1\n+  number_of_netty_threads=32\n+  job_queue_size=1000\n+  model_store=/home/model-server/shared/model-store\n+  model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}\n+  ```\n+\n+  \n+\n+  Now copy the files over to PersistentVolume using the following commands.\n+\n+  \n+\n+  ```bash\n+  wget https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar\n+  wget https://torchserve.s3.amazonaws.com/mar_files/mnist.mar\n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\n+  kubectl cp squeezenet1_1.mar model-store-pod:/pv/model-store/squeezenet1_1.mar\n+  kubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n+  \n+  \n+  kubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\n+  kubectl cp config.properties model-store-pod:/pv/config/config.properties\n+  ```\n+\n+  \n+\n+  Verify that the files have been copied by executing ```kubectl exec --tty pod/model-store-pod -- find /pv/``` . You should get an output similar to,\n+\n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec --tty pod/model-store-pod -- find /pv/\n+  /pv/\n+  /pv/config\n+  /pv/config/config.properties\n+  /pv/model-store\n+  /pv/model-store/squeezenet1_1.mar\n+  /pv/model-store/mnist.mar\n+  ```\n+\n+  \n+\n+  Finally terminate the pod - `kubectl delete pod/model-store-pod`.\n+\n+  \n+\n+  ## Deploy TorchServe using Helm Charts\n+\n+  \n+  The following table describes all the parameters for the Helm Chart.\n+\n+  | Parameter          | Description              | Default                         |\n+  | ------------------ | ------------------------ | ------------------------------- |\n+  | `image`            | Torchserve Serving image | `pytorch/torchserve:latest-gpu` |\n+  | `management-port`  | TS Inference port        | `8080`                          |\n+  | `inference-port`   | TS Management port       | `8081`                          |\n+  | `replicas`         | K8S deployment replicas  | `1`                             |\n+  | `model-store`      | EFS mountpath            | `/home/model-server/shared/`    |\n+  | `persistence.size` | Storage size to request  | `1Gi`                           |\n+  | `n_gpu`            | Number of GPU            | `1`                             |\n+  | `n_cpu`            | Number of CPU            | `1`                             |\n+  | `memory_limit`     | TS Pod memory limit      | `4Gi`                           |\n+  | `memory_request`   | TS Pod memory request    | `1Gi`                           |\n+\n+\n+  Edit the values in `values.yaml` with the right parameters. \n+  \n+\n+  ```yaml\n+  # Default values for torchserve helm chart.\n+  \n+  torchserve_image: pytorch/torchserve:latest-gpu\n+  \n+  namespace: torchserve\n+  \n+  torchserve:\n+    management_port: 8081\n+    inference_port: 8080\n+    pvd_mount: /home/model-server/shared/\n+    n_gpu: 1\n+    n_cpu: 1\n+    memory_limit: 4Gi\n+    memory_request: 1Gi\n+  \n+  deployment:\n+    replicas: 1 # Changes this to number of node in Node Group\n+  \n+  persitant_volume:\n+    size: 1Gi\n+  ```\n+\n+\n+  To install Torchserve run ```helm install ts .```  \n+  \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ helm install ts .\n+  NAME: ts\n+  LAST DEPLOYED: Wed Jul 29 08:29:04 2020\n+  NAMESPACE: default\n+  STATUS: deployed\n+  REVISION: 1\n+  TEST SUITE: None\n+  ```\n+  \n+\n+  Verify that torchserve has succesfully started by executing ```kubectl exec pod/torchserve-fff -- cat logs/ts_log.log``` on your torchserve pod. You can get this id by lookingup `kubectl get po --all-namespaces`\n+\n+  \n+\n+  Your output should should look similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-50-36:~/serve/kubernetes$ kubectl exec pod/torchserve-fff -- cat logs/ts_log.log\n+  2020-07-29 08:29:08,295 [INFO ] main org.pytorch.serve.ModelServer -\n+  Torchserve version: 0.1.1\n+  TS Home: /home/venv/lib/python3.6/site-packages\n+  Current directory: /home/model-server\n+  ......\n+  ```\n+\n+\n+  ## Test Torchserve Installation\n+\n+  Fetch the Load Balancer Extenal IP by executing \n+\n+  ```bash\n+  kubectl get svc\n+  ```\n+\n+  You should see an entry similar to \n+\n+  ```bash\n+  ubuntu@ip-172-31-65-0:~/ts/rel/serve$ kubectl get svc\n+  NAME         TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)                         AGE\n+  torchserve   LoadBalancer   10.100.142.22   your_elb.us-west-2.elb.amazonaws.com   8080:31115/TCP,8081:31751/TCP   14m\n+  ```\n+\n+  Now execute the following commands to test Management / Prediction APIs\n+  ```bash\n+  curl http://your_elb.us-west-2.elb.amazonaws.com:8081/models\n+  \n+  # You should something similar to the following\n+  {\n+    \"models\": [\n+      {\n+        \"modelName\": \"mnist\",\n+        \"modelUrl\": \"mnist.mar\"\n+      },\n+      {\n+        \"modelName\": \"squeezenet1_1\",\n+        \"modelUrl\": \"squeezenet1_1.mar\"\n+      }\n+    ]\n+  }\n+  \n+  \n+  curl http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8081/models/squeezenet1_1\n+  \n+  # You should see something similar to the following\n+  [\n+    {\n+      \"modelName\": \"squeezenet1_1\",\n+      \"modelVersion\": \"1.0\",\n+      \"modelUrl\": \"squeezenet1_1.mar\",\n+      \"runtime\": \"python\",\n+      \"minWorkers\": 3,\n+      \"maxWorkers\": 3,\n+      \"batchSize\": 1,\n+      \"maxBatchDelay\": 100,\n+      \"loadedAtStartup\": false,\n+      \"workers\": [\n+        {\n+          \"id\": \"9000\",\n+          \"startTime\": \"2020-07-23T18:34:33.201Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177491968\n+        },\n+        {\n+          \"id\": \"9001\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177569792\n+        },\n+        {\n+          \"id\": \"9002\",\n+          \"startTime\": \"2020-07-23T18:34:33.204Z\",\n+          \"status\": \"READY\",\n+          \"gpu\": true,\n+          \"memoryUsage\": 177872896\n+        }\n+      ]\n+    }\n+  ]\n+  \n+  \n+  wget https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\n+  curl -X POST  http://your_elb.us-west-2.elb.amazonaws.com.us-west-2.elb.amazonaws.com:8080/predictions/squeezenet1_1 -T kitten_small.jpg\n+  \n+  # You should something similar to the following\n+  [\n+    {\n+      \"lynx\": 0.5370921492576599\n+    },\n+    {\n+      \"tabby\": 0.28355881571769714\n+    },\n+    {\n+      \"Egyptian_cat\": 0.10669822245836258\n+    },\n+    {\n+      \"tiger_cat\": 0.06301568448543549\n+    },\n+    {\n+      \"leopard\": 0.006023923866450787\n+    }\n+  ]\n+  ```\n+  \n+\n+  ## Troubleshooting\n+  \n+\n+  **Troubleshooting EKCTL Cluster Creation**\n+\n+  Possible errors in this step may be a result of \n+\n+  * AWS Account limits. \n+  * IAM Policy of the role used during cluster creation - [Minimum IAM Policy](https://eksctl.io/usage/minimum-iam-policies/)\n+\n+  Inspect your Cloudformation console' events tab, to diagonize any possible issues. You should able be able to find the following resources at the end of this step in the respective AWS consoles\n+\n+  * EKS Cluser in the EKS UI\n+  * AutoScaling Group of the Node Groups\n+  * EC2 Node correponding to the node groups.\n+\n+  \n+\n+  Also, take a look at [eksctl](https://eksctl.io/introduction/) website / [github repo](https://github.com/weaveworks/eksctl/issues) for any issues. \n+\n+  \n+\n+  **Troubleshooting EFS Persitant Volume Creation** \n+\n+  \n+\n+  Possible error in this step may be a result of one of the following. Your pod my be struck in *Init / Creating* forever / persitant volume claim may be in *Pending* forever.\n+\n+  * Incorrect CLUSTER_NAME or Duplicate MOUNT_TARGET_GROUP_NAME in `setup_efs.sh` \n+\n+    * Rerun the script with a different `MOUNT_TARGET_GROUP_NAME` to avoid conflict with a previous run\n+\n+  * Faulty execution of ``setup_efs.sh`` \n+\n+    * Look up the screenshots above for the expected AWS Console UI for Security group & EFS. If you dont see the Ingress permissions / Mount points created, Execute steps from ```setup_efs.sh``` to make sure that they complete as expected.  We need 1 Mount point for every region where Nodes would be deployed. *This step is very critical to the setup* . If you run to any errors the `aws-efs-csi` driver might throw errors which might be hard to diagonize.\n+\n+  * EFS CSI Driver installation\n+\n+    * Ensure that ```--set efsProvisioner.efsFileSystemId=YOUR-EFS-FS-ID --set efsProvisioner.awsRegion=us-west-2 --set efsProvisioner.reclaimPolicy=Retain --generate-name``` is set correctly \n+\n+    * You may inspect the values by running ``helm list`` and ```helm get all YOUR_RELEASE_ID``` to verify if the values used for the installation\n+\n+    * You can execute the following commands to inspect the pods / events to debug EFS / CSI Issues\n+\n+      ```bash\n+      kubectl get events --sort-by='.metadata.creationTimestamp'\n+      \n+      kubectl get pod --all-namespaces # Get the Pod ID\n+      \n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl logs pod/efs-provisioner-YOUR_POD\n+      kubectl describe pod/efs-provisioner-YOUR_POD\n+      ```\n+\n+    * A more involved debugging step would involve installing a simple example app to verify EFS / EKS setup as described [here](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html) (Section : *To deploy a sample application and verify that the CSI driver is working*)\n+\n+    * More info about the driver can be found at \n+\n+      * [Github Page](https://github.com/kubernetes-sigs/aws-efs-csi-driver/) / [Helm Chart](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs) / [EKS Workshop](https://www.eksworkshop.com/beginner/190_efs/efs-provisioner/) / [AWS Docs](https://aws.amazon.com/premiumsupport/knowledge-center/eks-persistent-storage/)\n+\n+  \n+\n+  \n+\n+  **Troubleshooting Torchserve Helm Chart**\n+\n+  \n+\n+  Possible errors in this step may be a result of \n+\n+  * Incorrect values in ``values.yaml``\n+    * Changing values in `torchserve.pvd_mount`  would need corresponding change in `config.properties`\n+  * Invalid `config.properties`\n+    * You can verify these values by running this for local TS installation\n+  * TS Pods in *Pending* state\n+    * Ensure you have available Nodes in Node Group\n+\n+  * Helm Installation\n+    * You may inspect the values by running ``helm list`` and `helm get all ts` to verify if the values used for the installation\n+    * You can uninstall / reinstall the helm chart by executing  `helm uninstall ts` and `helm install ts .`\n+    * If you get an error `invalid: data: Too long: must have at most 1048576 characters`, ensure that you dont have any stale files in your kubernetes dir. Else add them to .helmignore file.\n+\n+  \n+\n+  ## Deleting Resources\n+\n+  \n+\n+  * Delete EKS cluster `eksctl delete cluster -name YOUR_CLUSTER_NAME`\n+  * Delete EFS `aws efs delete-file-system --file-system-id YOUR_EFS_FS_ID`\n+  * Delete Security Groups ``aws ec2 delete-security-group --group-id YOUR_SECURITY_GRP_ID` ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyOTk4NQ=="}, "originalCommit": {"oid": "bf00e1a72ef37589a175439141591a4ac6c1cd41"}, "originalPosition": 804}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1568, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}