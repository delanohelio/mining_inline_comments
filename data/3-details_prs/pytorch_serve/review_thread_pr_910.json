{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxMjc0OTAy", "number": 910, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNzozMTowNVrOFGT84Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxODo1MzoxNlrOFGWJWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTYyNjU3OnYy", "diffSide": "RIGHT", "path": "examples/image_classifier/mnist/README.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNzozMTowNVrOIHQ6KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyMjo0OFrOIHsIfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ4Nzk3Nw==", "bodyText": "@abishekchiffon Please change to \"TorchServe supports Captum explanations for eager mode only\"  this is for all models and nothing specific to mnist model", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544487977", "createdAt": "2020-12-16T17:31:05Z", "author": {"login": "chauhang"}, "path": "examples/image_classifier/mnist/README.md", "diffHunk": "@@ -51,6 +51,8 @@ When a json file is passed as a request format to the curl, Torchserve unwraps t\n \n The explain is called with the following request api http://127.0.0.1:8080/explanations/mnist_explain\n \n+Torchserve supports Captum Explanations for Eager models of mnist only.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ4ODkxMA==", "bodyText": "Same comment for other places as well", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544488910", "createdAt": "2020-12-16T17:32:21Z", "author": {"login": "chauhang"}, "path": "examples/image_classifier/mnist/README.md", "diffHunk": "@@ -51,6 +51,8 @@ When a json file is passed as a request format to the curl, Torchserve unwraps t\n \n The explain is called with the following request api http://127.0.0.1:8080/explanations/mnist_explain\n \n+Torchserve supports Captum Explanations for Eager models of mnist only.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ4Nzk3Nw=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNDAxMw==", "bodyText": "@chauhang  Changed in all the places.", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544934013", "createdAt": "2020-12-17T09:22:48Z", "author": {"login": "abishekchiffon"}, "path": "examples/image_classifier/mnist/README.md", "diffHunk": "@@ -51,6 +51,8 @@ When a json file is passed as a request format to the curl, Torchserve unwraps t\n \n The explain is called with the following request api http://127.0.0.1:8080/explanations/mnist_explain\n \n+Torchserve supports Captum Explanations for Eager models of mnist only.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ4Nzk3Nw=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTY0NDc0OnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/Huggingface_readme.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNzozNToxMFrOIHRFKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyMzoyN1rOIHsKJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ5MDc5NA==", "bodyText": "Please change to \"The service_envelope=kfserving setting is needed when deploying models on KFServing\"", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544490794", "createdAt": "2020-12-16T17:35:10Z", "author": {"login": "chauhang"}, "path": "kubernetes/kfserving/Huggingface_readme.md", "diffHunk": "@@ -14,17 +14,23 @@ This produces all the required files for packaging using a huggingface transform\n The .mar file creation command is as below:\n \n ```\n-torch-model-archiver --model-name BERTSeqClassification --version 1.0 --serialized-file serve/examples/Huggingface_Transformers/Transformer_model/pytorch_model.bin --handler serve/examples/Huggingface_Transformers/Transformer_model/Transformer_handler_generalized.py --source-vocab serve/examples/Huggingface_Transformers/Transformer_model/vocab.txt --extra-files \"Transformer_model/config.json,serve/examples/Huggingface_Transformers/Transformer_model/setup_config.json,serve/examples/Huggingface_Transformers/Transformer_model/index_to_name.json\"\n+torch-model-archiver --model-name BERTSeqClassification --version 1.0 --serialized-file serve/examples/Huggingface_Transformers/Transformer_model/pytorch_model.bin --handler serve/examples/Huggingface_Transformers/Transformer_model/Transformer_handler_generalized.py --extra-files \"serve/examples/Huggingface_Transformers/Transformer_model/vocab.txt,serve/examples/Huggingface_Transformers/Transformer_model/config.json,serve/examples/Huggingface_Transformers/Transformer_model/setup_config.json,serve/examples/Huggingface_Transformers/Transformer_model/index_to_name.json\"\n ```\n \n-## Starting Torchserve\n+## Starting Torchserve for KFServing Predictor\n To serve an Inference Request for Torchserve using the KFServing Spec, follow the below:\n \n * create a config.properties file and specify the details as shown:\n ```\n+inference_address=http://127.0.0.0:8085\n+management_address=http://127.0.0.0:8081\n+number_of_netty_threads=4\n service_envelope=kfserving\n+job_queue_size=10\n+model_store=model-store\n+\n ```\n-The Service Envelope field is mandatory for Torchserve to process the KFServing Input Request Format.\n+The Service Envelope field should be set as kfserving and it is mandatory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNDQzOQ==", "bodyText": "@chauhang  Changed them in the respective places", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544934439", "createdAt": "2020-12-17T09:23:27Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/Huggingface_readme.md", "diffHunk": "@@ -14,17 +14,23 @@ This produces all the required files for packaging using a huggingface transform\n The .mar file creation command is as below:\n \n ```\n-torch-model-archiver --model-name BERTSeqClassification --version 1.0 --serialized-file serve/examples/Huggingface_Transformers/Transformer_model/pytorch_model.bin --handler serve/examples/Huggingface_Transformers/Transformer_model/Transformer_handler_generalized.py --source-vocab serve/examples/Huggingface_Transformers/Transformer_model/vocab.txt --extra-files \"Transformer_model/config.json,serve/examples/Huggingface_Transformers/Transformer_model/setup_config.json,serve/examples/Huggingface_Transformers/Transformer_model/index_to_name.json\"\n+torch-model-archiver --model-name BERTSeqClassification --version 1.0 --serialized-file serve/examples/Huggingface_Transformers/Transformer_model/pytorch_model.bin --handler serve/examples/Huggingface_Transformers/Transformer_model/Transformer_handler_generalized.py --extra-files \"serve/examples/Huggingface_Transformers/Transformer_model/vocab.txt,serve/examples/Huggingface_Transformers/Transformer_model/config.json,serve/examples/Huggingface_Transformers/Transformer_model/setup_config.json,serve/examples/Huggingface_Transformers/Transformer_model/index_to_name.json\"\n ```\n \n-## Starting Torchserve\n+## Starting Torchserve for KFServing Predictor\n To serve an Inference Request for Torchserve using the KFServing Spec, follow the below:\n \n * create a config.properties file and specify the details as shown:\n ```\n+inference_address=http://127.0.0.0:8085\n+management_address=http://127.0.0.0:8081\n+number_of_netty_threads=4\n service_envelope=kfserving\n+job_queue_size=10\n+model_store=model-store\n+\n ```\n-The Service Envelope field is mandatory for Torchserve to process the KFServing Input Request Format.\n+The Service Envelope field should be set as kfserving and it is mandatory.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ5MDc5NA=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTk2MzY4OnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/Huggingface_readme.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxODo0Nzo0OVrOIHUCUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyMzo0OVrOIHsLMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUzOTIxNw==", "bodyText": "Nitpick: Shouldn't this be json?", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544539217", "createdAt": "2020-12-16T18:47:49Z", "author": {"login": "fbbradheintz"}, "path": "kubernetes/kfserving/Huggingface_readme.md", "diffHunk": "@@ -102,6 +112,20 @@ The Explanation response is as below :\n   ]\n }\n ```\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:\n+```bash", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNDcwNg==", "bodyText": "@fbbradheintz Modified", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544934706", "createdAt": "2020-12-17T09:23:49Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/Huggingface_readme.md", "diffHunk": "@@ -102,6 +112,20 @@ The Explanation response is as below :\n   ]\n }\n ```\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:\n+```bash", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUzOTIxNw=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTk4MDY4OnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxODo1MTo1NVrOIHUMMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyMzo1M1rOIHsLTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MTc0NQ==", "bodyText": "Should this be json?", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544541745", "createdAt": "2020-12-16T18:51:55Z", "author": {"login": "fbbradheintz"}, "path": "kubernetes/kfserving/README.md", "diffHunk": "@@ -231,7 +237,25 @@ The response is as below :\n   ]\n }\n ```\n-For the request and response of BERT and Text Classifier models, refer the \"Request and Response\" section of section of [BERT Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/Huggingface_readme.md) and [Text Classifier Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/text_classifier_readme.md). .\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:\n+\n+```bash", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNDczNQ==", "bodyText": "@fbbradheintz Modified", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544934735", "createdAt": "2020-12-17T09:23:53Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/README.md", "diffHunk": "@@ -231,7 +237,25 @@ The response is as below :\n   ]\n }\n ```\n-For the request and response of BERT and Text Classifier models, refer the \"Request and Response\" section of section of [BERT Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/Huggingface_readme.md) and [Text Classifier Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/text_classifier_readme.md). .\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:\n+\n+```bash", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MTc0NQ=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTk4NTIyOnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxODo1MzowMFrOIHUPAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyNDo0M1rOIHsNjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MjQ2NQ==", "bodyText": "Somewhere, we should add that when performing static batching this way, the user needs to make sure they're using batch size of 1 (even though more than one input instance is specified in the input). Doing otherwise will cause a failure.", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544542465", "createdAt": "2020-12-16T18:53:00Z", "author": {"login": "fbbradheintz"}, "path": "kubernetes/kfserving/README.md", "diffHunk": "@@ -231,7 +237,25 @@ The response is as below :\n   ]\n }\n ```\n-For the request and response of BERT and Text Classifier models, refer the \"Request and Response\" section of section of [BERT Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/Huggingface_readme.md) and [Text Classifier Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/text_classifier_readme.md). .\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNTMxMA==", "bodyText": "@fbbradheintz Added it", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544935310", "createdAt": "2020-12-17T09:24:43Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/README.md", "diffHunk": "@@ -231,7 +237,25 @@ The response is as below :\n   ]\n }\n ```\n-For the request and response of BERT and Text Classifier models, refer the \"Request and Response\" section of section of [BERT Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/Huggingface_readme.md) and [Text Classifier Readme file](https://github.com/pytorch/serve/blob/master/kubernetes/kfserving/text_classifier_readme.md). .\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MjQ2NQ=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMTk4NjE2OnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/Huggingface_readme.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxODo1MzoxNlrOIHUPqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOToyNDo0N1rOIHsNwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MjYzMw==", "bodyText": "Somewhere, we should add that when performing static batching this way, the user needs to make sure they're using batch size of 1 (even though more than one input instance is specified in the input). Doing otherwise will cause a failure.", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544542633", "createdAt": "2020-12-16T18:53:16Z", "author": {"login": "fbbradheintz"}, "path": "kubernetes/kfserving/Huggingface_readme.md", "diffHunk": "@@ -102,6 +112,20 @@ The Explanation response is as below :\n   ]\n }\n ```\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkzNTM2MQ==", "bodyText": "@fbbradheintz Added it", "url": "https://github.com/pytorch/serve/pull/910#discussion_r544935361", "createdAt": "2020-12-17T09:24:47Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/Huggingface_readme.md", "diffHunk": "@@ -102,6 +112,20 @@ The Explanation response is as below :\n   ]\n }\n ```\n+\n+KFServing supports Static batching by adding new examples in the instances key of the request:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MjYzMw=="}, "originalCommit": {"oid": "18e9e072ade1a0c17f2a28bf31dfb11ab380e788"}, "originalPosition": 58}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1369, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}