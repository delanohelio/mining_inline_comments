{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5MDEyNTk3", "number": 750, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1MjowNFrOEx96AQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxODoyNDoyM1rOE83f0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODI5OTUzOnYy", "diffSide": "RIGHT", "path": "ts/model_loader.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1MjowNFrOHoWOYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwOToxMjoxOVrOHox_cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2OTIxNg==", "bodyText": "@abishekchiffon Why has the logic for handling multiple files been removed? Please check rest of this class and make sure the code is merged properly", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512069216", "createdAt": "2020-10-26T15:52:04Z", "author": {"login": "chauhang"}, "path": "ts/model_loader.py", "diffHunk": "@@ -74,48 +76,71 @@ def load(self, model_name, model_dir, handler, gpu_id, batch_size):\n             with open(manifest_file) as f:\n                 manifest = json.load(f)\n \n+        function_name = None\n         try:\n-            temp = handler.split(\":\", 1)\n-            module_name = temp[0]\n-            function_name = None if len(temp) == 1 else temp[1]\n-            if module_name.endswith(\".py\"):\n-                module_name = module_name[:-3]\n-            module_name = module_name.split(\"/\")[-1]\n-            module = importlib.import_module(module_name)\n-            # pylint: disable=unused-variable\n-        except ImportError as e:\n-            module_name = \".{0}\".format(handler)\n-            module = importlib.import_module(module_name, 'ts.torch_handler')\n-            function_name = None\n+            module, function_name = self._load_handler_file(handler)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUyNDE0Nw==", "bodyText": "The logic has not been removed. It has been moved to the function _load_handler_file of the same TsModelLoader class.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512524147", "createdAt": "2020-10-27T09:12:19Z", "author": {"login": "abishekchiffon"}, "path": "ts/model_loader.py", "diffHunk": "@@ -74,48 +76,71 @@ def load(self, model_name, model_dir, handler, gpu_id, batch_size):\n             with open(manifest_file) as f:\n                 manifest = json.load(f)\n \n+        function_name = None\n         try:\n-            temp = handler.split(\":\", 1)\n-            module_name = temp[0]\n-            function_name = None if len(temp) == 1 else temp[1]\n-            if module_name.endswith(\".py\"):\n-                module_name = module_name[:-3]\n-            module_name = module_name.split(\"/\")[-1]\n-            module = importlib.import_module(module_name)\n-            # pylint: disable=unused-variable\n-        except ImportError as e:\n-            module_name = \".{0}\".format(handler)\n-            module = importlib.import_module(module_name, 'ts.torch_handler')\n-            function_name = None\n+            module, function_name = self._load_handler_file(handler)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2OTIxNg=="}, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODMyMjIxOnYy", "diffSide": "RIGHT", "path": "ts/torch_handler/vision_handler.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1NjozNVrOHoWcZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNTo1MTo0M1rOHosWFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MjgwNg==", "bodyText": "@abishekchiffon Why is this dependency needed in the base vision handler?", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512072806", "createdAt": "2020-10-26T15:56:35Z", "author": {"login": "chauhang"}, "path": "ts/torch_handler/vision_handler.py", "diffHunk": "@@ -8,14 +8,16 @@\n import torch\n from PIL import Image\n from .base_handler import BaseHandler\n+from captum.attr import IntegratedGradients", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzMTYzOA==", "bodyText": "Inference does not require Integrated Gradients. The explain functions were in vision_handler, it is the remnant of the explain. The line will be removed.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512431638", "createdAt": "2020-10-27T05:51:43Z", "author": {"login": "abishekchiffon"}, "path": "ts/torch_handler/vision_handler.py", "diffHunk": "@@ -8,14 +8,16 @@\n import torch\n from PIL import Image\n from .base_handler import BaseHandler\n+from captum.attr import IntegratedGradients", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MjgwNg=="}, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODMyNTI1OnYy", "diffSide": "RIGHT", "path": "ts/torch_handler/image_classifier.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1NzoxM1rOHoWeVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwNTo1MDoyOFrOHosUvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MzMwMg==", "bodyText": "@abishekchiffon Why do we need this in the base image handler?", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512073302", "createdAt": "2020-10-26T15:57:13Z", "author": {"login": "chauhang"}, "path": "ts/torch_handler/image_classifier.py", "diffHunk": "@@ -4,6 +4,10 @@\n import torch\n import torch.nn.functional as F\n from torchvision import transforms\n+from torchvision import models", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzMTI5NQ==", "bodyText": "We don't need it, Geeta. Lines 7, 9, 10 will be removed.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512431295", "createdAt": "2020-10-27T05:50:28Z", "author": {"login": "abishekchiffon"}, "path": "ts/torch_handler/image_classifier.py", "diffHunk": "@@ -4,6 +4,10 @@\n import torch\n import torch.nn.functional as F\n from torchvision import transforms\n+from torchvision import models", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3MzMwMg=="}, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODMzMzc5OnYy", "diffSide": "RIGHT", "path": "kubernetes/kf_predictor_docker/Dockerfile_kf.dev", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1OTowM1rOHoWjyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo1OTowM1rOHoWjyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA3NDY5OA==", "bodyText": "@abishekchiffon Why is a separate Dockerfile needed for KFServing integration? Can we need have a combined docker for TorchServe?", "url": "https://github.com/pytorch/serve/pull/750#discussion_r512074698", "createdAt": "2020-10-26T15:59:03Z", "author": {"login": "chauhang"}, "path": "kubernetes/kf_predictor_docker/Dockerfile_kf.dev", "diffHunk": "@@ -0,0 +1,99 @@\n+# syntax = docker/dockerfile:experimental", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaf64add5e4763faf9c3dce20415f53647ed1f44"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMjUwNTYzOnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/kf_predictor_docker/config_kf.properties", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxODowMzoxN1rOH5O7Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwODowOTo1MFrOH5nYmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3NTQ0Ng==", "bodyText": "NIT: Is there any reason for not exposing metrics_address?", "url": "https://github.com/pytorch/serve/pull/750#discussion_r529775446", "createdAt": "2020-11-24T18:03:17Z", "author": {"login": "maaquib"}, "path": "kubernetes/kfserving/kf_predictor_docker/config_kf.properties", "diffHunk": "@@ -0,0 +1,9 @@\n+#Sample config.properties. In production config.properties at /mnt/models/config/config.properties will be used\n+inference_address=http://0.0.0.0:8085\n+management_address=http://0.0.0.0:8081", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE3NjE1Mg==", "bodyText": "Added the metrics address. The kfserving wrapper does not implement metrics API as the 8082 port is not exposed in kfserver but the metrics API can be accessed inside the cluster.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r530176152", "createdAt": "2020-11-25T08:09:50Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/kf_predictor_docker/config_kf.properties", "diffHunk": "@@ -0,0 +1,9 @@\n+#Sample config.properties. In production config.properties at /mnt/models/config/config.properties will be used\n+inference_address=http://0.0.0.0:8085\n+management_address=http://0.0.0.0:8081", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3NTQ0Ng=="}, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMjUwOTMxOnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/mnist_readme.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxODowNDoxM1rOH5O9jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwODowODo1OFrOH5nWwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3NjAxMg==", "bodyText": "NIT: Formatting", "url": "https://github.com/pytorch/serve/pull/750#discussion_r529776012", "createdAt": "2020-11-24T18:04:13Z", "author": {"login": "maaquib"}, "path": "kubernetes/kfserving/mnist_readme.md", "diffHunk": "@@ -0,0 +1,64 @@\n+# Serve a MNIST Model for Inference on the KFServing side:\n+\n+In this document, the .mar file creation, request & response on the KFServing side and the KFServing changes to the handler files for Image Classification model using Torchserve's default vision handler.\n+\n+## .mar file creation\n+\n+The .mar file creation command is as below:\n+```bash\n+torch-model-archiver --model-name mnist --version 1.0 --model-file serve/examples/image_classifier/mnist/mnist.py --serialized-file serve/examples/image_classifier/mnist/mnist_cnn.pt --handler  serve/examples/image_classifier/mnist/mnist_handler.py\n+```\n+\n+## Request and Response\n+\n+The curl request is as below:\n+\n+```bash\n+ curl -H \"Content-Type: application/json\" --data @kubernetes/kfserving/kf_request_json/mnist.json http://127.0.0.1:8085/v1/models/mnist:predict\n+```\n+\n+The Prediction response is as below :\n+\n+```bash\n+{\n+\t\"predictions\" : [\n+\n+\t2", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE3NTY4MA==", "bodyText": "Formatted the readme.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r530175680", "createdAt": "2020-11-25T08:08:58Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/mnist_readme.md", "diffHunk": "@@ -0,0 +1,64 @@\n+# Serve a MNIST Model for Inference on the KFServing side:\n+\n+In this document, the .mar file creation, request & response on the KFServing side and the KFServing changes to the handler files for Image Classification model using Torchserve's default vision handler.\n+\n+## .mar file creation\n+\n+The .mar file creation command is as below:\n+```bash\n+torch-model-archiver --model-name mnist --version 1.0 --model-file serve/examples/image_classifier/mnist/mnist.py --serialized-file serve/examples/image_classifier/mnist/mnist_cnn.pt --handler  serve/examples/image_classifier/mnist/mnist_handler.py\n+```\n+\n+## Request and Response\n+\n+The curl request is as below:\n+\n+```bash\n+ curl -H \"Content-Type: application/json\" --data @kubernetes/kfserving/kf_request_json/mnist.json http://127.0.0.1:8085/v1/models/mnist:predict\n+```\n+\n+The Prediction response is as below :\n+\n+```bash\n+{\n+\t\"predictions\" : [\n+\n+\t2", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3NjAxMg=="}, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMjU2MzA1OnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/image_transformer/image_transformer/transformer_model_repository.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxODoxNzozNlrOH5Pdmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwODowNzozOVrOH5nUGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc4NDIxOA==", "bodyText": "NIT: Unused imports", "url": "https://github.com/pytorch/serve/pull/750#discussion_r529784218", "createdAt": "2020-11-24T18:17:36Z", "author": {"login": "maaquib"}, "path": "kubernetes/kfserving/image_transformer/image_transformer/transformer_model_repository.py", "diffHunk": "@@ -0,0 +1,35 @@\n+import os\n+\n+import logging\n+import tornado.ioloop", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE3NTAwMw==", "bodyText": "The imports are Removed.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r530175003", "createdAt": "2020-11-25T08:07:39Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/image_transformer/image_transformer/transformer_model_repository.py", "diffHunk": "@@ -0,0 +1,35 @@\n+import os\n+\n+import logging\n+import tornado.ioloop", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc4NDIxOA=="}, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMjU2NDUxOnYy", "diffSide": "RIGHT", "path": "examples/image_classifier/mnist/mnist_handler.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxODoxNzo1NVrOH5PeZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwODowNzoyNlrOH5nTpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc4NDQyMg==", "bodyText": "NIT: Unused imports", "url": "https://github.com/pytorch/serve/pull/750#discussion_r529784422", "createdAt": "2020-11-24T18:17:55Z", "author": {"login": "maaquib"}, "path": "examples/image_classifier/mnist/mnist_handler.py", "diffHunk": "@@ -4,6 +4,8 @@\n from torchvision import transforms\n \n from ts.torch_handler.image_classifier import ImageClassifier\n+import base64", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE3NDg4NA==", "bodyText": "The imports are Removed.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r530174884", "createdAt": "2020-11-25T08:07:26Z", "author": {"login": "abishekchiffon"}, "path": "examples/image_classifier/mnist/mnist_handler.py", "diffHunk": "@@ -4,6 +4,8 @@\n from torchvision import transforms\n \n from ts.torch_handler.image_classifier import ImageClassifier\n+import base64", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc4NDQyMg=="}, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMjU5MjgxOnYy", "diffSide": "RIGHT", "path": "kubernetes/kfserving/kfserving_wrapper/TorchserveModel.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxODoyNDoyM1rOH5Puvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwODowNzoxMVrOH5nTKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc4ODYwNg==", "bodyText": "Given the request could be fairly large, should this be a debug level log?", "url": "https://github.com/pytorch/serve/pull/750#discussion_r529788606", "createdAt": "2020-11-24T18:24:23Z", "author": {"login": "maaquib"}, "path": "kubernetes/kfserving/kfserving_wrapper/TorchserveModel.py", "diffHunk": "@@ -0,0 +1,83 @@\n+import os\n+import json\n+from typing import Dict\n+import logging\n+import requests\n+import kfserving\n+import tornado.web\n+\n+logging.basicConfig(level=kfserving.constants.KFSERVING_LOGLEVEL)\n+\n+\n+REGISTER_URL_FORMAT = \"{0}/models?initial_workers=1&url={1}\"\n+UNREGISTER_URL_FORMAT = \"{0}/models/{1}\"\n+\n+PREDICTOR_URL_FORMAT = \"http://{0}/v1/models/{1}:predict\"\n+\n+\n+class TorchserveModel(kfserving.KFModel):\n+    \"\"\"In this class, the torchserve side inference and explain end-points requests are handled to\n+    return a KFServing side response\n+\n+    Args:\n+        kfserving.KFModel(class object): The predict and explain methods are overriden by torchserve\n+        side predict and explain http requests.\n+    \"\"\"\n+    def __init__(self, name, inference_address, management_address, model_dir):\n+        \"\"\"The Model Name, Inference Address, Management Address and the model directory\n+        are specified.\n+\n+        Args:\n+            name (str): Model Name\n+            inference_address (str): The Inference Address in which we hit the inference end point\n+            management_address (str): The Management Address in which we register the model.\n+            model_dir (str): The location of the model artefacts.\n+        \"\"\"\n+        super().__init__(name)\n+\n+        if not self.predictor_host:\n+            self.predictor_host = inference_address.split(\"//\")[1]\n+        if not self.explainer_host:\n+            self.explainer_host = self.predictor_host\n+\n+        self.inference_address = inference_address\n+        self.management_address = management_address\n+        self.model_dir = model_dir\n+\n+        logging.info(\"kfmodel Predict URL set to %s\", self.predictor_host)\n+        self.explainer_host = self.predictor_host\n+        logging.info(\"kfmodel Explain URL set to %s\", self.explainer_host)\n+\n+    async def predict(self, request: Dict) -> Dict:\n+        \"\"\"The predict method is called when we hit the inference endpoint and handles the inference request and\n+        response from the Torchserve side and passes it on to the KFServing side.\n+\n+        Args:\n+            request (Dict): Input request from the http client side.\n+\n+        Raises:\n+            NotImplementedError: If the predictor host on the KFServing side is not\n+                                 available.\n+\n+            tornado.web.HTTPError: If there is a bad response from the http client.\n+\n+        Returns:\n+            Dict: The Response from the input from the inference endpoint.\n+        \"\"\"\n+        if not self.predictor_host:\n+            raise NotImplementedError\n+        logging.info(\"kfmodel predict request is %s\", json.dumps(request))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE3NDc2Mw==", "bodyText": "Yes, Changed it to debug level.", "url": "https://github.com/pytorch/serve/pull/750#discussion_r530174763", "createdAt": "2020-11-25T08:07:11Z", "author": {"login": "abishekchiffon"}, "path": "kubernetes/kfserving/kfserving_wrapper/TorchserveModel.py", "diffHunk": "@@ -0,0 +1,83 @@\n+import os\n+import json\n+from typing import Dict\n+import logging\n+import requests\n+import kfserving\n+import tornado.web\n+\n+logging.basicConfig(level=kfserving.constants.KFSERVING_LOGLEVEL)\n+\n+\n+REGISTER_URL_FORMAT = \"{0}/models?initial_workers=1&url={1}\"\n+UNREGISTER_URL_FORMAT = \"{0}/models/{1}\"\n+\n+PREDICTOR_URL_FORMAT = \"http://{0}/v1/models/{1}:predict\"\n+\n+\n+class TorchserveModel(kfserving.KFModel):\n+    \"\"\"In this class, the torchserve side inference and explain end-points requests are handled to\n+    return a KFServing side response\n+\n+    Args:\n+        kfserving.KFModel(class object): The predict and explain methods are overriden by torchserve\n+        side predict and explain http requests.\n+    \"\"\"\n+    def __init__(self, name, inference_address, management_address, model_dir):\n+        \"\"\"The Model Name, Inference Address, Management Address and the model directory\n+        are specified.\n+\n+        Args:\n+            name (str): Model Name\n+            inference_address (str): The Inference Address in which we hit the inference end point\n+            management_address (str): The Management Address in which we register the model.\n+            model_dir (str): The location of the model artefacts.\n+        \"\"\"\n+        super().__init__(name)\n+\n+        if not self.predictor_host:\n+            self.predictor_host = inference_address.split(\"//\")[1]\n+        if not self.explainer_host:\n+            self.explainer_host = self.predictor_host\n+\n+        self.inference_address = inference_address\n+        self.management_address = management_address\n+        self.model_dir = model_dir\n+\n+        logging.info(\"kfmodel Predict URL set to %s\", self.predictor_host)\n+        self.explainer_host = self.predictor_host\n+        logging.info(\"kfmodel Explain URL set to %s\", self.explainer_host)\n+\n+    async def predict(self, request: Dict) -> Dict:\n+        \"\"\"The predict method is called when we hit the inference endpoint and handles the inference request and\n+        response from the Torchserve side and passes it on to the KFServing side.\n+\n+        Args:\n+            request (Dict): Input request from the http client side.\n+\n+        Raises:\n+            NotImplementedError: If the predictor host on the KFServing side is not\n+                                 available.\n+\n+            tornado.web.HTTPError: If there is a bad response from the http client.\n+\n+        Returns:\n+            Dict: The Response from the input from the inference endpoint.\n+        \"\"\"\n+        if not self.predictor_host:\n+            raise NotImplementedError\n+        logging.info(\"kfmodel predict request is %s\", json.dumps(request))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc4ODYwNg=="}, "originalCommit": {"oid": "9bf0c0499fee7e10d07d38071a1c6de9185c95ae"}, "originalPosition": 69}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1458, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}