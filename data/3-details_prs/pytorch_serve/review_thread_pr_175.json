{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxNjQzNzgx", "number": 175, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozODo0NVrODwzAlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1OToxN1rODwzWyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDkzOTczOnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozODo0NVrOGD_d7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozODo0NVrOGD_d7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzODc2Nw==", "bodyText": "This is more about how long it should wait to fill the batch before sending a request with something less than the preferred batch size.", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406838767", "createdAt": "2020-04-10T16:38:45Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -9,50 +10,66 @@\n \n ## Introduction\n \n-Batching in the Machine-Learning/Deep-Learning is a process of aggregating inference-requests and sending this aggregated requests through the ML/DL framework for inference at once.\n-TorchServe was designed to natively support batching of incoming inference requests. This functionality provides customer using TorchServe to optimally utilize their host resources, because most ML/DL frameworks\n-are optimized for batch requests. This optimal utilization of host resources in turn reduces the operational expense of hosting an inference service using TorchServe. In this document we will go through an example of how this is done\n-and compare the performance of running a batched inference against running single inference.\n+Batch inference is a process of aggregating inference requests and sending this aggregated requests through the ML/DL framework for inference all at once.\n+TorchServe was designed to natively support batching of incoming inference requests. This functionality enables you to use your host resources optimally,\n+because most ML/DL frameworks are optimized for batch requests.\n+This optimal use of host resources in turn reduces the operational expense of hosting an inference service using TorchServe.\n+In this document we show an example of how this is done and compare the performance of running a batched inference against running single inference.\n+\n+## Prerequisites\n+\n+Before jumping into this document, read the following docs:\n \n-## Prerequisites:\n-Before jumping into this document, please go over the following docs\n 1. [What is TorchServe?](../README.md)\n 1. [What is custom service code?](custom_service.md)\n \n ## Batch Inference with TorchServe's default handlers\n+\n TorchServe's default handlers do not support batch inference.\n \n ## Batch Inference with TorchServe using ResNet-152 model\n-To support batching of inference requests, TorchServe needs the following:\n-1. TorchServe Model Configuration: TorchServe provides means to configure \"Max Batch Size\" and \"Max Batch Delay\" through \"POST /models\" API. \n-   TorchServe needs to know the maximum batch size that the model can handle and the maximum delay that TorchServe should wait for, to form this request-batch. \n-2. Model Handler code: TorchServe requires the Model Handler to handle the batch of inference requests. \n \n-For a full working code of a custom model handler with batch processing, refer to [resnet152_handler.py](../examples/image_classifier/resnet_152_batch/resnet152_handler.py)\n+To support batch inference, TorchServe needs the following:\n+\n+1. TorchServe model configuration: Configure `batch_size` and `max_batch_delay` by using the  \"POST /models\" management API.\n+   TorchServe needs to know the maximum batch size that the model can handle and the maximum time that TorchServe should wait for each batch request.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk0MzExOnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozOTo1M1rOGD_f6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozOTo1M1rOGD_f6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzOTI3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               TorchServe needs to know the maximum batch size that the model can handle and the maximum time that TorchServe should wait for each batch request.\n          \n          \n            \n               TorchServe needs to know the maximum batch size that the model can handle and the maximum time that TorchServe should wait to fill each batch request.", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406839272", "createdAt": "2020-04-10T16:39:53Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -9,50 +10,66 @@\n \n ## Introduction\n \n-Batching in the Machine-Learning/Deep-Learning is a process of aggregating inference-requests and sending this aggregated requests through the ML/DL framework for inference at once.\n-TorchServe was designed to natively support batching of incoming inference requests. This functionality provides customer using TorchServe to optimally utilize their host resources, because most ML/DL frameworks\n-are optimized for batch requests. This optimal utilization of host resources in turn reduces the operational expense of hosting an inference service using TorchServe. In this document we will go through an example of how this is done\n-and compare the performance of running a batched inference against running single inference.\n+Batch inference is a process of aggregating inference requests and sending this aggregated requests through the ML/DL framework for inference all at once.\n+TorchServe was designed to natively support batching of incoming inference requests. This functionality enables you to use your host resources optimally,\n+because most ML/DL frameworks are optimized for batch requests.\n+This optimal use of host resources in turn reduces the operational expense of hosting an inference service using TorchServe.\n+In this document we show an example of how this is done and compare the performance of running a batched inference against running single inference.\n+\n+## Prerequisites\n+\n+Before jumping into this document, read the following docs:\n \n-## Prerequisites:\n-Before jumping into this document, please go over the following docs\n 1. [What is TorchServe?](../README.md)\n 1. [What is custom service code?](custom_service.md)\n \n ## Batch Inference with TorchServe's default handlers\n+\n TorchServe's default handlers do not support batch inference.\n \n ## Batch Inference with TorchServe using ResNet-152 model\n-To support batching of inference requests, TorchServe needs the following:\n-1. TorchServe Model Configuration: TorchServe provides means to configure \"Max Batch Size\" and \"Max Batch Delay\" through \"POST /models\" API. \n-   TorchServe needs to know the maximum batch size that the model can handle and the maximum delay that TorchServe should wait for, to form this request-batch. \n-2. Model Handler code: TorchServe requires the Model Handler to handle the batch of inference requests. \n \n-For a full working code of a custom model handler with batch processing, refer to [resnet152_handler.py](../examples/image_classifier/resnet_152_batch/resnet152_handler.py)\n+To support batch inference, TorchServe needs the following:\n+\n+1. TorchServe model configuration: Configure `batch_size` and `max_batch_delay` by using the  \"POST /models\" management API.\n+   TorchServe needs to know the maximum batch size that the model can handle and the maximum time that TorchServe should wait for each batch request.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk1ODM0OnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0NToyNFrOGD_pDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0NToyNFrOGD_pDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0MTYxNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Follow the main [Readme](../README.md) and install all the required packages including \"torchserve\"\n          \n          \n            \n            Follow the main [Readme](../README.md) and install all the required packages including \"torchserve\".", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406841615", "createdAt": "2020-04-10T16:45:24Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -9,50 +10,66 @@\n \n ## Introduction\n \n-Batching in the Machine-Learning/Deep-Learning is a process of aggregating inference-requests and sending this aggregated requests through the ML/DL framework for inference at once.\n-TorchServe was designed to natively support batching of incoming inference requests. This functionality provides customer using TorchServe to optimally utilize their host resources, because most ML/DL frameworks\n-are optimized for batch requests. This optimal utilization of host resources in turn reduces the operational expense of hosting an inference service using TorchServe. In this document we will go through an example of how this is done\n-and compare the performance of running a batched inference against running single inference.\n+Batch inference is a process of aggregating inference requests and sending this aggregated requests through the ML/DL framework for inference all at once.\n+TorchServe was designed to natively support batching of incoming inference requests. This functionality enables you to use your host resources optimally,\n+because most ML/DL frameworks are optimized for batch requests.\n+This optimal use of host resources in turn reduces the operational expense of hosting an inference service using TorchServe.\n+In this document we show an example of how this is done and compare the performance of running a batched inference against running single inference.\n+\n+## Prerequisites\n+\n+Before jumping into this document, read the following docs:\n \n-## Prerequisites:\n-Before jumping into this document, please go over the following docs\n 1. [What is TorchServe?](../README.md)\n 1. [What is custom service code?](custom_service.md)\n \n ## Batch Inference with TorchServe's default handlers\n+\n TorchServe's default handlers do not support batch inference.\n \n ## Batch Inference with TorchServe using ResNet-152 model\n-To support batching of inference requests, TorchServe needs the following:\n-1. TorchServe Model Configuration: TorchServe provides means to configure \"Max Batch Size\" and \"Max Batch Delay\" through \"POST /models\" API. \n-   TorchServe needs to know the maximum batch size that the model can handle and the maximum delay that TorchServe should wait for, to form this request-batch. \n-2. Model Handler code: TorchServe requires the Model Handler to handle the batch of inference requests. \n \n-For a full working code of a custom model handler with batch processing, refer to [resnet152_handler.py](../examples/image_classifier/resnet_152_batch/resnet152_handler.py)\n+To support batch inference, TorchServe needs the following:\n+\n+1. TorchServe model configuration: Configure `batch_size` and `max_batch_delay` by using the  \"POST /models\" management API.\n+   TorchServe needs to know the maximum batch size that the model can handle and the maximum time that TorchServe should wait for each batch request.\n+2. Model handler code: TorchServe requires the Model handler to handle batch inference requests.\n+\n+For a full working example of a custom model handler with batch processing, see [resnet152_handler.py](../examples/image_classifier/resnet_152_batch/resnet152_handler.py)\n \n ### TorchServe Model Configuration\n-To configure TorchServe to use the batching feature, you would have to provide the batch configuration information through [**POST /models** API](management_api.md#register-a-model).\n-The configuration that we are interested in is the following: \n-1. `batch_size`: This is the maximum batch size that a model is expected to handle. \n-2. `max_batch_delay`: This is the maximum batch delay time TorchServe waits to receive `batch_size` number of requests. If TorchServe doesn't receive `batch_size` number of requests\n-before this timer time's out, it sends what ever requests that were received to the model `handler`.\n+\n+To configure TorchServe to use the batching feature, provide the batch configuration information through [**POST /models** API](management_api.md#register-a-model).\n+\n+The configuration that we are interested in is the following:\n+\n+1. `batch_size`: This is the maximum batch size that a model is expected to handle.\n+2. `max_batch_delay`: This is the maximum batch delay time TorchServe waits to receive `batch_size` number of requests. If TorchServe doesn't receive `batch_size` number of\n+requests before this timer time's out, it sends what ever requests that were received to the model `handler`.\n \n Let's look at an example using this configuration\n+\n ```bash\n # The following command will register a model \"resnet-152.mar\" and configure TorchServe to use a batch_size of 8 and a max batch delay of 50 milli seconds. \n curl -X POST \"localhost:8081/models?url=resnet-152.mar&batch_size=8&max_batch_delay=50\"\n ```\n- \n-These configurations are used both in TorchServe and in the model's custom-service-code (a.k.a the handler code). TorchServe associates the batch related configuration with each model. The frontend then tries to aggregate the batch-size number of requests and send it to the backend.\n+\n+These configurations are used both in TorchServe and in the model's custom service code (a.k.a the handler code).\n+TorchServe associates the batch related configuration with each model.\n+The frontend then tries to aggregate the batch-size number of requests and send it to the backend.\n \n ## Demo to configure TorchServe with batch-supported model\n+\n In this section lets bring up model server and launch Resnet-152 model, which has been built to handle a batch of request. \n \n-### Pre-requisites\n+### Prerequisites\n+\n Follow the main [Readme](../README.md) and install all the required packages including \"torchserve\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk2NDYzOnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0Nzo0OVrOGD_tBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0Nzo0OVrOGD_tBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0MjYyOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Now lets launch resnet-152 model, which we have built to handle batch inference. Because this is an example, we are going to launch 1 worker which handles a batch size of 8 with a max-batch-delay of 10ms.\n          \n          \n            \n            * Now let's launch resnet-152 model, which we have built to handle batch inference. Because this is an example, we are going to launch 1 worker which handles a batch size of 8 with a `max_batch_delay` of 10ms.", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406842629", "createdAt": "2020-04-10T16:47:49Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -62,18 +79,20 @@ management_address=http://0.0.0.0:8081\n $ torchserve --start --model-store model_store\n ```\n \n-Note :  This example assumes that the resnet-152.mar file is available in the torchserve model_store. For more details on creating resnet-152 mar file and serving it on TorchServe refer [resnet152 image classification example](../examples/image_classifier/resnet_152_batch/README.md)\n+**Note**: This example assumes that the resnet-152.mar file is available in the `model_store`.\n+For more details on creating resnet-152 mar file and serving it on TorchServe refer [resnet152 image classification example](../examples/image_classifier/resnet_152_batch/README.md)\n+\n+* Verify that TorchServe is up and running\n \n-* Verify that the TorchServe is up and running\n ```text\n $ curl localhost:8080/ping\n {\n   \"status\": \"Healthy\"\n }\n ```\n \n-* Now lets launch resnet-152 model, which we have built to handle batch inference. Since this is an example, we are going to launch 1 worker which handles a batch size of 8\n-with a max-batch-delay of 10ms. \n+* Now lets launch resnet-152 model, which we have built to handle batch inference. Because this is an example, we are going to launch 1 worker which handles a batch size of 8 with a max-batch-delay of 10ms.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk2NzA1OnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0ODo0MlrOGD_uhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0ODo0MlrOGD_uhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0MzAxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Verify that the workers were started properly\n          \n          \n            \n            * Verify that the workers were started properly.", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406843013", "createdAt": "2020-04-10T16:48:42Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -82,6 +101,7 @@ $ curl -X POST \"localhost:8081/models?url=resnet-152.mar&batch_size=8&max_batch_\n ```\n \n * Verify that the workers were started properly", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk2NzQ3OnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0ODo1MVrOGD_uyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0ODo1MVrOGD_uyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0MzA4MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              * Run inference to test the model\n          \n          \n            \n              * Run inference to test the model.", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406843081", "createdAt": "2020-04-10T16:48:51Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -104,12 +124,15 @@ $ curl localhost:8081/models/resnet-152\n }\n ```\n \n-* Now let's test this service. \n+* Now let's test this service.\n+\n   * Get an image to test this service\n     ```text\n     $ curl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n     ``` \n+\n   * Run inference to test the model", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk2OTAxOnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0OToxN1rOGD_vpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo0OToxN1rOGD_vpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0MzMwMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows:\n          \n          \n            \n            Before running this test, we need to first install `apache-bench` on our system. Since we were running this on an Ubuntu host, we install `apache-bench` as follows:", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406843301", "createdAt": "2020-04-10T16:49:17Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -133,25 +156,30 @@ $ curl localhost:8081/models/resnet-152\n         \"class\": \"n02129604 tiger, Panthera tigris\"\n       }\n     ```\n-    \n-* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use \n+\n+* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use\n [apache-bench](https://httpd.apache.org/docs/2.4/programs/ab.html) to run our performance tests. We chose `apache-bench` for our tests because of the ease of installation and ease of running tests.\n-Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows\n+\n+Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk4MTY4OnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1Mzo1NVrOGD_3Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1Mzo1NVrOGD_3Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NTI4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            its advantageous to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available\n          \n          \n            \n            it's advantageous to batch the requests. This allows for maximally utilizing the compute resources, especially GPU resources, which are more expensive. But customers should do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406845282", "createdAt": "2020-04-10T16:53:55Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -133,25 +156,30 @@ $ curl localhost:8081/models/resnet-152\n         \"class\": \"n02129604 tiger, Panthera tigris\"\n       }\n     ```\n-    \n-* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use \n+\n+* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use\n [apache-bench](https://httpd.apache.org/docs/2.4/programs/ab.html) to run our performance tests. We chose `apache-bench` for our tests because of the ease of installation and ease of running tests.\n-Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows\n+\n+Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows:\n+\n ```bash\n $ sudo apt-get udpate && sudo apt-get install apache2-utils\n-```   \n-Now that installation is done, we can run performance benchmark test as follows. \n+```\n+\n+Now that installation is done, we can run performance benchmark test as follows.\n+\n ```text\n $ ab -k -l -n 10000 -c 1000 -T \"image/jpeg\" -p kitten.jpg localhost:8080/predictions/resnet-152\n ```\n+\n The above test simulates TorchServe receiving 1000 concurrent requests at once and a total of 10,000 requests. All of these requests are directed to the endpoint \"localhost:8080/predictions/resnet-152\", which assumes\n that resnet-152 is already registered and scaled-up on TorchServe. We had done this registration and scaling up in the above steps.\n- \n+\n ## Conclusion\n-The take away from the experiments is that batching is a very useful feature. In cases where the services receive heavy load of requests or each request has high I/O, its advantageous\n-to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should\n-do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available and number of models loaded per GPU. Customers should also\n-analyze their traffic patterns before enabling the batch-inference. As shown in the above experiments, services receiving TPS lesser than the batch size would lead to consistent\n-\"batch delay\" timeouts and cause the response latency per request to spike. As any cutting edge technology, batch-inference is definitely a double edged sword. \n \n-   \n+The take away from this example is that batching is a very useful feature. In cases where the services receive heavy load of requests or each request has high I/O,\n+its advantageous to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk4MjU2OnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1NDoxN1rOGD_3_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1NDoxN1rOGD_3_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NTQzNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Customers should also analyze their traffic patterns before enabling the batch-inference. As shown in the above experiments,\n          \n          \n            \n            You should also analyze your traffic patterns before enabling the batch inference. As shown in the above experiments,", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406845436", "createdAt": "2020-04-10T16:54:17Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -133,25 +156,30 @@ $ curl localhost:8081/models/resnet-152\n         \"class\": \"n02129604 tiger, Panthera tigris\"\n       }\n     ```\n-    \n-* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use \n+\n+* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use\n [apache-bench](https://httpd.apache.org/docs/2.4/programs/ab.html) to run our performance tests. We chose `apache-bench` for our tests because of the ease of installation and ease of running tests.\n-Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows\n+\n+Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows:\n+\n ```bash\n $ sudo apt-get udpate && sudo apt-get install apache2-utils\n-```   \n-Now that installation is done, we can run performance benchmark test as follows. \n+```\n+\n+Now that installation is done, we can run performance benchmark test as follows.\n+\n ```text\n $ ab -k -l -n 10000 -c 1000 -T \"image/jpeg\" -p kitten.jpg localhost:8080/predictions/resnet-152\n ```\n+\n The above test simulates TorchServe receiving 1000 concurrent requests at once and a total of 10,000 requests. All of these requests are directed to the endpoint \"localhost:8080/predictions/resnet-152\", which assumes\n that resnet-152 is already registered and scaled-up on TorchServe. We had done this registration and scaling up in the above steps.\n- \n+\n ## Conclusion\n-The take away from the experiments is that batching is a very useful feature. In cases where the services receive heavy load of requests or each request has high I/O, its advantageous\n-to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should\n-do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available and number of models loaded per GPU. Customers should also\n-analyze their traffic patterns before enabling the batch-inference. As shown in the above experiments, services receiving TPS lesser than the batch size would lead to consistent\n-\"batch delay\" timeouts and cause the response latency per request to spike. As any cutting edge technology, batch-inference is definitely a double edged sword. \n \n-   \n+The take away from this example is that batching is a very useful feature. In cases where the services receive heavy load of requests or each request has high I/O,\n+its advantageous to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available\n+and number of models loaded per GPU.\n+Customers should also analyze their traffic patterns before enabling the batch-inference. As shown in the above experiments,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk4MzI3OnYy", "diffSide": "RIGHT", "path": "docs/batch_inference_with_ts.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1NDozM1rOGD_4aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1NDozM1rOGD_4aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NTU0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            As any cutting edge technology, batch-inference is definitely a double-edged sword.\n          \n          \n            \n            As with any cutting edge technology, batch inference is definitely a double-edged sword.", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406845544", "createdAt": "2020-04-10T16:54:33Z", "author": {"login": "aaronmarkham"}, "path": "docs/batch_inference_with_ts.md", "diffHunk": "@@ -133,25 +156,30 @@ $ curl localhost:8081/models/resnet-152\n         \"class\": \"n02129604 tiger, Panthera tigris\"\n       }\n     ```\n-    \n-* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use \n+\n+* Now that we have the service up and running, we could run performance tests with the same kitten image as follows. There are multiple tools to measure performance of web-servers. We will use\n [apache-bench](https://httpd.apache.org/docs/2.4/programs/ab.html) to run our performance tests. We chose `apache-bench` for our tests because of the ease of installation and ease of running tests.\n-Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows\n+\n+Before running this test, we need to first install `apache-bench` on our System. Since we were running this on a ubuntu host, we installed apache-bench as follows:\n+\n ```bash\n $ sudo apt-get udpate && sudo apt-get install apache2-utils\n-```   \n-Now that installation is done, we can run performance benchmark test as follows. \n+```\n+\n+Now that installation is done, we can run performance benchmark test as follows.\n+\n ```text\n $ ab -k -l -n 10000 -c 1000 -T \"image/jpeg\" -p kitten.jpg localhost:8080/predictions/resnet-152\n ```\n+\n The above test simulates TorchServe receiving 1000 concurrent requests at once and a total of 10,000 requests. All of these requests are directed to the endpoint \"localhost:8080/predictions/resnet-152\", which assumes\n that resnet-152 is already registered and scaled-up on TorchServe. We had done this registration and scaling up in the above steps.\n- \n+\n ## Conclusion\n-The take away from the experiments is that batching is a very useful feature. In cases where the services receive heavy load of requests or each request has high I/O, its advantageous\n-to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should\n-do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available and number of models loaded per GPU. Customers should also\n-analyze their traffic patterns before enabling the batch-inference. As shown in the above experiments, services receiving TPS lesser than the batch size would lead to consistent\n-\"batch delay\" timeouts and cause the response latency per request to spike. As any cutting edge technology, batch-inference is definitely a double edged sword. \n \n-   \n+The take away from this example is that batching is a very useful feature. In cases where the services receive heavy load of requests or each request has high I/O,\n+its advantageous to batch the requests. This allows for maximally utilizing the compute resources, especially GPU compute which are also more often than not more expensive. But customers should do their due diligence and perform enough tests to find optimal batch size depending on the number of GPUs available\n+and number of models loaded per GPU.\n+Customers should also analyze their traffic patterns before enabling the batch-inference. As shown in the above experiments,\n+services receiving TPS lesser than the batch size would lead to consistent \"batch delay\" timeouts and cause the response latency per request to spike.\n+As any cutting edge technology, batch-inference is definitely a double-edged sword.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5MjE1OnYy", "diffSide": "RIGHT", "path": "docs/code_coverage.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1Nzo0NFrOGD_-EQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1Nzo0NFrOGD_-EQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0Njk5Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ## Pre-requisites\n          \n          \n            \n            ## Prerequisites", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406846993", "createdAt": "2020-04-10T16:57:44Z", "author": {"login": "aaronmarkham"}, "path": "docs/code_coverage.md", "diffHunk": "@@ -1,8 +1,8 @@\n-## To execute unit testing and generate code coverage report follow these steps:\n+# Execute unit testing and generate a code coverage report\n \n ## Pre-requisites", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5MzA1OnYy", "diffSide": "RIGHT", "path": "docs/code_coverage.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODowNlrOGD_-ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODowNlrOGD_-ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzEzNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The reports can be accessed at the following path :\n          \n          \n            \n            The reports can be accessed at the following paths:", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406847134", "createdAt": "2020-04-10T16:58:06Z", "author": {"login": "aaronmarkham"}, "path": "docs/code_coverage.md", "diffHunk": "@@ -25,7 +25,6 @@ cd serve\n \n The reports can be accessed at the following path :", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5NDk2OnYy", "diffSide": "RIGHT", "path": "docs/code_coverage.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODo0N1rOGD__yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODo0N1rOGD__yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzQzNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * TorchServe frontende : serve/frontend/server/build/reports\n          \n          \n            \n            * TorchServe frontend: `serve/frontend/server/build/reports`", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406847435", "createdAt": "2020-04-10T16:58:47Z", "author": {"login": "aaronmarkham"}, "path": "docs/code_coverage.md", "diffHunk": "@@ -25,7 +25,6 @@ cd serve\n \n The reports can be accessed at the following path :\n \n-- TorchServe frontende : serve/frontend/server/build/reports\n-- TorchServe backend : serve/htmlcov\n-- torch-model-archiver : serve/model-archiver/htmlcov\n-\n+* TorchServe frontende : serve/frontend/server/build/reports", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5NTU3OnYy", "diffSide": "RIGHT", "path": "docs/code_coverage.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODo1OFrOGEAALA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODo1OFrOGEAALA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzUzMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * TorchServe backend : serve/htmlcov\n          \n          \n            \n            * TorchServe backend: `serve/htmlcov`", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406847532", "createdAt": "2020-04-10T16:58:58Z", "author": {"login": "aaronmarkham"}, "path": "docs/code_coverage.md", "diffHunk": "@@ -25,7 +25,6 @@ cd serve\n \n The reports can be accessed at the following path :\n \n-- TorchServe frontende : serve/frontend/server/build/reports\n-- TorchServe backend : serve/htmlcov\n-- torch-model-archiver : serve/model-archiver/htmlcov\n-\n+* TorchServe frontende : serve/frontend/server/build/reports\n+* TorchServe backend : serve/htmlcov", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5NjU4OnYy", "diffSide": "RIGHT", "path": "docs/code_coverage.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1OToxN1rOGEAAvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1OToxN1rOGEAAvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzY3OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * torch-model-archiver : serve/model-archiver/htmlcov\n          \n          \n            \n            * torch-model-archiver: `serve/model-archiver/htmlcov`", "url": "https://github.com/pytorch/serve/pull/175#discussion_r406847679", "createdAt": "2020-04-10T16:59:17Z", "author": {"login": "aaronmarkham"}, "path": "docs/code_coverage.md", "diffHunk": "@@ -25,7 +25,6 @@ cd serve\n \n The reports can be accessed at the following path :\n \n-- TorchServe frontende : serve/frontend/server/build/reports\n-- TorchServe backend : serve/htmlcov\n-- torch-model-archiver : serve/model-archiver/htmlcov\n-\n+* TorchServe frontende : serve/frontend/server/build/reports\n+* TorchServe backend : serve/htmlcov\n+* torch-model-archiver : serve/model-archiver/htmlcov", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bf9c8dd6f77c7a0ed1c3f3e2dd1d3bcc57b8e97"}, "originalPosition": 21}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1581, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}