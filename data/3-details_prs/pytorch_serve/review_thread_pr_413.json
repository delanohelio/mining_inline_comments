{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0NzgyNDA0", "number": 413, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMToyMjo1MFrOEDrcGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODo1NToxOFrOEDxm2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMjkyODg4OnYy", "diffSide": "RIGHT", "path": "docs/configuration.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwMToyMjo1MFrOGg1mAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQwMDoyNToyM1rOGhg0CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NTY5OQ==", "bodyText": "Maybe add a bit of background about how Netty is used in TorchServe", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437085699", "createdAt": "2020-06-09T01:22:50Z", "author": {"login": "mycpuorg"}, "path": "docs/configuration.md", "diffHunk": "@@ -188,8 +188,8 @@ By default, TorchServe uses all available GPUs for inference. Use `number_of_gpu\n Most of the following properties are designed for performance tuning. Adjusting these numbers will impact scalability and throughput.\n \n * `enable_envvars_config`: Enable configuring TorchServe through environment variables. When this option is set to \"true\", all the static configurations of TorchServe can come through environment variables as well. Default: false\n-* `number_of_netty_threads`: number frontend netty thread. Default: number of logical processors available to the JVM.\n-* `netty_client_threads`: number of backend netty thread. Default: number of logical processors available to the JVM.\n+* `number_of_netty_threads`: number frontend netty thread. This specifies the numer of threads in the child [EventLoopGroup](https://livebook.manning.com/book/netty-in-action/chapter-8) of the frontend netty server. This group provides EventLoops for processing Netty Channel events (namely inference and management requests) from accepted connections. Default: number of logical processors available to the JVM.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "142fc595b4b8a4ed10c22f4cd100b33acc22195d"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc5MzgwMA==", "bodyText": "Yes that will be helpful to get some clarity on the internal workings and how to think about tuning the different threads related configuration parameters", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437793800", "createdAt": "2020-06-10T00:25:23Z", "author": {"login": "chauhang"}, "path": "docs/configuration.md", "diffHunk": "@@ -188,8 +188,8 @@ By default, TorchServe uses all available GPUs for inference. Use `number_of_gpu\n Most of the following properties are designed for performance tuning. Adjusting these numbers will impact scalability and throughput.\n \n * `enable_envvars_config`: Enable configuring TorchServe through environment variables. When this option is set to \"true\", all the static configurations of TorchServe can come through environment variables as well. Default: false\n-* `number_of_netty_threads`: number frontend netty thread. Default: number of logical processors available to the JVM.\n-* `netty_client_threads`: number of backend netty thread. Default: number of logical processors available to the JVM.\n+* `number_of_netty_threads`: number frontend netty thread. This specifies the numer of threads in the child [EventLoopGroup](https://livebook.manning.com/book/netty-in-action/chapter-8) of the frontend netty server. This group provides EventLoops for processing Netty Channel events (namely inference and management requests) from accepted connections. Default: number of logical processors available to the JVM.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA4NTY5OQ=="}, "originalCommit": {"oid": "142fc595b4b8a4ed10c22f4cd100b33acc22195d"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzkxMDk5OnYy", "diffSide": "RIGHT", "path": "README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODo0ODoxMlrOGg_EaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODo0ODoxMlrOGg_EaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzI0MDkzNw==", "bodyText": "In the diagram, we should name Thread1/Thread2 as Worker1/Worker2.\nAlso, will it be a good idea to define 'Model Handler' as it has been referred to in the diagram.\nAnd some reference to MAR under 'Model' terminology.", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437240937", "createdAt": "2020-06-09T08:48:12Z", "author": {"login": "dhaniram-kshirsagar"}, "path": "README.md", "diffHunk": "@@ -4,6 +4,16 @@ TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n **For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n+## TorchServe Architecture\n+![Architecture Diagram](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)\n+\n+### Terminology:\n+* **Frontend**: The request/response handling component of TorchServe. This portion of the serving component handles both request/response coming from clients as well manages the models lifecycle.\n+* **Model Workers**: These workers are responsible for running the actual inference on the models. These are actual running instances of the models.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "142fc595b4b8a4ed10c22f4cd100b33acc22195d"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMzkzOTQ0OnYy", "diffSide": "RIGHT", "path": "README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODo1NToxOFrOGg_Wig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODo1NToxOFrOGg_Wig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzI0NTU3OA==", "bodyText": "Though not related -> 'Install Java 11' step has been repeated in 'install using pip' and 'install using conda'.", "url": "https://github.com/pytorch/serve/pull/413#discussion_r437245578", "createdAt": "2020-06-09T08:55:18Z", "author": {"login": "dhaniram-kshirsagar"}, "path": "README.md", "diffHunk": "@@ -4,6 +4,16 @@ TorchServe is a flexible and easy to use tool for serving PyTorch models.\n \n **For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n \n+## TorchServe Architecture\n+![Architecture Diagram](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)\n+\n+### Terminology:\n+* **Frontend**: The request/response handling component of TorchServe. This portion of the serving component handles both request/response coming from clients as well manages the models lifecycle.\n+* **Model Workers**: These workers are responsible for running the actual inference on the models. These are actual running instances of the models.\n+* **Model**: Models could be a `script_module` (JIT saved models) or `eager_mode_models`. These models can provide custom pre- and post-processing of data along with any other model artifacts such as state_dicts. Models can be loaded from cloud storage or from local hosts.\n+* **Plugins**: These are custom endpoints or authz/authn or batching algorithms that can be dropped into TorchServe at startup time.\n+* **Model Store**: This is a directory in which all the loadable models exist.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "142fc595b4b8a4ed10c22f4cd100b33acc22195d"}, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1494, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}