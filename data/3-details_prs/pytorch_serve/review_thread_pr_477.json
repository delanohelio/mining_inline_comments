{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4Nzk1MzE4", "number": 477, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDozMDo0NFrOEIH48Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyMTo0NFrOEII6IQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTUzMzI5OnYy", "diffSide": "RIGHT", "path": "examples/Huggingface_Transformers/Download_Transformer_models.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDozMDo0NFrOGn5biQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMjo0ODozNlrOGn9Ldg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4ODU4NQ==", "bodyText": "What is the purpose of setting the seed? We are not training the model here, only downloading a trained model, so not clear why this is needed", "url": "https://github.com/pytorch/serve/pull/477#discussion_r444488585", "createdAt": "2020-06-23T20:30:44Z", "author": {"login": "chauhang"}, "path": "examples/Huggingface_Transformers/Download_Transformer_models.py", "diffHunk": "@@ -5,25 +5,26 @@\n import torch\n from transformers import (AutoModelForSequenceClassification, AutoTokenizer, AutoModelForQuestionAnswering,\n  AutoModelForTokenClassification, AutoConfig)\n+from transformers import set_seed\n \"\"\" This function, save the checkpoint, config file along with tokenizer config and vocab files\n     of a transformer model of your choice.\n \"\"\"\n print('Transformers version',transformers.__version__)\n-\n-def transformers_model_dowloader(mode,pretrained_model_name,num_labels,do_lower_case,max_length,torchscript):\n+set_seed(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ae6c28d8d90ea11ab6908657a2285fd04a494f9a"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU1MDAwNg==", "bodyText": "Thanks Geeta, without setting the seed in the Download script, it does not show consistent behavior/ reproducible results  between eager mode and torchscript, when we are not using fine-tuned models. I believe this can be result of using torch.jit.trace as it may introduce some randomness.", "url": "https://github.com/pytorch/serve/pull/477#discussion_r444550006", "createdAt": "2020-06-23T22:48:36Z", "author": {"login": "HamidShojanazeri"}, "path": "examples/Huggingface_Transformers/Download_Transformer_models.py", "diffHunk": "@@ -5,25 +5,26 @@\n import torch\n from transformers import (AutoModelForSequenceClassification, AutoTokenizer, AutoModelForQuestionAnswering,\n  AutoModelForTokenClassification, AutoConfig)\n+from transformers import set_seed\n \"\"\" This function, save the checkpoint, config file along with tokenizer config and vocab files\n     of a transformer model of your choice.\n \"\"\"\n print('Transformers version',transformers.__version__)\n-\n-def transformers_model_dowloader(mode,pretrained_model_name,num_labels,do_lower_case,max_length,torchscript):\n+set_seed(1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4ODU4NQ=="}, "originalCommit": {"oid": "ae6c28d8d90ea11ab6908657a2285fd04a494f9a"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTUzOTUwOnYy", "diffSide": "RIGHT", "path": "examples/Huggingface_Transformers/Transformer_handler_generalized.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMDozMjo0M1rOGn5fjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMjo0ODo1MFrOGn9LxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4OTYxMw==", "bodyText": "Why is this needed for model predictions?", "url": "https://github.com/pytorch/serve/pull/477#discussion_r444489613", "createdAt": "2020-06-23T20:32:43Z", "author": {"login": "chauhang"}, "path": "examples/Huggingface_Transformers/Transformer_handler_generalized.py", "diffHunk": "@@ -5,11 +5,11 @@\n import ast\n import torch\n from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForQuestionAnswering,AutoModelForTokenClassification\n-\n+from transformers import set_seed\n from ts.torch_handler.base_handler import BaseHandler\n \n logger = logging.getLogger(__name__)\n-\n+set_seed(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ae6c28d8d90ea11ab6908657a2285fd04a494f9a"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU1MDA4NQ==", "bodyText": "it's been removed.", "url": "https://github.com/pytorch/serve/pull/477#discussion_r444550085", "createdAt": "2020-06-23T22:48:50Z", "author": {"login": "HamidShojanazeri"}, "path": "examples/Huggingface_Transformers/Transformer_handler_generalized.py", "diffHunk": "@@ -5,11 +5,11 @@\n import ast\n import torch\n from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForQuestionAnswering,AutoModelForTokenClassification\n-\n+from transformers import set_seed\n from ts.torch_handler.base_handler import BaseHandler\n \n logger = logging.getLogger(__name__)\n-\n+set_seed(1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4OTYxMw=="}, "originalCommit": {"oid": "ae6c28d8d90ea11ab6908657a2285fd04a494f9a"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2OTcwMDE3OnYy", "diffSide": "RIGHT", "path": "examples/Huggingface_Transformers/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMToyMTo0NFrOGn7GYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMjo1MTozMVrOGn9PYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNTkzNg==", "bodyText": "There is already a model_name field. Can that not be used for fine tuned models name/path for the finetuned models as well? Please explain why we need a second field.", "url": "https://github.com/pytorch/serve/pull/477#discussion_r444515936", "createdAt": "2020-06-23T21:21:44Z", "author": {"login": "chauhang"}, "path": "examples/Huggingface_Transformers/README.md", "diffHunk": "@@ -58,6 +58,8 @@ In the setup_config.json :\n \n *model_name* : bert-base-uncased , roberta-base or other available pre-trained models.\n \n+*fine_tuned* : name or path to the finetuned model if available, for example \"distilbert-base-cased-distilled-squad\", leave it blank if not available.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ae6c28d8d90ea11ab6908657a2285fd04a494f9a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU1MTAwOA==", "bodyText": "In the current setting , when we do not pass the vocab.txt as the artifacts, tokenizer uses the pretrained version as being passed by Model_name. In that case fine_tuned version can not be used for the tokenizer.", "url": "https://github.com/pytorch/serve/pull/477#discussion_r444551008", "createdAt": "2020-06-23T22:51:31Z", "author": {"login": "HamidShojanazeri"}, "path": "examples/Huggingface_Transformers/README.md", "diffHunk": "@@ -58,6 +58,8 @@ In the setup_config.json :\n \n *model_name* : bert-base-uncased , roberta-base or other available pre-trained models.\n \n+*fine_tuned* : name or path to the finetuned model if available, for example \"distilbert-base-cased-distilled-squad\", leave it blank if not available.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNTkzNg=="}, "originalCommit": {"oid": "ae6c28d8d90ea11ab6908657a2285fd04a494f9a"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1542, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}