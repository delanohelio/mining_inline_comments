{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg4ODI0NTQ0", "number": 687, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODozNzoxM1rOEosb7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTowMjoxOFrOE_VqUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTA2NTQxOnYy", "diffSide": "RIGHT", "path": "docs/configuration.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODozNzoxM1rOHZ7xzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NjozNlrOH9M0rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1NTg1Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * `metrics_format` : Use this to specify metric report format . At present, the only supported and default value for this is `prometheus'\n          \n          \n            \n            * `metrics_format` : Use this to specify metric report format . At present, the only supported and default value for this is `prometheus`", "url": "https://github.com/pytorch/serve/pull/687#discussion_r496955853", "createdAt": "2020-09-29T18:37:13Z", "author": {"login": "maaquib"}, "path": "docs/configuration.md", "diffHunk": "@@ -201,6 +219,12 @@ By default, TorchServe uses all available GPUs for inference. Use `number_of_gpu\n * `metrics_format` : Use this to specify metric report format . At present, the only supported and default value for this is `prometheus'\n \t\t     This is used in conjunction with `enable_meterics_api` option above.\n \n+### Enable metrics api\n+* `enable_metrics_api` : Enable or disable metric apis i.e. it can be either `true` or `false`. Default: true (Enabled)\n+* `metrics_format` : Use this to specify metric report format . At present, the only supported and default value for this is `prometheus'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNTI3OQ==", "bodyText": "Done.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533935279", "createdAt": "2020-12-02T06:56:36Z", "author": {"login": "harshbafna"}, "path": "docs/configuration.md", "diffHunk": "@@ -201,6 +219,12 @@ By default, TorchServe uses all available GPUs for inference. Use `number_of_gpu\n * `metrics_format` : Use this to specify metric report format . At present, the only supported and default value for this is `prometheus'\n \t\t     This is used in conjunction with `enable_meterics_api` option above.\n \n+### Enable metrics api\n+* `enable_metrics_api` : Enable or disable metric apis i.e. it can be either `true` or `false`. Default: true (Enabled)\n+* `metrics_format` : Use this to specify metric report format . At present, the only supported and default value for this is `prometheus'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1NTg1Mw=="}, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTA3MjQ1OnYy", "diffSide": "RIGHT", "path": "docs/inference_api.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODozOToyNFrOHZ72Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NjoyNFrOH9M0bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1Njk5OQ==", "bodyText": "NIT: Why the phrase \"closely follows\"?", "url": "https://github.com/pytorch/serve/pull/687#discussion_r496956999", "createdAt": "2020-09-29T18:39:24Z", "author": {"login": "maaquib"}, "path": "docs/inference_api.md", "diffHunk": "@@ -22,6 +22,8 @@ The out is OpenAPI 3.0.1 json format. You can use it to generate client code, se\n \n ## Health check API\n \n+This API closely follows the [InferenceAPIsService.Ping](../frontend/server/src/main/resources/proto/inference.proto) gRPC API. It returns the status of a model in the ModelServer.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNTIxNQ==", "bodyText": "Removed.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533935215", "createdAt": "2020-12-02T06:56:24Z", "author": {"login": "harshbafna"}, "path": "docs/inference_api.md", "diffHunk": "@@ -22,6 +22,8 @@ The out is OpenAPI 3.0.1 json format. You can use it to generate client code, se\n \n ## Health check API\n \n+This API closely follows the [InferenceAPIsService.Ping](../frontend/server/src/main/resources/proto/inference.proto) gRPC API. It returns the status of a model in the ModelServer.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1Njk5OQ=="}, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTA3NTEyOnYy", "diffSide": "RIGHT", "path": "frontend/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo0MDowNFrOHZ732Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NjoxNVrOH9M0Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1NzQwMQ==", "bodyText": "NIT: Indentation", "url": "https://github.com/pytorch/serve/pull/687#discussion_r496957401", "createdAt": "2020-09-29T18:40:04Z", "author": {"login": "maaquib"}, "path": "frontend/build.gradle", "diffHunk": "@@ -3,10 +3,15 @@ buildscript {\n     spotbugsVersion = '4.0.2'\n     toolVersion = '4.0.2'\n   }\n+  dependencies {\n+        classpath 'com.google.protobuf:protobuf-gradle-plugin:0.8.13'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNTE2Mg==", "bodyText": "Done.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533935162", "createdAt": "2020-12-02T06:56:15Z", "author": {"login": "harshbafna"}, "path": "frontend/build.gradle", "diffHunk": "@@ -3,10 +3,15 @@ buildscript {\n     spotbugsVersion = '4.0.2'\n     toolVersion = '4.0.2'\n   }\n+  dependencies {\n+        classpath 'com.google.protobuf:protobuf-gradle-plugin:0.8.13'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk1NzQwMQ=="}, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTExNTg4OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ModelServer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo1MToxNFrOHZ8Q4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo1MToxNFrOHZ8Q4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MzgxMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    inferencegRPCServer.shutdown();\n          \n          \n            \n                    inferencegRPCServer.shutdown();\n          \n          \n            \n                    try {\n          \n          \n            \n                            inferencegRPCServer.awaitTermination(1, TimeUnit.MINUTES);\n          \n          \n            \n                    } catch (InterruptedException e) {\n          \n          \n            \n                            logger.error(\"Inference gRPC server graceful shutdown failed\", e);\n          \n          \n            \n                    } finally {\n          \n          \n            \n                            inferencegRPCServer.shutdownNow();\n          \n          \n            \n                    }", "url": "https://github.com/pytorch/serve/pull/687#discussion_r496963810", "createdAt": "2020-09-29T18:51:14Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ModelServer.java", "diffHunk": "@@ -380,8 +403,22 @@ public boolean isRunning() {\n     }\n \n     public void stop() {\n-        if (stopped.get()) {\n-            return;\n+        inferencegRPCServer.shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTEyODAzOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxODo1NTowMFrOHZ8YuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NTo0NVrOH9MziA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2NTgxNw==", "bodyText": "Can we refactor these messages to a util class to be re-used by REST and gRPC, so that we don't have to make changes in two place?", "url": "https://github.com/pytorch/serve/pull/687#discussion_r496965817", "createdAt": "2020-09-29T18:55:00Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "diffHunk": "@@ -0,0 +1,117 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Empty;\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.net.HttpURLConnection;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.inference.PredictionResponse;\n+import org.pytorch.serve.grpc.inference.PredictionsRequest;\n+import org.pytorch.serve.grpc.inference.TorchServeHealthResponse;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.job.GRPCJob;\n+import org.pytorch.serve.job.Job;\n+import org.pytorch.serve.metrics.api.MetricAggregator;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.pytorch.serve.util.messages.WorkerCommands;\n+import org.pytorch.serve.wlm.ModelManager;\n+\n+public class InferenceImpl extends InferenceAPIsServiceImplBase {\n+\n+    @Override\n+    public void ping(Empty request, StreamObserver<TorchServeHealthResponse> responseObserver) {\n+        Runnable r =\n+                () -> {\n+                    String response = ApiUtils.getWorkerStatus();\n+                    TorchServeHealthResponse reply =\n+                            TorchServeHealthResponse.newBuilder()\n+                                    .setHealth(\n+                                            JsonUtils.GSON_PRETTY_EXPOSED.toJson(\n+                                                    new StatusResponse(\n+                                                            response, HttpURLConnection.HTTP_OK)))\n+                                    .build();\n+                    responseObserver.onNext(reply);\n+                    responseObserver.onCompleted();\n+                };\n+        ApiUtils.getTorchServeHealth(r);\n+    }\n+\n+    @Override\n+    public void predictions(\n+            PredictionsRequest request, StreamObserver<PredictionResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        if (modelName == null || (\"\").equals(modelName)) {\n+            BadRequestException e = new BadRequestException(\"Parameter model_name is required.\");\n+            responseObserver.onError(\n+                    Status.INTERNAL\n+                            .withDescription(e.getMessage())\n+                            .augmentDescription(\"BadRequestException.()\")\n+                            .withCause(e)\n+                            .asRuntimeException());\n+            return;\n+        }\n+\n+        if (modelVersion == null || (\"\").equals(modelVersion)) {\n+            modelVersion = null;\n+        }\n+\n+        String requestId = UUID.randomUUID().toString();\n+        RequestInput inputData = new RequestInput(requestId);\n+\n+        for (Map.Entry<String, ByteString> entry : request.getInputMap().entrySet()) {\n+            inputData.addParameter(\n+                    new InputParameter(entry.getKey(), entry.getValue().toByteArray()));\n+        }\n+\n+        MetricAggregator.handleInferenceMetric(modelName, modelVersion);\n+        Job job =\n+                new GRPCJob(\n+                        responseObserver,\n+                        modelName,\n+                        modelVersion,\n+                        WorkerCommands.PREDICT,\n+                        inputData);\n+\n+        try {\n+            if (!ModelManager.getInstance().addJob(job)) {\n+                String responseMessage =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNDk4NA==", "bodyText": "Done.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533934984", "createdAt": "2020-12-02T06:55:45Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "diffHunk": "@@ -0,0 +1,117 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Empty;\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.net.HttpURLConnection;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.inference.PredictionResponse;\n+import org.pytorch.serve.grpc.inference.PredictionsRequest;\n+import org.pytorch.serve.grpc.inference.TorchServeHealthResponse;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.job.GRPCJob;\n+import org.pytorch.serve.job.Job;\n+import org.pytorch.serve.metrics.api.MetricAggregator;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.pytorch.serve.util.messages.WorkerCommands;\n+import org.pytorch.serve.wlm.ModelManager;\n+\n+public class InferenceImpl extends InferenceAPIsServiceImplBase {\n+\n+    @Override\n+    public void ping(Empty request, StreamObserver<TorchServeHealthResponse> responseObserver) {\n+        Runnable r =\n+                () -> {\n+                    String response = ApiUtils.getWorkerStatus();\n+                    TorchServeHealthResponse reply =\n+                            TorchServeHealthResponse.newBuilder()\n+                                    .setHealth(\n+                                            JsonUtils.GSON_PRETTY_EXPOSED.toJson(\n+                                                    new StatusResponse(\n+                                                            response, HttpURLConnection.HTTP_OK)))\n+                                    .build();\n+                    responseObserver.onNext(reply);\n+                    responseObserver.onCompleted();\n+                };\n+        ApiUtils.getTorchServeHealth(r);\n+    }\n+\n+    @Override\n+    public void predictions(\n+            PredictionsRequest request, StreamObserver<PredictionResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        if (modelName == null || (\"\").equals(modelName)) {\n+            BadRequestException e = new BadRequestException(\"Parameter model_name is required.\");\n+            responseObserver.onError(\n+                    Status.INTERNAL\n+                            .withDescription(e.getMessage())\n+                            .augmentDescription(\"BadRequestException.()\")\n+                            .withCause(e)\n+                            .asRuntimeException());\n+            return;\n+        }\n+\n+        if (modelVersion == null || (\"\").equals(modelVersion)) {\n+            modelVersion = null;\n+        }\n+\n+        String requestId = UUID.randomUUID().toString();\n+        RequestInput inputData = new RequestInput(requestId);\n+\n+        for (Map.Entry<String, ByteString> entry : request.getInputMap().entrySet()) {\n+            inputData.addParameter(\n+                    new InputParameter(entry.getKey(), entry.getValue().toByteArray()));\n+        }\n+\n+        MetricAggregator.handleInferenceMetric(modelName, modelVersion);\n+        Job job =\n+                new GRPCJob(\n+                        responseObserver,\n+                        modelName,\n+                        modelVersion,\n+                        WorkerCommands.PREDICT,\n+                        inputData);\n+\n+        try {\n+            if (!ModelManager.getInstance().addJob(job)) {\n+                String responseMessage =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2NTgxNw=="}, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMjAyNjgwOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjo0Njo0MVrOHaFADw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjozODoyOVrOH9Mbiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEwNjk1OQ==", "bodyText": "Shouldn't this be getScheduled? Why are we changing how inferTime is measured?", "url": "https://github.com/pytorch/serve/pull/687#discussion_r497106959", "createdAt": "2020-09-29T22:46:41Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java", "diffHunk": "@@ -15,69 +15,30 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-public class Job {\n+public class RestJob extends Job {\n \n     private static final Logger logger = LoggerFactory.getLogger(Job.class);\n \n     private ChannelHandlerContext ctx;\n \n-    private String modelName;\n-    private String modelVersion;\n-    private WorkerCommands cmd; // Else its data msg or inf requests\n-    private RequestInput input;\n-    private long begin;\n-    private long scheduled;\n-\n-    public Job(\n+    public RestJob(\n             ChannelHandlerContext ctx,\n             String modelName,\n             String version,\n             WorkerCommands cmd,\n             RequestInput input) {\n+        super(modelName, version, cmd, input);\n         this.ctx = ctx;\n-        this.modelName = modelName;\n-        this.cmd = cmd;\n-        this.input = input;\n-        this.modelVersion = version;\n-        begin = System.nanoTime();\n-        scheduled = begin;\n-    }\n-\n-    public String getJobId() {\n-        return input.getRequestId();\n-    }\n-\n-    public String getModelName() {\n-        return modelName;\n-    }\n-\n-    public String getModelVersion() {\n-        return modelVersion;\n-    }\n-\n-    public WorkerCommands getCmd() {\n-        return cmd;\n-    }\n-\n-    public boolean isControlCmd() {\n-        return !WorkerCommands.PREDICT.equals(cmd);\n-    }\n-\n-    public RequestInput getPayload() {\n-        return input;\n-    }\n-\n-    public void setScheduled() {\n-        scheduled = System.nanoTime();\n     }\n \n+    @Override\n     public void response(\n             byte[] body,\n             CharSequence contentType,\n             int statusCode,\n             String statusPhrase,\n             Map<String, String> responseHeaders) {\n-        long inferTime = System.nanoTime() - scheduled;\n+        long inferTime = System.nanoTime() - getBegin();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkyODg0Mw==", "bodyText": "This was changed as part of #666. At the time of job queue both begin and scheduled hold the same value. Also, the scheduled parameter provides a setter that will overwrite the value with System.nanoTime(); whenever called.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533928843", "createdAt": "2020-12-02T06:38:29Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java", "diffHunk": "@@ -15,69 +15,30 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-public class Job {\n+public class RestJob extends Job {\n \n     private static final Logger logger = LoggerFactory.getLogger(Job.class);\n \n     private ChannelHandlerContext ctx;\n \n-    private String modelName;\n-    private String modelVersion;\n-    private WorkerCommands cmd; // Else its data msg or inf requests\n-    private RequestInput input;\n-    private long begin;\n-    private long scheduled;\n-\n-    public Job(\n+    public RestJob(\n             ChannelHandlerContext ctx,\n             String modelName,\n             String version,\n             WorkerCommands cmd,\n             RequestInput input) {\n+        super(modelName, version, cmd, input);\n         this.ctx = ctx;\n-        this.modelName = modelName;\n-        this.cmd = cmd;\n-        this.input = input;\n-        this.modelVersion = version;\n-        begin = System.nanoTime();\n-        scheduled = begin;\n-    }\n-\n-    public String getJobId() {\n-        return input.getRequestId();\n-    }\n-\n-    public String getModelName() {\n-        return modelName;\n-    }\n-\n-    public String getModelVersion() {\n-        return modelVersion;\n-    }\n-\n-    public WorkerCommands getCmd() {\n-        return cmd;\n-    }\n-\n-    public boolean isControlCmd() {\n-        return !WorkerCommands.PREDICT.equals(cmd);\n-    }\n-\n-    public RequestInput getPayload() {\n-        return input;\n-    }\n-\n-    public void setScheduled() {\n-        scheduled = System.nanoTime();\n     }\n \n+    @Override\n     public void response(\n             byte[] body,\n             CharSequence contentType,\n             int statusCode,\n             String statusPhrase,\n             Map<String, String> responseHeaders) {\n-        long inferTime = System.nanoTime() - scheduled;\n+        long inferTime = System.nanoTime() - getBegin();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEwNjk1OQ=="}, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMjAzODk0OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMjo0OToxOVrOHaFH2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo0MjozM1rOH9MhCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEwODk1Mw==", "bodyText": "Whats the reason for this remapping? Seems like 413 is appropriate", "url": "https://github.com/pytorch/serve/pull/687#discussion_r497108953", "createdAt": "2020-09-29T22:49:19Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java", "diffHunk": "@@ -102,29 +63,33 @@ public void response(\n          */\n         if (ctx != null) {\n             MetricAggregator.handleInferenceMetric(\n-                    modelName, modelVersion, scheduled - begin, inferTime);\n+                    getModelName(), getModelVersion(), getScheduled() - getBegin(), inferTime);\n             NettyUtils.sendHttpResponse(ctx, resp, true);\n         }\n         logger.debug(\n                 \"Waiting time ns: {}, Backend time ns: {}\",\n-                scheduled - begin,\n-                System.nanoTime() - scheduled);\n+                getScheduled() - getBegin(),\n+                System.nanoTime() - getScheduled());\n     }\n \n-    public void sendError(HttpResponseStatus status, String error) {\n+    @Override\n+    public void sendError(int status, String error) {\n         /*\n          * We can load the models based on the configuration file.Since this Job is\n          * not driven by the external connections, we could have a empty context for\n          * this job. We shouldn't try to send a response to ctx if this is not triggered\n          * by external clients.\n          */\n         if (ctx != null) {\n-            NettyUtils.sendError(ctx, status, new InternalServerException(error));\n+            // Mapping HTTPURLConnection's HTTP_ENTITY_TOO_LARGE to Netty's INSUFFICIENT_STORAGE\n+            status = (status == 413) ? 507 : status;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzMDI0OQ==", "bodyText": "This was because we refactored all the error codes sent from the WLM package to use HTTP Status code  Java's inbuilt HTTPURLConnection instead of Netty's HTTPResponseStatus. And HTTPURLConnection doesn't have the 507 code available. Hence, we used the 413 code instead and re-mapped it to 507 to keep the error codes backward compatible.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533930249", "createdAt": "2020-12-02T06:42:33Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java", "diffHunk": "@@ -102,29 +63,33 @@ public void response(\n          */\n         if (ctx != null) {\n             MetricAggregator.handleInferenceMetric(\n-                    modelName, modelVersion, scheduled - begin, inferTime);\n+                    getModelName(), getModelVersion(), getScheduled() - getBegin(), inferTime);\n             NettyUtils.sendHttpResponse(ctx, resp, true);\n         }\n         logger.debug(\n                 \"Waiting time ns: {}, Backend time ns: {}\",\n-                scheduled - begin,\n-                System.nanoTime() - scheduled);\n+                getScheduled() - getBegin(),\n+                System.nanoTime() - getScheduled());\n     }\n \n-    public void sendError(HttpResponseStatus status, String error) {\n+    @Override\n+    public void sendError(int status, String error) {\n         /*\n          * We can load the models based on the configuration file.Since this Job is\n          * not driven by the external connections, we could have a empty context for\n          * this job. We shouldn't try to send a response to ctx if this is not triggered\n          * by external clients.\n          */\n         if (ctx != null) {\n-            NettyUtils.sendError(ctx, status, new InternalServerException(error));\n+            // Mapping HTTPURLConnection's HTTP_ENTITY_TOO_LARGE to Netty's INSUFFICIENT_STORAGE\n+            status = (status == 413) ? 507 : status;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEwODk1Mw=="}, "originalCommit": {"oid": "0c1795dc84065cbc908119e09e8b5a87d9bc3edb"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3MzcxMzc0OnYy", "diffSide": "RIGHT", "path": "docs/grpc_api.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMDoxMToyNlrOHjObmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMDoxMToyNlrOHjObmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjY5ODY1MA==", "bodyText": "\"../frontend/server/src/main/resources/proto/management.proto\" should be changed to \"../frontend/server/src/main/resources/proto/inference.proto\"\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * [Inference API](../frontend/server/src/main/resources/proto/management.proto)\n          \n          \n            \n            * [Inference API](../frontend/server/src/main/resources/proto/inference.proto)", "url": "https://github.com/pytorch/serve/pull/687#discussion_r506698650", "createdAt": "2020-10-16T20:11:26Z", "author": {"login": "amtagrwl"}, "path": "docs/grpc_api.md", "diffHunk": "@@ -0,0 +1,70 @@\n+# TorchServe gRPC API\n+\n+TorchServe also supports [gRPC APIs](../frontend/server/src/main/resources/proto) for both inference and management calls.\n+\n+TorchServe provides following gRPCs apis\n+\n+* [Inference API](../frontend/server/src/main/resources/proto/management.proto)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b867a0da6d1896166692e15b12991546cbd6968"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjUzMTAzOnYy", "diffSide": "RIGHT", "path": "README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzowNzo1MFrOHlIbMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNTo1NTozNlrOH9LjXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5NzM5NA==", "bodyText": "Should this be added to a new requirements file?", "url": "https://github.com/pytorch/serve/pull/687#discussion_r508697394", "createdAt": "2020-10-20T17:07:50Z", "author": {"login": "maaquib"}, "path": "README.md", "diffHunk": "@@ -135,7 +135,29 @@ After you execute the `torchserve` command above, TorchServe runs on your host,\n \n ### Get predictions from a model\n \n-To test the model server, send a request to the server's `predictions` API.\n+To test the model server, send a request to the server's `predictions` API. TorchServe supports all [inference](docs/inference_api.md) and [management](docs/management_api.md) api's through both [gRPC](docs/grpc_api.md) and [HTTP/REST](docs/grpc_api.md).\n+\n+#### Using GRPC APIs through python client\n+\n+ - Install grpc python dependencies :\n+ \n+```bash\n+pip install -U grpcio protobuf grpcio-tools", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkxNDQ2MA==", "bodyText": "Already a part of developer.txt file. However, other packages are not required in case a user is just planning to implement a TorchServe gRPC client.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533914460", "createdAt": "2020-12-02T05:55:36Z", "author": {"login": "harshbafna"}, "path": "README.md", "diffHunk": "@@ -135,7 +135,29 @@ After you execute the `torchserve` command above, TorchServe runs on your host,\n \n ### Get predictions from a model\n \n-To test the model server, send a request to the server's `predictions` API.\n+To test the model server, send a request to the server's `predictions` API. TorchServe supports all [inference](docs/inference_api.md) and [management](docs/management_api.md) api's through both [gRPC](docs/grpc_api.md) and [HTTP/REST](docs/grpc_api.md).\n+\n+#### Using GRPC APIs through python client\n+\n+ - Install grpc python dependencies :\n+ \n+```bash\n+pip install -U grpcio protobuf grpcio-tools", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY5NzM5NA=="}, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjYwMTgxOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzoyNToyMFrOHlJG4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NjowMlrOH9Mz6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODU3Ng==", "bodyText": "NIT:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (modelName == null || (\"\").equals(modelName)) {\n          \n          \n            \n                    if (modelName == null || \"\".equals(modelName)) {", "url": "https://github.com/pytorch/serve/pull/687#discussion_r508708576", "createdAt": "2020-10-20T17:25:20Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "diffHunk": "@@ -0,0 +1,117 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Empty;\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.net.HttpURLConnection;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.inference.PredictionResponse;\n+import org.pytorch.serve.grpc.inference.PredictionsRequest;\n+import org.pytorch.serve.grpc.inference.TorchServeHealthResponse;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.job.GRPCJob;\n+import org.pytorch.serve.job.Job;\n+import org.pytorch.serve.metrics.api.MetricAggregator;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.pytorch.serve.util.messages.WorkerCommands;\n+import org.pytorch.serve.wlm.ModelManager;\n+\n+public class InferenceImpl extends InferenceAPIsServiceImplBase {\n+\n+    @Override\n+    public void ping(Empty request, StreamObserver<TorchServeHealthResponse> responseObserver) {\n+        Runnable r =\n+                () -> {\n+                    String response = ApiUtils.getWorkerStatus();\n+                    TorchServeHealthResponse reply =\n+                            TorchServeHealthResponse.newBuilder()\n+                                    .setHealth(\n+                                            JsonUtils.GSON_PRETTY_EXPOSED.toJson(\n+                                                    new StatusResponse(\n+                                                            response, HttpURLConnection.HTTP_OK)))\n+                                    .build();\n+                    responseObserver.onNext(reply);\n+                    responseObserver.onCompleted();\n+                };\n+        ApiUtils.getTorchServeHealth(r);\n+    }\n+\n+    @Override\n+    public void predictions(\n+            PredictionsRequest request, StreamObserver<PredictionResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        if (modelName == null || (\"\").equals(modelName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNTA4MA==", "bodyText": "Done.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533935080", "createdAt": "2020-12-02T06:56:02Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "diffHunk": "@@ -0,0 +1,117 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Empty;\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.net.HttpURLConnection;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.inference.PredictionResponse;\n+import org.pytorch.serve.grpc.inference.PredictionsRequest;\n+import org.pytorch.serve.grpc.inference.TorchServeHealthResponse;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.job.GRPCJob;\n+import org.pytorch.serve.job.Job;\n+import org.pytorch.serve.metrics.api.MetricAggregator;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.pytorch.serve.util.messages.WorkerCommands;\n+import org.pytorch.serve.wlm.ModelManager;\n+\n+public class InferenceImpl extends InferenceAPIsServiceImplBase {\n+\n+    @Override\n+    public void ping(Empty request, StreamObserver<TorchServeHealthResponse> responseObserver) {\n+        Runnable r =\n+                () -> {\n+                    String response = ApiUtils.getWorkerStatus();\n+                    TorchServeHealthResponse reply =\n+                            TorchServeHealthResponse.newBuilder()\n+                                    .setHealth(\n+                                            JsonUtils.GSON_PRETTY_EXPOSED.toJson(\n+                                                    new StatusResponse(\n+                                                            response, HttpURLConnection.HTTP_OK)))\n+                                    .build();\n+                    responseObserver.onNext(reply);\n+                    responseObserver.onCompleted();\n+                };\n+        ApiUtils.getTorchServeHealth(r);\n+    }\n+\n+    @Override\n+    public void predictions(\n+            PredictionsRequest request, StreamObserver<PredictionResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        if (modelName == null || (\"\").equals(modelName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODU3Ng=="}, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjYwNjAxOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzoyNjoyN1rOHlJJow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NTo1M1rOH9Mzvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwOTI4Mw==", "bodyText": "NIT:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (modelVersion == null || (\"\").equals(modelVersion)) {\n          \n          \n            \n                    if (modelVersion == null || \"\".equals(modelVersion)) {", "url": "https://github.com/pytorch/serve/pull/687#discussion_r508709283", "createdAt": "2020-10-20T17:26:27Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "diffHunk": "@@ -0,0 +1,117 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Empty;\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.net.HttpURLConnection;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.inference.PredictionResponse;\n+import org.pytorch.serve.grpc.inference.PredictionsRequest;\n+import org.pytorch.serve.grpc.inference.TorchServeHealthResponse;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.job.GRPCJob;\n+import org.pytorch.serve.job.Job;\n+import org.pytorch.serve.metrics.api.MetricAggregator;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.pytorch.serve.util.messages.WorkerCommands;\n+import org.pytorch.serve.wlm.ModelManager;\n+\n+public class InferenceImpl extends InferenceAPIsServiceImplBase {\n+\n+    @Override\n+    public void ping(Empty request, StreamObserver<TorchServeHealthResponse> responseObserver) {\n+        Runnable r =\n+                () -> {\n+                    String response = ApiUtils.getWorkerStatus();\n+                    TorchServeHealthResponse reply =\n+                            TorchServeHealthResponse.newBuilder()\n+                                    .setHealth(\n+                                            JsonUtils.GSON_PRETTY_EXPOSED.toJson(\n+                                                    new StatusResponse(\n+                                                            response, HttpURLConnection.HTTP_OK)))\n+                                    .build();\n+                    responseObserver.onNext(reply);\n+                    responseObserver.onCompleted();\n+                };\n+        ApiUtils.getTorchServeHealth(r);\n+    }\n+\n+    @Override\n+    public void predictions(\n+            PredictionsRequest request, StreamObserver<PredictionResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        if (modelName == null || (\"\").equals(modelName)) {\n+            BadRequestException e = new BadRequestException(\"Parameter model_name is required.\");\n+            responseObserver.onError(\n+                    Status.INTERNAL\n+                            .withDescription(e.getMessage())\n+                            .augmentDescription(\"BadRequestException.()\")\n+                            .withCause(e)\n+                            .asRuntimeException());\n+            return;\n+        }\n+\n+        if (modelVersion == null || (\"\").equals(modelVersion)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNTAzOA==", "bodyText": "Done.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533935038", "createdAt": "2020-12-02T06:55:53Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java", "diffHunk": "@@ -0,0 +1,117 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Empty;\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.net.HttpURLConnection;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.inference.PredictionResponse;\n+import org.pytorch.serve.grpc.inference.PredictionsRequest;\n+import org.pytorch.serve.grpc.inference.TorchServeHealthResponse;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.job.GRPCJob;\n+import org.pytorch.serve.job.Job;\n+import org.pytorch.serve.metrics.api.MetricAggregator;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+import org.pytorch.serve.util.messages.InputParameter;\n+import org.pytorch.serve.util.messages.RequestInput;\n+import org.pytorch.serve.util.messages.WorkerCommands;\n+import org.pytorch.serve.wlm.ModelManager;\n+\n+public class InferenceImpl extends InferenceAPIsServiceImplBase {\n+\n+    @Override\n+    public void ping(Empty request, StreamObserver<TorchServeHealthResponse> responseObserver) {\n+        Runnable r =\n+                () -> {\n+                    String response = ApiUtils.getWorkerStatus();\n+                    TorchServeHealthResponse reply =\n+                            TorchServeHealthResponse.newBuilder()\n+                                    .setHealth(\n+                                            JsonUtils.GSON_PRETTY_EXPOSED.toJson(\n+                                                    new StatusResponse(\n+                                                            response, HttpURLConnection.HTTP_OK)))\n+                                    .build();\n+                    responseObserver.onNext(reply);\n+                    responseObserver.onCompleted();\n+                };\n+        ApiUtils.getTorchServeHealth(r);\n+    }\n+\n+    @Override\n+    public void predictions(\n+            PredictionsRequest request, StreamObserver<PredictionResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        if (modelName == null || (\"\").equals(modelName)) {\n+            BadRequestException e = new BadRequestException(\"Parameter model_name is required.\");\n+            responseObserver.onError(\n+                    Status.INTERNAL\n+                            .withDescription(e.getMessage())\n+                            .augmentDescription(\"BadRequestException.()\")\n+                            .withCause(e)\n+                            .asRuntimeException());\n+            return;\n+        }\n+\n+        if (modelVersion == null || (\"\").equals(modelVersion)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwOTI4Mw=="}, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjYxNjAyOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/ManagementImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzoyOToxOFrOHlJP9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjozMDoyOFrOH9MRTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcxMDkwMg==", "bodyText": "This and following util methods should be refactored to the base class so that they can be used in the InferenceImpl.java too", "url": "https://github.com/pytorch/serve/pull/687#discussion_r508710902", "createdAt": "2020-10-20T17:29:18Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/ManagementImpl.java", "diffHunk": "@@ -0,0 +1,193 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.util.concurrent.ExecutionException;\n+import org.pytorch.serve.archive.ModelException;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.management.DescribeModelRequest;\n+import org.pytorch.serve.grpc.management.ListModelsRequest;\n+import org.pytorch.serve.grpc.management.ManagementAPIsServiceGrpc.ManagementAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.management.ManagementResponse;\n+import org.pytorch.serve.grpc.management.RegisterModelRequest;\n+import org.pytorch.serve.grpc.management.ScaleWorkerRequest;\n+import org.pytorch.serve.grpc.management.SetDefaultRequest;\n+import org.pytorch.serve.grpc.management.UnregisterModelRequest;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.GRPCUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+\n+public class ManagementImpl extends ManagementAPIsServiceImplBase {\n+\n+    @Override\n+    public void describeModel(\n+            DescribeModelRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        String resp;\n+        try {\n+            resp =\n+                    JsonUtils.GSON_PRETTY.toJson(\n+                            ApiUtils.getModelDescription(modelName, modelVersion));\n+            sendResponse(responseObserver, resp);\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        }\n+    }\n+\n+    @Override\n+    public void listModels(\n+            ListModelsRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        int limit = request.getLimit();\n+        int pageToken = request.getNextPageToken();\n+\n+        String modelList = JsonUtils.GSON_PRETTY.toJson(ApiUtils.getModelList(limit, pageToken));\n+        sendResponse(responseObserver, modelList);\n+    }\n+\n+    @Override\n+    public void registerModel(\n+            RegisterModelRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        org.pytorch.serve.http.messages.RegisterModelRequest registerModelRequest =\n+                new org.pytorch.serve.http.messages.RegisterModelRequest(request);\n+\n+        StatusResponse statusResponse;\n+        try {\n+            statusResponse = ApiUtils.registerModel(registerModelRequest);\n+            sendStatusResponse(responseObserver, statusResponse);\n+        } catch (InternalServerException e) {\n+            sendException(responseObserver, e, null);\n+        } catch (ExecutionException | InterruptedException e) {\n+            sendException(responseObserver, e, \"Error while creating workers\");\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        } catch (ModelException | BadRequestException e) {\n+            sendErrorResponse(responseObserver, Status.INVALID_ARGUMENT, e);\n+        }\n+    }\n+\n+    @Override\n+    public void scaleWorker(\n+            ScaleWorkerRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        int minWorkers = GRPCUtils.getRegisterParam(request.getMinWorker(), 1);\n+        int maxWorkers = GRPCUtils.getRegisterParam(request.getMaxWorker(), minWorkers);\n+        String modelName = GRPCUtils.getRegisterParam(request.getModelName(), null);\n+        String modelVersion = GRPCUtils.getRegisterParam(request.getModelVersion(), null);\n+        boolean synchronous = request.getSynchronous();\n+\n+        StatusResponse statusResponse;\n+        try {\n+            statusResponse =\n+                    ApiUtils.updateModelWorkers(\n+                            modelName,\n+                            modelVersion,\n+                            minWorkers,\n+                            maxWorkers,\n+                            synchronous,\n+                            false,\n+                            null);\n+            sendStatusResponse(responseObserver, statusResponse);\n+        } catch (ExecutionException | InterruptedException e) {\n+            sendException(responseObserver, e, \"Error while creating workers\");\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        } catch (BadRequestException e) {\n+            sendErrorResponse(responseObserver, Status.INVALID_ARGUMENT, e);\n+        }\n+    }\n+\n+    @Override\n+    public void setDefault(\n+            SetDefaultRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String newModelVersion = request.getModelVersion();\n+\n+        try {\n+            String msg = ApiUtils.setDefault(modelName, newModelVersion);\n+            sendResponse(responseObserver, msg);\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        }\n+    }\n+\n+    @Override\n+    public void unregisterModel(\n+            UnregisterModelRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        try {\n+            String modelName = request.getModelName();\n+            if (modelName == null || (\"\").equals(modelName)) {\n+                sendErrorResponse(\n+                        responseObserver,\n+                        Status.INVALID_ARGUMENT,\n+                        new BadRequestException(\"Parameter url is required.\"));\n+            }\n+\n+            String modelVersion = request.getModelVersion();\n+\n+            if ((\"\").equals(modelVersion)) {\n+                modelVersion = null;\n+            }\n+            ApiUtils.unregisterModel(modelName, modelVersion);\n+            String msg = \"Model \\\"\" + modelName + \"\\\" unregistered\";\n+            sendResponse(responseObserver, msg);\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        } catch (BadRequestException e) {\n+            sendErrorResponse(responseObserver, Status.INVALID_ARGUMENT, e);\n+        }\n+    }\n+\n+    private void sendResponse(StreamObserver<ManagementResponse> responseObserver, String msg) {\n+        ManagementResponse reply = ManagementResponse.newBuilder().setMsg(msg).build();\n+        responseObserver.onNext(reply);\n+        responseObserver.onCompleted();\n+    }\n+\n+    private void sendErrorResponse(\n+            StreamObserver<ManagementResponse> responseObserver,\n+            Status status,\n+            String description,\n+            String errorClass) {\n+        responseObserver.onError(\n+                status.withDescription(description)\n+                        .augmentDescription(errorClass)\n+                        .asRuntimeException());\n+    }\n+\n+    private void sendErrorResponse(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkyNjIyMg==", "bodyText": "@maaquib: Both Inference APIs and Management APIs use different responseObserver types. Also, the inference API has only one main API (predict). Thus for avoiding one minor code duplication we will need to add a bunch of complicated type castings.\nI have refactored the InferenceImpl.java to avoid code duplication within the file for sending error response.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533926222", "createdAt": "2020-12-02T06:30:28Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/grpcimpl/ManagementImpl.java", "diffHunk": "@@ -0,0 +1,193 @@\n+package org.pytorch.serve.grpcimpl;\n+\n+import io.grpc.Status;\n+import io.grpc.stub.StreamObserver;\n+import java.util.concurrent.ExecutionException;\n+import org.pytorch.serve.archive.ModelException;\n+import org.pytorch.serve.archive.ModelNotFoundException;\n+import org.pytorch.serve.archive.ModelVersionNotFoundException;\n+import org.pytorch.serve.grpc.management.DescribeModelRequest;\n+import org.pytorch.serve.grpc.management.ListModelsRequest;\n+import org.pytorch.serve.grpc.management.ManagementAPIsServiceGrpc.ManagementAPIsServiceImplBase;\n+import org.pytorch.serve.grpc.management.ManagementResponse;\n+import org.pytorch.serve.grpc.management.RegisterModelRequest;\n+import org.pytorch.serve.grpc.management.ScaleWorkerRequest;\n+import org.pytorch.serve.grpc.management.SetDefaultRequest;\n+import org.pytorch.serve.grpc.management.UnregisterModelRequest;\n+import org.pytorch.serve.http.BadRequestException;\n+import org.pytorch.serve.http.InternalServerException;\n+import org.pytorch.serve.http.StatusResponse;\n+import org.pytorch.serve.util.ApiUtils;\n+import org.pytorch.serve.util.GRPCUtils;\n+import org.pytorch.serve.util.JsonUtils;\n+\n+public class ManagementImpl extends ManagementAPIsServiceImplBase {\n+\n+    @Override\n+    public void describeModel(\n+            DescribeModelRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+\n+        String modelName = request.getModelName();\n+        String modelVersion = request.getModelVersion();\n+\n+        String resp;\n+        try {\n+            resp =\n+                    JsonUtils.GSON_PRETTY.toJson(\n+                            ApiUtils.getModelDescription(modelName, modelVersion));\n+            sendResponse(responseObserver, resp);\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        }\n+    }\n+\n+    @Override\n+    public void listModels(\n+            ListModelsRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        int limit = request.getLimit();\n+        int pageToken = request.getNextPageToken();\n+\n+        String modelList = JsonUtils.GSON_PRETTY.toJson(ApiUtils.getModelList(limit, pageToken));\n+        sendResponse(responseObserver, modelList);\n+    }\n+\n+    @Override\n+    public void registerModel(\n+            RegisterModelRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        org.pytorch.serve.http.messages.RegisterModelRequest registerModelRequest =\n+                new org.pytorch.serve.http.messages.RegisterModelRequest(request);\n+\n+        StatusResponse statusResponse;\n+        try {\n+            statusResponse = ApiUtils.registerModel(registerModelRequest);\n+            sendStatusResponse(responseObserver, statusResponse);\n+        } catch (InternalServerException e) {\n+            sendException(responseObserver, e, null);\n+        } catch (ExecutionException | InterruptedException e) {\n+            sendException(responseObserver, e, \"Error while creating workers\");\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        } catch (ModelException | BadRequestException e) {\n+            sendErrorResponse(responseObserver, Status.INVALID_ARGUMENT, e);\n+        }\n+    }\n+\n+    @Override\n+    public void scaleWorker(\n+            ScaleWorkerRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        int minWorkers = GRPCUtils.getRegisterParam(request.getMinWorker(), 1);\n+        int maxWorkers = GRPCUtils.getRegisterParam(request.getMaxWorker(), minWorkers);\n+        String modelName = GRPCUtils.getRegisterParam(request.getModelName(), null);\n+        String modelVersion = GRPCUtils.getRegisterParam(request.getModelVersion(), null);\n+        boolean synchronous = request.getSynchronous();\n+\n+        StatusResponse statusResponse;\n+        try {\n+            statusResponse =\n+                    ApiUtils.updateModelWorkers(\n+                            modelName,\n+                            modelVersion,\n+                            minWorkers,\n+                            maxWorkers,\n+                            synchronous,\n+                            false,\n+                            null);\n+            sendStatusResponse(responseObserver, statusResponse);\n+        } catch (ExecutionException | InterruptedException e) {\n+            sendException(responseObserver, e, \"Error while creating workers\");\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        } catch (BadRequestException e) {\n+            sendErrorResponse(responseObserver, Status.INVALID_ARGUMENT, e);\n+        }\n+    }\n+\n+    @Override\n+    public void setDefault(\n+            SetDefaultRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        String modelName = request.getModelName();\n+        String newModelVersion = request.getModelVersion();\n+\n+        try {\n+            String msg = ApiUtils.setDefault(modelName, newModelVersion);\n+            sendResponse(responseObserver, msg);\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        }\n+    }\n+\n+    @Override\n+    public void unregisterModel(\n+            UnregisterModelRequest request, StreamObserver<ManagementResponse> responseObserver) {\n+        try {\n+            String modelName = request.getModelName();\n+            if (modelName == null || (\"\").equals(modelName)) {\n+                sendErrorResponse(\n+                        responseObserver,\n+                        Status.INVALID_ARGUMENT,\n+                        new BadRequestException(\"Parameter url is required.\"));\n+            }\n+\n+            String modelVersion = request.getModelVersion();\n+\n+            if ((\"\").equals(modelVersion)) {\n+                modelVersion = null;\n+            }\n+            ApiUtils.unregisterModel(modelName, modelVersion);\n+            String msg = \"Model \\\"\" + modelName + \"\\\" unregistered\";\n+            sendResponse(responseObserver, msg);\n+        } catch (ModelNotFoundException | ModelVersionNotFoundException e) {\n+            sendErrorResponse(responseObserver, Status.NOT_FOUND, e);\n+        } catch (BadRequestException e) {\n+            sendErrorResponse(responseObserver, Status.INVALID_ARGUMENT, e);\n+        }\n+    }\n+\n+    private void sendResponse(StreamObserver<ManagementResponse> responseObserver, String msg) {\n+        ManagementResponse reply = ManagementResponse.newBuilder().setMsg(msg).build();\n+        responseObserver.onNext(reply);\n+        responseObserver.onCompleted();\n+    }\n+\n+    private void sendErrorResponse(\n+            StreamObserver<ManagementResponse> responseObserver,\n+            Status status,\n+            String description,\n+            String errorClass) {\n+        responseObserver.onError(\n+                status.withDescription(description)\n+                        .augmentDescription(errorClass)\n+                        .asRuntimeException());\n+    }\n+\n+    private void sendErrorResponse(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcxMDkwMg=="}, "originalCommit": {"oid": "3d7b0b9add224eee1017ca4ba0b13ba583ed5a65"}, "originalPosition": 163}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODM5MDg3OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/resources/proto/inference.proto", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNzoxMTozNVrOHrYWTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNTowMDoxNlrOHsgUSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI0OTc0Mw==", "bodyText": "Is it possible to align with existing Http request structure to make code consistent? Existing http request includes\n\ncommand or customer command\nRequestInput\n-- requestId\n-- headers\n-- parameters", "url": "https://github.com/pytorch/serve/pull/687#discussion_r515249743", "createdAt": "2020-10-30T17:11:35Z", "author": {"login": "lxning"}, "path": "frontend/server/src/main/resources/proto/inference.proto", "diffHunk": "@@ -0,0 +1,35 @@\n+syntax = \"proto3\";\n+\n+package org.pytorch.serve.grpc.inference;\n+\n+import \"google/protobuf/empty.proto\";\n+\n+option java_multiple_files = true;\n+\n+message PredictionsRequest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc8e410e4b1ff891f361a970ea609947324ecc0d"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyODg3NQ==", "bodyText": "Please refer my inputs on the other comment", "url": "https://github.com/pytorch/serve/pull/687#discussion_r516428875", "createdAt": "2020-11-03T05:00:16Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/resources/proto/inference.proto", "diffHunk": "@@ -0,0 +1,35 @@\n+syntax = \"proto3\";\n+\n+package org.pytorch.serve.grpc.inference;\n+\n+import \"google/protobuf/empty.proto\";\n+\n+option java_multiple_files = true;\n+\n+message PredictionsRequest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI0OTc0Mw=="}, "originalCommit": {"oid": "dc8e410e4b1ff891f361a970ea609947324ecc0d"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODQwODQzOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/resources/proto/inference.proto", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNzoxNToxM1rOHrYhDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNDo1OTozNVrOHsgTsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI1MjQ5Mw==", "bodyText": "Same for prediction response, the existing Http prediction response can also handle failure.  It includes:\n\nrequestId\nstatusCode\nreasonPhrase\ncontentType\nheaders\nresult", "url": "https://github.com/pytorch/serve/pull/687#discussion_r515252493", "createdAt": "2020-10-30T17:15:13Z", "author": {"login": "lxning"}, "path": "frontend/server/src/main/resources/proto/inference.proto", "diffHunk": "@@ -0,0 +1,35 @@\n+syntax = \"proto3\";\n+\n+package org.pytorch.serve.grpc.inference;\n+\n+import \"google/protobuf/empty.proto\";\n+\n+option java_multiple_files = true;\n+\n+message PredictionsRequest {\n+    // Name of model.\n+    string model_name = 1; //required\n+\n+    // Version of model to run prediction on.\n+    string model_version = 2; //optional\n+\n+    // input data for model prediction\n+    map<string, bytes> input = 3; //required\n+}\n+\n+message PredictionResponse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc8e410e4b1ff891f361a970ea609947324ecc0d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyODcyMA==", "bodyText": "@lxning: These are all HTTP layer params and gRPC is a wrapper on top of HTTP/2.0 protocol and all these parameters are populated by the gRPC framework.\nRegarding API failures, we send back a proper gRPC error where all our existing HTTP error codes map to corresponding gRPC error codes and gRPC client gets an exception with the error message and status code.\nRefer following code snippets\nManagement api error response\nInference API error response\nHTTP to gRPC error code mapping", "url": "https://github.com/pytorch/serve/pull/687#discussion_r516428720", "createdAt": "2020-11-03T04:59:35Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/resources/proto/inference.proto", "diffHunk": "@@ -0,0 +1,35 @@\n+syntax = \"proto3\";\n+\n+package org.pytorch.serve.grpc.inference;\n+\n+import \"google/protobuf/empty.proto\";\n+\n+option java_multiple_files = true;\n+\n+message PredictionsRequest {\n+    // Name of model.\n+    string model_name = 1; //required\n+\n+    // Version of model to run prediction on.\n+    string model_version = 2; //optional\n+\n+    // input data for model prediction\n+    map<string, bytes> input = 3; //required\n+}\n+\n+message PredictionResponse {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI1MjQ5Mw=="}, "originalCommit": {"oid": "dc8e410e4b1ff891f361a970ea609947324ecc0d"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyMzYxMTk2OnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/ModelServer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMjozNDo1OVrOH5ZgmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwODowODoxNFrOH5nVTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0ODgyNQ==", "bodyText": "it is better to be configurable: REST/gRPC/both", "url": "https://github.com/pytorch/serve/pull/687#discussion_r529948825", "createdAt": "2020-11-24T22:34:59Z", "author": {"login": "lxning"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ModelServer.java", "diffHunk": "@@ -104,7 +111,10 @@ public void startAndWait()\n             throws InterruptedException, IOException, GeneralSecurityException,\n                     InvalidSnapshotException {\n         try {\n-            List<ChannelFuture> channelFutures = start();\n+            List<ChannelFuture> channelFutures = startRESTserver();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ab7be17b43621435568f1ed349c194ea91d73116"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE3NTMwOA==", "bodyText": "@lxning: Logged following enhancement ticket to track this: #799. This is not a blocker item for the 0.3.0 release.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r530175308", "createdAt": "2020-11-25T08:08:14Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/ModelServer.java", "diffHunk": "@@ -104,7 +111,10 @@ public void startAndWait()\n             throws InterruptedException, IOException, GeneralSecurityException,\n                     InvalidSnapshotException {\n         try {\n-            List<ChannelFuture> channelFutures = start();\n+            List<ChannelFuture> channelFutures = startRESTserver();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0ODgyNQ=="}, "originalCommit": {"oid": "ab7be17b43621435568f1ed349c194ea91d73116"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODUwNjQwOnYy", "diffSide": "RIGHT", "path": "frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTowMjoxOFrOH8_ouA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwNjo1NDo1MlrOH9MyMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcxOTIyNA==", "bodyText": "Same as above, can you explain whats the reason for change 507 to 513?", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533719224", "createdAt": "2020-12-01T21:02:18Z", "author": {"login": "maaquib"}, "path": "frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java", "diffHunk": "@@ -447,7 +446,12 @@ public void channelRead0(ChannelHandlerContext ctx, ModelWorkerResponse msg) {\n         public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n             logger.error(\"Unknown exception\", cause);\n             if (cause instanceof OutOfMemoryError) {\n-                NettyUtils.sendError(ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, cause);\n+                ModelWorkerResponse msg = new ModelWorkerResponse();\n+                msg.setCode(HttpURLConnection.HTTP_ENTITY_TOO_LARGE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d62f22db63e9349a93a40c658bdde37b103fc9d"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkzNDY0MQ==", "bodyText": "HttpURLConnection doesn't support 507 code.", "url": "https://github.com/pytorch/serve/pull/687#discussion_r533934641", "createdAt": "2020-12-02T06:54:52Z", "author": {"login": "harshbafna"}, "path": "frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java", "diffHunk": "@@ -447,7 +446,12 @@ public void channelRead0(ChannelHandlerContext ctx, ModelWorkerResponse msg) {\n         public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n             logger.error(\"Unknown exception\", cause);\n             if (cause instanceof OutOfMemoryError) {\n-                NettyUtils.sendError(ctx, HttpResponseStatus.INSUFFICIENT_STORAGE, cause);\n+                ModelWorkerResponse msg = new ModelWorkerResponse();\n+                msg.setCode(HttpURLConnection.HTTP_ENTITY_TOO_LARGE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcxOTIyNA=="}, "originalCommit": {"oid": "5d62f22db63e9349a93a40c658bdde37b103fc9d"}, "originalPosition": 122}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1431, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}