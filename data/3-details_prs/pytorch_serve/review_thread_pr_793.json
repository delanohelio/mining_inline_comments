{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI1Mjc3Nzk2", "number": 793, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNDoxOVrOE9yYEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwMjo1ODowMFrOE-eotQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMjIzOTUzOnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNDoxOVrOH6ptDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNDoxOVrOH6ptDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI2MjczMw==", "bodyText": "@jagadeeshi2i Please add steps for cleanup of the nfs-disk as well", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531262733", "createdAt": "2020-11-27T00:34:19Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423\n+kubeconfig entry generated for ts.\n+NAME  LOCATION  MASTER_VERSION   MASTER_IP      MACHINE_TYPE   NODE_VERSION     NUM_NODES  STATUS\n+ts    us-west1  1.16.13-gke.401  34.83.140.167  n1-standard-4  1.16.13-gke.401  1          RUNNING\n+```\n+\n+#### 1.3 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use GKE Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [gcloud components install](https://kubernetes.io/docs/tasks/tools/install-kubectl/) command:\n+\n+```bash\n+gcloud components install kubectl\n+```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [gcloud container clusters get-credentials](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```bash\n+gcloud container clusters get-credentials torchserve --region us-west1 --project pytorch-tests-261423\n+Fetching cluster endpoint and auth data.\n+kubeconfig entry generated for torchserve.\n+```\n+\n+#### 1.4 Install helm\n+\n+```bash\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on GKE\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```bash\n+git clone https://github.com/pytorch/serve.git\n+\n+cd serve/kubernetes/GKE\n+```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```bash\n+kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n+daemonset.apps/nvidia-driver-installer created\n+```\n+\n+```bash\n+kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,MEMORY:.status.allocatable.memory,CPU:.status.allocatable.cpu,GPU:.status.allocatable.nvidia\\.com/gpu\"\n+``` \n+should show something similar to:\n+\n+```bash\n+NAME                                        MEMORY       CPU     GPU\n+gke-torchserve-default-pool-aa9f7d99-ggc9   12698376Ki   3920m   1\n+```\n+\n+#### 2.3 Create a storage disk\n+\n+A standard storage class is created with google compute disk. If multiple pods need concurrent access to the same storage volume, you need Google NFS. Create the storage disk named *nfs-disk* with the following command:\n+\n+```bash\n+gcloud compute disks create --size=200GB --zone=us-west1-a nfs-disk\n+\n+NAME     ZONE        SIZE_GB  TYPE         STATUS\n+nfs-disk  us-west1-a  200      pd-standard  READY\n+\n+New disks are unformatted. You must format and mount a disk before it\n+can be used. You can find instructions on how to do this at:\n+\n+https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting\n+\n+```\n+\n+#### 2.4 Create NFS Server\n+\n+Modify the values.yaml in the nfs-provisioner with persistent volume name, disk name and node affinity zones and install nfs-provisioner using helm.\n+\n+```bash\n+cd GKE\n+\n+helm install mynfs ./nfs-provisioner/\n+```\n+\n+```kubectl get pods``` should show something similiar to:\n+\n+```bash\n+NAME                                             READY   STATUS    RESTARTS   AGE\n+pod/mynfs-nfs-provisioner-bcc7c96cc-5xr2k   1/1     Running   0          19h\n+```\n+\n+#### 2.5 Create PV and PVC\n+\n+Run the below command and get NFS server IP:\n+\n+```bash\n+kubectl get svc -n default mynfs-nfs-provisioner -o jsonpath='{.spec.clusterIP}'\n+```\n+\n+Replace storage size and server IP in pv_pvc.yaml with the server IP got from above command. Run the below kubectl command and create PV and PVC\n+\n+```bash\n+kubectl apply -f templates/pv_pvc.yaml -n default\n+```\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```bash\n+kubectl get pvc,pv -n default\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE\n+persistentvolume/nfs   10Gi       RWX            Retain           Bound    default/nfs                           20h\n+\n+NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+persistentvolumeclaim/nfs   Bound    nfs      10Gi       RWX                           20h\n+```\n+\n+#### 2.6 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```bash\n+kubectl apply -f templates/pod.yaml\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+pod/model-store-pod created\n+```\n+\n+Verify that the pod is created by excuting.\n+\n+```bash\n+kubectl get po\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+NAME                                              READY   STATUS    RESTARTS   AGE\n+model-store-pod                                   1/1     Running   0          143m\n+```\n+\n+#### 2.6 Down and copy MAR / config files\n+\n+```bash\n+wget https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar\n+wget https://torchserve.pytorch.org/mar_files/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\n+kubectl cp squeezenet1_1.mar model-store-pod:/pv/model-store/squeezenet1_1.mar\n+kubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n+\n+kubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\n+kubectl cp config.properties model-store-pod:/pv/config/config.properties\n+```\n+\n+Verify that the MAR / config files have been copied to the directory.\n+\n+```bash\n+kubectl exec --tty pod/model-store-pod -- ls -lR /pv/\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+/pv/:\n+total 28\n+drwxr-xr-x 2 root root  4096 Nov 21 16:30 config\n+-rw-r--r-- 1 root root    16 Nov 21 15:42 index.html\n+drwx------ 2 root root 16384 Nov 21 15:42 lost+found\n+drwxr-xr-x 2 root root  4096 Nov 21 16:12 model-store\n+\n+/pv/config:\n+total 4\n+-rw-rw-r-- 1 1000 1000 579 Nov 21 16:30 config.properties\n+\n+/pv/lost+found:\n+total 0\n+\n+/pv/model-store:\n+total 8864\n+-rw-rw-r-- 1 1000 1000 4463882 Nov 21 16:12 mnist.mar\n+-rw-rw-r-- 1 1000 1000 4609382 Nov 21 16:11 squeezenet1_1.mar\n+```\n+\n+Delete model store pod.\n+\n+```bash\n+kubectl delete pod/model-store-pod\n+pod \"model-store-pod\" deleted\n+```\n+\n+#### 2.7 Install Torchserve using Helm Charts\n+\n+Enter the Helm directory and install TorchServe using Helm Charts.\n+\n+```bash\n+cd ../Helm\n+```\n+\n+```bash\n+helm install ts .\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+NAME: ts\n+LAST DEPLOYED: Thu Aug 20 02:07:38 2020\n+NAMESPACE: default\n+STATUS: deployed\n+REVISION: 1\n+TEST SUITE: None\n+```\n+\n+#### 2.8 Check the status of TorchServe\n+\n+```bash\n+kubectl get po\n+```\n+\n+The installation will take a few minutes. Output like this means the installation is not completed yet.\n+\n+```bash\n+NAME                               READY   STATUS              RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        0/1     ContainerCreating   0          6s\n+\n+Output like this means the installation is completed.\n+\n+NAME                               READY   STATUS    RESTARTS   AGE\n+torchserve-75f5b67469-5hnsn        1/1     Running   0          2m36s\n+```\n+\n+### 3 Test Torchserve Installation\n+\n+#### 3.1 Fetch the Load Balancer Extenal IP\n+\n+Fetch the Load Balancer Extenal IP by executing.\n+\n+```bash\n+kubectl get svc\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+NAME               TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)                         AGE\n+kubernetes         ClusterIP      10.0.0.1       <none>         443/TCP                         5d19h\n+torchserve         LoadBalancer   10.0.39.88     your-external-IP   8080:30306/TCP,8081:30442/TCP   48s\n+```\n+\n+#### 3.2 Test Management / Prediction APIs\n+\n+```bash\n+curl http://your-external-IP:8081/models\n+```\n+\n+Your output should look similar to\n+\n+```json\n+{\n+  \"models\": [\n+    {\n+      \"modelName\": \"mnist\",\n+      \"modelUrl\": \"mnist.mar\"\n+    },\n+    {\n+      \"modelName\": \"squeezenet1_1\",\n+      \"modelUrl\": \"squeezenet1_1.mar\"\n+    }\n+  ]\n+}\n+```\n+\n+```bash\n+curl http://your-external-IP:8081/models/mnist\n+```\n+\n+Your output should look similar to\n+\n+```json\n+[\n+  {\n+    \"modelName\": \"mnist\",\n+    \"modelVersion\": \"1.0\",\n+    \"modelUrl\": \"mnist.mar\",\n+    \"runtime\": \"python\",\n+    \"minWorkers\": 5,\n+    \"maxWorkers\": 5,\n+    \"batchSize\": 1,\n+    \"maxBatchDelay\": 200,\n+    \"loadedAtStartup\": false,\n+    \"workers\": [\n+      {\n+        \"id\": \"9003\",\n+        \"startTime\": \"2020-08-20T03:06:38.435Z\",\n+        \"status\": \"READY\",\n+        \"gpu\": false,\n+        \"memoryUsage\": 32194560\n+      },\n+      {\n+        \"id\": \"9004\",\n+        \"startTime\": \"2020-08-20T03:06:38.436Z\",\n+        \"status\": \"READY\",\n+        \"gpu\": false,\n+        \"memoryUsage\": 31842304\n+      },\n+      {\n+        \"id\": \"9005\",\n+        \"startTime\": \"2020-08-20T03:06:38.436Z\",\n+        \"status\": \"READY\",\n+        \"gpu\": false,\n+        \"memoryUsage\": 44621824\n+      },\n+      {\n+        \"id\": \"9006\",\n+        \"startTime\": \"2020-08-20T03:06:38.436Z\",\n+        \"status\": \"READY\",\n+        \"gpu\": false,\n+        \"memoryUsage\": 42045440\n+      },\n+      {\n+        \"id\": \"9007\",\n+        \"startTime\": \"2020-08-20T03:06:38.436Z\",\n+        \"status\": \"READY\",\n+        \"gpu\": false,\n+        \"memoryUsage\": 31584256\n+      }\n+    ]\n+  }\n+]\n+```\n+\n+```bash\n+curl -v http://your-external-IP:8080/predictions/mnist -T ../../examples/image_classifier/mnist/test_data/0.png\n+\n+*   Trying 34.82.176.215...\n+* Connected to 34.82.176.215 (34.82.176.215) port 8080 (#0)\n+> PUT /predictions/mnist HTTP/1.1\n+> Host: 34.82.176.215:8080\n+> User-Agent: curl/7.47.0\n+> Accept: */*\n+> Content-Length: 272\n+> Expect: 100-continue\n+> \n+< HTTP/1.1 100 Continue\n+* We are completely uploaded and fine\n+< HTTP/1.1 200 \n+< x-request-id: 0a70761f-0df4-40dd-9620-2cd98cb810d5\n+< Pragma: no-cache\n+< Cache-Control: no-cache; no-store, must-revalidate, private\n+< Expires: Thu, 01 Jan 1970 00:00:00 UTC\n+< content-length: 1\n+< connection: keep-alive\n+< \n+* Connection #0 to host 34.82.176.215 left intact\n+0\n+```\n+\n+### 4 Troubleshooting\n+\n+#### 4.1 Troubleshooting GKE Cluster Creation**\n+\n+Possible errors in this step may be a result of\n+\n+* IAM limits.\n+* Quota restrictions during cluster creation - [GKE Quotas](https://cloud.google.com/compute/quotas)\n+\n+You should able be able to find the following resources at the end of this step in the respective Gcoud consoles\n+\n+* GKE -> Cluser in the Gcloud Console\n+\n+#### 4.2 Troubleshooting NFS Persitant Volume Creation\n+\n+Possible error in this step may be a result of one of the following. Your pod my be struck in *Init / Creating* forever / persitant volume claim may be in *Pending* forever.\n+\n+* Storage disk not created / wrong storage disk name.\n+\n+  * Check if the storage is created and the correct name is given in the values.yaml\n+\n+* Node affinity does not match\n+\n+  * Check affinity zone in values.yaml to match the storage disk.\n+\n+* Wrong Server IP\n+  \n+  * Check the server IP mentioned in the pv_pvc.yaml\n+\n+* You can execute the following commands to inspect the pods / events to debug NFS Issues\n+\n+    ```bash\n+    kubectl get events --sort-by='.metadata.creationTimestamp'\n+    kubectl get pod --all-namespaces # Get the Pod ID\n+    kubectl logs pod/mynfs-nfs-provisioner-YOUR_POD\n+    kubectl logs pod/mynfs-nfs-provisioner-YOUR_POD\n+    kubectl describe pod/mynfs-nfs-provisioner-YOUR_POD\n+    ```\n+\n+#### 4.3 Troubleshooting Torchserve Helm Chart\n+\n+Possible errors in this step may be a result of\n+\n+* Incorrect values in ``values.yaml``\n+  * Changing values in `torchserve.pvd_mount`  would need corresponding change in `config.properties`\n+* Invalid `config.properties`\n+  * You can verify these values by running this for local TS installation\n+* TS Pods in *Pending* state\n+  * Ensure you have available Nodes in Node Group\n+\n+* Helm Installation\n+  * You may inspect the values by running ``helm list`` and `helm get all ts` to verify if the values used for the installation\n+  * You can uninstall / reinstall the helm chart by executing  `helm uninstall ts` and `helm install ts .`\n+  * If you get an error `invalid: data: Too long: must have at most 1048576 characters`, ensure that you dont have any stale files in your kubernetes dir. Else add them to .helmignore file.\n+\n+#### 4.4 Troubleshooting Torchserve\n+\n+* Check pod logs\n+\n+  ```bash\n+  kubectl logs torchserve-75f5b67469-5hnsn -c torchserve -n default\n+\n+  2020-11-26 14:33:22,974 [INFO ] main org.pytorch.serve.ModelServer - \n+  Torchserve version: 0.1.1\n+  TS Home: /home/venv/lib/python3.6/site-packages\n+  Current directory: /home/model-server\n+  Temp directory: /home/model-server/tmp\n+  Number of GPUs: 1\n+  Number of CPUs: 1\n+  Max heap size: 989 M\n+  Python executable: /home/venv/bin/python3\n+  Config file: /home/model-server/shared/config/config.properties\n+  Inference address: http://0.0.0.0:8080\n+  Management address: http://0.0.0.0:8081\n+  Model Store: /home/model-server/shared/model-store\n+  Initial Models: N/A\n+  Log dir: /home/model-server/logs\n+  Metrics dir: /home/model-server/logs\n+  Netty threads: 32\n+  Netty client threads: 0\n+  Default workers per model: 1\n+  Blacklist Regex: N/A\n+  Maximum Response Size: 6553500\n+  Maximum Request Size: 6553500\n+  Prefer direct buffer: false\n+  2020-11-26 14:33:23,004 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {\"name\":\"startup.cfg\",\"modelCount\":2,\"models\":{\"squeezenet1_1\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"squeezenet1_1.mar\",\"minWorkers\":3,\"maxWorkers\":3,\"batchSize\":1,\"maxBatchDelay\":100,\"responseTimeout\":120}},\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":5,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":200,\"responseTimeout\":60}}}}\n+  ```\n+\n+* Inspect pod\n+\n+  ```bash\n+  kubectl exec -it torchserve-75f5b67469-5hnsn -c torchserve -n default -- bash\n+  ```\n+\n+#### 4.5 Delete the Resources", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 490}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMjI0MDM0OnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNTowMFrOH6pteg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNTowMFrOH6pteg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI2Mjg0Mg==", "bodyText": "Obfuscate the project name in the logs", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531262842", "createdAt": "2020-11-27T00:35:00Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMjI0MjM1OnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNjo0N1rOH6pung==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNjo0N1rOH6pung==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI2MzEzNA==", "bodyText": "mention user has to specify their own project name. It will be better to define it as a ENV config variable at the beginning and just reference that here", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531263134", "createdAt": "2020-11-27T00:36:47Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423\n+kubeconfig entry generated for ts.\n+NAME  LOCATION  MASTER_VERSION   MASTER_IP      MACHINE_TYPE   NODE_VERSION     NUM_NODES  STATUS\n+ts    us-west1  1.16.13-gke.401  34.83.140.167  n1-standard-4  1.16.13-gke.401  1          RUNNING\n+```\n+\n+#### 1.3 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use GKE Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [gcloud components install](https://kubernetes.io/docs/tasks/tools/install-kubectl/) command:\n+\n+```bash\n+gcloud components install kubectl\n+```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [gcloud container clusters get-credentials](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```bash\n+gcloud container clusters get-credentials torchserve --region us-west1 --project pytorch-tests-261423", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMjI0MzEyOnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNzoyNlrOH6pu_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozNzoyNlrOH6pu_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI2MzIzMQ==", "bodyText": "@jagadeeshi2i Please include steps for CPU only cluster as well. Current steps are for a GPU cluster", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531263231", "createdAt": "2020-11-27T00:37:26Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423\n+kubeconfig entry generated for ts.\n+NAME  LOCATION  MASTER_VERSION   MASTER_IP      MACHINE_TYPE   NODE_VERSION     NUM_NODES  STATUS\n+ts    us-west1  1.16.13-gke.401  34.83.140.167  n1-standard-4  1.16.13-gke.401  1          RUNNING\n+```\n+\n+#### 1.3 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use GKE Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [gcloud components install](https://kubernetes.io/docs/tasks/tools/install-kubectl/) command:\n+\n+```bash\n+gcloud components install kubectl\n+```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [gcloud container clusters get-credentials](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```bash\n+gcloud container clusters get-credentials torchserve --region us-west1 --project pytorch-tests-261423\n+Fetching cluster endpoint and auth data.\n+kubeconfig entry generated for torchserve.\n+```\n+\n+#### 1.4 Install helm\n+\n+```bash\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on GKE\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```bash\n+git clone https://github.com/pytorch/serve.git\n+\n+cd serve/kubernetes/GKE\n+```\n+\n+#### 2.2 Install NVIDIA device plugin", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMjI0NDA2OnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozODo0MVrOH6pvjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDozODo0MVrOH6pvjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI2MzM3NQ==", "bodyText": "@jagadeeshi2i Please mention that one needs to update the nfs-provisioner/values.yaml and change the  to match the name that user used for creating", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531263375", "createdAt": "2020-11-27T00:38:41Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423\n+kubeconfig entry generated for ts.\n+NAME  LOCATION  MASTER_VERSION   MASTER_IP      MACHINE_TYPE   NODE_VERSION     NUM_NODES  STATUS\n+ts    us-west1  1.16.13-gke.401  34.83.140.167  n1-standard-4  1.16.13-gke.401  1          RUNNING\n+```\n+\n+#### 1.3 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use GKE Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [gcloud components install](https://kubernetes.io/docs/tasks/tools/install-kubectl/) command:\n+\n+```bash\n+gcloud components install kubectl\n+```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [gcloud container clusters get-credentials](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```bash\n+gcloud container clusters get-credentials torchserve --region us-west1 --project pytorch-tests-261423\n+Fetching cluster endpoint and auth data.\n+kubeconfig entry generated for torchserve.\n+```\n+\n+#### 1.4 Install helm\n+\n+```bash\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on GKE\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```bash\n+git clone https://github.com/pytorch/serve.git\n+\n+cd serve/kubernetes/GKE\n+```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```bash\n+kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n+daemonset.apps/nvidia-driver-installer created\n+```\n+\n+```bash\n+kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,MEMORY:.status.allocatable.memory,CPU:.status.allocatable.cpu,GPU:.status.allocatable.nvidia\\.com/gpu\"\n+``` \n+should show something similar to:\n+\n+```bash\n+NAME                                        MEMORY       CPU     GPU\n+gke-torchserve-default-pool-aa9f7d99-ggc9   12698376Ki   3920m   1\n+```\n+\n+#### 2.3 Create a storage disk\n+\n+A standard storage class is created with google compute disk. If multiple pods need concurrent access to the same storage volume, you need Google NFS. Create the storage disk named *nfs-disk* with the following command:\n+\n+```bash\n+gcloud compute disks create --size=200GB --zone=us-west1-a nfs-disk\n+\n+NAME     ZONE        SIZE_GB  TYPE         STATUS\n+nfs-disk  us-west1-a  200      pd-standard  READY\n+\n+New disks are unformatted. You must format and mount a disk before it\n+can be used. You can find instructions on how to do this at:\n+\n+https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting\n+\n+```\n+\n+#### 2.4 Create NFS Server\n+\n+Modify the values.yaml in the nfs-provisioner with persistent volume name, disk name and node affinity zones and install nfs-provisioner using helm.\n+\n+```bash\n+cd GKE\n+\n+helm install mynfs ./nfs-provisioner/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMjMwMDEyOnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMDo1MDoxM1rOH6qUuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwMTowMDozNlrOH6q4ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI3Mjg4OQ==", "bodyText": "@jagadeeshi2i The model-store pod is not getting created, using the commands specified Seeing following error in the describe pods:\n\n(base) gchauhan-mbp:GKE gchauhan$ kubectl describe pod/model-store-pod\nName:         model-store-pod\nNamespace:    default\nPriority:     0\nNode:         \nLabels:       \nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\n{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"model-store-pod\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"args\"...\nkubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container model-store\nStatus:       Pending\nIP:\nIPs:          \nContainers:\nmodel-store:\nImage:      ubuntu\nPort:       \nHost Port:  \nCommand:\nsleep\nArgs:\ninfinity\nRequests:\ncpu:        100m\nEnvironment:  \nMounts:\n/pv from mypvc (rw)\n/var/run/secrets/kubernetes.io/serviceaccount from default-token-8qsvr (ro)\nConditions:\nType           Status\nPodScheduled   False\nVolumes:\nmypvc:\nType:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\nClaimName:  nfs\nReadOnly:   false\ndefault-token-8qsvr:\nType:        Secret (a volume populated by a Secret)\nSecretName:  default-token-8qsvr\nOptional:    false\nQoS Class:       Burstable\nNode-Selectors:  \nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\nnode.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\nType     Reason             Age                  From                Message\n\nNormal   NotTriggerScaleUp  2m57s                cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added):\nWarning  FailedScheduling   14s (x3 over 2m59s)  default-scheduler   persistentvolumeclaim \"nfs\" not found\n\nChecking the pvc claim returns\n\n(base) gchauhan-mbp:GKE gchauhan$ kubectl get pvc,pv -n default\nNAME                                      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/model-store-claim   Bound    nfs      10Gi       RWX                           3m51s\nNAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE\npersistentvolume/nfs   10Gi       RWX            Retain           Bound    default/model-store-claim                           3m51s", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531272889", "createdAt": "2020-11-27T00:50:13Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423\n+kubeconfig entry generated for ts.\n+NAME  LOCATION  MASTER_VERSION   MASTER_IP      MACHINE_TYPE   NODE_VERSION     NUM_NODES  STATUS\n+ts    us-west1  1.16.13-gke.401  34.83.140.167  n1-standard-4  1.16.13-gke.401  1          RUNNING\n+```\n+\n+#### 1.3 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use GKE Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [gcloud components install](https://kubernetes.io/docs/tasks/tools/install-kubectl/) command:\n+\n+```bash\n+gcloud components install kubectl\n+```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [gcloud container clusters get-credentials](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```bash\n+gcloud container clusters get-credentials torchserve --region us-west1 --project pytorch-tests-261423\n+Fetching cluster endpoint and auth data.\n+kubeconfig entry generated for torchserve.\n+```\n+\n+#### 1.4 Install helm\n+\n+```bash\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on GKE\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```bash\n+git clone https://github.com/pytorch/serve.git\n+\n+cd serve/kubernetes/GKE\n+```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```bash\n+kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n+daemonset.apps/nvidia-driver-installer created\n+```\n+\n+```bash\n+kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,MEMORY:.status.allocatable.memory,CPU:.status.allocatable.cpu,GPU:.status.allocatable.nvidia\\.com/gpu\"\n+``` \n+should show something similar to:\n+\n+```bash\n+NAME                                        MEMORY       CPU     GPU\n+gke-torchserve-default-pool-aa9f7d99-ggc9   12698376Ki   3920m   1\n+```\n+\n+#### 2.3 Create a storage disk\n+\n+A standard storage class is created with google compute disk. If multiple pods need concurrent access to the same storage volume, you need Google NFS. Create the storage disk named *nfs-disk* with the following command:\n+\n+```bash\n+gcloud compute disks create --size=200GB --zone=us-west1-a nfs-disk\n+\n+NAME     ZONE        SIZE_GB  TYPE         STATUS\n+nfs-disk  us-west1-a  200      pd-standard  READY\n+\n+New disks are unformatted. You must format and mount a disk before it\n+can be used. You can find instructions on how to do this at:\n+\n+https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting\n+\n+```\n+\n+#### 2.4 Create NFS Server\n+\n+Modify the values.yaml in the nfs-provisioner with persistent volume name, disk name and node affinity zones and install nfs-provisioner using helm.\n+\n+```bash\n+cd GKE\n+\n+helm install mynfs ./nfs-provisioner/\n+```\n+\n+```kubectl get pods``` should show something similiar to:\n+\n+```bash\n+NAME                                             READY   STATUS    RESTARTS   AGE\n+pod/mynfs-nfs-provisioner-bcc7c96cc-5xr2k   1/1     Running   0          19h\n+```\n+\n+#### 2.5 Create PV and PVC\n+\n+Run the below command and get NFS server IP:\n+\n+```bash\n+kubectl get svc -n default mynfs-nfs-provisioner -o jsonpath='{.spec.clusterIP}'\n+```\n+\n+Replace storage size and server IP in pv_pvc.yaml with the server IP got from above command. Run the below kubectl command and create PV and PVC\n+\n+```bash\n+kubectl apply -f templates/pv_pvc.yaml -n default\n+```\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```bash\n+kubectl get pvc,pv -n default\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE\n+persistentvolume/nfs   10Gi       RWX            Retain           Bound    default/nfs                           20h\n+\n+NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+persistentvolumeclaim/nfs   Bound    nfs      10Gi       RWX                           20h\n+```\n+\n+#### 2.6 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```bash\n+kubectl apply -f templates/pod.yaml\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+pod/model-store-pod created\n+```\n+\n+Verify that the pod is created by excuting.\n+\n+```bash\n+kubectl get po", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI4MjAyNg==", "bodyText": "Had to change claim name from nfs to model-store-claim in the templates/pod.yaml file to make the above steps work. @jagadeeshi2i Please update with correct values in the readme", "url": "https://github.com/pytorch/serve/pull/793#discussion_r531282026", "createdAt": "2020-11-27T01:00:36Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -0,0 +1,520 @@\n+## TorchServe on Google Kubernetes Engine (AKS)\n+\n+### 1 Create an GKE cluster\n+\n+This quickstart requires that you are running the gcloud version 319.0.0 or later. Run `gcloud --version` to find the version. If you need to install or upgrade, see [Install gcloud SDK](https://cloud.google.com/sdk/docs/install).\n+\n+#### 1.1 Set Gcloud account information\n+\n+```bash\n+gcloud init\n+```\n+\n+#### 1.2 Create GKE cluster\n+\n+Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n+\n+```bash\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+\n+WARNING: Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication\n+WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n+WARNING: Newly created clusters and node-pools will have node auto-upgrade enabled by default. This can be disabled using the `--no-enable-autoupgrade` flag.\n+WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n+WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n+WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n+Machines with GPUs have certain limitations which may affect your workflow. Learn more at https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n+Creating cluster ts in us-west1... Cluster is being health-checked (master is healthy)...done.                                                                    \n+Created [https://container.googleapis.com/v1/projects/pytorch-tests-261423/zones/us-west1/clusters/ts].\n+To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-west1/ts?project=pytorch-tests-261423\n+kubeconfig entry generated for ts.\n+NAME  LOCATION  MASTER_VERSION   MASTER_IP      MACHINE_TYPE   NODE_VERSION     NUM_NODES  STATUS\n+ts    us-west1  1.16.13-gke.401  34.83.140.167  n1-standard-4  1.16.13-gke.401  1          RUNNING\n+```\n+\n+#### 1.3 Connect to the cluster\n+\n+To manage a Kubernetes cluster, you use [kubectl](https://kubernetes.io/docs/user-guide/kubectl/), the Kubernetes command-line client. If you use GKE Cloud Shell, `kubectl` is already installed. To install `kubectl` locally, use the [gcloud components install](https://kubernetes.io/docs/tasks/tools/install-kubectl/) command:\n+\n+```bash\n+gcloud components install kubectl\n+```\n+\n+To configure `kubectl` to connect to your Kubernetes cluster, use the [gcloud container clusters get-credentials](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials) command. This command downloads credentials and configures the Kubernetes CLI to use them.\n+\n+```bash\n+gcloud container clusters get-credentials torchserve --region us-west1 --project pytorch-tests-261423\n+Fetching cluster endpoint and auth data.\n+kubeconfig entry generated for torchserve.\n+```\n+\n+#### 1.4 Install helm\n+\n+```bash\n+curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\n+chmod 700 get_helm.sh\n+./get_helm.sh\n+```\n+\n+### 2 Deploy TorchServe on GKE\n+\n+#### 2.1 Download the github repository and enter the kubernetes directory\n+\n+```bash\n+git clone https://github.com/pytorch/serve.git\n+\n+cd serve/kubernetes/GKE\n+```\n+\n+#### 2.2 Install NVIDIA device plugin\n+\n+Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.\n+\n+```bash\n+kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n+daemonset.apps/nvidia-driver-installer created\n+```\n+\n+```bash\n+kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,MEMORY:.status.allocatable.memory,CPU:.status.allocatable.cpu,GPU:.status.allocatable.nvidia\\.com/gpu\"\n+``` \n+should show something similar to:\n+\n+```bash\n+NAME                                        MEMORY       CPU     GPU\n+gke-torchserve-default-pool-aa9f7d99-ggc9   12698376Ki   3920m   1\n+```\n+\n+#### 2.3 Create a storage disk\n+\n+A standard storage class is created with google compute disk. If multiple pods need concurrent access to the same storage volume, you need Google NFS. Create the storage disk named *nfs-disk* with the following command:\n+\n+```bash\n+gcloud compute disks create --size=200GB --zone=us-west1-a nfs-disk\n+\n+NAME     ZONE        SIZE_GB  TYPE         STATUS\n+nfs-disk  us-west1-a  200      pd-standard  READY\n+\n+New disks are unformatted. You must format and mount a disk before it\n+can be used. You can find instructions on how to do this at:\n+\n+https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting\n+\n+```\n+\n+#### 2.4 Create NFS Server\n+\n+Modify the values.yaml in the nfs-provisioner with persistent volume name, disk name and node affinity zones and install nfs-provisioner using helm.\n+\n+```bash\n+cd GKE\n+\n+helm install mynfs ./nfs-provisioner/\n+```\n+\n+```kubectl get pods``` should show something similiar to:\n+\n+```bash\n+NAME                                             READY   STATUS    RESTARTS   AGE\n+pod/mynfs-nfs-provisioner-bcc7c96cc-5xr2k   1/1     Running   0          19h\n+```\n+\n+#### 2.5 Create PV and PVC\n+\n+Run the below command and get NFS server IP:\n+\n+```bash\n+kubectl get svc -n default mynfs-nfs-provisioner -o jsonpath='{.spec.clusterIP}'\n+```\n+\n+Replace storage size and server IP in pv_pvc.yaml with the server IP got from above command. Run the below kubectl command and create PV and PVC\n+\n+```bash\n+kubectl apply -f templates/pv_pvc.yaml -n default\n+```\n+\n+Verify that the PVC / PV is created by excuting.\n+\n+```bash\n+kubectl get pvc,pv -n default\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE\n+persistentvolume/nfs   10Gi       RWX            Retain           Bound    default/nfs                           20h\n+\n+NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n+persistentvolumeclaim/nfs   Bound    nfs      10Gi       RWX                           20h\n+```\n+\n+#### 2.6 Create a pod and copy MAR / config files\n+\n+Create a pod named `pod/model-store-pod` with PersistentVolume mounted so that we can copy the MAR / config files.\n+\n+```bash\n+kubectl apply -f templates/pod.yaml\n+```\n+\n+Your output should look similar to\n+\n+```bash\n+pod/model-store-pod created\n+```\n+\n+Verify that the pod is created by excuting.\n+\n+```bash\n+kubectl get po", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI3Mjg4OQ=="}, "originalCommit": {"oid": "c9543cd29dcd05055e62c0765f5828924f4e42c5"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTQ4NjE1OnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwMjo1NDo1N1rOH7qWlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwMjo1NDo1N1rOH7qWlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjMyMTk0Mg==", "bodyText": "@jagadeeshi2i In addition to these, when the cluster is created one will use CPU node pool. Mention that for the cluster creation", "url": "https://github.com/pytorch/serve/pull/793#discussion_r532321942", "createdAt": "2020-11-30T02:54:57Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -66,7 +81,15 @@ git clone https://github.com/pytorch/serve.git\n cd serve/kubernetes/GKE\n ```\n \n-#### 2.2 Install NVIDIA device plugin\n+**_NOTE:_** By default the helm chart installs GPU version of torchserve. Follow steps in section [2.2](####-2.2-For-CPU-setup) for running in a CPU only cluster. For GPU setup section [2.2](####-2.2-For-CPU-setup) can be skipped.\n+\n+#### 2.2 For CPU setup\n+\n+* Change torchserve image in Helm/values.yaml to the CPU version\n+* Set `n_gpu` to `0` in Helm/values.yaml\n+* Skip NVIDIA plugin installation in section [2.3](#####-2.3-Install-NVIDIA-device-plugin)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "194b5651308252d596eef8a7b50156a314d24af3"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTQ4NzYxOnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwMjo1NTozNlrOH7qXVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwOTozMzo1MVrOH7ylOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjMyMjEzMw==", "bodyText": "Add a note that this is only needed if deploying for GPU nodes", "url": "https://github.com/pytorch/serve/pull/793#discussion_r532322133", "createdAt": "2020-11-30T02:55:36Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -66,7 +81,15 @@ git clone https://github.com/pytorch/serve.git\n cd serve/kubernetes/GKE\n ```\n \n-#### 2.2 Install NVIDIA device plugin\n+**_NOTE:_** By default the helm chart installs GPU version of torchserve. Follow steps in section [2.2](####-2.2-For-CPU-setup) for running in a CPU only cluster. For GPU setup section [2.2](####-2.2-For-CPU-setup) can be skipped.\n+\n+#### 2.2 For CPU setup\n+\n+* Change torchserve image in Helm/values.yaml to the CPU version\n+* Set `n_gpu` to `0` in Helm/values.yaml\n+* Skip NVIDIA plugin installation in section [2.3](#####-2.3-Install-NVIDIA-device-plugin)\n+  \n+#### 2.3 Install NVIDIA device plugin\n \n Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "194b5651308252d596eef8a7b50156a314d24af3"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjQ1Njc2MQ==", "bodyText": "By default Torchserve uses GPU, i have added in section 2.2 CPU setup we have to set n_gpu to 0 and skip nvidia Daemoset.", "url": "https://github.com/pytorch/serve/pull/793#discussion_r532456761", "createdAt": "2020-11-30T09:33:51Z", "author": {"login": "jagadeeshi2i"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -66,7 +81,15 @@ git clone https://github.com/pytorch/serve.git\n cd serve/kubernetes/GKE\n ```\n \n-#### 2.2 Install NVIDIA device plugin\n+**_NOTE:_** By default the helm chart installs GPU version of torchserve. Follow steps in section [2.2](####-2.2-For-CPU-setup) for running in a CPU only cluster. For GPU setup section [2.2](####-2.2-For-CPU-setup) can be skipped.\n+\n+#### 2.2 For CPU setup\n+\n+* Change torchserve image in Helm/values.yaml to the CPU version\n+* Set `n_gpu` to `0` in Helm/values.yaml\n+* Skip NVIDIA plugin installation in section [2.3](#####-2.3-Install-NVIDIA-device-plugin)\n+  \n+#### 2.3 Install NVIDIA device plugin\n \n Before the GPUs in the nodes can be used, you must deploy a DaemonSet for the NVIDIA device plugin. This DaemonSet runs a pod on each node to provide the required drivers for the GPUs.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjMyMjEzMw=="}, "originalCommit": {"oid": "194b5651308252d596eef8a7b50156a314d24af3"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzOTQ5MTA5OnYy", "diffSide": "RIGHT", "path": "kubernetes/GKE/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwMjo1ODowMFrOH7qZTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQwNzowMjo1MlrOH7uCww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjMyMjYzNg==", "bodyText": "Will these steps work if no explicit region is specified?\nAlso add a note that \"accelerator\" parameter is optional, use it for creating cluster for GPU nodes. It will be help to include example of command for CPU cluster to remove an confusion", "url": "https://github.com/pytorch/serve/pull/793#discussion_r532322636", "createdAt": "2020-11-30T02:58:00Z", "author": {"login": "chauhang"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -6,16 +6,31 @@ This quickstart requires that you are running the gcloud version 319.0.0 or late\n \n #### 1.1 Set Gcloud account information\n \n+Create a gcloud configuration with account and project info.\n+\n ```bash\n gcloud init\n ```\n \n+Set compute/region and compute/zone in config\n+\n+```bash\n+gcloud config set compute/region us-west1\n+gcloud config set compute/zone us-west1-a\n+```\n+\n+If you have multiple configurations, activate required config.\n+\n+```bash\n+gcloud config configurations activate <config_name>\n+```\n+\n #### 1.2 Create GKE cluster\n \n Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n \n ```bash\n-gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "194b5651308252d596eef8a7b50156a314d24af3"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjM4MjQwMw==", "bodyText": "Yes, the region will be set from glcoud config.", "url": "https://github.com/pytorch/serve/pull/793#discussion_r532382403", "createdAt": "2020-11-30T07:02:52Z", "author": {"login": "jagadeeshi2i"}, "path": "kubernetes/GKE/README.md", "diffHunk": "@@ -6,16 +6,31 @@ This quickstart requires that you are running the gcloud version 319.0.0 or late\n \n #### 1.1 Set Gcloud account information\n \n+Create a gcloud configuration with account and project info.\n+\n ```bash\n gcloud init\n ```\n \n+Set compute/region and compute/zone in config\n+\n+```bash\n+gcloud config set compute/region us-west1\n+gcloud config set compute/zone us-west1-a\n+```\n+\n+If you have multiple configurations, activate required config.\n+\n+```bash\n+gcloud config configurations activate <config_name>\n+```\n+\n #### 1.2 Create GKE cluster\n \n Use the [gcloud container clusters create](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster) command to create an GKE cluster. The following example creates a cluster named *torchserve* with one node with a *nvidia-tesla-t4* GPU. This will take several minutes to complete.\n \n ```bash\n-gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1 --region us-west1 --node-locations us-west1-a\n+gcloud container clusters create torchserve --machine-type n1-standard-4 --accelerator type=nvidia-tesla-t4,count=1 --num-nodes 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjMyMjYzNg=="}, "originalCommit": {"oid": "194b5651308252d596eef8a7b50156a314d24af3"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1347, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}