{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyMTU3OTA1", "number": 388, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjozNzo1NlrOD_FlrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjo0MDo1OVrOD_Fmww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDc4NDQ0OnYy", "diffSide": "RIGHT", "path": "docker/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjozNzo1NlrOGZladA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjozNzo1NlrOGZladA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDU2NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n          \n          \n            \n            You may want to consider the following aspects / docker options when deploying torchserve in Production with Docker.", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429480564", "createdAt": "2020-05-22T22:37:56Z", "author": {"login": "maaquib"}, "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDc4NjkxOnYy", "diffSide": "RIGHT", "path": "docker/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjo0MDo0MlrOGZlcHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjo0MDo0MlrOGZlcHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDk5MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n          \n          \n            \n                *  ```-p8080:p8080 -p8081:8081``` TorchServe uses default ports 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429480991", "createdAt": "2020-05-22T22:40:42Z", "author": {"login": "maaquib"}, "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n+\n+\n+* Shared Memory Size \n+\n+    * ```shm-size``` - The shm-size parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n+\n+\n+* User Limits for System Resources\n+    \n+    * ```--ulimit memlock=-1``` : Maximum locked-in-memory address space. \n+    * ```--ulimit stack``` : Linux stack size \n+\n+    The current ulimit values can be viewed by executing ```ulimit -a```. A more exhaustive set of options for resource constraining can be found in the Docker Documentation [here](https://docs.docker.com/config/containers/resource_constraints/), [here](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit) and [here](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n+\n+\n+* Exposing specific ports / volumes between the host & docker env.\n+\n+    *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDc4NzIzOnYy", "diffSide": "RIGHT", "path": "docker/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjo0MDo1OVrOGZlcVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQyMjo0MDo1OVrOGZlcVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MTA0Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                * The model store is passed to torchserve with the --model-store option. You may want want to consider using a shared volume if you prefer pre populating models in model-store directory.\n          \n          \n            \n                * The model store is passed to torchserve with the --model-store option. You may want to consider using a shared volume if you prefer pre populating models in model-store directory.", "url": "https://github.com/pytorch/serve/pull/388#discussion_r429481046", "createdAt": "2020-05-22T22:40:59Z", "author": {"login": "maaquib"}, "path": "docker/README.md", "diffHunk": "@@ -161,3 +162,37 @@ torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/m\n Refer [torch-model-archiver](../model-archiver/README.md) for details.\n \n 4. desnet161.mar file should be present at /home/model-server/model-store\n+\n+# Running TorchServe in a Production Docker Environment.\n+\n+You may want to consider about the following aspects / docker options when deploying torchserve in Production with Docker.\n+\n+\n+* Shared Memory Size \n+\n+    * ```shm-size``` - The shm-size parameter allows you to specify the shared memory that a container can use. It enables memory-intensive containers to run faster by giving more access to allocated memory.\n+\n+\n+* User Limits for System Resources\n+    \n+    * ```--ulimit memlock=-1``` : Maximum locked-in-memory address space. \n+    * ```--ulimit stack``` : Linux stack size \n+\n+    The current ulimit values can be viewed by executing ```ulimit -a```. A more exhaustive set of options for resource constraining can be found in the Docker Documentation [here](https://docs.docker.com/config/containers/resource_constraints/), [here](https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit) and [here](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)\n+\n+\n+* Exposing specific ports / volumes between the host & docker env.\n+\n+    *  ```-p8080:p8080 -p8081:8081``` TorchServe uses 8080 / 8081 for inference & management APIs. You may want to expose these ports to the host for HTTP Requests between Docker & Host.\n+    * The model store is passed to torchserve with the --model-store option. You may want want to consider using a shared volume if you prefer pre populating models in model-store directory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddcae3e531eb6d0fbae83c8b63d98ccbb9ed20ef"}, "originalPosition": 34}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1483, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}