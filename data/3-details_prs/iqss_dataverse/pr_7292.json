{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk3MDQ5MjU0", "number": 7292, "title": "Iqss/7140 google cloud archiver", "bodyText": "What this PR does / why we need it: This PR includes an archiver class to send Bags with datacite.xml and OAI_ORE metadata files to Google cloud storage that is analogous to the existing DuraCloud and Local File archivers. It also includes an update to the DuraCloud archiver to add some timing checks developed when testing the Google version that should improve robustness. There is also a one character fix in the documentation of the Local File archiver to replace a non-functional smart quote in a curl command needed to configure it (the smart quote breaks cut/paste from the doc).\nThe Google Cloud Archiver was developed by/for QDR but should be useful for other community members.\nWhich issue(s) this PR closes:\nCloses #7140\nSpecial notes for your reviewer:\nSuggestions on how to test this: Testing requires a Google Cloud account - we can probably coordinate to add the config/key for QDR's dev bucket to an ansible-launched test machine and screen share to verify the Bag arrives at Google.\nDoes this PR introduce a user interface change? If mockups are available, please link/include them here: no\nIs there a release notes update needed for this change?: could note that archiving to Google is possible\nAdditional documentation:", "createdAt": "2020-10-02T17:47:46Z", "url": "https://github.com/IQSS/dataverse/pull/7292", "merged": true, "mergeCommit": {"oid": "62b7fdf785886200a9b7459c37f9e61ea96ef5ec"}, "closed": true, "closedAt": "2020-10-09T00:02:17Z", "author": {"login": "qqmyers"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdOm01ygH2gAyNDk3MDQ5MjU0OmJmNmNhMDBhOGQxZjRkMGUxZTk0MWMzYWM3YmQ0MGRjYzI5YzdhNTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQnj-egH2gAyNDk3MDQ5MjU0OjM0ODRiYjZiNmEyY2RkMDhhMmUzNjFkOGQzMWRlOWRjNTY4NDA5Mzk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "bf6ca00a8d1f4d0e1e941c3ac7bd40dcc29c7a59", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/bf6ca00a8d1f4d0e1e941c3ac7bd40dcc29c7a59", "committedDate": "2020-10-02T14:26:17Z", "message": "add google archiver and dependencies"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee08e9c2e2ec518248e5bb252ab7f7f1cf41876d", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/ee08e9c2e2ec518248e5bb252ab7f7f1cf41876d", "committedDate": "2020-10-02T17:32:57Z", "message": "documentation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dfbae394f875edf2ce8c724f9e3bcce45b8f7e0", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/5dfbae394f875edf2ce8c724f9e3bcce45b8f7e0", "committedDate": "2020-10-02T17:33:33Z", "message": "update DuraCloud archiver with enhancements from Google archiver"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fed5e456bd6381e3758d637cf69b5aec00641843", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/fed5e456bd6381e3758d637cf69b5aec00641843", "committedDate": "2020-10-02T17:37:14Z", "message": "typos"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d1b4b00bc90389947192340ff9a8a226a0b457e", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/4d1b4b00bc90389947192340ff9a8a226a0b457e", "committedDate": "2020-10-02T17:39:08Z", "message": "capitalization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/3eb0ebcab2facd9d79c40821f2903b3ae1c72155", "committedDate": "2020-10-02T17:40:08Z", "message": "for example"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMzQ4OTM0", "url": "https://github.com/IQSS/dataverse/pull/7292#pullrequestreview-501348934", "createdAt": "2020-10-02T18:14:00Z", "commit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxNDowMFrOHb3DYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxNjo1OVrOHb3N-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3NTU4NQ==", "bodyText": "When all the BagIt export stuff was initially merged, it didn't dawn on me that we lost our comprehensive list of settings in one place.\nI think it would be nice to document :ArchiverSettings in the big list and probably all the various sub-settings like :GoogleCloudBucket, :GoogleCloudProject, and the older ones (:BagItLocalPath, etc.).\nFrom a code perspective, I think I'd also like to see strings like :GoogleCloudBucket, :GoogleCloudProject, :BagItLocalPath, etc. added to the \"Key\" enum in SettingsServiceBean.java. That way, developers can also have a comprehensive list of all settings.", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498975585", "createdAt": "2020-10-02T18:14:00Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -827,10 +827,36 @@ ArchiverClassName - the fully qualified class to be used for archiving. For exam\n \n \\:ArchiverSettings - the archiver class can access required settings including existing Dataverse settings and dynamically defined ones specific to the class. This setting is a comma-separated list of those settings. For example\\:\n \n-``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":BagItLocalPath\u201d``\n+``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":BagItLocalPath\"``\n \n :BagItLocalPath is the file path that you've set in :ArchiverSettings.\n \n+.. _Google Cloud Configuration:\n+\n+Google Cloud Configuration\n+++++++++++++++++++++++++++\n+\n+The Google Cloud Archiver can send Dataverse Bags to a bucket in Google's cloud, including those in the 'Coldline' storage class (cheaper, with slower access) \n+\n+``curl http://localhost:8080/api/admin/settings/:ArchiverClassName -X PUT -d \"edu.harvard.iq.dataverse.engine.command.impl.GoogleCloudSubmitToArchiveCommand\"``\n+\n+``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":\":GoogleCloudBucket, :GoogleCloudProject\"``\n+\n+The Google Cloud Archiver defines two custom settings, both are required:\n+\n+\\:GoogleCloudBucket - the name of the bucket to use. For example:\n+\n+``curl http://localhost:8080/api/admin/settings/:GoogleCloudBucket -X PUT -d \"qdr-archive\"``\n+\n+\\:GoogleCloudProject - the name of the project managing the bucket. For example:\n+\n+``curl http://localhost:8080/api/admin/settings/:GoogleCloudProject -X PUT -d \"qdr-project\"``", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3ODI5OA==", "bodyText": "Do failures like this show up in the UI? If so, do they need to be translatable into languages other than English?", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498978298", "createdAt": "2020-10-02T18:16:59Z", "author": {"login": "pdurbin"}, "path": "src/main/java/edu/harvard/iq/dataverse/engine/command/impl/GoogleCloudSubmitToArchiveCommand.java", "diffHunk": "@@ -0,0 +1,228 @@\n+package edu.harvard.iq.dataverse.engine.command.impl;\n+\n+import edu.harvard.iq.dataverse.DOIDataCiteRegisterService;\n+import edu.harvard.iq.dataverse.DataCitation;\n+import edu.harvard.iq.dataverse.Dataset;\n+import edu.harvard.iq.dataverse.DatasetVersion;\n+import edu.harvard.iq.dataverse.DatasetLock.Reason;\n+import edu.harvard.iq.dataverse.authorization.Permission;\n+import edu.harvard.iq.dataverse.authorization.users.ApiToken;\n+import edu.harvard.iq.dataverse.engine.command.Command;\n+import edu.harvard.iq.dataverse.engine.command.DataverseRequest;\n+import edu.harvard.iq.dataverse.engine.command.RequiredPermissions;\n+import edu.harvard.iq.dataverse.util.bagit.BagGenerator;\n+import edu.harvard.iq.dataverse.util.bagit.OREMap;\n+import edu.harvard.iq.dataverse.workflow.step.Failure;\n+import edu.harvard.iq.dataverse.workflow.step.WorkflowStepResult;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n+import java.nio.charset.Charset;\n+import java.security.DigestInputStream;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.util.Map;\n+import java.util.logging.Logger;\n+\n+import org.apache.commons.codec.binary.Hex;\n+import com.google.auth.oauth2.ServiceAccountCredentials;\n+import com.google.cloud.storage.Blob;\n+import com.google.cloud.storage.Bucket;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageOptions;\n+\n+@RequiredPermissions(Permission.PublishDataset)\n+public class GoogleCloudSubmitToArchiveCommand extends AbstractSubmitToArchiveCommand implements Command<DatasetVersion> {\n+\n+    private static final Logger logger = Logger.getLogger(GoogleCloudSubmitToArchiveCommand.class.getName());\n+    private static final String GOOGLECLOUD_BUCKET = \":GoogleCloudBucket\";\n+    private static final String GOOGLECLOUD_PROJECT = \":GoogleCloudProject\";\n+\n+    public GoogleCloudSubmitToArchiveCommand(DataverseRequest aRequest, DatasetVersion version) {\n+        super(aRequest, version);\n+    }\n+\n+    @Override\n+    public WorkflowStepResult performArchiveSubmission(DatasetVersion dv, ApiToken token, Map<String, String> requestedSettings) {\n+        logger.fine(\"In GoogleCloudSubmitToArchiveCommand...\");\n+        String bucketName = requestedSettings.get(GOOGLECLOUD_BUCKET);\n+        String projectName = requestedSettings.get(GOOGLECLOUD_PROJECT);\n+        logger.fine(\"Project: \" + projectName + \" Bucket: \" + bucketName);\n+        if (bucketName != null && projectName != null) {\n+            Storage storage;\n+            try {\n+                FileInputStream fis = new FileInputStream(System.getProperty(\"dataverse.files.directory\") + System.getProperty(\"file.separator\")+ \"googlecloudkey.json\");\n+                storage = StorageOptions.newBuilder()\n+                        .setCredentials(ServiceAccountCredentials.fromStream(fis))\n+                        .setProjectId(projectName)\n+                        .build()\n+                        .getService();\n+                Bucket bucket = storage.get(bucketName);\n+\n+                Dataset dataset = dv.getDataset();\n+                if (dataset.getLockFor(Reason.finalizePublication) == null) {\n+\n+                    String spaceName = dataset.getGlobalId().asString().replace(':', '-').replace('/', '-')\n+                            .replace('.', '-').toLowerCase();\n+\n+                    DataCitation dc = new DataCitation(dv);\n+                    Map<String, String> metadata = dc.getDataCiteMetadata();\n+                    String dataciteXml = DOIDataCiteRegisterService.getMetadataFromDvObject(\n+                            dv.getDataset().getGlobalId().asString(), metadata, dv.getDataset());\n+                    String blobIdString = null;\n+                    MessageDigest messageDigest = MessageDigest.getInstance(\"MD5\");\n+                    try (PipedInputStream dataciteIn = new PipedInputStream(); DigestInputStream digestInputStream = new DigestInputStream(dataciteIn, messageDigest)) {\n+                        // Add datacite.xml file\n+\n+                        new Thread(new Runnable() {\n+                            public void run() {\n+                                try (PipedOutputStream dataciteOut = new PipedOutputStream(dataciteIn)) {\n+\n+                                    dataciteOut.write(dataciteXml.getBytes(Charset.forName(\"utf-8\")));\n+                                    dataciteOut.close();\n+                                } catch (Exception e) {\n+                                    logger.severe(\"Error creating datacite.xml: \" + e.getMessage());\n+                                    // TODO Auto-generated catch block\n+                                    e.printStackTrace();\n+                                    throw new RuntimeException(\"Error creating datacite.xml: \" + e.getMessage());\n+                                }\n+                            }\n+                        }).start();\n+                        //Have seen broken pipe in PostPublishDataset workflow without this delay\n+                        int i=0;\n+                        while(digestInputStream.available()<=0 && i<100) {\n+                            Thread.sleep(10);\n+                            i++;\n+                        }\n+                        Blob dcXml = bucket.create(spaceName + \"/datacite.v\" + dv.getFriendlyVersionNumber()+\".xml\", digestInputStream, \"text/xml\", Bucket.BlobWriteOption.doesNotExist());\n+                        String checksum = dcXml.getMd5ToHexString();\n+                        logger.fine(\"Content: datacite.xml added with checksum: \" + checksum);\n+                        String localchecksum = Hex.encodeHexString(digestInputStream.getMessageDigest().digest());\n+                        if (!checksum.equals(localchecksum)) {\n+                            logger.severe(checksum + \" not equal to \" + localchecksum);\n+                            return new Failure(\"Error in transferring DataCite.xml file to GoogleCloud\",\n+                                    \"GoogleCloud Submission Failure: incomplete metadata transfer\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "originalPosition": 108}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e5eecfae790dce775b194e5ca34ca0e61cbef50", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/3e5eecfae790dce775b194e5ca34ca0e61cbef50", "committedDate": "2020-10-02T19:47:05Z", "message": "adding settings to master list"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ede2a221389839a4e6f70bcdacf6b4b58f45205e", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/ede2a221389839a4e6f70bcdacf6b4b58f45205e", "committedDate": "2020-10-02T19:48:54Z", "message": "add formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a33b9b167a1c0b825966a55515388dcfbfdbd9dc", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/a33b9b167a1c0b825966a55515388dcfbfdbd9dc", "committedDate": "2020-10-02T19:54:52Z", "message": "simplify links"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4aeccfdf861cb3fb4e5710e8248a3ad5fe658ccb", "author": {"user": {"login": "djbrooke", "name": "Danny Brooke"}}, "url": "https://github.com/IQSS/dataverse/commit/4aeccfdf861cb3fb4e5710e8248a3ad5fe658ccb", "committedDate": "2020-10-02T20:07:47Z", "message": "add release notes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9", "author": {"user": {"login": "djbrooke", "name": "Danny Brooke"}}, "url": "https://github.com/IQSS/dataverse/commit/558f008218f0dee1b0d956ee763c5700b24d88e9", "committedDate": "2020-10-02T20:08:53Z", "message": "Merge branch 'IQSS/7140-GoogleCloudArchiver' of https://github.com/QualitativeDataRepository/dataverse into IQSS/7140-GoogleCloudArchiver"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyMjY5MTc3", "url": "https://github.com/IQSS/dataverse/pull/7292#pullrequestreview-502269177", "createdAt": "2020-10-05T17:26:31Z", "commit": {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNjozMVrOHcmyaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNzozMlrOHcm0pA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NzY3NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Dataverse can export archival \"Bag' files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).\n          \n          \n            \n            Dataverse can export archival \"Bag\" files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r499757675", "createdAt": "2020-10-05T17:26:31Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -2120,3 +2148,40 @@ To enable redirects to the zipper installed on the same server as the main Datav\n To enable redirects to the zipper on a different server: \n \n ``curl -X PUT -d 'https://zipper.example.edu/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl`` \n+\n+:ArchiverClassName\n+++++++++++++++++++\n+\n+Dataverse can export archival \"Bag' files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1ODI0NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Each Archiver class may have it's own custom settings. Along with setting which Archiver class to use, one must use this setting to identify which setting values should be sent to it when it is invoked. The value should be a comma-spearate list of setting names. \n          \n          \n            \n            Each Archiver class may have its own custom settings. Along with setting which Archiver class to use, one must use this setting to identify which setting values should be sent to it when it is invoked. The value should be a comma-separated list of setting names.", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r499758244", "createdAt": "2020-10-05T17:27:32Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -2120,3 +2148,40 @@ To enable redirects to the zipper installed on the same server as the main Datav\n To enable redirects to the zipper on a different server: \n \n ``curl -X PUT -d 'https://zipper.example.edu/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl`` \n+\n+:ArchiverClassName\n+++++++++++++++++++\n+\n+Dataverse can export archival \"Bag' files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).\n+This setting specifies which storage system to use by identifying the particular Java class that should be run. Current options include DuraCloudSubmitToArchiveCommand, LocalSubmitToArchiveCommand, and GoogleCloudSubmitToArchiveCommand.\n+\n+``curl -X PUT -d 'LocalSubmitToArchiveCommand' http://localhost:8080/api/admin/settings/:ArchiverClassName`` \n+ \n+:ArchiverSettings\n++++++++++++++++++\n+\n+Each Archiver class may have it's own custom settings. Along with setting which Archiver class to use, one must use this setting to identify which setting values should be sent to it when it is invoked. The value should be a comma-spearate list of setting names. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9"}, "originalPosition": 72}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c257a1ea5e2960459bf5bd9325994afee55f7a7c", "author": {"user": {"login": "djbrooke", "name": "Danny Brooke"}}, "url": "https://github.com/IQSS/dataverse/commit/c257a1ea5e2960459bf5bd9325994afee55f7a7c", "committedDate": "2020-10-05T17:29:09Z", "message": "Update doc/sphinx-guides/source/installation/config.rst\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57e6b426f48d1a0952694a451391d51fc8e85af6", "author": {"user": {"login": "djbrooke", "name": "Danny Brooke"}}, "url": "https://github.com/IQSS/dataverse/commit/57e6b426f48d1a0952694a451391d51fc8e85af6", "committedDate": "2020-10-05T17:29:28Z", "message": "Update doc/sphinx-guides/source/installation/config.rst\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyMjcyNDgy", "url": "https://github.com/IQSS/dataverse/pull/7292#pullrequestreview-502272482", "createdAt": "2020-10-05T17:31:01Z", "commit": {"oid": "57e6b426f48d1a0952694a451391d51fc8e85af6"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "babb316d729817421c93421d8e5edda29e2dac90", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/babb316d729817421c93421d8e5edda29e2dac90", "committedDate": "2020-10-05T17:47:41Z", "message": "typo fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2b1b44c993d9cf196f580385f9f7d634353f36b", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/f2b1b44c993d9cf196f580385f9f7d634353f36b", "committedDate": "2020-10-05T17:56:41Z", "message": "Merge branch 'IQSS/7140-GoogleCloudArchiver' of https://github.com/QualitativeDataRepository/dataverse.git into IQSS/7140-GoogleCloudArchiver"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71dd1b9b685fefb1b9e11f5083bf91f1e4f51fc0", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/71dd1b9b685fefb1b9e11f5083bf91f1e4f51fc0", "committedDate": "2020-10-06T15:17:29Z", "message": "Merge remote-tracking branch 'IQSS/develop' into IQSS/7140-GoogleCloudArchiver"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed90534d0150709ecba8882bc764955295a99433", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/ed90534d0150709ecba8882bc764955295a99433", "committedDate": "2020-10-08T18:36:13Z", "message": "Merge remote-tracking branch 'IQSS/develop' into IQSS/7140-GoogleCloudArchiver"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab29f15fa9b34b0806ad79f463a3f4339fed9d72", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/ab29f15fa9b34b0806ad79f463a3f4339fed9d72", "committedDate": "2020-10-08T19:27:29Z", "message": "typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af6dc98af51abe8b241dc00d4417e89ec4248e64", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/af6dc98af51abe8b241dc00d4417e89ec4248e64", "committedDate": "2020-10-08T20:22:10Z", "message": "expanded directions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3484bb6b6a2cdd08a2e361d8d31de9dc56840939", "author": {"user": {"login": "qqmyers", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/3484bb6b6a2cdd08a2e361d8d31de9dc56840939", "committedDate": "2020-10-08T20:25:37Z", "message": "typos"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 767, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}