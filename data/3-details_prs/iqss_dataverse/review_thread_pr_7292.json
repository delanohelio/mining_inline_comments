{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk3MDQ5MjU0", "number": 7292, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxNDowMFrOEp55nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNzozMlrOEqafVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzc1NzExOnYy", "diffSide": "RIGHT", "path": "doc/sphinx-guides/source/installation/config.rst", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxNDowMFrOHb3DYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODozNzo0NVrOHb33Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3NTU4NQ==", "bodyText": "When all the BagIt export stuff was initially merged, it didn't dawn on me that we lost our comprehensive list of settings in one place.\nI think it would be nice to document :ArchiverSettings in the big list and probably all the various sub-settings like :GoogleCloudBucket, :GoogleCloudProject, and the older ones (:BagItLocalPath, etc.).\nFrom a code perspective, I think I'd also like to see strings like :GoogleCloudBucket, :GoogleCloudProject, :BagItLocalPath, etc. added to the \"Key\" enum in SettingsServiceBean.java. That way, developers can also have a comprehensive list of all settings.", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498975585", "createdAt": "2020-10-02T18:14:00Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -827,10 +827,36 @@ ArchiverClassName - the fully qualified class to be used for archiving. For exam\n \n \\:ArchiverSettings - the archiver class can access required settings including existing Dataverse settings and dynamically defined ones specific to the class. This setting is a comma-separated list of those settings. For example\\:\n \n-``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":BagItLocalPath\u201d``\n+``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":BagItLocalPath\"``\n \n :BagItLocalPath is the file path that you've set in :ArchiverSettings.\n \n+.. _Google Cloud Configuration:\n+\n+Google Cloud Configuration\n+++++++++++++++++++++++++++\n+\n+The Google Cloud Archiver can send Dataverse Bags to a bucket in Google's cloud, including those in the 'Coldline' storage class (cheaper, with slower access) \n+\n+``curl http://localhost:8080/api/admin/settings/:ArchiverClassName -X PUT -d \"edu.harvard.iq.dataverse.engine.command.impl.GoogleCloudSubmitToArchiveCommand\"``\n+\n+``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":\":GoogleCloudBucket, :GoogleCloudProject\"``\n+\n+The Google Cloud Archiver defines two custom settings, both are required:\n+\n+\\:GoogleCloudBucket - the name of the bucket to use. For example:\n+\n+``curl http://localhost:8080/api/admin/settings/:GoogleCloudBucket -X PUT -d \"qdr-archive\"``\n+\n+\\:GoogleCloudProject - the name of the project managing the bucket. For example:\n+\n+``curl http://localhost:8080/api/admin/settings/:GoogleCloudProject -X PUT -d \"qdr-project\"``", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk4ODg5MA==", "bodyText": "I'll add them to the guides list. The :ArchiverSettings is a Key already. For the others, I think earlier discussions questioned whether the archive-specific classes would end up being deployed separately, in which case having their settings in the core would be odd. Since that's not on the roadmap at this point, I could move them temporarily if you think its worth the effort.", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498988890", "createdAt": "2020-10-02T18:37:45Z", "author": {"login": "qqmyers"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -827,10 +827,36 @@ ArchiverClassName - the fully qualified class to be used for archiving. For exam\n \n \\:ArchiverSettings - the archiver class can access required settings including existing Dataverse settings and dynamically defined ones specific to the class. This setting is a comma-separated list of those settings. For example\\:\n \n-``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":BagItLocalPath\u201d``\n+``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":BagItLocalPath\"``\n \n :BagItLocalPath is the file path that you've set in :ArchiverSettings.\n \n+.. _Google Cloud Configuration:\n+\n+Google Cloud Configuration\n+++++++++++++++++++++++++++\n+\n+The Google Cloud Archiver can send Dataverse Bags to a bucket in Google's cloud, including those in the 'Coldline' storage class (cheaper, with slower access) \n+\n+``curl http://localhost:8080/api/admin/settings/:ArchiverClassName -X PUT -d \"edu.harvard.iq.dataverse.engine.command.impl.GoogleCloudSubmitToArchiveCommand\"``\n+\n+``curl http://localhost:8080/api/admin/settings/:ArchiverSettings -X PUT -d \":\":GoogleCloudBucket, :GoogleCloudProject\"``\n+\n+The Google Cloud Archiver defines two custom settings, both are required:\n+\n+\\:GoogleCloudBucket - the name of the bucket to use. For example:\n+\n+``curl http://localhost:8080/api/admin/settings/:GoogleCloudBucket -X PUT -d \"qdr-archive\"``\n+\n+\\:GoogleCloudProject - the name of the project managing the bucket. For example:\n+\n+``curl http://localhost:8080/api/admin/settings/:GoogleCloudProject -X PUT -d \"qdr-project\"``", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3NTU4NQ=="}, "originalCommit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzc3MTUwOnYy", "diffSide": "RIGHT", "path": "src/main/java/edu/harvard/iq/dataverse/engine/command/impl/GoogleCloudSubmitToArchiveCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODoxNjo1OVrOHb3N-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxODo0NTozNlrOHb4Erg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3ODI5OA==", "bodyText": "Do failures like this show up in the UI? If so, do they need to be translatable into languages other than English?", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498978298", "createdAt": "2020-10-02T18:16:59Z", "author": {"login": "pdurbin"}, "path": "src/main/java/edu/harvard/iq/dataverse/engine/command/impl/GoogleCloudSubmitToArchiveCommand.java", "diffHunk": "@@ -0,0 +1,228 @@\n+package edu.harvard.iq.dataverse.engine.command.impl;\n+\n+import edu.harvard.iq.dataverse.DOIDataCiteRegisterService;\n+import edu.harvard.iq.dataverse.DataCitation;\n+import edu.harvard.iq.dataverse.Dataset;\n+import edu.harvard.iq.dataverse.DatasetVersion;\n+import edu.harvard.iq.dataverse.DatasetLock.Reason;\n+import edu.harvard.iq.dataverse.authorization.Permission;\n+import edu.harvard.iq.dataverse.authorization.users.ApiToken;\n+import edu.harvard.iq.dataverse.engine.command.Command;\n+import edu.harvard.iq.dataverse.engine.command.DataverseRequest;\n+import edu.harvard.iq.dataverse.engine.command.RequiredPermissions;\n+import edu.harvard.iq.dataverse.util.bagit.BagGenerator;\n+import edu.harvard.iq.dataverse.util.bagit.OREMap;\n+import edu.harvard.iq.dataverse.workflow.step.Failure;\n+import edu.harvard.iq.dataverse.workflow.step.WorkflowStepResult;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n+import java.nio.charset.Charset;\n+import java.security.DigestInputStream;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.util.Map;\n+import java.util.logging.Logger;\n+\n+import org.apache.commons.codec.binary.Hex;\n+import com.google.auth.oauth2.ServiceAccountCredentials;\n+import com.google.cloud.storage.Blob;\n+import com.google.cloud.storage.Bucket;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageOptions;\n+\n+@RequiredPermissions(Permission.PublishDataset)\n+public class GoogleCloudSubmitToArchiveCommand extends AbstractSubmitToArchiveCommand implements Command<DatasetVersion> {\n+\n+    private static final Logger logger = Logger.getLogger(GoogleCloudSubmitToArchiveCommand.class.getName());\n+    private static final String GOOGLECLOUD_BUCKET = \":GoogleCloudBucket\";\n+    private static final String GOOGLECLOUD_PROJECT = \":GoogleCloudProject\";\n+\n+    public GoogleCloudSubmitToArchiveCommand(DataverseRequest aRequest, DatasetVersion version) {\n+        super(aRequest, version);\n+    }\n+\n+    @Override\n+    public WorkflowStepResult performArchiveSubmission(DatasetVersion dv, ApiToken token, Map<String, String> requestedSettings) {\n+        logger.fine(\"In GoogleCloudSubmitToArchiveCommand...\");\n+        String bucketName = requestedSettings.get(GOOGLECLOUD_BUCKET);\n+        String projectName = requestedSettings.get(GOOGLECLOUD_PROJECT);\n+        logger.fine(\"Project: \" + projectName + \" Bucket: \" + bucketName);\n+        if (bucketName != null && projectName != null) {\n+            Storage storage;\n+            try {\n+                FileInputStream fis = new FileInputStream(System.getProperty(\"dataverse.files.directory\") + System.getProperty(\"file.separator\")+ \"googlecloudkey.json\");\n+                storage = StorageOptions.newBuilder()\n+                        .setCredentials(ServiceAccountCredentials.fromStream(fis))\n+                        .setProjectId(projectName)\n+                        .build()\n+                        .getService();\n+                Bucket bucket = storage.get(bucketName);\n+\n+                Dataset dataset = dv.getDataset();\n+                if (dataset.getLockFor(Reason.finalizePublication) == null) {\n+\n+                    String spaceName = dataset.getGlobalId().asString().replace(':', '-').replace('/', '-')\n+                            .replace('.', '-').toLowerCase();\n+\n+                    DataCitation dc = new DataCitation(dv);\n+                    Map<String, String> metadata = dc.getDataCiteMetadata();\n+                    String dataciteXml = DOIDataCiteRegisterService.getMetadataFromDvObject(\n+                            dv.getDataset().getGlobalId().asString(), metadata, dv.getDataset());\n+                    String blobIdString = null;\n+                    MessageDigest messageDigest = MessageDigest.getInstance(\"MD5\");\n+                    try (PipedInputStream dataciteIn = new PipedInputStream(); DigestInputStream digestInputStream = new DigestInputStream(dataciteIn, messageDigest)) {\n+                        // Add datacite.xml file\n+\n+                        new Thread(new Runnable() {\n+                            public void run() {\n+                                try (PipedOutputStream dataciteOut = new PipedOutputStream(dataciteIn)) {\n+\n+                                    dataciteOut.write(dataciteXml.getBytes(Charset.forName(\"utf-8\")));\n+                                    dataciteOut.close();\n+                                } catch (Exception e) {\n+                                    logger.severe(\"Error creating datacite.xml: \" + e.getMessage());\n+                                    // TODO Auto-generated catch block\n+                                    e.printStackTrace();\n+                                    throw new RuntimeException(\"Error creating datacite.xml: \" + e.getMessage());\n+                                }\n+                            }\n+                        }).start();\n+                        //Have seen broken pipe in PostPublishDataset workflow without this delay\n+                        int i=0;\n+                        while(digestInputStream.available()<=0 && i<100) {\n+                            Thread.sleep(10);\n+                            i++;\n+                        }\n+                        Blob dcXml = bucket.create(spaceName + \"/datacite.v\" + dv.getFriendlyVersionNumber()+\".xml\", digestInputStream, \"text/xml\", Bucket.BlobWriteOption.doesNotExist());\n+                        String checksum = dcXml.getMd5ToHexString();\n+                        logger.fine(\"Content: datacite.xml added with checksum: \" + checksum);\n+                        String localchecksum = Hex.encodeHexString(digestInputStream.getMessageDigest().digest());\n+                        if (!checksum.equals(localchecksum)) {\n+                            logger.severe(checksum + \" not equal to \" + localchecksum);\n+                            return new Failure(\"Error in transferring DataCite.xml file to GoogleCloud\",\n+                                    \"GoogleCloud Submission Failure: incomplete metadata transfer\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk5MjMwMg==", "bodyText": "They don't appear anywhere except the log AFAIK, even if you run them as part of a post-publish workflow instead of the API. (Since they run asynchronously, the API just returns an OK, it's launched response.\nThe /api/datasets/{id}/actions/:publish also runs archiving, synchronously, if configured, and it does have internationalizable success/failure messages.", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r498992302", "createdAt": "2020-10-02T18:45:36Z", "author": {"login": "qqmyers"}, "path": "src/main/java/edu/harvard/iq/dataverse/engine/command/impl/GoogleCloudSubmitToArchiveCommand.java", "diffHunk": "@@ -0,0 +1,228 @@\n+package edu.harvard.iq.dataverse.engine.command.impl;\n+\n+import edu.harvard.iq.dataverse.DOIDataCiteRegisterService;\n+import edu.harvard.iq.dataverse.DataCitation;\n+import edu.harvard.iq.dataverse.Dataset;\n+import edu.harvard.iq.dataverse.DatasetVersion;\n+import edu.harvard.iq.dataverse.DatasetLock.Reason;\n+import edu.harvard.iq.dataverse.authorization.Permission;\n+import edu.harvard.iq.dataverse.authorization.users.ApiToken;\n+import edu.harvard.iq.dataverse.engine.command.Command;\n+import edu.harvard.iq.dataverse.engine.command.DataverseRequest;\n+import edu.harvard.iq.dataverse.engine.command.RequiredPermissions;\n+import edu.harvard.iq.dataverse.util.bagit.BagGenerator;\n+import edu.harvard.iq.dataverse.util.bagit.OREMap;\n+import edu.harvard.iq.dataverse.workflow.step.Failure;\n+import edu.harvard.iq.dataverse.workflow.step.WorkflowStepResult;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n+import java.nio.charset.Charset;\n+import java.security.DigestInputStream;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.util.Map;\n+import java.util.logging.Logger;\n+\n+import org.apache.commons.codec.binary.Hex;\n+import com.google.auth.oauth2.ServiceAccountCredentials;\n+import com.google.cloud.storage.Blob;\n+import com.google.cloud.storage.Bucket;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageOptions;\n+\n+@RequiredPermissions(Permission.PublishDataset)\n+public class GoogleCloudSubmitToArchiveCommand extends AbstractSubmitToArchiveCommand implements Command<DatasetVersion> {\n+\n+    private static final Logger logger = Logger.getLogger(GoogleCloudSubmitToArchiveCommand.class.getName());\n+    private static final String GOOGLECLOUD_BUCKET = \":GoogleCloudBucket\";\n+    private static final String GOOGLECLOUD_PROJECT = \":GoogleCloudProject\";\n+\n+    public GoogleCloudSubmitToArchiveCommand(DataverseRequest aRequest, DatasetVersion version) {\n+        super(aRequest, version);\n+    }\n+\n+    @Override\n+    public WorkflowStepResult performArchiveSubmission(DatasetVersion dv, ApiToken token, Map<String, String> requestedSettings) {\n+        logger.fine(\"In GoogleCloudSubmitToArchiveCommand...\");\n+        String bucketName = requestedSettings.get(GOOGLECLOUD_BUCKET);\n+        String projectName = requestedSettings.get(GOOGLECLOUD_PROJECT);\n+        logger.fine(\"Project: \" + projectName + \" Bucket: \" + bucketName);\n+        if (bucketName != null && projectName != null) {\n+            Storage storage;\n+            try {\n+                FileInputStream fis = new FileInputStream(System.getProperty(\"dataverse.files.directory\") + System.getProperty(\"file.separator\")+ \"googlecloudkey.json\");\n+                storage = StorageOptions.newBuilder()\n+                        .setCredentials(ServiceAccountCredentials.fromStream(fis))\n+                        .setProjectId(projectName)\n+                        .build()\n+                        .getService();\n+                Bucket bucket = storage.get(bucketName);\n+\n+                Dataset dataset = dv.getDataset();\n+                if (dataset.getLockFor(Reason.finalizePublication) == null) {\n+\n+                    String spaceName = dataset.getGlobalId().asString().replace(':', '-').replace('/', '-')\n+                            .replace('.', '-').toLowerCase();\n+\n+                    DataCitation dc = new DataCitation(dv);\n+                    Map<String, String> metadata = dc.getDataCiteMetadata();\n+                    String dataciteXml = DOIDataCiteRegisterService.getMetadataFromDvObject(\n+                            dv.getDataset().getGlobalId().asString(), metadata, dv.getDataset());\n+                    String blobIdString = null;\n+                    MessageDigest messageDigest = MessageDigest.getInstance(\"MD5\");\n+                    try (PipedInputStream dataciteIn = new PipedInputStream(); DigestInputStream digestInputStream = new DigestInputStream(dataciteIn, messageDigest)) {\n+                        // Add datacite.xml file\n+\n+                        new Thread(new Runnable() {\n+                            public void run() {\n+                                try (PipedOutputStream dataciteOut = new PipedOutputStream(dataciteIn)) {\n+\n+                                    dataciteOut.write(dataciteXml.getBytes(Charset.forName(\"utf-8\")));\n+                                    dataciteOut.close();\n+                                } catch (Exception e) {\n+                                    logger.severe(\"Error creating datacite.xml: \" + e.getMessage());\n+                                    // TODO Auto-generated catch block\n+                                    e.printStackTrace();\n+                                    throw new RuntimeException(\"Error creating datacite.xml: \" + e.getMessage());\n+                                }\n+                            }\n+                        }).start();\n+                        //Have seen broken pipe in PostPublishDataset workflow without this delay\n+                        int i=0;\n+                        while(digestInputStream.available()<=0 && i<100) {\n+                            Thread.sleep(10);\n+                            i++;\n+                        }\n+                        Blob dcXml = bucket.create(spaceName + \"/datacite.v\" + dv.getFriendlyVersionNumber()+\".xml\", digestInputStream, \"text/xml\", Bucket.BlobWriteOption.doesNotExist());\n+                        String checksum = dcXml.getMd5ToHexString();\n+                        logger.fine(\"Content: datacite.xml added with checksum: \" + checksum);\n+                        String localchecksum = Hex.encodeHexString(digestInputStream.getMessageDigest().digest());\n+                        if (!checksum.equals(localchecksum)) {\n+                            logger.severe(checksum + \" not equal to \" + localchecksum);\n+                            return new Failure(\"Error in transferring DataCite.xml file to GoogleCloud\",\n+                                    \"GoogleCloud Submission Failure: incomplete metadata transfer\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODk3ODI5OA=="}, "originalCommit": {"oid": "3eb0ebcab2facd9d79c40821f2903b3ae1c72155"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTA5MzA0OnYy", "diffSide": "RIGHT", "path": "doc/sphinx-guides/source/installation/config.rst", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNjozMVrOHcmyaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNjozMVrOHcmyaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NzY3NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Dataverse can export archival \"Bag' files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).\n          \n          \n            \n            Dataverse can export archival \"Bag\" files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r499757675", "createdAt": "2020-10-05T17:26:31Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -2120,3 +2148,40 @@ To enable redirects to the zipper installed on the same server as the main Datav\n To enable redirects to the zipper on a different server: \n \n ``curl -X PUT -d 'https://zipper.example.edu/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl`` \n+\n+:ArchiverClassName\n+++++++++++++++++++\n+\n+Dataverse can export archival \"Bag' files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTA5NjUyOnYy", "diffSide": "RIGHT", "path": "doc/sphinx-guides/source/installation/config.rst", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNzozMlrOHcm0pA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyNzozMlrOHcm0pA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1ODI0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Each Archiver class may have it's own custom settings. Along with setting which Archiver class to use, one must use this setting to identify which setting values should be sent to it when it is invoked. The value should be a comma-spearate list of setting names. \n          \n          \n            \n            Each Archiver class may have its own custom settings. Along with setting which Archiver class to use, one must use this setting to identify which setting values should be sent to it when it is invoked. The value should be a comma-separated list of setting names.", "url": "https://github.com/IQSS/dataverse/pull/7292#discussion_r499758244", "createdAt": "2020-10-05T17:27:32Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -2120,3 +2148,40 @@ To enable redirects to the zipper installed on the same server as the main Datav\n To enable redirects to the zipper on a different server: \n \n ``curl -X PUT -d 'https://zipper.example.edu/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl`` \n+\n+:ArchiverClassName\n+++++++++++++++++++\n+\n+Dataverse can export archival \"Bag' files to an extensible set of storage systems (see :ref:`BagIt Export` above for details about this and for further explanation of the other archiving related settings below).\n+This setting specifies which storage system to use by identifying the particular Java class that should be run. Current options include DuraCloudSubmitToArchiveCommand, LocalSubmitToArchiveCommand, and GoogleCloudSubmitToArchiveCommand.\n+\n+``curl -X PUT -d 'LocalSubmitToArchiveCommand' http://localhost:8080/api/admin/settings/:ArchiverClassName`` \n+ \n+:ArchiverSettings\n++++++++++++++++++\n+\n+Each Archiver class may have it's own custom settings. Along with setting which Archiver class to use, one must use this setting to identify which setting values should be sent to it when it is invoked. The value should be a comma-spearate list of setting names. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558f008218f0dee1b0d956ee763c5700b24d88e9"}, "originalPosition": 72}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2397, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}