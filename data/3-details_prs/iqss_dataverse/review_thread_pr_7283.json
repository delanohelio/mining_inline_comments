{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzMTU5MTM5", "number": 7283, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNTo1NDo0MVrOEng0BA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoxNTo1NlrOEou5ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5ODY3NTI0OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.1-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNTo1NDo0MVrOHYJYFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjowOTowNlrOHYJ5xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4MTQ5Mg==", "bodyText": "when downloaded / the name of downloaded files", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495081492", "createdAt": "2020-09-25T15:54:41Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.\n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d194201d37b9b6eda91a357a8092a3188c2a6483"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5MDExNw==", "bodyText": "thanks, addressed in 2f7fb8a", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495090117", "createdAt": "2020-09-25T16:09:06Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.\n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4MTQ5Mg=="}, "originalCommit": {"oid": "d194201d37b9b6eda91a357a8092a3188c2a6483"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5ODc1NTU5OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.1-release-notes.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjoxNjoxN1rOHYKJNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzo0MjoyM1rOHZ5zYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5NDA3MQ==", "bodyText": "This is both API and UI! :-) (and DVUploader which uses the API) But only for direct uploads to S3.\nAlso, not sure how much goes in the release notes but it might be good to note that:\n\npart size is configurable:\nThe new multipart functionality is used by default when direct upload files are > 1 GB. This value is configurable per store - see the Guides table of S3 settings. (Range is from 5 MB to 5GB per part if you want to list that here)\nAdministrators should note that charges may be incurred for storage reserved for multipart uploads that are not completed or cancelled. Administrators may want to do periodic manual or automated checks for open multipart uploads.\nWhile multipart uploads can support much larger files, and can have advantages in terms of robust transfer and speed, they are more complex than single part direct uploads. Administrators should consider taking advantage of the options to limit use of multipart uploads to specific users (by using multiple stores and configuring access to stores with high file size limits to specific Dataverses or Datasets).\n\nIt's been long enough that I've forgotten how much documentation is out but I'll plan to create some additional info - perhaps for the guides or just wiki/faq/debug ideas or even some demo videos.", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495094071", "createdAt": "2020-09-25T16:16:17Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d194201d37b9b6eda91a357a8092a3188c2a6483"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk1MDczMw==", "bodyText": "Thanks @qqmyers - will address later today.", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495950733", "createdAt": "2020-09-28T13:45:21Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5NDA3MQ=="}, "originalCommit": {"oid": "d194201d37b9b6eda91a357a8092a3188c2a6483"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkyMzQ5MA==", "bodyText": "@qqmyers - I addressed much of this in f0a0d0b ... If there's some better docs we can add pointers later.", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r496923490", "createdAt": "2020-09-29T17:42:23Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA5NDA3MQ=="}, "originalCommit": {"oid": "d194201d37b9b6eda91a357a8092a3188c2a6483"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5ODgyOTU5OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.1-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjozODoxNlrOHYK3oQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjozODoxNlrOHYK3oQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNTk1Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. (Issue #4225, PR #7211)\n          \n          \n            \n            - Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause stale search results to not load. (Issue #4225, PR #7211)", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495105953", "createdAt": "2020-09-25T16:38:16Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.\n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. (Issue #4225, PR #7211)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f7fb8ae0a2e3d5a05aecfd045d05ff5bb4749c2"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5ODgzNjg1OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.1-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjo0MDoyNlrOHYK8BQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxODowMToxMVrOHYNcrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNzA3Nw==", "bodyText": "Maybe this should be step 0 below? And not have its own heading?", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495107077", "createdAt": "2020-09-25T16:40:26Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.\n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+### Upgrade from Glassfish 4.1 to Payara 5\n+\n+These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0). ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f7fb8ae0a2e3d5a05aecfd045d05ff5bb4749c2"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0ODIwNw==", "bodyText": "makes sense, addressed in b86a9fc", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495148207", "createdAt": "2020-09-25T18:01:11Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,78 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.\n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+### Upgrade from Glassfish 4.1 to Payara 5\n+\n+These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0). ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNzA3Nw=="}, "originalCommit": {"oid": "2f7fb8ae0a2e3d5a05aecfd045d05ff5bb4749c2"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5ODg0MTg4OnYy", "diffSide": "RIGHT", "path": "doc/sphinx-guides/source/admin/solr-search-index.rst", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNjo0MTo1NVrOHYK--Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzo0MTozMlrOHZ5xNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNzgzMw==", "bodyText": "Is there any particular reason we're changing this from \"orphaned\" to \"detached\"? The API endpoint is called \"clear-orphans\".", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r495107833", "createdAt": "2020-09-25T16:41:55Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/admin/solr-search-index.rst", "diffHunk": "@@ -22,7 +22,7 @@ Get a list of all database objects that are missing in Solr, and Solr documents\n \n ``curl http://localhost:8080/api/admin/index/status``\n \n-Remove all Solr documents that are orphaned (ie not associated with objects in the database):\n+Remove all Solr documents that are detached (ie not associated with objects in the database):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f7fb8ae0a2e3d5a05aecfd045d05ff5bb4749c2"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkyMjkzMg==", "bodyText": "addressed in f0a0d0b", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r496922932", "createdAt": "2020-09-29T17:41:32Z", "author": {"login": "djbrooke"}, "path": "doc/sphinx-guides/source/admin/solr-search-index.rst", "diffHunk": "@@ -22,7 +22,7 @@ Get a list of all database objects that are missing in Solr, and Solr documents\n \n ``curl http://localhost:8080/api/admin/index/status``\n \n-Remove all Solr documents that are orphaned (ie not associated with objects in the database):\n+Remove all Solr documents that are detached (ie not associated with objects in the database):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEwNzgzMw=="}, "originalCommit": {"oid": "2f7fb8ae0a2e3d5a05aecfd045d05ff5bb4749c2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTQ1MTA5OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.1-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoxMDo0OFrOHZ_XPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoyOTozMFrOHZ_--Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxNDU4OQ==", "bodyText": "This is only needed if you update the metadata block in step 1? Or were there other changes?", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r497014589", "createdAt": "2020-09-29T20:10:48Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,100 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API and UI (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.  \n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause stale search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Multipart Upload Storage Monitoring, Recommended Use for Multipart Upload\n+\n+Charges may be incurred for storage reserved for multipart uploads that are not completed or cancelled. Administrators may want to do periodic manual or automated checks for open multipart uploads. Learn more in the Big Data Support section of the [Developers Guide](http://guides.dataverse.org/en/5.1/developer/big-data-support.html).\n+\n+While multipart uploads can support much larger files, and can have advantages in terms of robust transfer and speed, they are more complex than single part direct uploads. Administrators should consider taking advantage of the options to limit use of multipart uploads to specific users by using multiple stores and configuring access to stores with high file size limits to specific Dataverses (added in 4.20) or Datasets (added in this release).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+### Biomedical Metadata Block Updated\n+\n+The Life Science Metadata block (biomedical.tsv) was updated.  \"Other Design Type\", \"Other Factor Type\", \"Other Technology Type\", \"Other Technology Platform\" boxes were added. See the \"Additional Upgrade Steps\" below if you use this in your installation.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+0. These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0).\n+\n+1. Undeploy the previous version.\n+\n+<payara install path>/payara/bin/asadmin list-applications\n+<payara install path>/payara/bin/asadmin undeploy dataverse\n+\n+2. Stop payara and remove the generated directory, start.\n+\n+- service payara stop\n+- remove the generated directory: rm -rf <payara install path>payara/payara/domains/domain1/generated\n+- service payara start\n+\n+3. Deploy this version.\n+<payara install path>/payara/bin/asadmin deploy <path>dataverse-5.1.war\n+\n+4. Restart payara\n+\n+### Additional Upgrade Steps\n+\n+1. Update Biomedical Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.1/biomedical.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @biomedical.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+2. Updated Solr XML and Reload Solr", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727106b8a1eb238cfc83a2551fa5b9cc6d0446cc"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyNDc2MQ==", "bodyText": "Just needed if you do step one. I collapsed into one step.", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r497024761", "createdAt": "2020-09-29T20:29:30Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,100 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API and UI (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.  \n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause stale search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Multipart Upload Storage Monitoring, Recommended Use for Multipart Upload\n+\n+Charges may be incurred for storage reserved for multipart uploads that are not completed or cancelled. Administrators may want to do periodic manual or automated checks for open multipart uploads. Learn more in the Big Data Support section of the [Developers Guide](http://guides.dataverse.org/en/5.1/developer/big-data-support.html).\n+\n+While multipart uploads can support much larger files, and can have advantages in terms of robust transfer and speed, they are more complex than single part direct uploads. Administrators should consider taking advantage of the options to limit use of multipart uploads to specific users by using multiple stores and configuring access to stores with high file size limits to specific Dataverses (added in 4.20) or Datasets (added in this release).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+### Biomedical Metadata Block Updated\n+\n+The Life Science Metadata block (biomedical.tsv) was updated.  \"Other Design Type\", \"Other Factor Type\", \"Other Technology Type\", \"Other Technology Platform\" boxes were added. See the \"Additional Upgrade Steps\" below if you use this in your installation.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+0. These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0).\n+\n+1. Undeploy the previous version.\n+\n+<payara install path>/payara/bin/asadmin list-applications\n+<payara install path>/payara/bin/asadmin undeploy dataverse\n+\n+2. Stop payara and remove the generated directory, start.\n+\n+- service payara stop\n+- remove the generated directory: rm -rf <payara install path>payara/payara/domains/domain1/generated\n+- service payara start\n+\n+3. Deploy this version.\n+<payara install path>/payara/bin/asadmin deploy <path>dataverse-5.1.war\n+\n+4. Restart payara\n+\n+### Additional Upgrade Steps\n+\n+1. Update Biomedical Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.1/biomedical.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @biomedical.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+2. Updated Solr XML and Reload Solr", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxNDU4OQ=="}, "originalCommit": {"oid": "727106b8a1eb238cfc83a2551fa5b9cc6d0446cc"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTQ2ODU4OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.1-release-notes.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoxNTo1NlrOHZ_hxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDozNDoxOFrOHaAJ3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxNzI4NA==", "bodyText": "Same here - none of the PRs changes the export formats do they? Neither of these hurt if you do them when not needed, but if it really is simpler, it would be nice to show that, esp. after 5.0.", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r497017284", "createdAt": "2020-09-29T20:15:56Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,100 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API and UI (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.  \n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause stale search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Multipart Upload Storage Monitoring, Recommended Use for Multipart Upload\n+\n+Charges may be incurred for storage reserved for multipart uploads that are not completed or cancelled. Administrators may want to do periodic manual or automated checks for open multipart uploads. Learn more in the Big Data Support section of the [Developers Guide](http://guides.dataverse.org/en/5.1/developer/big-data-support.html).\n+\n+While multipart uploads can support much larger files, and can have advantages in terms of robust transfer and speed, they are more complex than single part direct uploads. Administrators should consider taking advantage of the options to limit use of multipart uploads to specific users by using multiple stores and configuring access to stores with high file size limits to specific Dataverses (added in 4.20) or Datasets (added in this release).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+### Biomedical Metadata Block Updated\n+\n+The Life Science Metadata block (biomedical.tsv) was updated.  \"Other Design Type\", \"Other Factor Type\", \"Other Technology Type\", \"Other Technology Platform\" boxes were added. See the \"Additional Upgrade Steps\" below if you use this in your installation.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+0. These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0).\n+\n+1. Undeploy the previous version.\n+\n+<payara install path>/payara/bin/asadmin list-applications\n+<payara install path>/payara/bin/asadmin undeploy dataverse\n+\n+2. Stop payara and remove the generated directory, start.\n+\n+- service payara stop\n+- remove the generated directory: rm -rf <payara install path>payara/payara/domains/domain1/generated\n+- service payara start\n+\n+3. Deploy this version.\n+<payara install path>/payara/bin/asadmin deploy <path>dataverse-5.1.war\n+\n+4. Restart payara\n+\n+### Additional Upgrade Steps\n+\n+1. Update Biomedical Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.1/biomedical.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @biomedical.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+2. Updated Solr XML and Reload Solr\n+\n+-  copy schema_dv_mdb_fields.xml and schema_dv_mdb_copies.xml to solr server, for example into /usr/local/solr/solr-7.7.2/server/solr/collection1/conf/ directory\n+-  reload Solr, for example, http://localhost:8983/solr/admin/cores?action=RELOAD&core=collection1\n+\n+3. (Recommended) Run ReExportall to update JSON Exports  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727106b8a1eb238cfc83a2551fa5b9cc6d0446cc"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyNTU2Mw==", "bodyText": "I thought if we were updating a metadata block we recommended a JSON reexport to capture those changes?\nre: formatting, I collapsed this into a single step in af09c9a", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r497025563", "createdAt": "2020-09-29T20:30:49Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,100 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API and UI (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.  \n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause stale search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Multipart Upload Storage Monitoring, Recommended Use for Multipart Upload\n+\n+Charges may be incurred for storage reserved for multipart uploads that are not completed or cancelled. Administrators may want to do periodic manual or automated checks for open multipart uploads. Learn more in the Big Data Support section of the [Developers Guide](http://guides.dataverse.org/en/5.1/developer/big-data-support.html).\n+\n+While multipart uploads can support much larger files, and can have advantages in terms of robust transfer and speed, they are more complex than single part direct uploads. Administrators should consider taking advantage of the options to limit use of multipart uploads to specific users by using multiple stores and configuring access to stores with high file size limits to specific Dataverses (added in 4.20) or Datasets (added in this release).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+### Biomedical Metadata Block Updated\n+\n+The Life Science Metadata block (biomedical.tsv) was updated.  \"Other Design Type\", \"Other Factor Type\", \"Other Technology Type\", \"Other Technology Platform\" boxes were added. See the \"Additional Upgrade Steps\" below if you use this in your installation.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+0. These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0).\n+\n+1. Undeploy the previous version.\n+\n+<payara install path>/payara/bin/asadmin list-applications\n+<payara install path>/payara/bin/asadmin undeploy dataverse\n+\n+2. Stop payara and remove the generated directory, start.\n+\n+- service payara stop\n+- remove the generated directory: rm -rf <payara install path>payara/payara/domains/domain1/generated\n+- service payara start\n+\n+3. Deploy this version.\n+<payara install path>/payara/bin/asadmin deploy <path>dataverse-5.1.war\n+\n+4. Restart payara\n+\n+### Additional Upgrade Steps\n+\n+1. Update Biomedical Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.1/biomedical.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @biomedical.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+2. Updated Solr XML and Reload Solr\n+\n+-  copy schema_dv_mdb_fields.xml and schema_dv_mdb_copies.xml to solr server, for example into /usr/local/solr/solr-7.7.2/server/solr/collection1/conf/ directory\n+-  reload Solr, for example, http://localhost:8983/solr/admin/cores?action=RELOAD&core=collection1\n+\n+3. (Recommended) Run ReExportall to update JSON Exports  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxNzI4NA=="}, "originalCommit": {"oid": "727106b8a1eb238cfc83a2551fa5b9cc6d0446cc"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyNzU1MQ==", "bodyText": "Ah yeah - if you update a block where there's existing metadata, you could be changing the title, or the formal URI associated with it, which would mean exports like JSON and OAI-ORE could change.", "url": "https://github.com/IQSS/dataverse/pull/7283#discussion_r497027551", "createdAt": "2020-09-29T20:34:18Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.1-release-notes.md", "diffHunk": "@@ -0,0 +1,100 @@\n+# Dataverse 5.1\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+## Release Highlights\n+\n+### Large File Upload for Installations Using AWS S3\n+\n+The added support for multipart upload through the API and UI (Issue #6763) will allow files larger than 5 GB to be uploaded to Dataverse when an installation is running on AWS S3. Previously, only non-AWS S3 storage configurations would allow uploads larger than 5 GB.  \n+\n+### Dataset-Specific Stores\n+\n+In previous releases, configuration options were added that allow each dataverse to have a specific store enabled. This release adds even more granularity, with the ability to set a dataset-level store.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users can now upload files larger than 5 GB on installations running AWS S3 (Issue #6763, PR #6995)\n+- Administrators will now be able to specify a store at the dataset level in addition to the Dataverse level (Issue #6872, PR #7272)\n+- Users will have their dataset's directory structure retained when uploading a dataset with shapefiles (Issue #6873, PR #7279)\n+- Users will now be able to download zip files through the experimental Zipper service when the set of downloaded files have duplicate names (Issue [#80](https://github.com/IQSS/dataverse.harvard.edu/issues/80), PR #7276)\n+- Users will now be able to download zip files with the proper file structure through the experiment Zipper service (Issue #7255, PR #7258)\n+- Administrators will be able to use new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause stale search results to not load. (Issue #4225, PR #7211)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### New API for setting a Dataset-level Store\n+\n+- This release adds a new API for setting a dataset-specific store. Learn more in the Managing Dataverse and Datasets section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Multipart Upload Storage Monitoring, Recommended Use for Multipart Upload\n+\n+Charges may be incurred for storage reserved for multipart uploads that are not completed or cancelled. Administrators may want to do periodic manual or automated checks for open multipart uploads. Learn more in the Big Data Support section of the [Developers Guide](http://guides.dataverse.org/en/5.1/developer/big-data-support.html).\n+\n+While multipart uploads can support much larger files, and can have advantages in terms of robust transfer and speed, they are more complex than single part direct uploads. Administrators should consider taking advantage of the options to limit use of multipart uploads to specific users by using multiple stores and configuring access to stores with high file size limits to specific Dataverses (added in 4.20) or Datasets (added in this release).\n+\n+### New APIs for keeping Solr records in sync\n+\n+This release adds new APIs to keep the Solr index and the DB in sync, allowing easier resolution of an issue that would occasionally cause search results to not load. Learn more in the Solr section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/solr-search-index.html).\n+\n+### Documentation for Purging the Ingest Queue\n+\n+At times, it may be necessary to cancel long-running Ingest jobs in the interest of system stability. The Troubleshooting section of the [Admin Guide](http://guides.dataverse.org/en/5.1/admin/) now has specific steps.\n+\n+### Biomedical Metadata Block Updated\n+\n+The Life Science Metadata block (biomedical.tsv) was updated.  \"Other Design Type\", \"Other Factor Type\", \"Other Technology Type\", \"Other Technology Platform\" boxes were added. See the \"Additional Upgrade Steps\" below if you use this in your installation.\n+\n+## Notes for Tool Developers and Integrators\n+\n+### Spaces in File Names\n+\n+Dataverse Installations using S3 storage will no longer replace spaces in file names of downloaded files with the + character. If your tool or integration has any special handling around this, you may need to make further adjustments to maintain backwards compatibility while also supporting Dataverse installations on 5.1+.\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.1 Milestone](https://github.com/IQSS/dataverse/milestone/90?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.1/installation/)\n+\n+## Upgrade Instructions\n+\n+0. These instructions assume that you've already successfully upgraded from Dataverse 4.x to  Dataverse 5 following the instructions in the [Dataverse 5 Release Notes](https://github.com/IQSS/dataverse/releases/tag/v5.0).\n+\n+1. Undeploy the previous version.\n+\n+<payara install path>/payara/bin/asadmin list-applications\n+<payara install path>/payara/bin/asadmin undeploy dataverse\n+\n+2. Stop payara and remove the generated directory, start.\n+\n+- service payara stop\n+- remove the generated directory: rm -rf <payara install path>payara/payara/domains/domain1/generated\n+- service payara start\n+\n+3. Deploy this version.\n+<payara install path>/payara/bin/asadmin deploy <path>dataverse-5.1.war\n+\n+4. Restart payara\n+\n+### Additional Upgrade Steps\n+\n+1. Update Biomedical Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.1/biomedical.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @biomedical.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+2. Updated Solr XML and Reload Solr\n+\n+-  copy schema_dv_mdb_fields.xml and schema_dv_mdb_copies.xml to solr server, for example into /usr/local/solr/solr-7.7.2/server/solr/collection1/conf/ directory\n+-  reload Solr, for example, http://localhost:8983/solr/admin/cores?action=RELOAD&core=collection1\n+\n+3. (Recommended) Run ReExportall to update JSON Exports  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxNzI4NA=="}, "originalCommit": {"oid": "727106b8a1eb238cfc83a2551fa5b9cc6d0446cc"}, "originalPosition": 98}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2393, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}