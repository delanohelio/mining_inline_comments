{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM1MjA0MjI3", "number": 6986, "title": "6505 optimize zip downloads", "bodyText": "What this PR does / why we need it:\nThe much debated \"zipper service\" - an experimental way to take zipped file downloads (extra long running jobs by design) outside of the main Application Service.\nWhich issue(s) this PR closes:\nCloses #6505\nSpecial notes for your reviewer:\nSuggestions on how to test this:\nDoes this PR introduce a user interface change? If mockups are available, please link/include them here:\nIs there a release notes update needed for this change?:\nAdditional documentation:", "createdAt": "2020-06-16T13:11:48Z", "url": "https://github.com/IQSS/dataverse/pull/6986", "merged": true, "mergeCommit": {"oid": "875374fded0e2a4ae8af1af0b89ef01300421fd4"}, "closed": true, "closedAt": "2020-07-16T19:24:28Z", "author": {"login": "landreev"}, "timelineItems": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcmDpaOgH2gAyNDM1MjA0MjI3OmI4ZDI2OGExYTJhMTFjMjI3MzljZThhNWE4YWEyNDU3ZjBkMTc3MTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc05poGAH2gAyNDM1MjA0MjI3OjI1NTM4NDViM2RmMmMyNzUyYjY1MmMyNDM5ZWZmZDU1ZGI5MDQxZDg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b8d268a1a2a11c22739ce8a5a8aa2457f0d17711", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/b8d268a1a2a11c22739ce8a5a8aa2457f0d17711", "committedDate": "2020-05-29T14:50:09Z", "message": "The (very limited) changes that went into the application to accommodate the\nexternal \"custom download\" service. Everything else is done by an outside\nstandalone program (a java program with its own pom file). (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b1765a5ec6cfd30412db7486ea23955d112c7da", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/8b1765a5ec6cfd30412db7486ea23955d112c7da", "committedDate": "2020-05-29T23:33:26Z", "message": "components of the standalone zipper (#6505).\nstill working on the documentation, so will need to check it in later."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3973d1a90a31295a868223da52e21a5d99290e1", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/e3973d1a90a31295a868223da52e21a5d99290e1", "committedDate": "2020-06-02T13:42:03Z", "message": "handling of folders added to the zipper;\nadded some info to the documentation explaining how the zipper does its thing. (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad1787a4b22cbcf5d7ab0f25a9754a2a8fbdb753", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/ad1787a4b22cbcf5d7ab0f25a9754a2a8fbdb753", "committedDate": "2020-06-02T16:46:19Z", "message": "cosmetic (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1dc597b0fd0d5a83bacd7220619a97fec9bdc9cf", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/1dc597b0fd0d5a83bacd7220619a97fec9bdc9cf", "committedDate": "2020-06-16T13:05:06Z", "message": "The modifications allowing the use of the \"custom zipper\" with the API as well.(#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ddfc88c3ca221bccb7df2a0bdd454fc8d56040ea", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/ddfc88c3ca221bccb7df2a0bdd454fc8d56040ea", "committedDate": "2020-06-23T01:07:38Z", "message": "uncommented the line that cleans the request table, on the service executable side. (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5402e07eb1afa5370d91c3383089a4341fb57036", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/5402e07eb1afa5370d91c3383089a4341fb57036", "committedDate": "2020-06-23T01:28:08Z", "message": "Merge branch 'develop' into 6505-optimize-zip-downloads\n(fixed merge conflicts w/develop - mostly the POST handling added for the /api/access/datafiles/ API)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa923ba34ff812091ace1fd56f3dc65822646838", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/aa923ba34ff812091ace1fd56f3dc65822646838", "committedDate": "2020-06-23T02:11:54Z", "message": "a release note for the \"zipper tool\". (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/5d27982b1b4ebd7f56adc349138bd9de3fc22670", "committedDate": "2020-06-23T02:34:24Z", "message": "added a section on the zipper service to the \"installation/advanced\" section (#6505)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2OTE3MTU2", "url": "https://github.com/IQSS/dataverse/pull/6986#pullrequestreview-436917156", "createdAt": "2020-06-24T18:50:19Z", "commit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1MDoyMFrOGoe0wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDo1NzozNlrOGoi3zA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwMTI0OQ==", "bodyText": "When I read, \"If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested...\" I think it means that if :CustomZipDownloadServiceUrl is enabled, :ZipDownloadLimit is ignored. However, without looking at the code, I have some doubt so we should probably make this more explicit.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445101249", "createdAt": "2020-06-24T18:50:20Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/advanced.rst", "diffHunk": "@@ -35,3 +35,42 @@ If you have successfully installed multiple app servers behind a load balancer y\n You would repeat the steps above for all of your app servers. If users seem to be having a problem with a particular server, you can ask them to visit https://dataverse.example.edu/host.txt and let you know what they see there (e.g. \"server1.example.edu\") to help you know which server to troubleshoot.\n \n Please note that :ref:`network-ports` under the Configuration section has more information on fronting your app server with Apache. The :doc:`shibboleth` section talks about the use of ``ProxyPassMatch``.\n+\n+Optional Components\n+-------------------\n+\n+Standalone \"Zipper\" Service Tool\n+++++++++++++++++++++++++++++++++\n+\n+As of Dataverse v5.0 we offer an experimental optimization for the\n+multi-file, download-as-zip functionality. If this option is enabled,\n+instead of enforcing size limits, we attempt to serve all the files\n+that the user requested (that they are authorized to download), but", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwMTc3OQ==", "bodyText": "Just confirming that we're asking installations to build the zipper themselves for now?", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445101779", "createdAt": "2020-06-24T18:51:15Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/advanced.rst", "diffHunk": "@@ -35,3 +35,42 @@ If you have successfully installed multiple app servers behind a load balancer y\n You would repeat the steps above for all of your app servers. If users seem to be having a problem with a particular server, you can ask them to visit https://dataverse.example.edu/host.txt and let you know what they see there (e.g. \"server1.example.edu\") to help you know which server to troubleshoot.\n \n Please note that :ref:`network-ports` under the Configuration section has more information on fronting your app server with Apache. The :doc:`shibboleth` section talks about the use of ``ProxyPassMatch``.\n+\n+Optional Components\n+-------------------\n+\n+Standalone \"Zipper\" Service Tool\n+++++++++++++++++++++++++++++++++\n+\n+As of Dataverse v5.0 we offer an experimental optimization for the\n+multi-file, download-as-zip functionality. If this option is enabled,\n+instead of enforcing size limits, we attempt to serve all the files\n+that the user requested (that they are authorized to download), but\n+the request is redirected to a standalone zipper service running as a\n+cgi-bin executable under Apache. Thus moving these potentially\n+long-running jobs completely outside the Application Server (Payara);\n+and preventing worker threads from becoming locked serving them. Since\n+zipping is also a CPU-intensive task, it is possible to have this\n+service running on a different host system, thus freeing the cycles on\n+the main Application Server. (The system running the service needs to\n+have access to the database as well as to the storage filesystem,\n+and/or S3 bucket).\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5\n+source tree for more information. \n+\n+To install: follow the instructions in the file above to build\n+``ZipDownloadService-v1.0.0.jar``. Copy it, together with the shell", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwNTg3Nw==", "bodyText": "I know this whole zipper thing is somewhat experimental but it might be nice to list :CustomZipDownloadServiceUrl in config.rst along side the other database options.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445105877", "createdAt": "2020-06-24T18:58:46Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/advanced.rst", "diffHunk": "@@ -35,3 +35,42 @@ If you have successfully installed multiple app servers behind a load balancer y\n You would repeat the steps above for all of your app servers. If users seem to be having a problem with a particular server, you can ask them to visit https://dataverse.example.edu/host.txt and let you know what they see there (e.g. \"server1.example.edu\") to help you know which server to troubleshoot.\n \n Please note that :ref:`network-ports` under the Configuration section has more information on fronting your app server with Apache. The :doc:`shibboleth` section talks about the use of ``ProxyPassMatch``.\n+\n+Optional Components\n+-------------------\n+\n+Standalone \"Zipper\" Service Tool\n+++++++++++++++++++++++++++++++++\n+\n+As of Dataverse v5.0 we offer an experimental optimization for the\n+multi-file, download-as-zip functionality. If this option is enabled,\n+instead of enforcing size limits, we attempt to serve all the files\n+that the user requested (that they are authorized to download), but\n+the request is redirected to a standalone zipper service running as a\n+cgi-bin executable under Apache. Thus moving these potentially\n+long-running jobs completely outside the Application Server (Payara);\n+and preventing worker threads from becoming locked serving them. Since\n+zipping is also a CPU-intensive task, it is possible to have this\n+service running on a different host system, thus freeing the cycles on\n+the main Application Server. (The system running the service needs to\n+have access to the database as well as to the storage filesystem,\n+and/or S3 bucket).\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5\n+source tree for more information. \n+\n+To install: follow the instructions in the file above to build\n+``ZipDownloadService-v1.0.0.jar``. Copy it, together with the shell\n+script scripts/zipdownload/cgi-bin/zipdownload to the cgi-bin\n+directory of the chosen Apache server (/var/www/cgi-bin standard).\n+Edit the config lines in the shell script (zipdownload) to configure\n+database access credentials. Do note that the executable does not need\n+access to the entire Dataverse database. A secuirity-conscious admin\n+can create a dedicated database user with access to just one table:\n+``CUSTOMZIPSERVICEREQUEST``.\n+\n+to activate in Dataverse::\n+\n+   curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwODY1NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            clean compile assembly:single\n          \n          \n            \n            mvn clean compile assembly:single", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445108655", "createdAt": "2020-06-24T19:03:54Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwOTMyMQ==", "bodyText": "We should add a step here:\ncd scripts/zipdownload", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445109321", "createdAt": "2020-06-24T19:05:13Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEyMTUzNw==", "bodyText": "@landreev and I talked about this but this is a reminder to add the following line to wherever the ProxyPass config is:\nProxyPassMatch ^/cgi-bin/zipdownload !\nOtherwise, you end up with errors like this:", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445121537", "createdAt": "2020-06-24T19:28:17Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEyMjk5Mg==", "bodyText": "I ran into problems with the CGI script setup:\n\n[Wed Jun 24 19:29:43.616634 2020] [cgid:error] [pid 57182:tid 140001334073088] [client 75.69.182.6:50927] AH01241: error spawning CGI child: exec of '/var/www/cgi-bin/zipdownload' failed (Permission denied): /var/www/cgi-bin/zipdownload, referer: http://ec2-34-224-6-242.compute-1.amazonaws.com/dataset.xhtml?persistentId=doi:10.5072/FK2/J5MISN&version=1.0\n[Wed Jun 24 19:29:43.616865 2020] [cgid:error] [pid 57182:tid 140001334073088] [client 75.69.182.6:50927] End of script output before headers: zipdownload, referer: http://ec2-34-224-6-242.compute-1.amazonaws.com/dataset.xhtml?persistentId=doi:10.5072/FK2/J5MISN&version=1.0\nTo overcome these problems I had to do two things, and both should be documented, I believe:\n\nchmod 755 zipdownload\nsetenforce Permissive\n\nIdeally, we would figure out how to run the zipper with SELinux enabled. I once wrote some tips at http://guides.dataverse.org/en/4.20/developers/selinux.html", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445122992", "createdAt": "2020-06-24T19:31:15Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEyNjk3NQ==", "bodyText": "This \"Thus moving\" sentence isn't a full sentence. It's a fragment.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445126975", "createdAt": "2020-06-24T19:39:00Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/advanced.rst", "diffHunk": "@@ -35,3 +35,42 @@ If you have successfully installed multiple app servers behind a load balancer y\n You would repeat the steps above for all of your app servers. If users seem to be having a problem with a particular server, you can ask them to visit https://dataverse.example.edu/host.txt and let you know what they see there (e.g. \"server1.example.edu\") to help you know which server to troubleshoot.\n \n Please note that :ref:`network-ports` under the Configuration section has more information on fronting your app server with Apache. The :doc:`shibboleth` section talks about the use of ``ProxyPassMatch``.\n+\n+Optional Components\n+-------------------\n+\n+Standalone \"Zipper\" Service Tool\n+++++++++++++++++++++++++++++++++\n+\n+As of Dataverse v5.0 we offer an experimental optimization for the\n+multi-file, download-as-zip functionality. If this option is enabled,\n+instead of enforcing size limits, we attempt to serve all the files\n+that the user requested (that they are authorized to download), but\n+the request is redirected to a standalone zipper service running as a\n+cgi-bin executable under Apache. Thus moving these potentially", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEzMDA2Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            access to the entire Dataverse database. A secuirity-conscious admin\n          \n          \n            \n            access to the entire Dataverse database. A security-conscious admin", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445130063", "createdAt": "2020-06-24T19:45:09Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/advanced.rst", "diffHunk": "@@ -35,3 +35,42 @@ If you have successfully installed multiple app servers behind a load balancer y\n You would repeat the steps above for all of your app servers. If users seem to be having a problem with a particular server, you can ask them to visit https://dataverse.example.edu/host.txt and let you know what they see there (e.g. \"server1.example.edu\") to help you know which server to troubleshoot.\n \n Please note that :ref:`network-ports` under the Configuration section has more information on fronting your app server with Apache. The :doc:`shibboleth` section talks about the use of ``ProxyPassMatch``.\n+\n+Optional Components\n+-------------------\n+\n+Standalone \"Zipper\" Service Tool\n+++++++++++++++++++++++++++++++++\n+\n+As of Dataverse v5.0 we offer an experimental optimization for the\n+multi-file, download-as-zip functionality. If this option is enabled,\n+instead of enforcing size limits, we attempt to serve all the files\n+that the user requested (that they are authorized to download), but\n+the request is redirected to a standalone zipper service running as a\n+cgi-bin executable under Apache. Thus moving these potentially\n+long-running jobs completely outside the Application Server (Payara);\n+and preventing worker threads from becoming locked serving them. Since\n+zipping is also a CPU-intensive task, it is possible to have this\n+service running on a different host system, thus freeing the cycles on\n+the main Application Server. (The system running the service needs to\n+have access to the database as well as to the storage filesystem,\n+and/or S3 bucket).\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5\n+source tree for more information. \n+\n+To install: follow the instructions in the file above to build\n+``ZipDownloadService-v1.0.0.jar``. Copy it, together with the shell\n+script scripts/zipdownload/cgi-bin/zipdownload to the cgi-bin\n+directory of the chosen Apache server (/var/www/cgi-bin standard).\n+Edit the config lines in the shell script (zipdownload) to configure\n+database access credentials. Do note that the executable does not need\n+access to the entire Dataverse database. A secuirity-conscious admin", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEzNDUwMQ==", "bodyText": "It's a little weird to use the first person here. The reader was directed to this README from the Installation Guide. If \"I\" is used this letter should probably be signed. (There's some \"me\" below too.)", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445134501", "createdAt": "2020-06-24T19:53:32Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+\n+to activate in Dataverse: \n+\n+curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl\n+\n+How it works:\n+=============\n+\n+The goal: to move this potentially long-running task out of the\n+Application Server. This is the sole focus of this implementation. It\n+does not attempt to make it faster.\n+\n+The rationale here is a zipped download of a large enough number of\n+large enough files will always be slow. Zipping (compressing) itself\n+is a fairly CPU-intensive task. This will most frequently be the\n+bottleneck of the service. Although with a slow storage location (S3\n+or Swift, with a slow link to the share) it may be the speed at which\n+the application accesses the raw bytes. The exact location of the\n+bottleneck is in a sense irrelevant. On a very fast system, with the\n+files stored on a very fast local RAID, the bottleneck for most users\n+will likely shift to the speed of their internet connection to the\n+Dataverse. Bottom line is, downloading this multi-file compressed\n+stream will take a long time no matter how you slice it. So this hack\n+addresses it by moving the task outside Payara, where it's not going\n+to hog any threads. \n+\n+A quick, somewhat unrelated note: attempting to download a multi-GB\n+stream over http will always have its own inherent risks. If the\n+download has to take hours or days to complete, it is very likely that\n+it'll break down somewhere in the middle. Do note that for a zipped\n+download our users will not be able to utilize `wget --continue`, or\n+any similar \"resume\" functionality - because it's impossible to resume\n+generating a zipped stream from a certain offset.\n+\n+The implementation is a hack. It relies on direct access to everything - storage locations (filesystem or S3) and the database.\n+\n+There are no network calls between the Application and the zipper (an\n+implementation relying on such a call was discussed early\n+on). Dataverse issues a \"job key\" and sends the user's browser to the\n+zipper (to, for ex., /cgi-bin/zipdownload?<job key>) instead of\n+/api/access/datafiles/<file ids>). To authorize the zipdownload for\n+the \"job key\", and inform the zipper on which files to zip and where\n+to find them, the application relies on a database table, that the\n+zipper also has access too. In other words, there is a saved state\n+information associated with each zipped download request. Zipper may\n+be given a limited database access - for example, via a user\n+authorized to access that one table only. After serving the files, the\n+zipper removes the database entries. Job records in the database have\n+time stamps, so on the application side, as an added level of cleanup,\n+it automatically deletes any records older than 5 minutes (can be\n+further reduced) every time the service adds new records; as an added\n+level of cleanup for any records that got stuck in the db because the\n+corresponding zipper jobs never completed. A paranoid admin may choose\n+to give the zipper read-only access to the database, and rely on a\n+cleanup solely on the application side.\n+\n+I have explored ways to avoid maintaining this state information. A", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEzNTM1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            presigns their access URLs. Jim Meyers has already speced out how this\n          \n          \n            \n            presigns their access URLs. Jim Myers has already speced out how this", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445135353", "createdAt": "2020-06-24T19:55:10Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+\n+to activate in Dataverse: \n+\n+curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl\n+\n+How it works:\n+=============\n+\n+The goal: to move this potentially long-running task out of the\n+Application Server. This is the sole focus of this implementation. It\n+does not attempt to make it faster.\n+\n+The rationale here is a zipped download of a large enough number of\n+large enough files will always be slow. Zipping (compressing) itself\n+is a fairly CPU-intensive task. This will most frequently be the\n+bottleneck of the service. Although with a slow storage location (S3\n+or Swift, with a slow link to the share) it may be the speed at which\n+the application accesses the raw bytes. The exact location of the\n+bottleneck is in a sense irrelevant. On a very fast system, with the\n+files stored on a very fast local RAID, the bottleneck for most users\n+will likely shift to the speed of their internet connection to the\n+Dataverse. Bottom line is, downloading this multi-file compressed\n+stream will take a long time no matter how you slice it. So this hack\n+addresses it by moving the task outside Payara, where it's not going\n+to hog any threads. \n+\n+A quick, somewhat unrelated note: attempting to download a multi-GB\n+stream over http will always have its own inherent risks. If the\n+download has to take hours or days to complete, it is very likely that\n+it'll break down somewhere in the middle. Do note that for a zipped\n+download our users will not be able to utilize `wget --continue`, or\n+any similar \"resume\" functionality - because it's impossible to resume\n+generating a zipped stream from a certain offset.\n+\n+The implementation is a hack. It relies on direct access to everything - storage locations (filesystem or S3) and the database.\n+\n+There are no network calls between the Application and the zipper (an\n+implementation relying on such a call was discussed early\n+on). Dataverse issues a \"job key\" and sends the user's browser to the\n+zipper (to, for ex., /cgi-bin/zipdownload?<job key>) instead of\n+/api/access/datafiles/<file ids>). To authorize the zipdownload for\n+the \"job key\", and inform the zipper on which files to zip and where\n+to find them, the application relies on a database table, that the\n+zipper also has access too. In other words, there is a saved state\n+information associated with each zipped download request. Zipper may\n+be given a limited database access - for example, via a user\n+authorized to access that one table only. After serving the files, the\n+zipper removes the database entries. Job records in the database have\n+time stamps, so on the application side, as an added level of cleanup,\n+it automatically deletes any records older than 5 minutes (can be\n+further reduced) every time the service adds new records; as an added\n+level of cleanup for any records that got stuck in the db because the\n+corresponding zipper jobs never completed. A paranoid admin may choose\n+to give the zipper read-only access to the database, and rely on a\n+cleanup solely on the application side.\n+\n+I have explored ways to avoid maintaining this state information. A\n+potential implementation we discussed early on, where the application\n+would make a network call to the zipper before redirecting the user\n+there, would NOT solve that problem - the state would need to somehow\n+be maintained on the zipper side. The only truly stateless\n+implementation would rely on including all the file information WITH\n+the redirect itself, with some pre-signed URL mechanism to make it\n+secure. Mechanisms for pre-signing requests are readily available and\n+simple to implement. We could go with something similar to how S3\n+presigns their access URLs. Jim Meyers has already speced out how this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE0MDk0MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Dataverse. Bottom line is, downloading this multi-file compressed\n          \n          \n            \n            Dataverse installation. The bottom line is, downloading this multi-file compressed", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445140941", "createdAt": "2020-06-24T20:05:43Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+\n+to activate in Dataverse: \n+\n+curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl\n+\n+How it works:\n+=============\n+\n+The goal: to move this potentially long-running task out of the\n+Application Server. This is the sole focus of this implementation. It\n+does not attempt to make it faster.\n+\n+The rationale here is a zipped download of a large enough number of\n+large enough files will always be slow. Zipping (compressing) itself\n+is a fairly CPU-intensive task. This will most frequently be the\n+bottleneck of the service. Although with a slow storage location (S3\n+or Swift, with a slow link to the share) it may be the speed at which\n+the application accesses the raw bytes. The exact location of the\n+bottleneck is in a sense irrelevant. On a very fast system, with the\n+files stored on a very fast local RAID, the bottleneck for most users\n+will likely shift to the speed of their internet connection to the\n+Dataverse. Bottom line is, downloading this multi-file compressed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE0Mjg2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            zipper also has access too. In other words, there is a saved state\n          \n          \n            \n            zipper also has access to. In other words, there is a saved state", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445142865", "createdAt": "2020-06-24T20:09:31Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+\n+to activate in Dataverse: \n+\n+curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl\n+\n+How it works:\n+=============\n+\n+The goal: to move this potentially long-running task out of the\n+Application Server. This is the sole focus of this implementation. It\n+does not attempt to make it faster.\n+\n+The rationale here is a zipped download of a large enough number of\n+large enough files will always be slow. Zipping (compressing) itself\n+is a fairly CPU-intensive task. This will most frequently be the\n+bottleneck of the service. Although with a slow storage location (S3\n+or Swift, with a slow link to the share) it may be the speed at which\n+the application accesses the raw bytes. The exact location of the\n+bottleneck is in a sense irrelevant. On a very fast system, with the\n+files stored on a very fast local RAID, the bottleneck for most users\n+will likely shift to the speed of their internet connection to the\n+Dataverse. Bottom line is, downloading this multi-file compressed\n+stream will take a long time no matter how you slice it. So this hack\n+addresses it by moving the task outside Payara, where it's not going\n+to hog any threads. \n+\n+A quick, somewhat unrelated note: attempting to download a multi-GB\n+stream over http will always have its own inherent risks. If the\n+download has to take hours or days to complete, it is very likely that\n+it'll break down somewhere in the middle. Do note that for a zipped\n+download our users will not be able to utilize `wget --continue`, or\n+any similar \"resume\" functionality - because it's impossible to resume\n+generating a zipped stream from a certain offset.\n+\n+The implementation is a hack. It relies on direct access to everything - storage locations (filesystem or S3) and the database.\n+\n+There are no network calls between the Application and the zipper (an\n+implementation relying on such a call was discussed early\n+on). Dataverse issues a \"job key\" and sends the user's browser to the\n+zipper (to, for ex., /cgi-bin/zipdownload?<job key>) instead of\n+/api/access/datafiles/<file ids>). To authorize the zipdownload for\n+the \"job key\", and inform the zipper on which files to zip and where\n+to find them, the application relies on a database table, that the\n+zipper also has access too. In other words, there is a saved state", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE0NDY2OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There are no network calls between the Application and the zipper (an\n          \n          \n            \n            There are no network calls between the application (Dataverse) and the zipper (an", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445144668", "createdAt": "2020-06-24T20:13:15Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+\n+to activate in Dataverse: \n+\n+curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl\n+\n+How it works:\n+=============\n+\n+The goal: to move this potentially long-running task out of the\n+Application Server. This is the sole focus of this implementation. It\n+does not attempt to make it faster.\n+\n+The rationale here is a zipped download of a large enough number of\n+large enough files will always be slow. Zipping (compressing) itself\n+is a fairly CPU-intensive task. This will most frequently be the\n+bottleneck of the service. Although with a slow storage location (S3\n+or Swift, with a slow link to the share) it may be the speed at which\n+the application accesses the raw bytes. The exact location of the\n+bottleneck is in a sense irrelevant. On a very fast system, with the\n+files stored on a very fast local RAID, the bottleneck for most users\n+will likely shift to the speed of their internet connection to the\n+Dataverse. Bottom line is, downloading this multi-file compressed\n+stream will take a long time no matter how you slice it. So this hack\n+addresses it by moving the task outside Payara, where it's not going\n+to hog any threads. \n+\n+A quick, somewhat unrelated note: attempting to download a multi-GB\n+stream over http will always have its own inherent risks. If the\n+download has to take hours or days to complete, it is very likely that\n+it'll break down somewhere in the middle. Do note that for a zipped\n+download our users will not be able to utilize `wget --continue`, or\n+any similar \"resume\" functionality - because it's impossible to resume\n+generating a zipped stream from a certain offset.\n+\n+The implementation is a hack. It relies on direct access to everything - storage locations (filesystem or S3) and the database.\n+\n+There are no network calls between the Application and the zipper (an", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE0NTUxOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            hash along with the request. Tempering with any of the parameters, or\n          \n          \n            \n            hash along with the request. Tampering with any of the parameters, or", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445145519", "createdAt": "2020-06-24T20:14:57Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/README.md", "diffHunk": "@@ -0,0 +1,93 @@\n+Work in progress!\n+\n+to build: \n+\n+clean compile assembly:single\n+\n+to install: \n+\n+install cgi-bin/zipdownload and ZipDownloadService-v1.0.0.jar in your cgi-bin directory (/var/www/cgi-bin standard). \n+Edit the config lines in the shell script (zipdownload) as needed. \n+\n+to activate in Dataverse: \n+\n+curl -X PUT -d '/cgi-bin/zipdownload' http://localhost:8080/api/admin/settings/:CustomZipDownloadServiceUrl\n+\n+How it works:\n+=============\n+\n+The goal: to move this potentially long-running task out of the\n+Application Server. This is the sole focus of this implementation. It\n+does not attempt to make it faster.\n+\n+The rationale here is a zipped download of a large enough number of\n+large enough files will always be slow. Zipping (compressing) itself\n+is a fairly CPU-intensive task. This will most frequently be the\n+bottleneck of the service. Although with a slow storage location (S3\n+or Swift, with a slow link to the share) it may be the speed at which\n+the application accesses the raw bytes. The exact location of the\n+bottleneck is in a sense irrelevant. On a very fast system, with the\n+files stored on a very fast local RAID, the bottleneck for most users\n+will likely shift to the speed of their internet connection to the\n+Dataverse. Bottom line is, downloading this multi-file compressed\n+stream will take a long time no matter how you slice it. So this hack\n+addresses it by moving the task outside Payara, where it's not going\n+to hog any threads. \n+\n+A quick, somewhat unrelated note: attempting to download a multi-GB\n+stream over http will always have its own inherent risks. If the\n+download has to take hours or days to complete, it is very likely that\n+it'll break down somewhere in the middle. Do note that for a zipped\n+download our users will not be able to utilize `wget --continue`, or\n+any similar \"resume\" functionality - because it's impossible to resume\n+generating a zipped stream from a certain offset.\n+\n+The implementation is a hack. It relies on direct access to everything - storage locations (filesystem or S3) and the database.\n+\n+There are no network calls between the Application and the zipper (an\n+implementation relying on such a call was discussed early\n+on). Dataverse issues a \"job key\" and sends the user's browser to the\n+zipper (to, for ex., /cgi-bin/zipdownload?<job key>) instead of\n+/api/access/datafiles/<file ids>). To authorize the zipdownload for\n+the \"job key\", and inform the zipper on which files to zip and where\n+to find them, the application relies on a database table, that the\n+zipper also has access too. In other words, there is a saved state\n+information associated with each zipped download request. Zipper may\n+be given a limited database access - for example, via a user\n+authorized to access that one table only. After serving the files, the\n+zipper removes the database entries. Job records in the database have\n+time stamps, so on the application side, as an added level of cleanup,\n+it automatically deletes any records older than 5 minutes (can be\n+further reduced) every time the service adds new records; as an added\n+level of cleanup for any records that got stuck in the db because the\n+corresponding zipper jobs never completed. A paranoid admin may choose\n+to give the zipper read-only access to the database, and rely on a\n+cleanup solely on the application side.\n+\n+I have explored ways to avoid maintaining this state information. A\n+potential implementation we discussed early on, where the application\n+would make a network call to the zipper before redirecting the user\n+there, would NOT solve that problem - the state would need to somehow\n+be maintained on the zipper side. The only truly stateless\n+implementation would rely on including all the file information WITH\n+the redirect itself, with some pre-signed URL mechanism to make it\n+secure. Mechanisms for pre-signing requests are readily available and\n+simple to implement. We could go with something similar to how S3\n+presigns their access URLs. Jim Meyers has already speced out how this\n+could be done for Dataverse access urls in a design document\n+(https://docs.google.com/document/d/1J8GW6zi-vSRKZdtFjLpmYJ2SUIcIkAEwHkP4q1fxL-s/edit#). (Basically,\n+you hash the product of your request parameters, the issue timestamp\n+AND some \"secret\" - like the user's API key - and send the resulting\n+hash along with the request. Tempering with any of the parameters, or", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE0OTg4Mw==", "bodyText": "I feel like this pom.xml could be made smaller. I can't imagine the PrimeFaces repo is used, for example.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445149883", "createdAt": "2020-06-24T20:23:30Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/pom.xml", "diffHunk": "@@ -0,0 +1,107 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+    <groupId>ZipDownloadService</groupId>\n+    <artifactId>ZipDownloadService</artifactId>\n+    <version>1.0.0</version>\n+    <properties>\n+        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+    </properties>\n+    <pluginRepositories>\n+        <pluginRepository>\n+            <id>central</id>\n+            <name>Central Repository</name>\n+            <url>https://repo.maven.apache.org/maven2</url>\n+            <layout>default</layout>\n+            <snapshots>\n+                <enabled>false</enabled>\n+            </snapshots>\n+            <releases>\n+                <updatePolicy>never</updatePolicy>\n+            </releases>\n+        </pluginRepository>\n+    </pluginRepositories>\n+    <repositories>\n+        <repository>\n+            <id>central-repo</id>\n+            <name>Central Repository</name>\n+            <url>https://repo1.maven.org/maven2</url>\n+            <layout>default</layout>\n+        </repository>\n+        <repository>\n+            <id>prime-repo</id>\n+            <name>PrimeFaces Maven Repository</name>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1MDk5Mg==", "bodyText": "Should this TODO about folders be removed? I see some folder handling below.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445150992", "createdAt": "2020-06-24T20:25:36Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/src/main/java/edu/harvard/iq/dataverse/custom/service/download/ZipDownloadService.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+   Copyright (C) 2005-2012, by the President and Fellows of Harvard College.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+         http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+\n+   Dataverse Network - A web application to share, preserve and analyze research data.\n+   Developed at the Institute for Quantitative Social Science, Harvard University.\n+   Version 3.0.\n+*/\n+package edu.harvard.iq.dataverse.custom.service.download;\n+\n+import edu.harvard.iq.dataverse.custom.service.util.DirectAccessUtil;\n+import static edu.harvard.iq.dataverse.custom.service.util.DatabaseAccessUtil.lookupZipJob;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipOutputStream;\n+\n+/**\n+ * Custom (standalone) download service for Dataverse\n+ * \n+ * @author Leonid Andreev\n+ */\n+public class ZipDownloadService { \n+    \n+    private static String jobKey = null;\n+    private List<String[]> jobFiles = null;\n+    private boolean zipOnly = false; \n+    \n+    private DirectAccessUtil directAccessUtil = null; \n+    private ZipOutputStream zipOutputStream = null;\n+\n+    public static void main(String args[]) throws Exception {\n+        \n+        ZipDownloadService zipperService = new ZipDownloadService();\n+        \n+        if (!zipperService.parseArgs(args)) {\n+            zipperService.usage();\n+            return; \n+        }\n+        \n+        zipperService.parseCgiQueryParameters();\n+               \n+        zipperService.execute(jobKey);\n+    }\n+\n+    private static void usage() {\n+        System.out.println(\"\\nUsage:\");\n+        System.out.println(\"  java -jar ZipDownloadService-1.0.0.jar [-ziponly]>\\n\");\n+\n+        System.out.println(\"  supported options:\");\n+        System.out.println(\"   -ziponly = output zip only, no http header/no chunking\");\n+        System.out.println(\"\");\n+\n+    }\n+\n+    // The only option supported at the moment is \"zip only\" - output just the\n+    // compressed stream, skip the HTTP header and chunking.\n+    public boolean parseArgs(String[] args) {\n+\n+        if (args == null || args.length == 0) {\n+            return true; \n+        } else if (args.length == 1) {\n+            if (args[0].equals(\"-ziponly\")) {\n+                this.zipOnly = true;\n+                return true;\n+            }\n+        }\n+        \n+        return false; \n+    }\n+    \n+    // Does not support any parameters, except the job-identifying token key, \n+    // supplied as the entire query string. \n+    public void parseCgiQueryParameters() {\n+        String queryString = System.getenv().get(\"QUERY_STRING\");\n+        if (queryString != null) {\n+            jobKey = queryString; \n+        }\n+    }\n+    \n+    public void print404() {\n+        System.out.println(\"Status: 404 Not Found\\r\");\n+        System.out.println(\"Content-Type: text/html\\r\");\n+        System.out.println(\"\\r\");\n+\n+        System.out.println(\"<h1>404 No such download job!</h1>\");\n+    }\n+    \n+    public void printZipHeader() {\n+        System.out.println(\"Content-disposition: attachment; filename=\\\"dataverse_files.zip\\\"\\r\");\n+        System.out.println(\"Content-Type: application/zip; name=\\\"dataverse_files.zip\\\"\\r\");\n+        System.out.println(\"Transfer-Encoding: chunked\\r\");\n+        System.out.println(\"\\r\");\n+        System.out.flush();\n+    }\n+    \n+    public void execute(String key) {\n+        \n+        jobFiles = lookupZipJob(key); \n+        \n+        if (jobFiles == null || jobFiles.size() == 0) {\n+            this.print404();\n+            System.exit(0);\n+        }\n+        \n+        this.processFiles();\n+    }\n+    \n+    public void processFiles() {\n+        \n+        if (!this.zipOnly) {\n+            this.printZipHeader();\n+        }\n+        \n+        Set<String> zippedFolders = new HashSet<>();\n+       \n+        for (String [] fileEntry : jobFiles) {\n+            String storageLocation = fileEntry[0];\n+            String fileName = fileEntry[1];\n+            \n+            //System.out.println(storageLocation + \":\" + fileName);\n+            \n+            if (this.zipOutputStream == null) {\n+                openZipStream();\n+            }\n+            \n+            if (this.directAccessUtil == null) {\n+                this.directAccessUtil = new DirectAccessUtil();\n+            }\n+            \n+            InputStream inputStream = this.directAccessUtil.openDirectAccess(storageLocation);\n+                \n+            // TODO: folders", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1MTQ1Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                System.err.println(\"Faile to compress \"+storageLocation);\n          \n          \n            \n                                System.err.println(\"Failed to compress \"+storageLocation);", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445151452", "createdAt": "2020-06-24T20:26:26Z", "author": {"login": "pdurbin"}, "path": "scripts/zipdownload/src/main/java/edu/harvard/iq/dataverse/custom/service/download/ZipDownloadService.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+   Copyright (C) 2005-2012, by the President and Fellows of Harvard College.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+         http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+\n+   Dataverse Network - A web application to share, preserve and analyze research data.\n+   Developed at the Institute for Quantitative Social Science, Harvard University.\n+   Version 3.0.\n+*/\n+package edu.harvard.iq.dataverse.custom.service.download;\n+\n+import edu.harvard.iq.dataverse.custom.service.util.DirectAccessUtil;\n+import static edu.harvard.iq.dataverse.custom.service.util.DatabaseAccessUtil.lookupZipJob;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipOutputStream;\n+\n+/**\n+ * Custom (standalone) download service for Dataverse\n+ * \n+ * @author Leonid Andreev\n+ */\n+public class ZipDownloadService { \n+    \n+    private static String jobKey = null;\n+    private List<String[]> jobFiles = null;\n+    private boolean zipOnly = false; \n+    \n+    private DirectAccessUtil directAccessUtil = null; \n+    private ZipOutputStream zipOutputStream = null;\n+\n+    public static void main(String args[]) throws Exception {\n+        \n+        ZipDownloadService zipperService = new ZipDownloadService();\n+        \n+        if (!zipperService.parseArgs(args)) {\n+            zipperService.usage();\n+            return; \n+        }\n+        \n+        zipperService.parseCgiQueryParameters();\n+               \n+        zipperService.execute(jobKey);\n+    }\n+\n+    private static void usage() {\n+        System.out.println(\"\\nUsage:\");\n+        System.out.println(\"  java -jar ZipDownloadService-1.0.0.jar [-ziponly]>\\n\");\n+\n+        System.out.println(\"  supported options:\");\n+        System.out.println(\"   -ziponly = output zip only, no http header/no chunking\");\n+        System.out.println(\"\");\n+\n+    }\n+\n+    // The only option supported at the moment is \"zip only\" - output just the\n+    // compressed stream, skip the HTTP header and chunking.\n+    public boolean parseArgs(String[] args) {\n+\n+        if (args == null || args.length == 0) {\n+            return true; \n+        } else if (args.length == 1) {\n+            if (args[0].equals(\"-ziponly\")) {\n+                this.zipOnly = true;\n+                return true;\n+            }\n+        }\n+        \n+        return false; \n+    }\n+    \n+    // Does not support any parameters, except the job-identifying token key, \n+    // supplied as the entire query string. \n+    public void parseCgiQueryParameters() {\n+        String queryString = System.getenv().get(\"QUERY_STRING\");\n+        if (queryString != null) {\n+            jobKey = queryString; \n+        }\n+    }\n+    \n+    public void print404() {\n+        System.out.println(\"Status: 404 Not Found\\r\");\n+        System.out.println(\"Content-Type: text/html\\r\");\n+        System.out.println(\"\\r\");\n+\n+        System.out.println(\"<h1>404 No such download job!</h1>\");\n+    }\n+    \n+    public void printZipHeader() {\n+        System.out.println(\"Content-disposition: attachment; filename=\\\"dataverse_files.zip\\\"\\r\");\n+        System.out.println(\"Content-Type: application/zip; name=\\\"dataverse_files.zip\\\"\\r\");\n+        System.out.println(\"Transfer-Encoding: chunked\\r\");\n+        System.out.println(\"\\r\");\n+        System.out.flush();\n+    }\n+    \n+    public void execute(String key) {\n+        \n+        jobFiles = lookupZipJob(key); \n+        \n+        if (jobFiles == null || jobFiles.size() == 0) {\n+            this.print404();\n+            System.exit(0);\n+        }\n+        \n+        this.processFiles();\n+    }\n+    \n+    public void processFiles() {\n+        \n+        if (!this.zipOnly) {\n+            this.printZipHeader();\n+        }\n+        \n+        Set<String> zippedFolders = new HashSet<>();\n+       \n+        for (String [] fileEntry : jobFiles) {\n+            String storageLocation = fileEntry[0];\n+            String fileName = fileEntry[1];\n+            \n+            //System.out.println(storageLocation + \":\" + fileName);\n+            \n+            if (this.zipOutputStream == null) {\n+                openZipStream();\n+            }\n+            \n+            if (this.directAccessUtil == null) {\n+                this.directAccessUtil = new DirectAccessUtil();\n+            }\n+            \n+            InputStream inputStream = this.directAccessUtil.openDirectAccess(storageLocation);\n+                \n+            // TODO: folders\n+            // TODO: String zipEntryName = checkZipEntryName(fileName);\n+            if (inputStream != null && this.zipOutputStream != null) {\n+                \n+                ZipEntry entry = new ZipEntry(fileName);\n+\n+                byte[] bytes = new byte[2 * 8192];\n+                int read = 0;\n+                long readSize = 0L;\n+\n+                try {\n+                    // Does this file have a folder name? \n+                    if (hasFolder(fileName)) {\n+                        addFolderToZipStream(getFolderName(fileName), zippedFolders);\n+                    }\n+\n+                    this.zipOutputStream.putNextEntry(entry);\n+\n+                    while ((read = inputStream.read(bytes)) != -1) {\n+                        this.zipOutputStream.write(bytes, 0, read);\n+                        readSize += read;\n+                    }\n+                    inputStream.close();\n+                    this.zipOutputStream.closeEntry();\n+\n+                    /*if (fileSize == readSize) {\n+                        //System.out.println(\"Read \"+readSize+\" bytes;\");\n+                    } else {\n+                        throw new IOException(\"Byte size mismatch: expected \" + fileSize + \", read: \" + readSize);\n+                    }*/\n+                } catch (IOException ioex) {\n+                    System.err.println(\"Faile to compress \"+storageLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE2NTg4Nw==", "bodyText": "Should we use our usual @Entity convention and have JPA create and manage this table? And remove this SQL script?", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445165887", "createdAt": "2020-06-24T20:54:25Z", "author": {"login": "pdurbin"}, "path": "src/main/resources/db/migration/V4.20.0.4__6505-zipdownload-jobs.sql", "diffHunk": "@@ -0,0 +1,2 @@\n+-- maybe temporary? - work in progress\n+CREATE TABLE IF NOT EXISTS CUSTOMZIPSERVICEREQUEST (KEY VARCHAR(63), STORAGELOCATION VARCHAR(255), FILENAME VARCHAR(255), ISSUETIME TIMESTAMP);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE2NzU2NA==", "bodyText": "I'm a little confused by how this \"original\" variable always seems to be true. Maybe I'm missing something.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r445167564", "createdAt": "2020-06-24T20:57:36Z", "author": {"login": "pdurbin"}, "path": "src/main/java/edu/harvard/iq/dataverse/FileDownloadServiceBean.java", "diffHunk": "@@ -90,6 +97,7 @@ public void writeGuestbookAndStartBatchDownload(GuestbookResponse guestbookRespo\n     }\n     \n     public void writeGuestbookAndStartBatchDownload(GuestbookResponse guestbookResponse, Boolean doNotSaveGuestbookRecord){\n+        boolean original = true; ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d27982b1b4ebd7f56adc349138bd9de3fc22670"}, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c99fa60c60993a5921ce261fd3a13babec56a4eb", "author": {"user": {"login": "djbrooke", "name": "Danny Brooke"}}, "url": "https://github.com/IQSS/dataverse/commit/c99fa60c60993a5921ce261fd3a13babec56a4eb", "committedDate": "2020-06-24T21:09:59Z", "message": "adding new setting to release notes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8dfe4c448c87c63e4fa9449eea8de49ab6382441", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/8dfe4c448c87c63e4fa9449eea8de49ab6382441", "committedDate": "2020-06-26T16:07:51Z", "message": "Update scripts/zipdownload/README.md\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46584da7cffb4cd017dae812aa085f2062caedce", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/46584da7cffb4cd017dae812aa085f2062caedce", "committedDate": "2020-06-26T16:09:50Z", "message": "Update scripts/zipdownload/src/main/java/edu/harvard/iq/dataverse/custom/service/download/ZipDownloadService.java\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5aaaff5664c2f610f654e941d0f5318dd1d385f1", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/5aaaff5664c2f610f654e941d0f5318dd1d385f1", "committedDate": "2020-06-26T16:56:59Z", "message": "Better/safer handling of database queries (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48a56df3b0b23dbf558de4dd1f400ef88cc8a483", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/48a56df3b0b23dbf558de4dd1f400ef88cc8a483", "committedDate": "2020-06-26T16:58:00Z", "message": "Merge branch '6505-optimize-zip-downloads' of https://github.com/IQSS/dataverse into 6505-optimize-zip-downloads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3eb3976192e7bcb5acd32f4d4786a72a04f09b8c", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/3eb3976192e7bcb5acd32f4d4786a72a04f09b8c", "committedDate": "2020-06-26T17:04:17Z", "message": "added a line about the Apache configuration to the installation instruction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1cd8629a26b1acef0941bc7025fd76ebf020c7d2", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/1cd8629a26b1acef0941bc7025fd76ebf020c7d2", "committedDate": "2020-06-26T17:06:47Z", "message": "line breaks in the readme (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69297fb256fe4e7db1ece6e30a7f2eb5b6a0a8a5", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/69297fb256fe4e7db1ece6e30a7f2eb5b6a0a8a5", "committedDate": "2020-06-26T17:19:08Z", "message": "small addition to the guide on installation (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e2e39650f8dfb0774a87cfe2d2a07cc7b80e07b", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/6e2e39650f8dfb0774a87cfe2d2a07cc7b80e07b", "committedDate": "2020-06-26T17:33:31Z", "message": "documents the zipper setting. (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72394a46edd4c4ef37922fd5f0409ac242a585c3", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/72394a46edd4c4ef37922fd5f0409ac242a585c3", "committedDate": "2020-06-26T17:39:04Z", "message": "fixes \"original\" always being true (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96c37086ce3f8f66b510cdf95f72bbe994eec127", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/96c37086ce3f8f66b510cdf95f72bbe994eec127", "committedDate": "2020-06-26T18:18:26Z", "message": "removed unnecessary repos from pom.xml; a few more words in the advanced guide; #6505"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e01c213d98c802dcb12ebe6ebe6a34abe5f74369", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/e01c213d98c802dcb12ebe6ebe6a34abe5f74369", "committedDate": "2020-06-26T18:22:33Z", "message": "Update doc/sphinx-guides/source/installation/advanced.rst\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e42aec35af9795ec43aa758a8b988612f3148d1", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/9e42aec35af9795ec43aa758a8b988612f3148d1", "committedDate": "2020-06-26T18:23:52Z", "message": "Update scripts/zipdownload/README.md\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6100ed62d70621b4e453161e125fe15d9b72a106", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/6100ed62d70621b4e453161e125fe15d9b72a106", "committedDate": "2020-06-26T18:31:25Z", "message": "style/grammar #6505"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aaaa035bf9411758a32cb2bcafb7fb7038295f0f", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/aaaa035bf9411758a32cb2bcafb7fb7038295f0f", "committedDate": "2020-06-26T18:37:35Z", "message": "Update scripts/zipdownload/README.md\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5cca50151d3c0a4c93679858e4c8a611cc2fee3", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/c5cca50151d3c0a4c93679858e4c8a611cc2fee3", "committedDate": "2020-06-26T18:39:04Z", "message": "Update scripts/zipdownload/README.md\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d4b83fb1755b1d0c839298e878c6bddc77cda0f", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/1d4b83fb1755b1d0c839298e878c6bddc77cda0f", "committedDate": "2020-06-26T18:39:38Z", "message": "Update scripts/zipdownload/README.md\n\nCo-authored-by: Philip Durbin <philip_durbin@harvard.edu>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NTI4MzAw", "url": "https://github.com/IQSS/dataverse/pull/6986#pullrequestreview-438528300", "createdAt": "2020-06-26T19:00:39Z", "commit": {"oid": "1d4b83fb1755b1d0c839298e878c6bddc77cda0f"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxOTowMDozOVrOGprkLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxOTowMDozOVrOGprkLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1ODU3NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The location of the \"Standalone Zipper\" service. If this option is specified, Dataverse will be redirecing bulk/mutli-file zip download requests to that location, instead of serving them internally. See the \"Advanced\" section of the Installation guide for information on how to installe the external zipper. (This is still an experimental feature, as of v5.0). \n          \n          \n            \n            The location of the \"Standalone Zipper\" service. If this option is specified, Dataverse will be redirecing bulk/mutli-file zip download requests to that location, instead of serving them internally. See the \"Advanced\" section of the Installation guide for information on how to install the external zipper. (This is still an experimental feature, as of v5.0).", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r446358574", "createdAt": "2020-06-26T19:00:39Z", "author": {"login": "pdurbin"}, "path": "doc/sphinx-guides/source/installation/config.rst", "diffHunk": "@@ -2117,3 +2117,16 @@ Unlike other facets, those indexed by Date/Year are sorted chronologically by de\n If you don\u2019t want date facets to be sorted chronologically, set:\n \n ``curl -X PUT -d 'false' http://localhost:8080/api/admin/settings/:ChronologicalDateFacets``\n+\n+:CustomZipDownloadServiceUrl\n+++++++++++++++++++++++++++++\n+\n+The location of the \"Standalone Zipper\" service. If this option is specified, Dataverse will be redirecing bulk/mutli-file zip download requests to that location, instead of serving them internally. See the \"Advanced\" section of the Installation guide for information on how to installe the external zipper. (This is still an experimental feature, as of v5.0). ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d4b83fb1755b1d0c839298e878c6bddc77cda0f"}, "originalPosition": 8}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d34eccaccc5411d115fb23ec0605b537d8821205", "author": {"user": {"login": "pdurbin", "name": "Philip Durbin"}}, "url": "https://github.com/IQSS/dataverse/commit/d34eccaccc5411d115fb23ec0605b537d8821205", "committedDate": "2020-06-26T19:03:00Z", "message": "typo"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ1MDAxODA0", "url": "https://github.com/IQSS/dataverse/pull/6986#pullrequestreview-445001804", "createdAt": "2020-07-08T17:54:48Z", "commit": {"oid": "d34eccaccc5411d115fb23ec0605b537d8821205"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxNzo1NDo0OFrOGuzKJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxNzo1NDo0OFrOGuzKJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTcyNTg2MQ==", "bodyText": "@landreev Sorry to drop $0.02 of code review in here while it's in QA...\n\nthis sentence should start with an uppercase \"To...\"\nthere only needs to be one colon : after \"...Dataverse:\"\nthe curl command in the next line, below is missing the double-`` formatting -- see curl commands examples you included in config.rst in this same PR.", "url": "https://github.com/IQSS/dataverse/pull/6986#discussion_r451725861", "createdAt": "2020-07-08T17:54:48Z", "author": {"login": "mheppler"}, "path": "doc/sphinx-guides/source/installation/advanced.rst", "diffHunk": "@@ -35,3 +35,48 @@ If you have successfully installed multiple app servers behind a load balancer y\n You would repeat the steps above for all of your app servers. If users seem to be having a problem with a particular server, you can ask them to visit https://dataverse.example.edu/host.txt and let you know what they see there (e.g. \"server1.example.edu\") to help you know which server to troubleshoot.\n \n Please note that :ref:`network-ports` under the Configuration section has more information on fronting your app server with Apache. The :doc:`shibboleth` section talks about the use of ``ProxyPassMatch``.\n+\n+Optional Components\n+-------------------\n+\n+Standalone \"Zipper\" Service Tool\n+++++++++++++++++++++++++++++++++\n+\n+As of Dataverse v5.0 we offer an experimental optimization for the\n+multi-file, download-as-zip functionality. If this option\n+(``:CustomZipDownloadServiceUrl``) is enabled, instead of enforcing\n+the size limit on multi-file zipped downloads (as normally specified\n+by the option ``:ZipDownloadLimit``), we attempt to serve all the\n+files that the user requested (that they are authorized to download),\n+but the request is redirected to a standalone zipper service running\n+as a cgi-bin executable under Apache. Thus moving these potentially\n+long-running jobs completely outside the Application Server (Payara);\n+and preventing worker threads from becoming locked serving them. Since\n+zipping is also a CPU-intensive task, it is possible to have this\n+service running on a different host system, freeing the cycles on the\n+main Application Server. (The system running the service needs to have\n+access to the database as well as to the storage filesystem, and/or S3\n+bucket).\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5\n+source tree for more information. \n+\n+To install: You can follow the instructions in the file above to build\n+``ZipDownloadService-v1.0.0.jar``. It will also be available, pre-built as part of the Dataverse release on GitHub. Copy it, together with the shell\n+script scripts/zipdownload/cgi-bin/zipdownload to the cgi-bin\n+directory of the chosen Apache server (/var/www/cgi-bin standard).\n+You may need to make extra Apache configuration changes to make sure /cgi-bin/zipdownload is accessible from the outside.\n+For example, if this is the same Apache that's in front of your Dataverse Payara instance, you will need to add another pass through statement to your configuration:\n+\n+``ProxyPassMatch ^/cgi-bin/zipdownload !``\n+\n+Edit the config lines in the shell script (zipdownload) to configure\n+database access credentials. Do note that the executable does not need\n+access to the entire Dataverse database. A security-conscious admin\n+can create a dedicated database user with access to just one table:\n+``CUSTOMZIPSERVICEREQUEST``.\n+\n+to activate in Dataverse::", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d34eccaccc5411d115fb23ec0605b537d8821205"}, "originalPosition": 45}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f580a80b5cf84c8eda824dd84d13aa351ee24e1c", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/f580a80b5cf84c8eda824dd84d13aa351ee24e1c", "committedDate": "2020-07-08T19:54:46Z", "message": "renamed the flyway script. #6505"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0e0a4522efadc5afcf2828bf293d80d22915e90", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/e0e0a4522efadc5afcf2828bf293d80d22915e90", "committedDate": "2020-07-08T19:57:00Z", "message": "Merge branch 'develop' into 6505-optimize-zip-downloads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fcdfac60444b4fc42e8e12bbd087de0ea1fc7eb", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/2fcdfac60444b4fc42e8e12bbd087de0ea1fc7eb", "committedDate": "2020-07-08T19:57:50Z", "message": "Merge branch '6505-optimize-zip-downloads' of https://github.com/IQSS/dataverse into 6505-optimize-zip-downloads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "757a1207524275da1874f96858864e68c046c134", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/757a1207524275da1874f96858864e68c046c134", "committedDate": "2020-07-09T19:45:55Z", "message": "added support for multiple file stores (#6505)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f2bf94579f4afeca5ef9edd499955b04888fdbf", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/7f2bf94579f4afeca5ef9edd499955b04888fdbf", "committedDate": "2020-07-09T20:07:12Z", "message": "extra words in the doc #6505"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2553845b3df2c2752b652c2439effd55db9041d8", "author": {"user": {"login": "landreev", "name": null}}, "url": "https://github.com/IQSS/dataverse/commit/2553845b3df2c2752b652c2439effd55db9041d8", "committedDate": "2020-07-14T17:40:12Z", "message": "Fixed the chunking encoding error, that was preventing the download from working in some browsers. (#6505)"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 858, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}