{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYzNzEzNDI5", "number": 7164, "reviewThreads": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozNzo0NFrOEV91Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMToyNTowMVrOEYPefg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY4NTc4OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozNzo0NFrOG9Bl2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozNzo0NFrOG9Bl2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MjM5NQ==", "bodyText": "I would consider moving Primefaces down in the list, even to the bottom. End users don't care, neither do sysadmins (who will need to care about Payara).", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466642395", "createdAt": "2020-08-06T19:37:44Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDcwNTQyOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0Mzo0N1rOG9Bxyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzoyNDowM1rOG9Zs7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTQ1MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n          \n          \n            \n            Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n          \n      \n    \n    \n  \n\nNow that pull request #7064 has been merged, it's not possible to run Dataverse 5 on Glassfish 4 without hacking on your pom.xml file (@mheppler may be able to verify this).\nSo, I would simply remove the line that suggests that it's possible to stay on Glassfish.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466645451", "createdAt": "2020-08-06T19:43:47Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzNzQyMg==", "bodyText": "Will remove", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467037422", "createdAt": "2020-08-07T13:24:03Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTQ1MQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDcxNzMwOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0NzowNlrOG9B4_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0NzowNlrOG9B4_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NzI5NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n          \n          \n            \n            The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466647295", "createdAt": "2020-08-06T19:47:06Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDcyNTY2OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0OTozNFrOG9B96A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzoyNToxN1rOG9Zv5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0ODU1Mg==", "bodyText": "\"Dataset and File Redesign\" as a proper name seems odd to me. Maybe \"the Dataset and File Redesign project\"?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466648552", "createdAt": "2020-08-06T19:49:34Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzODE4MA==", "bodyText": "Yes", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467038180", "createdAt": "2020-08-07T13:25:17Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0ODU1Mg=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDczMjc4OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo1MToyMlrOG9CB6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzoyNTo0OVrOG9ZxJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0OTU3OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n          \n          \n            \n            A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n          \n      \n    \n    \n  \n\nCurb your enthusiasm. \ud83d\ude04", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466649579", "createdAt": "2020-08-06T19:51:22Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzODUwMg==", "bodyText": "I was trying to channel @poikilotherm here!", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467038502", "createdAt": "2020-08-07T13:25:49Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0OTU3OQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDc3MzgzOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDowNDo0NFrOG9CbKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDowNDo0NFrOG9CbKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1NjA0MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n          \n          \n            \n            Users can now more easily download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n          \n      \n    \n    \n  \n\nAlso, I wonder if \"Dataverse Installation Administrators\" will grow on me. I guess it's fine. You could probably also say \"you\". Above we say \"your installation\".", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466656041", "createdAt": "2020-08-06T20:04:44Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDc5NjAwOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxMjoxMFrOG9Co4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNDo1NDo0MFrOG9dDdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1OTU1Mw==", "bodyText": "Wait, how is this \"Download All Option on the Dataset Page\" item different from the \"Download Dataset\" item, just above it? Should they be merged into one? If not, what issue numbers are we talking about?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466659553", "createdAt": "2020-08-06T20:12:10Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0OTUwMg==", "bodyText": "\"Download Dataset\" is a higher level heading and is an umbrella for both the Download All through the UI and the Download all through the API. I wanted to get in a plug for the zipper service here as well", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467049502", "createdAt": "2020-08-07T13:44:34Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1OTU1Mw=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5MjM0Mg==", "bodyText": "Ah, umbrella term and a higher level heading. Got it.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467092342", "createdAt": "2020-08-07T14:54:40Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1OTU1Mw=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDgwOTE4OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxNjozMVrOG9CxRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxNjozMVrOG9CxRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2MTcwMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Find all the database id of the files.\n          \n          \n            \n            - Find all the database ids of the files.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466661701", "createdAt": "2020-08-06T20:16:31Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDgxMjE2OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxNzoyOVrOG9CzLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxNzoyOVrOG9CzLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2MjE5MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n          \n          \n            \n            Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported and like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466662190", "createdAt": "2020-08-06T20:17:29Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDgyNzc0OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoyMjowOFrOG9C8fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoyMjowOFrOG9C8fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2NDU3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            #### Download All API\n          \n          \n            \n            #### Download All By Dataset API\n          \n      \n    \n    \n  \n\nI'm not sure what to call it but here's how we document it:", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466664574", "createdAt": "2020-08-06T20:22:08Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDg0NTA3OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoyNzozOVrOG9DHGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoyNzozOVrOG9DHGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2NzI5MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            XXXXX\n          \n          \n            \n            \n          \n          \n            \n            (my plan is to build the executable and to add it to the v5\n          \n          \n            \n            release files on github: - L.A.)\n          \n          \n            \n            <https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n          \n          \n            \n            <https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>\n          \n      \n    \n    \n  \n\nLet's not forget to actually do this. \ud83d\ude04", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466667290", "createdAt": "2020-08-06T20:27:39Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDkwMDQ0OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo0NTowOFrOG9DpDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo0NTowOFrOG9DpDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY3NTk4MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n          \n          \n            \n            Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the reservation (and publication, the switch to \"findable\") taking place with DataCite at publish time. This allows the DOI to be reserved earlier in the publication process.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466675981", "createdAt": "2020-08-06T20:45:08Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDkwODg3OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo0Nzo0OFrOG9DuKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo0Nzo0OFrOG9DuKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY3NzI4OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n          \n          \n            \n            - Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466677288", "createdAt": "2020-08-06T20:47:48Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDkzNDcyOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1NTo1NlrOG9D-HA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1NTo1NlrOG9D-HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4MTM3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n          \n          \n            \n            For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level persistent ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466681372", "createdAt": "2020-08-06T20:55:56Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk0MDMxOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1Nzo1MFrOG9EBow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1Nzo1MFrOG9EBow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4MjI3NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n          \n          \n            \n            Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466682275", "createdAt": "2020-08-06T20:57:50Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk0NDg1OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1OToxOFrOG9EEaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1OToxOFrOG9EEaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Mjk4NQ==", "bodyText": "Just noting that Title Case isn't used here. I see another instance of this below. Not sure how much we care.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466682985", "createdAt": "2020-08-06T20:59:18Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk1NTA1OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowMjozNVrOG9EKyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNDo1NzozNlrOG9dKYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NDYxOQ==", "bodyText": "Just noting that this sounds a little scary:\n\"Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.\"\nMaybe we could end with \"If you don't need this setting, don't set it.\"", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466684619", "createdAt": "2020-08-06T21:02:35Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY5MTQ0MA==", "bodyText": "Or 'see guide for proper use and details about the security concern.'", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466691440", "createdAt": "2020-08-06T21:17:45Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NDYxOQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NDExMw==", "bodyText": "Ah, I didn't realize there's more detail in the guides about the security concern. Sure, mentioning this would be good. Or we could remove the scary line and let people discover it in the guides. Less is more.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467094113", "createdAt": "2020-08-07T14:57:36Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NDYxOQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk3MDY0OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowNzo1N1rOG9EUkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowNzo1N1rOG9EUkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NzEyMg==", "bodyText": ":FileValidationOnPublishEnabled is a database setting, not a JVM option, so it should be moved down there.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466687122", "createdAt": "2020-08-06T21:07:57Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk3NTQ5OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowOTozNlrOG9EXaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNDozNzoyMlrOG9cZvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Nzg0OQ==", "bodyText": "@djbrooke - FWIW:  #7116 changes this but I see it didn't get tagged as 5.0 - as is there are two options that both point to the same URL:  doi.mdcbaseurlstring and :doi.baseurlstringnext.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466687849", "createdAt": "2020-08-06T21:09:36Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA2MTU0NQ==", "bodyText": "@qqmyers @scolapasta what is the preferred resolution here? Should #7116 be tagged as 5.0 or no?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467061545", "createdAt": "2020-08-07T14:04:29Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Nzg0OQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA2NTQ4NQ==", "bodyText": "I would recommend that - to avoid having duplicate jvm options in 5.0 that would then disappear.\nIf #7116 is a concern because it changes build scripts, a more minimalist approach would just be to replace uses of the newly created doi.baseurlstringnext with doi:mdcbasurlstring (and maybe note that mdcbaseurlstring must be set/is no longer optional/just for mdc) and then use an update to #7116 to give that jvm option a more neutral/correct name.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467065485", "createdAt": "2020-08-07T14:11:07Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Nzg0OQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA4MTY2MQ==", "bodyText": "Thanks @qqmyers I'll tag it as 5.0 and we can accept any feedback in that PR. I see there's a release note file as part of the changes in the PR, so it'll force us to revisit the correct phrasing here. :)\ncc: @pdurbin as I saw you brought this up on a Slack thread as well.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467081661", "createdAt": "2020-08-07T14:37:22Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Nzg0OQ=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk3NTY2OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowOTozOVrOG9EXfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowOTozOVrOG9EXfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Nzg3MQ==", "bodyText": "\"Application Service\" sounds weird to me. Maybe \"application server\"? Perhaps the idea is that there can be multiple application servers forming a service and the zipper can live outside all of that. Maybe just say \"the main application\"? I don't know.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466687871", "createdAt": "2020-08-06T21:09:39Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+- :http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main Application Service where zip downloads should be directed (instead of /api/access/datafiles/).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk4MDA4OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMTowNVrOG9EaGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMTowNVrOG9EaGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4ODUzNg==", "bodyText": "This is a setting not a jvm option", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466688536", "createdAt": "2020-08-06T21:11:05Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk4Mjg0OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMTo1N1rOG9EbqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMTo1N1rOG9EbqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4ODkzNw==", "bodyText": "After reading the first line I'm wondering how to update my custom analytics code.\nThe second line gives the link but perhaps these could be woven together a bit more smoothly.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466688937", "createdAt": "2020-08-06T21:11:57Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+- :http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main Application Service where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+We have updated the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDk4NDA5OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMjoxOFrOG9EcZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMjoxOFrOG9EcZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4OTEyNA==", "bodyText": "This is the other non Title Case I mentioned above.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466689124", "createdAt": "2020-08-06T21:12:18Z", "author": {"login": "pdurbin"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+- :http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main Application Service where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+We have updated the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking users' IP addresses behind an address-masking proxy", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNTAwMDQwOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxODoxNlrOG9EmPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMzo1NzozMlrOG9a7PA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY5MTY0Ng==", "bodyText": "THese jvm options also don't get a leading ':' like settings do they?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r466691646", "createdAt": "2020-08-06T21:18:16Z", "author": {"login": "qqmyers"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA1NzQ2OA==", "bodyText": "Only in @djbrooke's release note world. I'll update.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r467057468", "createdAt": "2020-08-07T13:57:32Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,313 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also be redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable eagerly anticipated upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5. If your installation upgrades to Dataverse 5 without upgrading to Payara, the first step of any troubleshooting will be to suggest an upgrade to Payara.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+### Download Dataset\n+\n+Users can now more easily Download all files in Dataset. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database id of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported like with the \"download metadata\" API you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0).\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+XXXXX\n+\n+(my plan is to build the executable and to add it to the v5\n+release files on github: - L.A.)\n+<https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip>.\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse Installations using DataCite will have DOIs reserved as \"Draft\" with DataCite when the Dataset is created, instead of the reservation taking place locally at create time and the reservation taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+For datasets with large numbers of files, this validation will be performed asynchronously, using the same mechanism as for the registration of the file-level global ids. The cutoff number of files is configured by the same database setting. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Config section of the Installation guide for more info.\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. (the fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database).\n+\n+### The setting :PIDAsynchRegFileCount is deprecated as of v5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- :doi.baseurlstringnext: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- :dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY5MTY0Ng=="}, "originalCommit": {"oid": "b7c947a6381a1bef87cead90f7a169798aeb2de6"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODUwMzc2OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMToxMToxOVrOHAd1Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxMjoyNjozMlrOHAys0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1MDc2Mg==", "bodyText": "crap, I've typed, and lost a comment here twice in a row... there may be something wrong with this paragraph: it appears to be saying \"dataverse will be reserving the DOI at create time, instead of reserving it at create time\"? - maybe not (it says \"locally\" in the second part?) - but there may be simply TMI in it. I would leave the technical details (about \"findable\" and \"draft\" etc.) in the upgrade instruction further down; but only say\n\"Dataverse installations using DataCite will be able to reserve the persistent identifiers for datasets ahead of publishing time.\" - or something along these lines - in this part of the release note?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470250762", "createdAt": "2020-08-13T21:11:19Z", "author": {"login": "landreev"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d1572deb7e8d2b20315436f011434a5deda443"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDU5MjcyMQ==", "bodyText": "Updated text in 2ec68b3\nI agree that at this point in the doc (release highlights) we can be less technical.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470592721", "createdAt": "2020-08-14T12:26:32Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1MDc2Mg=="}, "originalCommit": {"oid": "c0d1572deb7e8d2b20315436f011434a5deda443"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODUzNTM5OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMToyMDo0N1rOHAeH7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxMjozMDo0MVrOHAy0KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1NTU5OQ==", "bodyText": "Not sure why it was needed, mentioning the Xmx and Xms lines in this upgrade instruction. I mean, it's an existing installation - they must have these settings in their config already. I can see how this can confuse somebody.\nI would simply remove this paragraph and the last two jvm-options lines.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470255599", "createdAt": "2020-08-13T21:20:47Z", "author": {"login": "landreev"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Required\n+\n+If you are using DataCite as your DOI provider you must add a new JVM option called \"doi.dataciterestapiurlstring\" with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. More information about this JVM option can be found in the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/).\n+\n+\"doi.mdcbaseurlstring\" should be deleted if it was previously set.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Terms of Use Display Updates\n+\n+In this release we\u2019ve fixed an issue that would cause the Application Terms of Use to not display when the user's language is set to a language that does not match one of the languages for which terms were created and registered for that Dataverse installation. Instead of the expected Terms of Use, users signing up could receive the \u201cThere are no Terms of Use for this Dataverse installation\u201d message. This could potentially result in some users signing up for an account without having the proper Terms of Use displayed. This will only affect installations that use the :ApplicationTermsOfUse setting.\n+\n+Please note that there is not currently a native workflow in Dataverse to display updated Terms of Use to a user or to force re-agreement. This would only potentially affect users that have signed up since the upgrade to 4.17 (or a following release if 4.17 was skipped).\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+This validation will be performed asynchronously, the same way as the registration of the file-level persistent ids. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database.\n+\n+### The Setting :PIDAsynchRegFileCount is Deprecated as of 5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- doi.dataciterestapiurlstring: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address. See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst) for more information about proper use and security concerns.\n+- http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main application server where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+For more information, see the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking Users' IP Addresses Behind an Address-Masking Proxy\n+\n+It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See \"Recording User IP Addresses\" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+### Reload Astrophysics Metadata Block (if used)\n+\n+Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:\n+\n+`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+We've included this in the step-by-step instructions below.\n+\n+### Run ReExportall\n+\n+We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)\n+\n+We've included this in the step-by-step instructions below.\n+\n+## Notes for Tool Developers and Integrators\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)\n+\n+## Upgrade Dataverse from Glassfish 4.1 to Payara 5\n+\n+The instructions below describe the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data.\n+\n+1. Download Payara, v5.2020.2 as of this writing:\n+\n+   `curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip`\n+   `sha256sum payara-5.2020.2.zip`\n+      1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e\n+\n+2. Unzip it somewhere (/usr/local is a safe bet)\n+\n+   `sudo unzip payara-5.2020.2.zip -d /usr/local/`\n+\n+3. Copy the Postgres driver to /usr/local/payara5/glassfish/lib\n+\n+   `sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/`\n+\n+4. Move payara5/glassfish/domains/domain1 out of the way\n+\n+   `sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig`\n+\n+5. Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin list-applications`\n+   `sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20`\n+\n+6. Stop Glassfish; copy domain1 to Payara\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin stop-domain`\n+   `sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/`\n+\n+7. Remove the Glassfish cache directories\n+\n+   `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/`  `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/`\n+\n+8. In domain.xml:\n+\n+ Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.\n+\n+   <jvm-options>-XX:MetaspaceSize=256m</jvm-options>\n+   <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>\n+\n+Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher:\n+\n+   <jvm-options>-Xmx2048m</jvm-options>\n+   <jvm-options>-Xms2048m</jvm-options>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e96567fedf52577532676e0f3d6597354725f65"}, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDU5NDYwMQ==", "bodyText": "@landreev I removed this in c593f89\nheads up to @donsizemore in case this was included for some other reason we're not seeing", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470594601", "createdAt": "2020-08-14T12:30:41Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Required\n+\n+If you are using DataCite as your DOI provider you must add a new JVM option called \"doi.dataciterestapiurlstring\" with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. More information about this JVM option can be found in the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/).\n+\n+\"doi.mdcbaseurlstring\" should be deleted if it was previously set.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Terms of Use Display Updates\n+\n+In this release we\u2019ve fixed an issue that would cause the Application Terms of Use to not display when the user's language is set to a language that does not match one of the languages for which terms were created and registered for that Dataverse installation. Instead of the expected Terms of Use, users signing up could receive the \u201cThere are no Terms of Use for this Dataverse installation\u201d message. This could potentially result in some users signing up for an account without having the proper Terms of Use displayed. This will only affect installations that use the :ApplicationTermsOfUse setting.\n+\n+Please note that there is not currently a native workflow in Dataverse to display updated Terms of Use to a user or to force re-agreement. This would only potentially affect users that have signed up since the upgrade to 4.17 (or a following release if 4.17 was skipped).\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+This validation will be performed asynchronously, the same way as the registration of the file-level persistent ids. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database.\n+\n+### The Setting :PIDAsynchRegFileCount is Deprecated as of 5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- doi.dataciterestapiurlstring: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address. See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst) for more information about proper use and security concerns.\n+- http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main application server where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+For more information, see the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking Users' IP Addresses Behind an Address-Masking Proxy\n+\n+It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See \"Recording User IP Addresses\" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+### Reload Astrophysics Metadata Block (if used)\n+\n+Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:\n+\n+`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+We've included this in the step-by-step instructions below.\n+\n+### Run ReExportall\n+\n+We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)\n+\n+We've included this in the step-by-step instructions below.\n+\n+## Notes for Tool Developers and Integrators\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)\n+\n+## Upgrade Dataverse from Glassfish 4.1 to Payara 5\n+\n+The instructions below describe the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data.\n+\n+1. Download Payara, v5.2020.2 as of this writing:\n+\n+   `curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip`\n+   `sha256sum payara-5.2020.2.zip`\n+      1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e\n+\n+2. Unzip it somewhere (/usr/local is a safe bet)\n+\n+   `sudo unzip payara-5.2020.2.zip -d /usr/local/`\n+\n+3. Copy the Postgres driver to /usr/local/payara5/glassfish/lib\n+\n+   `sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/`\n+\n+4. Move payara5/glassfish/domains/domain1 out of the way\n+\n+   `sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig`\n+\n+5. Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin list-applications`\n+   `sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20`\n+\n+6. Stop Glassfish; copy domain1 to Payara\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin stop-domain`\n+   `sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/`\n+\n+7. Remove the Glassfish cache directories\n+\n+   `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/`  `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/`\n+\n+8. In domain.xml:\n+\n+ Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.\n+\n+   <jvm-options>-XX:MetaspaceSize=256m</jvm-options>\n+   <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>\n+\n+Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher:\n+\n+   <jvm-options>-Xmx2048m</jvm-options>\n+   <jvm-options>-Xms2048m</jvm-options>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1NTU5OQ=="}, "originalCommit": {"oid": "9e96567fedf52577532676e0f3d6597354725f65"}, "originalPosition": 260}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODU0NTEzOnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMToyNDowNlrOHAeN5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxMjozOTo0MFrOHAzDvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1NzEyNg==", "bodyText": "Since there is already a detailed section for this step, above - does it need to be here, in the upgrade-to-payara instruction - probably not?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470257126", "createdAt": "2020-08-13T21:24:06Z", "author": {"login": "landreev"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Required\n+\n+If you are using DataCite as your DOI provider you must add a new JVM option called \"doi.dataciterestapiurlstring\" with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. More information about this JVM option can be found in the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/).\n+\n+\"doi.mdcbaseurlstring\" should be deleted if it was previously set.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Terms of Use Display Updates\n+\n+In this release we\u2019ve fixed an issue that would cause the Application Terms of Use to not display when the user's language is set to a language that does not match one of the languages for which terms were created and registered for that Dataverse installation. Instead of the expected Terms of Use, users signing up could receive the \u201cThere are no Terms of Use for this Dataverse installation\u201d message. This could potentially result in some users signing up for an account without having the proper Terms of Use displayed. This will only affect installations that use the :ApplicationTermsOfUse setting.\n+\n+Please note that there is not currently a native workflow in Dataverse to display updated Terms of Use to a user or to force re-agreement. This would only potentially affect users that have signed up since the upgrade to 4.17 (or a following release if 4.17 was skipped).\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+This validation will be performed asynchronously, the same way as the registration of the file-level persistent ids. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database.\n+\n+### The Setting :PIDAsynchRegFileCount is Deprecated as of 5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- doi.dataciterestapiurlstring: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address. See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst) for more information about proper use and security concerns.\n+- http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main application server where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+For more information, see the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking Users' IP Addresses Behind an Address-Masking Proxy\n+\n+It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See \"Recording User IP Addresses\" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+### Reload Astrophysics Metadata Block (if used)\n+\n+Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:\n+\n+`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+We've included this in the step-by-step instructions below.\n+\n+### Run ReExportall\n+\n+We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)\n+\n+We've included this in the step-by-step instructions below.\n+\n+## Notes for Tool Developers and Integrators\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)\n+\n+## Upgrade Dataverse from Glassfish 4.1 to Payara 5\n+\n+The instructions below describe the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data.\n+\n+1. Download Payara, v5.2020.2 as of this writing:\n+\n+   `curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip`\n+   `sha256sum payara-5.2020.2.zip`\n+      1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e\n+\n+2. Unzip it somewhere (/usr/local is a safe bet)\n+\n+   `sudo unzip payara-5.2020.2.zip -d /usr/local/`\n+\n+3. Copy the Postgres driver to /usr/local/payara5/glassfish/lib\n+\n+   `sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/`\n+\n+4. Move payara5/glassfish/domains/domain1 out of the way\n+\n+   `sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig`\n+\n+5. Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin list-applications`\n+   `sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20`\n+\n+6. Stop Glassfish; copy domain1 to Payara\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin stop-domain`\n+   `sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/`\n+\n+7. Remove the Glassfish cache directories\n+\n+   `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/`  `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/`\n+\n+8. In domain.xml:\n+\n+ Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.\n+\n+   <jvm-options>-XX:MetaspaceSize=256m</jvm-options>\n+   <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>\n+\n+Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher:\n+\n+   <jvm-options>-Xmx2048m</jvm-options>\n+   <jvm-options>-Xms2048m</jvm-options>\n+\n+Add the below JVM options beneath the -Ddataverse settings:  \n+\n+   <jvm-options>-Dfish.payara.classloading.delegate=false</jvm-options>\n+   <jvm-options>-XX:+UseG1GC</jvm-options>\n+   <jvm-options>-XX:+UseStringDeduplication</jvm-options>\n+   <jvm-options>-XX:+DisableExplicitGC</jvm-options>\n+\n+9. Change any full pathnames /usr/local/glassfish4/... to /usr/local/payara5/... or whatever it is in your case. (Specifically check the -Ddataverse.files.directory and -Ddataverse.files.file.directory JVM options)\n+\n+10. In domain1/config/jhove.conf, change the hard-coded /usr/local/glassfish4 path, as above.\n+\n+(Optional): If you renamed your service account from glassfish to payara or appserver, update the ownership permissions. The Installation Guide recommends a service account of `dataverse`:\n+\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/domains/domain1`\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/lib`\n+\n+11. You will also need to check that the service account has write permission on the files directory, if they are located outside the old Glassfish domain. And/or make sure the service account has the correct AWS credentials, if you are using S3 for storage.\n+\n+12. Finally, start Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+13. Deploy the Dataverse 5 warfile:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin deploy /path/to/dataverse-5.0.war`\n+\n+14. Then restart Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin stop-domain`\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+15. Update Astrophysics Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.0/astrophysics.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+16. (Recommended) Run ReExportall to update JSON Exports  \n+\n+   <http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api>\n+\n+17. For installations using DataCite, pre-register DOIs:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e96567fedf52577532676e0f3d6597354725f65"}, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDU5ODU5MA==", "bodyText": "We've done this previously, that is, added it in the notes above and called out and individual step.\nI added a (Recommended for installations using DataCite) at the start of the instruction. I also added a note for Add doi.dataciterestapiurlstring\nLet me know what you think following the updates in abc4d9b", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470598590", "createdAt": "2020-08-14T12:39:40Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Required\n+\n+If you are using DataCite as your DOI provider you must add a new JVM option called \"doi.dataciterestapiurlstring\" with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. More information about this JVM option can be found in the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/).\n+\n+\"doi.mdcbaseurlstring\" should be deleted if it was previously set.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Terms of Use Display Updates\n+\n+In this release we\u2019ve fixed an issue that would cause the Application Terms of Use to not display when the user's language is set to a language that does not match one of the languages for which terms were created and registered for that Dataverse installation. Instead of the expected Terms of Use, users signing up could receive the \u201cThere are no Terms of Use for this Dataverse installation\u201d message. This could potentially result in some users signing up for an account without having the proper Terms of Use displayed. This will only affect installations that use the :ApplicationTermsOfUse setting.\n+\n+Please note that there is not currently a native workflow in Dataverse to display updated Terms of Use to a user or to force re-agreement. This would only potentially affect users that have signed up since the upgrade to 4.17 (or a following release if 4.17 was skipped).\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+This validation will be performed asynchronously, the same way as the registration of the file-level persistent ids. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database.\n+\n+### The Setting :PIDAsynchRegFileCount is Deprecated as of 5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- doi.dataciterestapiurlstring: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address. See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst) for more information about proper use and security concerns.\n+- http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main application server where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+For more information, see the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking Users' IP Addresses Behind an Address-Masking Proxy\n+\n+It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See \"Recording User IP Addresses\" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+### Reload Astrophysics Metadata Block (if used)\n+\n+Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:\n+\n+`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+We've included this in the step-by-step instructions below.\n+\n+### Run ReExportall\n+\n+We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)\n+\n+We've included this in the step-by-step instructions below.\n+\n+## Notes for Tool Developers and Integrators\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)\n+\n+## Upgrade Dataverse from Glassfish 4.1 to Payara 5\n+\n+The instructions below describe the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data.\n+\n+1. Download Payara, v5.2020.2 as of this writing:\n+\n+   `curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip`\n+   `sha256sum payara-5.2020.2.zip`\n+      1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e\n+\n+2. Unzip it somewhere (/usr/local is a safe bet)\n+\n+   `sudo unzip payara-5.2020.2.zip -d /usr/local/`\n+\n+3. Copy the Postgres driver to /usr/local/payara5/glassfish/lib\n+\n+   `sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/`\n+\n+4. Move payara5/glassfish/domains/domain1 out of the way\n+\n+   `sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig`\n+\n+5. Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin list-applications`\n+   `sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20`\n+\n+6. Stop Glassfish; copy domain1 to Payara\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin stop-domain`\n+   `sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/`\n+\n+7. Remove the Glassfish cache directories\n+\n+   `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/`  `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/`\n+\n+8. In domain.xml:\n+\n+ Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.\n+\n+   <jvm-options>-XX:MetaspaceSize=256m</jvm-options>\n+   <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>\n+\n+Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher:\n+\n+   <jvm-options>-Xmx2048m</jvm-options>\n+   <jvm-options>-Xms2048m</jvm-options>\n+\n+Add the below JVM options beneath the -Ddataverse settings:  \n+\n+   <jvm-options>-Dfish.payara.classloading.delegate=false</jvm-options>\n+   <jvm-options>-XX:+UseG1GC</jvm-options>\n+   <jvm-options>-XX:+UseStringDeduplication</jvm-options>\n+   <jvm-options>-XX:+DisableExplicitGC</jvm-options>\n+\n+9. Change any full pathnames /usr/local/glassfish4/... to /usr/local/payara5/... or whatever it is in your case. (Specifically check the -Ddataverse.files.directory and -Ddataverse.files.file.directory JVM options)\n+\n+10. In domain1/config/jhove.conf, change the hard-coded /usr/local/glassfish4 path, as above.\n+\n+(Optional): If you renamed your service account from glassfish to payara or appserver, update the ownership permissions. The Installation Guide recommends a service account of `dataverse`:\n+\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/domains/domain1`\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/lib`\n+\n+11. You will also need to check that the service account has write permission on the files directory, if they are located outside the old Glassfish domain. And/or make sure the service account has the correct AWS credentials, if you are using S3 for storage.\n+\n+12. Finally, start Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+13. Deploy the Dataverse 5 warfile:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin deploy /path/to/dataverse-5.0.war`\n+\n+14. Then restart Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin stop-domain`\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+15. Update Astrophysics Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.0/astrophysics.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+16. (Recommended) Run ReExportall to update JSON Exports  \n+\n+   <http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api>\n+\n+17. For installations using DataCite, pre-register DOIs:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1NzEyNg=="}, "originalCommit": {"oid": "9e96567fedf52577532676e0f3d6597354725f65"}, "originalPosition": 302}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODU0ODQ2OnYy", "diffSide": "RIGHT", "path": "doc/release-notes/5.0-release-notes.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMToyNTowMVrOHAePzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxMjo0MTo1OVrOHAzILw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1NzYxNA==", "bodyText": "similarly, this is already explained in its own section above - not sure it needs to be repeated in the upgrade-to-payara instruction - ?", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470257614", "createdAt": "2020-08-13T21:25:01Z", "author": {"login": "landreev"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Required\n+\n+If you are using DataCite as your DOI provider you must add a new JVM option called \"doi.dataciterestapiurlstring\" with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. More information about this JVM option can be found in the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/).\n+\n+\"doi.mdcbaseurlstring\" should be deleted if it was previously set.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Terms of Use Display Updates\n+\n+In this release we\u2019ve fixed an issue that would cause the Application Terms of Use to not display when the user's language is set to a language that does not match one of the languages for which terms were created and registered for that Dataverse installation. Instead of the expected Terms of Use, users signing up could receive the \u201cThere are no Terms of Use for this Dataverse installation\u201d message. This could potentially result in some users signing up for an account without having the proper Terms of Use displayed. This will only affect installations that use the :ApplicationTermsOfUse setting.\n+\n+Please note that there is not currently a native workflow in Dataverse to display updated Terms of Use to a user or to force re-agreement. This would only potentially affect users that have signed up since the upgrade to 4.17 (or a following release if 4.17 was skipped).\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+This validation will be performed asynchronously, the same way as the registration of the file-level persistent ids. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database.\n+\n+### The Setting :PIDAsynchRegFileCount is Deprecated as of 5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- doi.dataciterestapiurlstring: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address. See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst) for more information about proper use and security concerns.\n+- http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main application server where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+For more information, see the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking Users' IP Addresses Behind an Address-Masking Proxy\n+\n+It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See \"Recording User IP Addresses\" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+### Reload Astrophysics Metadata Block (if used)\n+\n+Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:\n+\n+`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+We've included this in the step-by-step instructions below.\n+\n+### Run ReExportall\n+\n+We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)\n+\n+We've included this in the step-by-step instructions below.\n+\n+## Notes for Tool Developers and Integrators\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)\n+\n+## Upgrade Dataverse from Glassfish 4.1 to Payara 5\n+\n+The instructions below describe the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data.\n+\n+1. Download Payara, v5.2020.2 as of this writing:\n+\n+   `curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip`\n+   `sha256sum payara-5.2020.2.zip`\n+      1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e\n+\n+2. Unzip it somewhere (/usr/local is a safe bet)\n+\n+   `sudo unzip payara-5.2020.2.zip -d /usr/local/`\n+\n+3. Copy the Postgres driver to /usr/local/payara5/glassfish/lib\n+\n+   `sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/`\n+\n+4. Move payara5/glassfish/domains/domain1 out of the way\n+\n+   `sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig`\n+\n+5. Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin list-applications`\n+   `sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20`\n+\n+6. Stop Glassfish; copy domain1 to Payara\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin stop-domain`\n+   `sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/`\n+\n+7. Remove the Glassfish cache directories\n+\n+   `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/`  `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/`\n+\n+8. In domain.xml:\n+\n+ Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.\n+\n+   <jvm-options>-XX:MetaspaceSize=256m</jvm-options>\n+   <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>\n+\n+Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher:\n+\n+   <jvm-options>-Xmx2048m</jvm-options>\n+   <jvm-options>-Xms2048m</jvm-options>\n+\n+Add the below JVM options beneath the -Ddataverse settings:  \n+\n+   <jvm-options>-Dfish.payara.classloading.delegate=false</jvm-options>\n+   <jvm-options>-XX:+UseG1GC</jvm-options>\n+   <jvm-options>-XX:+UseStringDeduplication</jvm-options>\n+   <jvm-options>-XX:+DisableExplicitGC</jvm-options>\n+\n+9. Change any full pathnames /usr/local/glassfish4/... to /usr/local/payara5/... or whatever it is in your case. (Specifically check the -Ddataverse.files.directory and -Ddataverse.files.file.directory JVM options)\n+\n+10. In domain1/config/jhove.conf, change the hard-coded /usr/local/glassfish4 path, as above.\n+\n+(Optional): If you renamed your service account from glassfish to payara or appserver, update the ownership permissions. The Installation Guide recommends a service account of `dataverse`:\n+\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/domains/domain1`\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/lib`\n+\n+11. You will also need to check that the service account has write permission on the files directory, if they are located outside the old Glassfish domain. And/or make sure the service account has the correct AWS credentials, if you are using S3 for storage.\n+\n+12. Finally, start Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+13. Deploy the Dataverse 5 warfile:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin deploy /path/to/dataverse-5.0.war`\n+\n+14. Then restart Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin stop-domain`\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+15. Update Astrophysics Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.0/astrophysics.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+16. (Recommended) Run ReExportall to update JSON Exports  \n+\n+   <http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api>\n+\n+17. For installations using DataCite, pre-register DOIs:\n+\n+   `/api/pids/unreserved`  will report the ids of the datasets\n+   `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+   Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+18. Set up Standalone Zipper Service if desired. Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree for more information. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9e96567fedf52577532676e0f3d6597354725f65"}, "originalPosition": 321}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDU5OTcyNw==", "bodyText": "I added the experimental note in abc4d9b. I think it's fine to add this in two different instruction formats and it matches what we've done in previous notes but if you feel strongly I can revisit. I don't do the upgrades myself so if this sort of thing is not helpful to mention multiple times I'm fine to adjust it.", "url": "https://github.com/IQSS/dataverse/pull/7164#discussion_r470599727", "createdAt": "2020-08-14T12:41:59Z", "author": {"login": "djbrooke"}, "path": "doc/release-notes/5.0-release-notes.md", "diffHunk": "@@ -0,0 +1,321 @@\n+# Dataverse 5.0\n+\n+This release brings new features, enhancements, and bug fixes to Dataverse. Thank you to all of the community members who contributed code, suggestions, bug reports, and other assistance across the project.\n+\n+Please note that this is a major release and these are long release notes. We offer no apologies. :)\n+\n+## Release Highlights\n+\n+### Continued Dataset and File Redesign: Dataset and File Button Redesign, Responsive Layout\n+\n+The buttons available on the Dataset and File pages have been redesigned. This change is to provide more scalability for future expanded options for data access and exploration, and to provide a consistent experience between the two pages. The dataset and file pages have also been redesigned to be more responsive and function better across multiple devices.\n+\n+This is an important step in the incremental process of the Dataset and File Redesign project, following the release of on-page previews, filtering and sorting options, tree view, and other enhancements. Additional features in support of these redesign efforts will follow in later 5.x releases.\n+\n+### Payara 5\n+\n+A major upgrade of the application server provides security updates, access to new features like MicroProfile Config API, and will enable upgrades to other core technologies.\n+\n+Note that moving from Glassfish to Payara will be required as part of the move to Dataverse 5.\n+\n+### Download Dataset\n+\n+Users can now more easily download all files in Dataset through both the UI and API. If this causes server instability, it's suggested that Dataverse Installation Administrators take advantage of the new Standalone Zipper Service described below.\n+\n+#### Download All Option on the Dataset Page\n+\n+In previous versions of Dataverse, downloading all files from a dataset meant several clicks to select files and initiate the download. The Dataset Page now includes a Download All option for both the original and archival formats of the files in a dataset under the \"Access Dataset\" button.\n+\n+#### Download All Files in a Dataset by API\n+\n+In previous versions of Dataverse, downloading all files from a dataset via API was a two step process:\n+\n+- Find all the database ids of the files.\n+- Download all the files, using those ids (comma-separated).\n+\n+Now you can download all files from a dataset (assuming you have access to them) via API by passing the dataset persistent ID (PID such as DOI or Handle) or the dataset's database id. Versions are also supported, and you can pass :draft, :latest, :latest-published, or numbers (1.1, 2.0) similar to the \"download metadata\" API.\n+\n+### A Multi-File, Zipped Download Optimization\n+\n+In this release we are offering an experimental optimization for the multi-file, download-as-zip functionality. If this option is enabled, instead of enforcing size limits, we attempt to serve all the files that the user requested (that they are authorized to download), but the request is redirected to a standalone zipper service running as a cgi executable. Thus moving these potentially long-running jobs completely outside the Application Server (Payara); and preventing service threads from becoming locked serving them. Since zipping is also a CPU-intensive task, it is possible to have this service running on a different host system, thus freeing the cycles on the main Application Server. The system running the service needs to have access to the database as well as to the storage filesystem, and/or S3 bucket.\n+\n+Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree.\n+\n+The components of the standalone \"zipper tool\" can also be downloaded\n+here:\n+\n+https://github.com/IQSS/dataverse/releases/download/v5.0/zipper.zip\n+\n+### Updated File Handling\n+\n+Files without extensions can now be uploaded through the UI. This release also changes the way Dataverse handles duplicate (filename or checksum) files in a dataset. Specifically:\n+\n+- Files with the same checksum can be included in a dataset, even if the files are in the same directory.\n+- Files with the same filename can be included in a dataset as long as the files are in different directories.\n+- If a user uploads a file to a directory where a file already exists with that directory/filename combination, Dataverse will adjust the file path and names by adding \"-1\" or \"-2\" as applicable. This change will be visible in the list of files being uploaded.\n+- If the directory or name of an existing or newly uploaded file is edited in such a way that would create a directory/filename combination that already exists, Dataverse will display an error.\n+- If a user attempts to replace a file with another file that has the same checksum, an error message will be displayed and the file will not be able to be replaced.\n+- If a user attempts to replace a file with a file that has the same checksum as a different file in the dataset, a warning will be displayed.\n+- Files without extensions can now be uploaded through the UI.\n+\n+### Pre-Publish DOI Reservation with DataCite\n+\n+Dataverse installations using DataCite will have DOIs reserved as \"draft\" with DataCite when the Dataset is created, instead of the DOI reservation taking place locally at create time and the reservation and switch to \"findable\" taking place with DataCite at publish time. This allows the DOI to reserved earlier in the publication process.\n+\n+### Primefaces 8\n+\n+Primefaces, the open source UI framework upon which the Dataverse front end is built, has been updated to the most recent version. This provides security updates and bug fixes and will also allow Dataverse developers to take advantage of new features and enhancements.\n+\n+## Major Use Cases\n+\n+Newly-supported use cases in this release include:\n+\n+- Users will be presented with a new workflow around dataset and file access and exploration. (Issue #6684, PR #6909)\n+- Users will experience a UI appropriate across a variety of device sizes. (Issue #6684, PR #6909)\n+- Users will be able to download an entire dataset without needing to select all the files in that dataset. (Issue #6564, PR #6262)\n+- Users will be able to download all files in a dataset with a single API call. (Issue #4529, PR #7086)\n+- Users will have DOIs reserved for their datasets upon dataset create instead of at publish time. (Issue #5093, PR #6901)\n+- Users will be able to upload files without extensions. (Issue #6634, PR #6804)\n+- Users will be able to upload files with the same name in a dataset, as long as a those files are in different file paths. (Issue #4813, PR #6924)\n+- Users will be able to upload files with the same checksum in a dataset. (Issue #4813, PR #6924)\n+- Users will be less likely to encounter locks during the publishing process due to PID providers being unavailable. (Issue #6918, PR #7118)\n+- Users will now have their files validated during publish, and in the unlikely event that anything has happened to the files between deposit and publish, they will be able to take corrective action. (Issue #6558, PR #6790)\n+- Administrators will likely see more success with Harvesting, as many minor harvesting issues have been resolved. (Issues #7127, #7128, #4597, #7056, #7052, #7023, #7009, and #7003)\n+- Administrators can now enable an external zip service that frees up application server resources and allows the zip download limit to be increased. (Issue #6505, PR #6986)\n+- Administrators can now create groups based on users' email domains. (Issue #6936, PR #6974)\n+- Administrators can now set date facets to be organized chronologically. (Issue #4977, PR #6958)\n+- Administrators can now link harvested datasets using an API. (Issue #5886, PR #6935)\n+- Administrators can now destroy datasets with mapped shapefiles. (Issue #4093, PR #6860)\n+\n+## Notes for Dataverse Installation Administrators\n+\n+### Glassfish to Payara\n+\n+This upgrade requires a few extra steps. See the detailed upgrade instructions below.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Required\n+\n+If you are using DataCite as your DOI provider you must add a new JVM option called \"doi.dataciterestapiurlstring\" with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. More information about this JVM option can be found in the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/).\n+\n+\"doi.mdcbaseurlstring\" should be deleted if it was previously set.\n+\n+### Dataverse Installations Using DataCite: Upgrade Action Recommended\n+\n+For installations that are using DataCite, Dataverse v5.0 introduces a change in the process of registering the Persistent Identifier (DOI) for a dataset. Instead of registering it when the dataset is published for the first time, Dataverse will try to \"reserve\" the DOI when it's created (by registering it as a \"draft\", using DataCite terminology). When the user publishes the dataset, the DOI will be publicized as well (by switching the registration status to \"findable\"). This approach makes the process of publishing datasets simpler and less error-prone.\n+\n+New APIs have been provided for finding any unreserved DataCite-issued DOIs in your Dataverse, and for reserving them (see below). While not required - the user can still attempt to publish a dataset with an unreserved DOI - having all the identifiers reserved ahead of time is recommended. If you are upgrading an installation that uses DataCite, we specifically recommend that you reserve the DOIs for all your pre-existing unpublished drafts as soon as Dataverse v5.0 is deployed, since none of them were registered at create time. This can be done using the following API calls:  \n+\n+- `/api/pids/unreserved`  will report the ids of the datasets\n+- `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+See the [Native API Guide](http://guides.dataverse.org/en/5.0/api/native-api.html) for more information.\n+\n+Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+Going forward, once all the DOIs have been reserved for the legacy drafts, you may still get an occasional dataset with an unreserved identifier. DataCite service instability would be a potential cause. There is no reason to expect that to happen often, but it is not impossible. You may consider running the script above (perhaps with some extra diagnostics added) regularly, from a cron job or otherwise, to address this preemptively.\n+\n+### Terms of Use Display Updates\n+\n+In this release we\u2019ve fixed an issue that would cause the Application Terms of Use to not display when the user's language is set to a language that does not match one of the languages for which terms were created and registered for that Dataverse installation. Instead of the expected Terms of Use, users signing up could receive the \u201cThere are no Terms of Use for this Dataverse installation\u201d message. This could potentially result in some users signing up for an account without having the proper Terms of Use displayed. This will only affect installations that use the :ApplicationTermsOfUse setting.\n+\n+Please note that there is not currently a native workflow in Dataverse to display updated Terms of Use to a user or to force re-agreement. This would only potentially affect users that have signed up since the upgrade to 4.17 (or a following release if 4.17 was skipped).\n+\n+### Datafiles Validation when Publishing Datasets\n+\n+When a user requests to publish a dataset, Dataverse will now attempt to validate the physical files in the dataset, by recalculating the checksums and verifying them against the values in the database. The goal is to prevent any corrupted files in published datasets. Most of all the instances of actual damage to physical files that we've seen in the past happened while the datafiles were still in the Draft state. (Physical files become essentially read-only once published). So this is the logical place to catch any such issues.\n+\n+If any files in the dataset fail the validation, the dataset does not get published, and the user is notified that they need to contact their Dataverse support in order to address the issue before another attempt to publish can be made. See the \"Troubleshooting\" section of the Guide on how to fix such problems.\n+\n+This validation will be performed asynchronously, the same way as the registration of the file-level persistent ids. Similarly to the file PID registration, this validation process can be disabled on your system, with the setting `:FileValidationOnPublishEnabled`. (A Dataverse admin may choose to disable it if, for example, they are already running an external auditing system to monitor the integrity of the files in their Dataverse, and would prefer the publishing process to take less time). See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+Please note that we are not aware of any bugs in the current versions of Dataverse that would result in damage to users' files. But you may have some legacy files in your archive that were affected by some issue in the past, or perhaps affected by something outside Dataverse, so we are adding this feature out of abundance of caution. An example of a problem we've experienced in the early versions of Dataverse was a possible scenario where a user actually attempted to delete a Draft file from an unpublished version, where the database transaction would fail for whatever reason, but only after the physical file had already been deleted from the filesystem. Thus resulting in a datafile entry remaining in the dataset, but with the corresponding physical file missing. The fix for this case, since the user wanted to delete the file in the first place, is simply to confirm it and purge the datafile entity from the database.\n+\n+### The Setting :PIDAsynchRegFileCount is Deprecated as of 5.0\n+\n+It used to specify the number of datafiles in the dataset to warrant adding a lock during publishing. As of v5.0 all datasets get locked for the duration of the publishing process. The setting will be ignored if present.\n+\n+### Location Changes for Related Projects\n+\n+The dataverse-ansible and dataverse-previewers repositories have been moved to the GDCC Organization on GitHub. If you have been referencing the dataverse-ansible repository from IQSS and the dataverse-previewers from QDR, please instead use them from their new locations:\n+\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-ansible>\n+<https://github.com/GlobalDataverseCommunityConsortium/dataverse-previewers>\n+\n+### Harvesting Improvements\n+\n+Many updates have been made to address common Harvesting failures. You may see Harvests complete more often and have a higher success rate on a dataset-by-dataset basis.\n+\n+### New JVM Options and Database Settings\n+\n+Several new JVM options and DB Settings have been added in this release. More documentation about each of these settings can be found in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+#### New JVM Options\n+\n+- doi.dataciterestapiurlstring: Set with a value of \"https://api.datacite.org\" for production environments and \"https://api.test.datacite.org\" for test environments. Must be set if you are using DataCite as your DOI provider.\n+- dataverse.useripaddresssourceheader: If set, specifies an HTTP Header such as X-Forwarded-For to use to retrieve the user's IP address. This setting is useful in cases such as running Dataverse behind load balancers where the default option of getting the Remote Address from the servlet isn't correct (e.g. it would be the load balancer IP address). Note that unless your installation always sets the header you configure here, this could be used as a way to spoof the user's address. See the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst) for more information about proper use and security concerns.\n+- http.request-timeout-seconds: To facilitate large file upload and download, the Dataverse installer bumps the Payara **server-config.network-config.protocols.protocol.http-listener-1.http.request-timeout-seconds** setting from its default 900 seconds (15 minutes) to 1800 (30 minutes).\n+\n+#### New Database Settings\n+\n+- :CustomZipDownloadServiceUrl: If defined, this is the URL of the zipping service outside the main application server where zip downloads should be directed (instead of /api/access/datafiles/).\n+- :ShibAttributeCharacterSetConversionEnabled: By default, all attributes received from Shibboleth are converted from ISO-8859-1 to UTF-8. You can disable this behavior by setting to false.\n+- :ChronologicalDateFacets: Facets with Date/Year are sorted chronologically by default, with the most recent value first. To have them sorted by number of hits, e.g. with the year with the most results first, set this to false.\n+- :NavbarGuidesUrl: Set to a fully-qualified URL which will be used for the \"User Guide\" link in the navbar.\n+- :FileValidationOnPublishEnabled: Toggles validation of the physical files in the dataset when it's published, by recalculating the checksums and comparing against the values stored in the DataFile table. By default this setting is absent and Dataverse assumes it to be true. If enabled, the validation will be performed asynchronously, similarly to how we handle assigning persistent identifiers to datafiles, with the dataset locked for the duration of the publishing process.\n+\n+### Custom Analytics Code Changes\n+\n+You should update your custom analytics code to implement necessary changes for tracking updated dataset and file buttons. There was also a fix to the analytics code that will now properly track downloads for tabular files.\n+\n+For more information, see the documentation and sample analytics code snippet provided in [Installation Guide > Configuration > Web Analytics Code](http://guides.dataverse.org/en/5.0/installation/config.html#web-analytics-code) to reflect the changes implemented in this version (#6938/#6684).\n+\n+### Tracking Users' IP Addresses Behind an Address-Masking Proxy\n+\n+It is now possible to collect real user IP addresses in MDC logs and/or set up an IP group on a system running behind a proxy/load balancer that hides the addresses of incoming requests. See \"Recording User IP Addresses\" in the Configuration section of the [Installation Guide](http://guides.dataverse.org/en/5.0/installation/config.rst).\n+\n+### Reload Astrophysics Metadata Block (if used)\n+\n+Tooltips have been updated for the Astrophysics Metadata Block. If you'd like these updated Tooltips to be displayed to users of your installation, you should update the Astrophysics Metadata Block:\n+\n+`curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+We've included this in the step-by-step instructions below.\n+\n+### Run ReExportall\n+\n+We made changes to the JSON Export in this release. If you'd like these changes to reflected in your JSON exports, you should run ReExportall as part of the upgrade process following the steps in [Admin Guide](http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api)\n+\n+We've included this in the step-by-step instructions below.\n+\n+## Notes for Tool Developers and Integrators\n+\n+## Complete List of Changes\n+\n+For the complete list of code changes in this release, see the [5.0 Milestone](https://github.com/IQSS/dataverse/milestone/89?closed=1) in Github.\n+\n+For help with upgrading, installing, or general questions please post to the [Dataverse Google Group](https://groups.google.com/forum/#!forum/dataverse-community) or email support@dataverse.org.\n+\n+## Installation\n+\n+If this is a new installation, please see our [Installation Guide](http://guides.dataverse.org/en/5.0/installation/)\n+\n+## Upgrade Dataverse from Glassfish 4.1 to Payara 5\n+\n+The instructions below describe the upgrade procedure based on moving an existing glassfish4 domain directory under Payara. We recommend this method instead of setting up a brand-new Payara domain using the installer because it appears to be the easiest way to recreate your current configuration and preserve all your data.\n+\n+1. Download Payara, v5.2020.2 as of this writing:\n+\n+   `curl -L -O https://github.com/payara/Payara/releases/download/payara-server-5.2020.2/payara-5.2020.2.zip`\n+   `sha256sum payara-5.2020.2.zip`\n+      1f5f7ea30901b1b4c7bcdfa5591881a700c9b7e2022ae3894192ba97eb83cc3e\n+\n+2. Unzip it somewhere (/usr/local is a safe bet)\n+\n+   `sudo unzip payara-5.2020.2.zip -d /usr/local/`\n+\n+3. Copy the Postgres driver to /usr/local/payara5/glassfish/lib\n+\n+   `sudo cp /usr/local/glassfish4/glassfish/lib/postgresql-42.2.9.jar /usr/local/payara5/glassfish/lib/`\n+\n+4. Move payara5/glassfish/domains/domain1 out of the way\n+\n+   `sudo mv /usr/local/payara5/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/domain1.orig`\n+\n+5. Undeploy the Dataverse web application (if deployed; version 4.20 is assumed in the example below)\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin list-applications`\n+   `sudo /usr/local/glassfish4/bin/asadmin undeploy dataverse-4.20`\n+\n+6. Stop Glassfish; copy domain1 to Payara\n+\n+   `sudo /usr/local/glassfish4/bin/asadmin stop-domain`\n+   `sudo cp -ar /usr/local/glassfish4/glassfish/domains/domain1 /usr/local/payara5/glassfish/domains/`\n+\n+7. Remove the Glassfish cache directories\n+\n+   `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/generated/`  `sudo rm -rf /usr/local/payara5/glassfish/domains/domain1/osgi-cache/`\n+\n+8. In domain.xml:\n+\n+ Replace the -XX:PermSize and -XX:MaxPermSize JVM options with -XX:MetaspaceSize and -XX:MaxMetaspaceSize.\n+\n+   <jvm-options>-XX:MetaspaceSize=256m</jvm-options>\n+   <jvm-options>-XX:MaxMetaspaceSize=512m</jvm-options>\n+\n+Set both Xmx and Xms at startup to avoid runtime re-allocation. Your Xmx value should likely be higher:\n+\n+   <jvm-options>-Xmx2048m</jvm-options>\n+   <jvm-options>-Xms2048m</jvm-options>\n+\n+Add the below JVM options beneath the -Ddataverse settings:  \n+\n+   <jvm-options>-Dfish.payara.classloading.delegate=false</jvm-options>\n+   <jvm-options>-XX:+UseG1GC</jvm-options>\n+   <jvm-options>-XX:+UseStringDeduplication</jvm-options>\n+   <jvm-options>-XX:+DisableExplicitGC</jvm-options>\n+\n+9. Change any full pathnames /usr/local/glassfish4/... to /usr/local/payara5/... or whatever it is in your case. (Specifically check the -Ddataverse.files.directory and -Ddataverse.files.file.directory JVM options)\n+\n+10. In domain1/config/jhove.conf, change the hard-coded /usr/local/glassfish4 path, as above.\n+\n+(Optional): If you renamed your service account from glassfish to payara or appserver, update the ownership permissions. The Installation Guide recommends a service account of `dataverse`:\n+\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/domains/domain1`\n+   `sudo chown -R dataverse /usr/local/payara5/glassfish/lib`\n+\n+11. You will also need to check that the service account has write permission on the files directory, if they are located outside the old Glassfish domain. And/or make sure the service account has the correct AWS credentials, if you are using S3 for storage.\n+\n+12. Finally, start Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+13. Deploy the Dataverse 5 warfile:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin deploy /path/to/dataverse-5.0.war`\n+\n+14. Then restart Payara:\n+\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin stop-domain`\n+   `sudo -u dataverse /usr/local/payara5/bin/asadmin start-domain`\n+\n+15. Update Astrophysics Metadata Block (if used)\n+\n+   `wget https://github.com/IQSS/dataverse/releases/download/5.0/astrophysics.tsv`\n+   `curl http://localhost:8080/api/admin/datasetfield/load -X POST --data-binary @astrophysics.tsv -H \"Content-type: text/tab-separated-values\"`\n+\n+16. (Recommended) Run ReExportall to update JSON Exports  \n+\n+   <http://guides.dataverse.org/en/5.0/admin/metadataexport.html?highlight=export#batch-exports-through-the-api>\n+\n+17. For installations using DataCite, pre-register DOIs:\n+\n+   `/api/pids/unreserved`  will report the ids of the datasets\n+   `/api/pids/:persistentId/reserve` reserves the assigned DOI with DataCite (will need to be run on every id reported by the the first API).\n+\n+   Scripted, the whole process would look as follows (adjust as needed):\n+\n+```\n+   API_TOKEN='xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n+\n+   curl -s -H \"X-Dataverse-key:$API_TOKEN\" http://localhost:8080/api/pids/unreserved |\n+   # the API outputs JSON; note the use of jq to parse it:\n+   jq '.data.count[].pid' | tr -d '\"' | \n+   while read doi\n+   do\n+      curl -s -H \"X-Dataverse-key:$API_TOKEN\" -X POST http://localhost:8080/api/pids/:persistentId/reserve?persistentId=$doi\n+   done\n+```\n+\n+18. Set up Standalone Zipper Service if desired. Please consult the scripts/zipdownload/README.md in the Dataverse 5 source tree for more information. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI1NzYxNA=="}, "originalCommit": {"oid": "9e96567fedf52577532676e0f3d6597354725f65"}, "originalPosition": 321}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2370, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}