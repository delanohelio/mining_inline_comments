{"pr_number": 1320, "pr_title": "Flink: Replace Row with RowData in flink write path.", "pr_createdAt": "2020-08-11T03:05:20Z", "pr_url": "https://github.com/apache/iceberg/pull/1320", "timeline": [{"oid": "a2baa142d733d869db50183b0bc1c4fff67841a3", "url": "https://github.com/apache/iceberg/commit/a2baa142d733d869db50183b0bc1c4fff67841a3", "message": "Flink: Replace Row with RowData in flink write path.", "committedDate": "2020-08-11T03:26:57Z", "type": "commit"}, {"oid": "a2baa142d733d869db50183b0bc1c4fff67841a3", "url": "https://github.com/apache/iceberg/commit/a2baa142d733d869db50183b0bc1c4fff67841a3", "message": "Flink: Replace Row with RowData in flink write path.", "committedDate": "2020-08-11T03:26:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgwODk2NQ==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r468808965", "bodyText": "In Spark, we had a bug where Spark may produce a row with a short, which is stored as an int in Iceberg. In a CTAS query, data would actually get passed to Iceberg with the short and we would end up with a ClassCastException. That's why we now pass the dataset schema when creating writers. You might want to watch out for a similar bug.", "author": "rdblue", "createdAt": "2020-08-11T19:17:14Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java", "diffHunk": "@@ -38,37 +40,38 @@\n import org.apache.iceberg.io.OutputFileFactory;\n import org.apache.iceberg.io.TaskWriter;\n import org.apache.iceberg.io.UnpartitionedWriter;\n-import org.apache.iceberg.parquet.Parquet;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n \n-class RowTaskWriterFactory implements TaskWriterFactory<Row> {\n+class RowDataTaskWriterFactory implements TaskWriterFactory<RowData> {\n   private final Schema schema;\n   private final PartitionSpec spec;\n   private final LocationProvider locations;\n   private final FileIO io;\n   private final EncryptionManager encryptionManager;\n   private final long targetFileSizeBytes;\n   private final FileFormat format;\n-  private final FileAppenderFactory<Row> appenderFactory;\n+  private final RowType flinkSchema;\n+  private final FileAppenderFactory<RowData> appenderFactory;\n \n   private OutputFileFactory outputFileFactory;\n \n-  RowTaskWriterFactory(Schema schema,\n-                       PartitionSpec spec,\n-                       LocationProvider locations,\n-                       FileIO io,\n-                       EncryptionManager encryptionManager,\n-                       long targetFileSizeBytes,\n-                       FileFormat format,\n-                       Map<String, String> tableProperties) {\n+  RowDataTaskWriterFactory(Schema schema,\n+                           PartitionSpec spec,\n+                           LocationProvider locations,\n+                           FileIO io,\n+                           EncryptionManager encryptionManager,\n+                           long targetFileSizeBytes,\n+                           FileFormat format,\n+                           Map<String, String> tableProperties) {\n     this.schema = schema;\n     this.spec = spec;\n     this.locations = locations;\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.targetFileSizeBytes = targetFileSizeBytes;\n     this.format = format;\n-    this.appenderFactory = new FlinkFileAppenderFactory(schema, tableProperties);\n+    this.flinkSchema = FlinkSchemaUtil.convert(schema);", "originalCommit": "a2baa142d733d869db50183b0bc1c4fff67841a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3Njc5NA==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r468976794", "bodyText": "Thanks for the great remanding, I think the bug you mentioned is this one #999. The flink don't support CTAS but support INSERT INTO iceberg_table SELECT * from table_2,  if the table_2 has a TINYINT or SMALLINT, them its BinaryRowData queried from SELECT will be byte or short,  we also need the raw flink's schema to read the values from BinaryRowData (rather than the flink schema converted from iceberg schema), and write those byte or short into integer.  Let me consider how to fix this.", "author": "openinx", "createdAt": "2020-08-12T02:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgwODk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ5ODg5Mg==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469498892", "bodyText": "This only appears in Spark with a CTAS query because that's the only time that Spark doesn't get a schema back from the table. When Spark has a table schema, it will automatically insert casts to the appropriate types so this problem doesn't happen. I'm not sure if Flink does that, but if it does then you wouldn't need to worry about that bug.", "author": "rdblue", "createdAt": "2020-08-12T19:45:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgwODk2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "f43ec6118f1af861f29a37f56605edc751868ead", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java b/flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java\nindex df9e54eb5..4c307adb4 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java\n\n@@ -56,6 +56,7 @@ class RowDataTaskWriterFactory implements TaskWriterFactory<RowData> {\n   private OutputFileFactory outputFileFactory;\n \n   RowDataTaskWriterFactory(Schema schema,\n+                           RowType flinkSchema,\n                            PartitionSpec spec,\n                            LocationProvider locations,\n                            FileIO io,\n"}}, {"oid": "f43ec6118f1af861f29a37f56605edc751868ead", "url": "https://github.com/apache/iceberg/commit/f43ec6118f1af861f29a37f56605edc751868ead", "message": "Address the schema issue from ryan", "committedDate": "2020-08-12T03:34:57Z", "type": "commit"}, {"oid": "44da7c33f883d472801b1195d25b54405846d4c5", "url": "https://github.com/apache/iceberg/commit/44da7c33f883d472801b1195d25b54405846d4c5", "message": "Add unit tests.", "committedDate": "2020-08-12T06:35:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA0MjY3Mw==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469042673", "bodyText": "I changed the type from iceberg type to flink's logical type here, because the value of tinyint & smallint is a byte  & short,  when cast to the byte or short to Integer here, it will throw a cast failure exception.  Using logical type here so that we could cast it to integer right way.", "author": "openinx", "createdAt": "2020-08-12T06:52:38Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -85,52 +85,51 @@ public int size() {\n   }\n \n   private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n-    switch (type.typeId()) {\n-      case STRING:\n+    switch (logicalType.getTypeRoot()) {", "originalCommit": "44da7c33f883d472801b1195d25b54405846d4c5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "018d16d5fa0273ee442135fe23ee65c4285931b1", "url": "https://github.com/apache/iceberg/commit/018d16d5fa0273ee442135fe23ee65c4285931b1", "message": "Minor changes.", "committedDate": "2020-08-12T07:01:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUwOTY4Mg==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469509682", "bodyText": "I think an identity check would be okay since this is an enum symbol, but either way is fine.", "author": "rdblue", "createdAt": "2020-08-12T20:06:16Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -85,52 +85,51 @@ public int size() {\n   }\n \n   private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n-    switch (type.typeId()) {\n-      case STRING:\n+    switch (logicalType.getTypeRoot()) {\n+      case TINYINT:\n+        return (row, pos) -> (int) row.getByte(pos);\n+      case SMALLINT:\n+        return (row, pos) -> (int) row.getShort(pos);\n+      case CHAR:\n+      case VARCHAR:\n         return (row, pos) -> row.getString(pos).toString();\n \n-      case FIXED:\n       case BINARY:\n-        return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n-\n-      case UUID:\n-        return (row, pos) -> {\n-          ByteBuffer bb = ByteBuffer.wrap(row.getBinary(pos));\n-          long mostSigBits = bb.getLong();\n-          long leastSigBits = bb.getLong();\n-          return new UUID(mostSigBits, leastSigBits);\n-        };\n+      case VARBINARY:\n+        if (Type.TypeID.UUID.equals(type.typeId())) {", "originalCommit": "018d16d5fa0273ee442135fe23ee65c4285931b1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUwOTk2Nw==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469509967", "bodyText": "Looks like another area where we should have a util method to convert (though it shouldn't block this commit).", "author": "rdblue", "createdAt": "2020-08-12T20:06:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -85,52 +85,51 @@ public int size() {\n   }\n \n   private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n-    switch (type.typeId()) {\n-      case STRING:\n+    switch (logicalType.getTypeRoot()) {\n+      case TINYINT:\n+        return (row, pos) -> (int) row.getByte(pos);\n+      case SMALLINT:\n+        return (row, pos) -> (int) row.getShort(pos);\n+      case CHAR:\n+      case VARCHAR:\n         return (row, pos) -> row.getString(pos).toString();\n \n-      case FIXED:\n       case BINARY:\n-        return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n-\n-      case UUID:\n-        return (row, pos) -> {\n-          ByteBuffer bb = ByteBuffer.wrap(row.getBinary(pos));\n-          long mostSigBits = bb.getLong();\n-          long leastSigBits = bb.getLong();\n-          return new UUID(mostSigBits, leastSigBits);\n-        };\n+      case VARBINARY:\n+        if (Type.TypeID.UUID.equals(type.typeId())) {\n+          return (row, pos) -> {\n+            ByteBuffer bb = ByteBuffer.wrap(row.getBinary(pos));\n+            long mostSigBits = bb.getLong();\n+            long leastSigBits = bb.getLong();\n+            return new UUID(mostSigBits, leastSigBits);", "originalCommit": "018d16d5fa0273ee442135fe23ee65c4285931b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1Njk0Ng==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469656946", "bodyText": "It's true,  we could have a separate pull request for this.", "author": "openinx", "createdAt": "2020-08-13T02:16:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUwOTk2Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUxMDc3Nw==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469510777", "bodyText": "Why not use the same logic here that is used in the other timestamp type? Both of the values are TimestampData that is returned by getTimestamp. It seems like converting directly to a microsecond value is better than going through LocalDateTime here.", "author": "rdblue", "createdAt": "2020-08-12T20:08:33Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -85,52 +85,51 @@ public int size() {\n   }\n \n   private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n-    switch (type.typeId()) {\n-      case STRING:\n+    switch (logicalType.getTypeRoot()) {\n+      case TINYINT:\n+        return (row, pos) -> (int) row.getByte(pos);\n+      case SMALLINT:\n+        return (row, pos) -> (int) row.getShort(pos);\n+      case CHAR:\n+      case VARCHAR:\n         return (row, pos) -> row.getString(pos).toString();\n \n-      case FIXED:\n       case BINARY:\n-        return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n-\n-      case UUID:\n-        return (row, pos) -> {\n-          ByteBuffer bb = ByteBuffer.wrap(row.getBinary(pos));\n-          long mostSigBits = bb.getLong();\n-          long leastSigBits = bb.getLong();\n-          return new UUID(mostSigBits, leastSigBits);\n-        };\n+      case VARBINARY:\n+        if (Type.TypeID.UUID.equals(type.typeId())) {\n+          return (row, pos) -> {\n+            ByteBuffer bb = ByteBuffer.wrap(row.getBinary(pos));\n+            long mostSigBits = bb.getLong();\n+            long leastSigBits = bb.getLong();\n+            return new UUID(mostSigBits, leastSigBits);\n+          };\n+        } else {\n+          return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n+        }\n \n       case DECIMAL:\n         DecimalType decimalType = (DecimalType) logicalType;\n         return (row, pos) -> row.getDecimal(pos, decimalType.getPrecision(), decimalType.getScale()).toBigDecimal();\n \n-      case TIME:\n+      case TIME_WITHOUT_TIME_ZONE:\n         // Time in RowData is in milliseconds (Integer), while iceberg's time is microseconds (Long).\n         return (row, pos) -> ((long) row.getInt(pos)) * 1_000;\n \n-      case TIMESTAMP:\n-        switch (logicalType.getTypeRoot()) {\n-          case TIMESTAMP_WITHOUT_TIME_ZONE:\n-            TimestampType timestampType = (TimestampType) logicalType;\n-            return (row, pos) -> {\n-              LocalDateTime localDateTime = row.getTimestamp(pos, timestampType.getPrecision()).toLocalDateTime();\n-              return DateTimeUtil.microsFromTimestamp(localDateTime);\n-            };\n-\n-          case TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n-            LocalZonedTimestampType lzTs = (LocalZonedTimestampType) logicalType;\n-            return (row, pos) -> {\n-              TimestampData timestampData = row.getTimestamp(pos, lzTs.getPrecision());\n-              return timestampData.getMillisecond() * 1000 + timestampData.getNanoOfMillisecond() / 1000;\n-            };\n-\n-          default:\n-            throw new IllegalArgumentException(\"Unhandled iceberg type: \" + type + \" corresponding flink type: \" +\n-                logicalType);\n-        }\n+      case TIMESTAMP_WITHOUT_TIME_ZONE:\n+        TimestampType timestampType = (TimestampType) logicalType;\n+        return (row, pos) -> {\n+          LocalDateTime localDateTime = row.getTimestamp(pos, timestampType.getPrecision()).toLocalDateTime();\n+          return DateTimeUtil.microsFromTimestamp(localDateTime);", "originalCommit": "018d16d5fa0273ee442135fe23ee65c4285931b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODA1NQ==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469658055", "bodyText": "Yeah, it could be the same to convert TimestampData to a long. I separate them because the TimestampType are different, and we are depending the TimestampType.getPrecision() or LocalZonedTimestampType.getPrecision() to get the precision (though we could use the constant 6 here, but better to use the timestamp's precision getter).", "author": "openinx", "createdAt": "2020-08-13T02:21:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUxMDc3Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUxMjEwOQ==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469512109", "bodyText": "This could also use the assertEquals implementation that @chenjunjiedada has added in #1266. That would be better than converting a specific record type.", "author": "rdblue", "createdAt": "2020-08-12T20:11:19Z", "path": "flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java", "diffHunk": "@@ -74,10 +76,16 @@ static Record createRecord(Integer id, String data) {\n     return record;\n   }\n \n-  static void assertTableRows(String tablePath, List<Row> rows) throws IOException {\n+  static RowData createRowData(Integer id, String data) {\n+    return GenericRowData.of(id, StringData.fromString(data));\n+  }\n+\n+  static void assertTableRows(String tablePath, List<RowData> rows) throws IOException {\n     List<Record> records = Lists.newArrayList();\n-    for (Row row : rows) {\n-      records.add(createRecord((Integer) row.getField(0), (String) row.getField(1)));\n+    for (RowData row : rows) {\n+      Integer id = row.isNullAt(0) ? null : row.getInt(0);\n+      String data = row.isNullAt(1) ? null : row.getString(1).toString();\n+      records.add(createRecord(id, data));", "originalCommit": "018d16d5fa0273ee442135fe23ee65c4285931b1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODE4NA==", "url": "https://github.com/apache/iceberg/pull/1320#discussion_r469658184", "bodyText": "Make sense.", "author": "openinx", "createdAt": "2020-08-13T02:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUxMjEwOQ=="}], "type": "inlineReview", "revised_code": null}]}