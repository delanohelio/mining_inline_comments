{"pr_number": 882, "pr_title": "fix: Failed to get status issue because of s3 eventual consistency", "pr_createdAt": "2020-03-30T17:39:40Z", "pr_url": "https://github.com/apache/iceberg/pull/882", "timeline": [{"oid": "20b1ab45ce803f814be23667625b9414b0b7209d", "url": "https://github.com/apache/iceberg/commit/20b1ab45ce803f814be23667625b9414b0b7209d", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-03-31T02:24:04Z", "type": "forcePushed"}, {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab", "url": "https://github.com/apache/iceberg/commit/7916d8fb9b9a421f597a6b95526ad1642ac542ab", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file when configured.\n\nadded new datasource option \"use-writer-length-as-file-size\" to control behaviour.", "committedDate": "2020-03-31T17:47:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401789045", "bodyText": "@rdblue additional 103 in writer length, can this be bug in writer factory which returns buffer size after flush?\n( no rush of merging this PR , I am trying to make sure changes are ok)", "author": "sudssf", "createdAt": "2020-04-01T17:32:39Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -201,6 +201,36 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n+  @Test\n+  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .option(\"use-writer-length-as-file-size\", true)\n+        .save(tableLocation);\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")", "originalCommit": "7916d8fb9b9a421f597a6b95526ad1642ac542ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDc1OQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401820759", "bodyText": "Does this happen for all formats, or just one?", "author": "rdblue", "createdAt": "2020-04-01T18:26:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk1NzQ3NA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401957474", "bodyText": "Do we know why this is happening? I would expect Parquet to return the correct size after close. We should find out what's going on.", "author": "rdblue", "createdAt": "2020-04-01T22:56:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2MDU5MA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401960590", "bodyText": "I think this happens only for parquet.\nhttps://github.com/apache/incubator-iceberg/blob/master/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java#L142\nwriteStore seems to return non zero results for getBufferedSize after close.", "author": "sudssf", "createdAt": "2020-04-01T23:04:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "3c5e52e53cd4a87a5c7b637e4a7ab2e9a58df466", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\nindex 1e87ed07b..8668b283c 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\n\n@@ -201,36 +201,6 @@ public class TestDataSourceOptions {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n-  @Test\n-  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n-\n-    HadoopTables tables = new HadoopTables(CONF);\n-    PartitionSpec spec = PartitionSpec.unpartitioned();\n-    Map<String, String> options = Maps.newHashMap();\n-    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n-    tables.create(SCHEMA, spec, options, tableLocation);\n-\n-    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n-        new SimpleRecord(1, \"a\"),\n-        new SimpleRecord(2, \"b\")\n-    );\n-    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n-    originalDf.select(\"id\", \"data\").write()\n-        .format(\"iceberg\")\n-        .mode(\"append\")\n-        .option(\"use-writer-length-as-file-size\", true)\n-        .save(tableLocation);\n-\n-    Dataset<Row> resultDf = spark.read()\n-        .format(\"iceberg\")\n-        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")\n-            // 103 is buffer size added by parquet writer\n-        .load(tableLocation);\n-\n-    Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n-  }\n-\n   @Test\n   public void testIncrementalScanOptions() throws IOException {\n     String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401820394", "bodyText": "This shouldn't be a new option. Let's remove it.", "author": "rdblue", "createdAt": "2020-04-01T18:25:29Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "originalCommit": "7916d8fb9b9a421f597a6b95526ad1642ac542ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkzOTk3Ng==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401939976", "bodyText": "@rdblue  do you suggestion always using writer length, instead of calling dataFileBuilder.withEncryptedOutputFile(currentFile) which internally calls getStatus for s3a?", "author": "sudssf", "createdAt": "2020-04-01T22:11:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk1NzMwOA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401957308", "bodyText": "Yes, I think it should always use the length reported by the writer.", "author": "rdblue", "createdAt": "2020-04-01T22:55:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2MDc0OQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401960749", "bodyText": "sounds good I will update PR.", "author": "sudssf", "createdAt": "2020-04-01T23:05:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjUyNjgwOQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r402526809", "bodyText": "@rdblue please take a look at updated PR at your convenience.", "author": "sudssf", "createdAt": "2020-04-02T18:32:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}], "type": "inlineReview", "revised_code": {"commit": "3c5e52e53cd4a87a5c7b637e4a7ab2e9a58df466", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java b/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java\nindex c86978a8d..844c6815b 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java\n\n@@ -115,7 +114,6 @@ class Writer implements DataSourceWriter {\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n-    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);\n   }\n \n   private FileFormat getFileFormat(Map<String, String> tableProperties, DataSourceOptions options) {\n"}}, {"oid": "3c5e52e53cd4a87a5c7b637e4a7ab2e9a58df466", "url": "https://github.com/apache/iceberg/commit/3c5e52e53cd4a87a5c7b637e4a7ab2e9a58df466", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-04-01T23:56:30Z", "type": "forcePushed"}, {"oid": "69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "url": "https://github.com/apache/iceberg/commit/69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-04-01T23:58:26Z", "type": "commit"}, {"oid": "69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "url": "https://github.com/apache/iceberg/commit/69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-04-01T23:58:26Z", "type": "forcePushed"}]}