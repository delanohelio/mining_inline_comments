{"pr_number": 805, "pr_title": "Add all_data_files, all_manifests, and all_entries metadata tables", "pr_createdAt": "2020-02-16T23:39:49Z", "pr_url": "https://github.com/apache/iceberg/pull/805", "timeline": [{"oid": "b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "url": "https://github.com/apache/iceberg/commit/b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "message": "Add all_data_files, all_manifests, and all_entries metadata tables.", "committedDate": "2020-02-17T00:59:52Z", "type": "commit"}, {"oid": "b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "url": "https://github.com/apache/iceberg/commit/b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "message": "Add all_data_files, all_manifests, and all_entries metadata tables.", "committedDate": "2020-02-17T00:59:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjI0MQ==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r380336241", "bodyText": "This may be a bit out of scope, but shouldn't we have a better way to represent reserved field ids than magic numbers?", "author": "danielcweeks", "createdAt": "2020-02-17T19:15:40Z", "path": "core/src/main/java/org/apache/iceberg/AllDataFilesTable.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Sets;\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.ResidualEvaluator;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.ParallelIterable;\n+import org.apache.iceberg.util.ThreadPools;\n+\n+/**\n+ * A {@link Table} implementation that exposes a table's valid data files as rows.\n+ * <p>\n+ * A valid data file is one that is readable from any snapshot currently tracked by the table.\n+ * <p>\n+ * This table may return duplicate rows.\n+ */\n+public class AllDataFilesTable extends BaseMetadataTable {\n+  private final TableOperations ops;\n+  private final Table table;\n+\n+  public AllDataFilesTable(TableOperations ops, Table table) {\n+    this.ops = ops;\n+    this.table = table;\n+  }\n+\n+  @Override\n+  Table table() {\n+    return table;\n+  }\n+\n+  @Override\n+  String metadataTableName() {\n+    return \"all_data_files\";\n+  }\n+\n+  @Override\n+  public TableScan newScan() {\n+    return new AllDataFilesTableScan(ops, table, schema());\n+  }\n+\n+  @Override\n+  public Schema schema() {\n+    Schema schema = new Schema(DataFile.getType(table.spec().partitionType()).fields());\n+    if (table.spec().fields().size() < 1) {\n+      // avoid returning an empty struct, which is not always supported. instead, drop the partition field (id 102)\n+      return TypeUtil.selectNot(schema, Sets.newHashSet(102));", "originalCommit": "b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM2NzE2Ng==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r380367166", "bodyText": "We can look up the number in the schema by using schema.findColumn(\"partition\").fieldId(), but that's really just looking up a constant value using another constant field name, \"partition\". I thought it was easier to use a comment to explain what's happening than to do the lookup. Maybe there's an alternative to this I didn't think of? Any idea?", "author": "rdblue", "createdAt": "2020-02-17T21:11:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjI0MQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjQzMw==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r380336433", "bodyText": "Assumes manifest list location?", "author": "danielcweeks", "createdAt": "2020-02-17T19:16:17Z", "path": "core/src/main/java/org/apache/iceberg/AllDataFilesTable.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Sets;\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.ResidualEvaluator;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.ParallelIterable;\n+import org.apache.iceberg.util.ThreadPools;\n+\n+/**\n+ * A {@link Table} implementation that exposes a table's valid data files as rows.\n+ * <p>\n+ * A valid data file is one that is readable from any snapshot currently tracked by the table.\n+ * <p>\n+ * This table may return duplicate rows.\n+ */\n+public class AllDataFilesTable extends BaseMetadataTable {\n+  private final TableOperations ops;\n+  private final Table table;\n+\n+  public AllDataFilesTable(TableOperations ops, Table table) {\n+    this.ops = ops;\n+    this.table = table;\n+  }\n+\n+  @Override\n+  Table table() {\n+    return table;\n+  }\n+\n+  @Override\n+  String metadataTableName() {\n+    return \"all_data_files\";\n+  }\n+\n+  @Override\n+  public TableScan newScan() {\n+    return new AllDataFilesTableScan(ops, table, schema());\n+  }\n+\n+  @Override\n+  public Schema schema() {\n+    Schema schema = new Schema(DataFile.getType(table.spec().partitionType()).fields());\n+    if (table.spec().fields().size() < 1) {\n+      // avoid returning an empty struct, which is not always supported. instead, drop the partition field (id 102)\n+      return TypeUtil.selectNot(schema, Sets.newHashSet(102));\n+    } else {\n+      return schema;\n+    }\n+  }\n+\n+  @Override\n+  public String location() {\n+    return table.currentSnapshot().manifestListLocation();", "originalCommit": "b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM2NzIzNg==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r380367236", "bodyText": "Yes, but it's okay if this is null.", "author": "rdblue", "createdAt": "2020-02-17T21:11:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjQzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5MzM3MQ==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r382693371", "bodyText": "@rdblue, a quick question. Why do we use manifestListLocation for AllDataFiles, AllEntriesFiles, DataFiles, ManifestEntries tables and ops.current().file().location() for others?\nThere is one rare case when this might fail. Our Reader in Spark will use table.location() to obtain a file system object. If the manifest list is null, it will fail.", "author": "aokolnychyi", "createdAt": "2020-02-21T16:54:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjQzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMzE4OA==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r382703188", "bodyText": "The check was added recently with locality for HDFS.", "author": "aokolnychyi", "createdAt": "2020-02-21T17:14:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjQzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwNjY0Ng==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r382706646", "bodyText": "For location, I suspect that it's just a copy/paste error. I was much more careful setting locations in the read tasks than for the metadata tables. We can clean that up.\nAlso, I forgot about the HDFS check. I think we should probably fix the cases where this may be null and return the table location. And we should also make sure that the locality logic doesn't cause a failure. That should be a warning for the optimization only.", "author": "rdblue", "createdAt": "2020-02-21T17:21:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjQzMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjk4Mg==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r380336982", "bodyText": "Seems like something we should map to a table/read property \\w default as opposed to hard coded.", "author": "danielcweeks", "createdAt": "2020-02-17T19:18:09Z", "path": "core/src/main/java/org/apache/iceberg/AllDataFilesTable.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Sets;\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.ResidualEvaluator;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.ParallelIterable;\n+import org.apache.iceberg.util.ThreadPools;\n+\n+/**\n+ * A {@link Table} implementation that exposes a table's valid data files as rows.\n+ * <p>\n+ * A valid data file is one that is readable from any snapshot currently tracked by the table.\n+ * <p>\n+ * This table may return duplicate rows.\n+ */\n+public class AllDataFilesTable extends BaseMetadataTable {\n+  private final TableOperations ops;\n+  private final Table table;\n+\n+  public AllDataFilesTable(TableOperations ops, Table table) {\n+    this.ops = ops;\n+    this.table = table;\n+  }\n+\n+  @Override\n+  Table table() {\n+    return table;\n+  }\n+\n+  @Override\n+  String metadataTableName() {\n+    return \"all_data_files\";\n+  }\n+\n+  @Override\n+  public TableScan newScan() {\n+    return new AllDataFilesTableScan(ops, table, schema());\n+  }\n+\n+  @Override\n+  public Schema schema() {\n+    Schema schema = new Schema(DataFile.getType(table.spec().partitionType()).fields());\n+    if (table.spec().fields().size() < 1) {\n+      // avoid returning an empty struct, which is not always supported. instead, drop the partition field (id 102)\n+      return TypeUtil.selectNot(schema, Sets.newHashSet(102));\n+    } else {\n+      return schema;\n+    }\n+  }\n+\n+  @Override\n+  public String location() {\n+    return table.currentSnapshot().manifestListLocation();\n+  }\n+\n+  public static class AllDataFilesTableScan extends BaseTableScan {\n+    private static final long TARGET_SPLIT_SIZE = 32 * 1024 * 1024; // 32 MB\n+    private final Schema fileSchema;\n+\n+    AllDataFilesTableScan(TableOperations ops, Table table, Schema fileSchema) {\n+      super(ops, table, fileSchema);\n+      this.fileSchema = fileSchema;\n+    }\n+\n+    private AllDataFilesTableScan(\n+        TableOperations ops, Table table, Long snapshotId, Schema schema, Expression rowFilter,\n+        boolean caseSensitive, boolean colStats, Collection<String> selectedColumns, Schema fileSchema,\n+        ImmutableMap<String, String> options) {\n+      super(ops, table, snapshotId, schema, rowFilter, caseSensitive, colStats, selectedColumns, options);\n+      this.fileSchema = fileSchema;\n+    }\n+\n+    @Override\n+    protected TableScan newRefinedScan(\n+        TableOperations ops, Table table, Long snapshotId, Schema schema, Expression rowFilter,\n+        boolean caseSensitive, boolean colStats, Collection<String> selectedColumns,\n+        ImmutableMap<String, String> options) {\n+      return new AllDataFilesTableScan(\n+          ops, table, snapshotId, schema, rowFilter, caseSensitive, colStats, selectedColumns, fileSchema, options);\n+    }\n+\n+    @Override\n+    public TableScan useSnapshot(long scanSnapshotId) {\n+      throw new UnsupportedOperationException(\"Cannot select snapshot: all_data_files is for all snapshots\");\n+    }\n+\n+    @Override\n+    public TableScan asOfTime(long timestampMillis) {\n+      throw new UnsupportedOperationException(\"Cannot select snapshot: all_data_files is for all snapshots\");\n+    }\n+\n+    @Override\n+    protected long targetSplitSize(TableOperations ops) {\n+      return TARGET_SPLIT_SIZE;", "originalCommit": "b446de9efa4ca37e851c5a3968b03ec49ce54eaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM2NzQ5Ng==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r380367496", "bodyText": "I think we can add this later if necessary. I'd like to keep these simple and add features as we go.", "author": "rdblue", "createdAt": "2020-02-17T21:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjk4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY5NjUzMA==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r382696530", "bodyText": "I believe it generally makes sense to configure the split size for metadata tables. In our tables, it is not uncommon to see 4-6 MB manifest files. If we have a reasonable cluster and allow split sizes to be 16 MB, metadata queries can be 2 times faster.\nI've created #817 so that we can discuss it.", "author": "aokolnychyi", "createdAt": "2020-02-21T17:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNjk4Mg=="}], "type": "inlineReview", "revised_code": null}, {"oid": "452082c9f05ba15f17dc394c4453a7a539d70d45", "url": "https://github.com/apache/iceberg/commit/452082c9f05ba15f17dc394c4453a7a539d70d45", "message": "Merge branch 'master' into add-new-metadata-tables", "committedDate": "2020-02-17T21:13:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxMTczNA==", "url": "https://github.com/apache/iceberg/pull/805#discussion_r382711734", "bodyText": "There are a couple of places when we define static methods in one metadata table class and call them in others. It seems we can put some of those in the parent BaseMetadataTable.", "author": "aokolnychyi", "createdAt": "2020-02-21T17:32:29Z", "path": "core/src/main/java/org/apache/iceberg/AllDataFilesTable.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Sets;\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.ResidualEvaluator;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.ParallelIterable;\n+import org.apache.iceberg.util.ThreadPools;\n+\n+/**\n+ * A {@link Table} implementation that exposes a table's valid data files as rows.\n+ * <p>\n+ * A valid data file is one that is readable from any snapshot currently tracked by the table.\n+ * <p>\n+ * This table may return duplicate rows.\n+ */\n+public class AllDataFilesTable extends BaseMetadataTable {\n+  private final TableOperations ops;\n+  private final Table table;\n+\n+  public AllDataFilesTable(TableOperations ops, Table table) {\n+    this.ops = ops;\n+    this.table = table;\n+  }\n+\n+  @Override\n+  Table table() {\n+    return table;\n+  }\n+\n+  @Override\n+  String metadataTableName() {\n+    return \"all_data_files\";\n+  }\n+\n+  @Override\n+  public TableScan newScan() {\n+    return new AllDataFilesTableScan(ops, table, schema());\n+  }\n+\n+  @Override\n+  public Schema schema() {\n+    Schema schema = new Schema(DataFile.getType(table.spec().partitionType()).fields());\n+    if (table.spec().fields().size() < 1) {\n+      // avoid returning an empty struct, which is not always supported. instead, drop the partition field (id 102)\n+      return TypeUtil.selectNot(schema, Sets.newHashSet(102));\n+    } else {\n+      return schema;\n+    }\n+  }\n+\n+  @Override\n+  public String location() {\n+    return table.currentSnapshot().manifestListLocation();\n+  }\n+\n+  public static class AllDataFilesTableScan extends BaseTableScan {\n+    private static final long TARGET_SPLIT_SIZE = 32 * 1024 * 1024; // 32 MB\n+    private final Schema fileSchema;\n+\n+    AllDataFilesTableScan(TableOperations ops, Table table, Schema fileSchema) {\n+      super(ops, table, fileSchema);\n+      this.fileSchema = fileSchema;\n+    }\n+\n+    private AllDataFilesTableScan(\n+        TableOperations ops, Table table, Long snapshotId, Schema schema, Expression rowFilter,\n+        boolean caseSensitive, boolean colStats, Collection<String> selectedColumns, Schema fileSchema,\n+        ImmutableMap<String, String> options) {\n+      super(ops, table, snapshotId, schema, rowFilter, caseSensitive, colStats, selectedColumns, options);\n+      this.fileSchema = fileSchema;\n+    }\n+\n+    @Override\n+    protected TableScan newRefinedScan(\n+        TableOperations ops, Table table, Long snapshotId, Schema schema, Expression rowFilter,\n+        boolean caseSensitive, boolean colStats, Collection<String> selectedColumns,\n+        ImmutableMap<String, String> options) {\n+      return new AllDataFilesTableScan(\n+          ops, table, snapshotId, schema, rowFilter, caseSensitive, colStats, selectedColumns, fileSchema, options);\n+    }\n+\n+    @Override\n+    public TableScan useSnapshot(long scanSnapshotId) {\n+      throw new UnsupportedOperationException(\"Cannot select snapshot: all_data_files is for all snapshots\");\n+    }\n+\n+    @Override\n+    public TableScan asOfTime(long timestampMillis) {\n+      throw new UnsupportedOperationException(\"Cannot select snapshot: all_data_files is for all snapshots\");\n+    }\n+\n+    @Override\n+    protected long targetSplitSize(TableOperations ops) {\n+      return TARGET_SPLIT_SIZE;\n+    }\n+\n+    @Override\n+    protected CloseableIterable<FileScanTask> planFiles(\n+        TableOperations ops, Snapshot snapshot, Expression rowFilter, boolean caseSensitive, boolean colStats) {\n+      CloseableIterable<ManifestFile> manifests = allManifestFiles(ops.current().snapshots());\n+      String schemaString = SchemaParser.toJson(schema());\n+      String specString = PartitionSpecParser.toJson(PartitionSpec.unpartitioned());\n+      ResidualEvaluator residuals = ResidualEvaluator.unpartitioned(rowFilter);\n+\n+      // Data tasks produce the table schema, not the projection schema and projection is done by processing engines.\n+      // This data task needs to use the table schema, which may not include a partition schema to avoid having an\n+      // empty struct in the schema for unpartitioned tables. Some engines, like Spark, can't handle empty structs in\n+      // all cases.\n+      return CloseableIterable.transform(manifests, manifest ->\n+          new DataFilesTable.ManifestReadTask(ops.io(), manifest, fileSchema, schemaString, specString, residuals));\n+    }\n+  }\n+\n+  static CloseableIterable<ManifestFile> allManifestFiles(List<Snapshot> snapshots) {", "originalCommit": "452082c9f05ba15f17dc394c4453a7a539d70d45", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}