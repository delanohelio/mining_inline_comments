{"pr_number": 1933, "pr_title": "Refactor Spark DF read and Write options", "pr_createdAt": "2020-12-15T01:34:23Z", "pr_url": "https://github.com/apache/iceberg/pull/1933", "timeline": [{"oid": "0ddfeaf99820ebd22491e604223a8f9daca33feb", "url": "https://github.com/apache/iceberg/commit/0ddfeaf99820ebd22491e604223a8f9daca33feb", "message": "Fixes", "committedDate": "2020-12-10T20:05:19Z", "type": "commit"}, {"oid": "9e74e1870055858beab94d451109f801de5d1931", "url": "https://github.com/apache/iceberg/commit/9e74e1870055858beab94d451109f801de5d1931", "message": "Fix checkstyle", "committedDate": "2020-12-10T20:15:04Z", "type": "commit"}, {"oid": "1fba22440b4a6ef8567eca49f034c264ccc9757d", "url": "https://github.com/apache/iceberg/commit/1fba22440b4a6ef8567eca49f034c264ccc9757d", "message": "Refactor", "committedDate": "2020-12-15T01:28:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI2MjU3MA==", "url": "https://github.com/apache/iceberg/pull/1933#discussion_r543262570", "bodyText": "I feel like there are more write properties. Is there a reason we don't include others?", "author": "aokolnychyi", "createdAt": "2020-12-15T11:25:37Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkWriteOptions.java", "diffHunk": "@@ -29,4 +29,7 @@ private SparkWriteOptions() {\n \n   // Overrides table property write.spark.fanout.enabled(default: false)\n   public static final String FANOUT_ENABLED = \"fanout-enabled\";\n+\n+  // Fileformat for write operations\n+  public static final String WRITE_FORMAT = \"write-format\";", "originalCommit": "1fba22440b4a6ef8567eca49f034c264ccc9757d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d102f88f4aa22367d6d0d486cc965bf91b752a42", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/SparkWriteOptions.java b/spark/src/main/java/org/apache/iceberg/spark/SparkWriteOptions.java\nindex d4b5461f6..9dfa0828a 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/SparkWriteOptions.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/SparkWriteOptions.java\n\n@@ -27,9 +27,24 @@ public class SparkWriteOptions {\n   private SparkWriteOptions() {\n   }\n \n+  // Fileformat for write operations(default: Table write.format.default )\n+  public static final String WRITE_FORMAT = \"write-format\";\n+\n+  // Overrides this table's write.target-file-size-bytes\n+  public static final String TARGET_FILE_SIZE_BYTES = \"target-file-size-bytes\";\n+\n+  //  Sets the nullable check on fields(default: true)\n+  public static final String CHECK_NULLABILITY = \"check-nullability\";\n+\n+  // Adds an entry with custom-key and corresponding value in the snapshot summary\n+  // ex: df.write().format(iceberg)\n+  //     .option(SparkWriteOptions.SNAPSHOT_PROPERTY_PREFIX.\"key1\", \"value1\")\n+  //     .save(location)\n+  public static final String SNAPSHOT_PROPERTY_PREFIX = \"snapshot-property\";\n+\n   // Overrides table property write.spark.fanout.enabled(default: false)\n   public static final String FANOUT_ENABLED = \"fanout-enabled\";\n \n-  // Fileformat for write operations\n-  public static final String WRITE_FORMAT = \"write-format\";\n+  // Checks if input schema and table schema are same(default: true)\n+  public static final String CHECK_ORDERING = \"check-ordering\";\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI2Mjc4MA==", "url": "https://github.com/apache/iceberg/pull/1933#discussion_r543262780", "bodyText": "There are also more read options. Why not include all of them?", "author": "aokolnychyi", "createdAt": "2020-12-15T11:25:58Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+/**\n+ * Spark DF read options\n+ */\n+public class SparkReadOptions {\n+\n+  private SparkReadOptions() {\n+  }\n+\n+  // Snapshot ID of the table snapshot to read\n+  public static final String SNAPSHOT_ID = \"snapshot-id\";", "originalCommit": "1fba22440b4a6ef8567eca49f034c264ccc9757d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d102f88f4aa22367d6d0d486cc965bf91b752a42", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java b/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java\nindex 0c091d742..e0242e062 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java\n\n@@ -41,4 +41,10 @@ public class SparkReadOptions {\n \n   // Overrides the table's read.split.open-file-cost\n   public static final String FILE_OPEN_COST = \"file-open-cost\";\n+\n+  // Overrides the table's read.split.open-file-cost\n+  public static final String VECTORIZATION_ENABLED = \"vectorization-enabled\";\n+\n+  // Overrides the table's read.parquet.vectorization.batch-size\n+  public static final String VECTORIZATION_BATCH_SIZE = \"batch-size\";\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI2MzUxNg==", "url": "https://github.com/apache/iceberg/pull/1933#discussion_r543263516", "bodyText": "Shall we use SparkReadOptions.SNAPSHOT_ID here?", "author": "aokolnychyi", "createdAt": "2020-12-15T11:27:09Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -107,7 +108,7 @@\n       boolean caseSensitive, DataSourceOptions options) {\n     this.table = table;\n     this.snapshotId = options.get(\"snapshot-id\").map(Long::parseLong).orElse(null);", "originalCommit": "1fba22440b4a6ef8567eca49f034c264ccc9757d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d102f88f4aa22367d6d0d486cc965bf91b752a42", "chunk": "diff --git a/spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex c34482de1..4b0723e7e 100644\n--- a/spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -107,7 +107,7 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   Reader(Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n       boolean caseSensitive, DataSourceOptions options) {\n     this.table = table;\n-    this.snapshotId = options.get(\"snapshot-id\").map(Long::parseLong).orElse(null);\n+    this.snapshotId = options.get(SparkReadOptions.SNAPSHOT_ID).map(Long::parseLong).orElse(null);\n     this.asOfTimestamp = options.get(SparkReadOptions.AS_OF_TIMESTAMP).map(Long::parseLong).orElse(null);\n     if (snapshotId != null && asOfTimestamp != null) {\n       throw new IllegalArgumentException(\n"}}, {"oid": "d102f88f4aa22367d6d0d486cc965bf91b752a42", "url": "https://github.com/apache/iceberg/commit/d102f88f4aa22367d6d0d486cc965bf91b752a42", "message": "Add options that were left out", "committedDate": "2020-12-15T19:53:57Z", "type": "commit"}, {"oid": "ade06aebf84a7a5f15b50be84d5a3e12bd3b6276", "url": "https://github.com/apache/iceberg/commit/ade06aebf84a7a5f15b50be84d5a3e12bd3b6276", "message": "Add section for read/write constanst", "committedDate": "2020-12-19T00:19:34Z", "type": "commit"}, {"oid": "1fffc4972993a97f1a33559a3bcc2a1e5fa9f804", "url": "https://github.com/apache/iceberg/commit/1fffc4972993a97f1a33559a3bcc2a1e5fa9f804", "message": "Address review comments", "committedDate": "2020-12-21T20:47:41Z", "type": "commit"}, {"oid": "975f8772e770354181fe2a06f5a6ac58319bfa23", "url": "https://github.com/apache/iceberg/commit/975f8772e770354181fe2a06f5a6ac58319bfa23", "message": "Update spark.md", "committedDate": "2020-12-28T21:53:27Z", "type": "commit"}, {"oid": "16dafbef2def35291ae740817bb675b8951b4ba4", "url": "https://github.com/apache/iceberg/commit/16dafbef2def35291ae740817bb675b8951b4ba4", "message": "Update spark.md", "committedDate": "2020-12-28T21:54:16Z", "type": "commit"}]}