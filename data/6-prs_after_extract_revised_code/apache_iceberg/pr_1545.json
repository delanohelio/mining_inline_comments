{"pr_number": 1545, "pr_title": "Spark3: Make views that reference Iceberg tables show current data", "pr_createdAt": "2020-10-02T16:12:41Z", "pr_url": "https://github.com/apache/iceberg/pull/1545", "timeline": [{"oid": "4be560495400b1b10c5368e3706cbc3a60c0f010", "url": "https://github.com/apache/iceberg/commit/4be560495400b1b10c5368e3706cbc3a60c0f010", "message": "Spark3: Make views that reference Iceberg tables show current data", "committedDate": "2020-10-02T15:56:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODkxNzkxMw==", "url": "https://github.com/apache/iceberg/pull/1545#discussion_r498917913", "bodyText": "This was failing for tables loaded through IcebergSource in Spark 3. Now, it matches Spark 2.", "author": "aokolnychyi", "createdAt": "2020-10-02T16:14:13Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -458,4 +458,55 @@ public void testWriteProjectionWithMiddle() throws IOException {\n     Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n     Assert.assertEquals(\"Result rows should match\", expected, actual);\n   }\n+\n+  @Test\n+  public void testViewsReturnRecentResults() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    tables.create(SCHEMA, spec, location.toString());\n+\n+    List<SimpleRecord> records = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    Dataset<Row> query = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString())\n+        .where(\"id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    List<SimpleRecord> actual1 = spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n+    List<SimpleRecord> expected1 = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\")\n+    );\n+    Assert.assertEquals(\"Number of rows should match\", expected1.size(), actual1.size());\n+    Assert.assertEquals(\"Result rows should match\", expected1, actual1);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    List<SimpleRecord> actual2 = spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();", "originalCommit": "4be560495400b1b10c5368e3706cbc3a60c0f010", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}