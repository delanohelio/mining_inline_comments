{"pr_number": 1641, "pr_title": "Add NaN counter to Metrics and implement in Parquet writers", "pr_createdAt": "2020-10-22T02:20:51Z", "pr_url": "https://github.com/apache/iceberg/pull/1641", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MDk3NA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r509840974", "bodyText": "We may potentially use ColumnDescriptor desc for determine id, but felt that explicitly passing id could provide better guarantee. Suggestions are welcomed.", "author": "yyanyy", "createdAt": "2020-10-22T02:23:48Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -137,11 +141,20 @@ public void write(int repetitionLevel, T value) {\n     public void setColumnStore(ColumnWriteStore columnStore) {\n       this.column.setColumnStore(columnStore);\n     }\n+\n+    @Override\n+    public Stream<FieldMetrics> metrics() {\n+      if (id != null) {\n+        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, 0L, null, null));", "originalCommit": "a47cf2cfd8091ea98452c06ab77a9a69f09c2ec6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMTU4Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514921582", "bodyText": "Yeah it's a messy change, it would be great if we can somehow not pass in the id", "author": "jackye1995", "createdAt": "2020-10-30T07:51:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MDk3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTA1Ng==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515671056", "bodyText": "I think this should be Stream.empty unless it is overridden for float and double.", "author": "rdblue", "createdAt": "2020-11-01T20:58:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MDk3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjIyNA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516246224", "bodyText": "I originally implemented it as Stream.empty and later added these since I wanted to match the set of ids being reported by NaN counter with the set from value/null counter from writer.metrics. But I guess since NaN will never be non-0 for non-float/double fields, and ORC reports a different set of ids than parquet anyway, there's no reason that the sets have to match. I'll update metrics() to only create metric object for float and double writers.", "author": "yyanyy", "createdAt": "2020-11-02T20:50:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MDk3NA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -141,20 +139,11 @@ public class ParquetValueWriters {\n     public void setColumnStore(ColumnWriteStore columnStore) {\n       this.column.setColumnStore(columnStore);\n     }\n-\n-    @Override\n-    public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, 0L, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n-    }\n   }\n \n   private static class UnboxedWriter<T> extends PrimitiveWriter<T> {\n-    private UnboxedWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private UnboxedWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     public void writeBoolean(int repetitionLevel, boolean value) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTU1NA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r509841554", "bodyText": "We could potentially use the schema from the footer itself in ParquetUtil instead of explicitly pass it here; currently have it this way since it's guaranteed that the schema will match.", "author": "yyanyy", "createdAt": "2020-10-22T02:25:57Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java", "diffHunk": "@@ -121,7 +123,7 @@ public void add(T value) {\n \n   @Override\n   public Metrics metrics() {\n-    return ParquetUtil.footerMetrics(writer.getFooter(), metricsConfig);\n+    return ParquetUtil.footerMetrics(writer.getFooter(), model.metrics(), inputSchema, metricsConfig);", "originalCommit": "a47cf2cfd8091ea98452c06ab77a9a69f09c2ec6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMTkxMw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514921913", "bodyText": "Yeah, as my comment in the ParquetUtil class, we should investigate if they are actually the same schema.", "author": "jackye1995", "createdAt": "2020-10-30T07:52:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTU1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjU1Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515672552", "bodyText": "It should be the same.", "author": "rdblue", "createdAt": "2020-11-01T21:11:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTU1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NjI1OA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516246258", "bodyText": "Thank you for the confirmation!", "author": "yyanyy", "createdAt": "2020-11-02T20:50:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MTU1NA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java\nindex bb7a9973b..8b5d00026 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java\n\n@@ -123,7 +121,7 @@ class ParquetWriter<T> implements FileAppender<T>, Closeable {\n \n   @Override\n   public Metrics metrics() {\n-    return ParquetUtil.footerMetrics(writer.getFooter(), model.metrics(), inputSchema, metricsConfig);\n+    return ParquetUtil.footerMetrics(writer.getFooter(), model.metrics(), metricsConfig);\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MjEyMQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r509842121", "bodyText": "I later noticed that there's a TestParquetMetrics as well, will see if I can merge this to that (and eventually toTestMetrics).\nUpdate: I plan to leave this class separately from TestParquetMetrics since it currently test metrics in a different angle than TestParquetMetrics and has more coverage for NaN use case. Will try merging it back to TestMetrics once I finished implementing NaN across all three writers. In the meanwhile I will develop the same test using flink/spark appender factory to ensure coverage. Let me know if you have better suggestions on this!", "author": "yyanyy", "createdAt": "2020-10-22T02:27:55Z", "path": "data/src/test/java/org/apache/iceberg/parquet/TestParquetMergingMetrics.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestParquetMergingMetrics {", "originalCommit": "a47cf2cfd8091ea98452c06ab77a9a69f09c2ec6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a128ae00949320577a279bb44d1245db5538db51", "chunk": "diff --git a/data/src/test/java/org/apache/iceberg/parquet/TestParquetMergingMetrics.java b/data/src/test/java/org/apache/iceberg/parquet/TestParquetMergingMetrics.java\nindex 942d8bf07..03118e3a6 100644\n--- a/data/src/test/java/org/apache/iceberg/parquet/TestParquetMergingMetrics.java\n+++ b/data/src/test/java/org/apache/iceberg/parquet/TestParquetMergingMetrics.java\n\n@@ -19,158 +19,23 @@\n \n package org.apache.iceberg.parquet;\n \n+import java.io.IOException;\n import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TestMergingMetrics;\n import org.apache.iceberg.data.GenericAppenderFactory;\n-import org.apache.iceberg.data.GenericRecord;\n-import org.apache.iceberg.data.RandomGenericData;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.io.FileAppender;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n-import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n \n-import static org.apache.iceberg.types.Types.NestedField.optional;\n-import static org.apache.iceberg.types.Types.NestedField.required;\n-\n-public class TestParquetMergingMetrics {\n-\n-  // all supported fields, except for UUID which is on deprecation path: see https://github.com/apache/iceberg/pull/1611\n-  private static final Types.NestedField ID_FIELD = required(1, \"id\", Types.IntegerType.get());\n-  private static final Types.NestedField DATA_FIELD = optional(2, \"data\", Types.StringType.get());\n-  private static final Types.NestedField FLOAT_FIELD = required(3, \"float\", Types.FloatType.get());\n-  private static final Types.NestedField DOUBLE_FIELD = optional(4, \"double\", Types.DoubleType.get());\n-  private static final Types.NestedField DECIMAL_FIELD = optional(5, \"decimal\", Types.DecimalType.of(5, 3));\n-  private static final Types.NestedField FIXED_FIELD = optional(7, \"fixed\", Types.FixedType.ofLength(4));\n-  private static final Types.NestedField BINARY_FIELD = optional(8, \"binary\", Types.BinaryType.get());\n-  private static final Types.NestedField FLOAT_LIST = optional(9, \"floatlist\",\n-      Types.ListType.ofRequired(10, Types.FloatType.get()));\n-  private static final Types.NestedField LONG_FIELD = optional(11, \"long\", Types.LongType.get());\n-\n-  private static final Types.NestedField MAP_FIELD_1 = optional(17, \"map1\",\n-      Types.MapType.ofOptional(18, 19, Types.FloatType.get(), Types.StringType.get())\n-  );\n-  private static final Types.NestedField MAP_FIELD_2 = optional(20, \"map2\",\n-      Types.MapType.ofOptional(21, 22, Types.IntegerType.get(), Types.DoubleType.get())\n-  );\n-  private static final Types.NestedField STRUCT_FIELD = optional(23, \"structField\", Types.StructType.of(\n-      required(24, \"booleanField\", Types.BooleanType.get()),\n-      optional(25, \"date\", Types.DateType.get()),\n-      optional(26, \"time\", Types.TimeType.get()),\n-      optional(27, \"timestamp\", Types.TimestampType.withZone()),\n-      optional(28, \"timestampWithoutZone\", Types.TimestampType.withoutZone())\n-  ));\n-\n-  private static final Set<Integer> IDS_WITH_ZERO_NAN_COUNT = ImmutableSet.of(1, 2, 5, 7, 8, 11, 24, 25, 26, 27,\n-      28);\n-  private static final Map<Types.NestedField, Integer> FIELDS_WITH_NAN_COUNT_TO_ID = ImmutableMap.of(\n-      FLOAT_FIELD, 3, DOUBLE_FIELD, 4, FLOAT_LIST, 10, MAP_FIELD_1, 18, MAP_FIELD_2, 22\n-  );\n-\n-  // create a schema with all supported fields\n-  private static final Schema SCHEMA = new Schema(\n-      ID_FIELD,\n-      DATA_FIELD,\n-      FLOAT_FIELD,\n-      DOUBLE_FIELD,\n-      DECIMAL_FIELD,\n-      FIXED_FIELD,\n-      BINARY_FIELD,\n-      FLOAT_LIST,\n-      LONG_FIELD,\n-      MAP_FIELD_1,\n-      MAP_FIELD_2,\n-      STRUCT_FIELD\n-  );\n-\n-  @Rule\n-  public TemporaryFolder temp = new TemporaryFolder();\n-\n-  @Test\n-  public void verifySingleRecordMetric() throws Exception {\n-    Record record = GenericRecord.create(SCHEMA);\n-    record.setField(\"id\", 3);\n-    record.setField(\"float\", Float.NaN); // FLOAT_FIELD - 1\n-    record.setField(\"double\", Double.NaN); // DOUBLE_FIELD - 1\n-    record.setField(\"floatlist\", ImmutableList.of(3.3F, 2.8F, Float.NaN, -25.1F, Float.NaN)); // FLOAT_LIST - 2\n-    record.setField(\"map1\", ImmutableMap.of(Float.NaN, \"a\", 0F, \"b\")); // MAP_FIELD_1 - 1\n-    record.setField(\"map2\", ImmutableMap.of(\n-        0, 0D, 1, Double.NaN, 2, 2D, 3, Double.NaN, 4, Double.NaN)); // MAP_FIELD_2 - 3\n+public class TestParquetMergingMetrics extends TestMergingMetrics<Record> {\n \n+  @Override\n+  protected FileAppender<Record> writeAndGetAppender(List<Record> records) throws IOException {\n     FileAppender<Record> appender = new GenericAppenderFactory(SCHEMA).newAppender(\n         org.apache.iceberg.Files.localOutput(temp.newFile()), FileFormat.PARQUET);\n     try (FileAppender<Record> fileAppender = appender) {\n-      fileAppender.add(record);\n+      records.forEach(fileAppender::add);\n     }\n-    Map<Integer, Long> nanValueCount = appender.metrics().nanValueCounts();\n-\n-    assertNaNCountMatch(1L, nanValueCount, FLOAT_FIELD);\n-    assertNaNCountMatch(1L, nanValueCount, DOUBLE_FIELD);\n-    assertNaNCountMatch(2L, nanValueCount, FLOAT_LIST);\n-    assertNaNCountMatch(1L, nanValueCount, MAP_FIELD_1);\n-    assertNaNCountMatch(3L, nanValueCount, MAP_FIELD_2);\n-  }\n-\n-  private void assertNaNCountMatch(Long expected, Map<Integer, Long> nanValueCount, Types.NestedField field) {\n-    Assert.assertEquals(\n-        String.format(\"NaN count for field %s does not match expected\", field.name()),\n-        expected, nanValueCount.get(FIELDS_WITH_NAN_COUNT_TO_ID.get(field)));\n-  }\n-\n-  @Test\n-  public void verifyRandomlyGeneratedRecordsMetric() throws Exception {\n-    List<Record> recordList = RandomGenericData.generate(SCHEMA, 50, 250L);\n-\n-    FileAppender<Record> appender = new GenericAppenderFactory(SCHEMA).newAppender(\n-        org.apache.iceberg.Files.localOutput(temp.newFile()), FileFormat.PARQUET);\n-    try (FileAppender<Record> fileAppender = appender) {\n-      fileAppender.addAll(recordList);\n-    }\n-    Map<Integer, Long> nanValueCount = appender.metrics().nanValueCounts();\n-\n-    IDS_WITH_ZERO_NAN_COUNT.forEach(i -> Assert.assertEquals(String.format(\"Field %s \" +\n-        \"shouldn't have non-zero nanValueCount\", i), Long.valueOf(0), nanValueCount.get(i)));\n-\n-    FIELDS_WITH_NAN_COUNT_TO_ID.forEach((key, value) -> Assert.assertEquals(\n-        String.format(\"NaN count for field %s does not match expected\", key.name()),\n-        getExpectedNaNCount(recordList, key),\n-        nanValueCount.get(value)));\n-  }\n-\n-  private Long getExpectedNaNCount(List<Record> expectedRecords, Types.NestedField field) {\n-    return expectedRecords.stream()\n-        .mapToLong(e -> {\n-          Object value = e.getField(field.name());\n-          if (value == null) {\n-            return 0;\n-          }\n-          if (FLOAT_FIELD.equals(field)) {\n-            return value.equals(Float.NaN) ? 1 : 0;\n-          } else if  (DOUBLE_FIELD.equals(field)) {\n-            return value.equals(Double.NaN) ? 1 : 0;\n-          } else if  (FLOAT_LIST.equals(field)) {\n-            return ((List<Float>) value).stream()\n-                .filter(val -> val != null && val.equals(Float.NaN))\n-                .count();\n-          } else if  (MAP_FIELD_1.equals(field)) {\n-            return ((Map<Float, ?>) value).keySet().stream()\n-                .filter(key -> key.equals(Float.NaN))\n-                .count();\n-          } else if  (MAP_FIELD_2.equals(field)) {\n-            return ((Map<?, Double>) value).values().stream()\n-                .filter(val -> val != null && val.equals(Double.NaN))\n-                .count();\n-          } else {\n-            throw new RuntimeException(\"unknown field name for getting expected NaN count: \" + field.name());\n-          }\n-        }).sum();\n+    return appender;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mjg4Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r509842882", "bodyText": "This has the similar problem I mentioned in pr description for importing spark table; if the file itself is directly passed in there's not much chance to get the additional metrics tracked by value writers. Currently fileMetrics are only used by tests. Do people have suggestions on this?", "author": "yyanyy", "createdAt": "2020-10-22T02:30:45Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java", "diffHunk": "@@ -65,28 +68,25 @@\n   private ParquetUtil() {\n   }\n \n-  // Access modifier is package-private, to only allow use from existing tests\n-  static Metrics fileMetrics(InputFile file) {\n-    return fileMetrics(file, MetricsConfig.getDefault());\n-  }\n-\n   public static Metrics fileMetrics(InputFile file, MetricsConfig metricsConfig) {\n     return fileMetrics(file, metricsConfig, null);\n   }\n \n   public static Metrics fileMetrics(InputFile file, MetricsConfig metricsConfig, NameMapping nameMapping) {\n     try (ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(file))) {\n-      return footerMetrics(reader.getFooter(), metricsConfig, nameMapping);\n+      return footerMetrics(reader.getFooter(), Stream.empty(), null, metricsConfig, nameMapping);", "originalCommit": "a47cf2cfd8091ea98452c06ab77a9a69f09c2ec6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDA2Ng==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515670066", "bodyText": "In that case, we should just set the NaN count map to null. I don't think that we want to scan imported files to create these metrics. Also, I believe that we can rely on recent Parquet versions to not produce min or max values that are NaN, so it should be safe to use these as long as we check that they are not NaN.", "author": "rdblue", "createdAt": "2020-11-01T20:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mjg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI4MjQyNw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516282427", "bodyText": "Thank you! I'll leave this as Stream.empty for now.\nRegarding relying on recent Parquet versions to not produce min or max values that are NaN, sounds like that also answers my question in the pr description (i.e. we won't follow this approach to populate upper and lower bounds). In my current code base it looks like parquet still gives us NaN as max; do you happen to have a reference to the parquet version that supports NaN properly? From a quick search I wasn't able to find it; I noticed this but I suspect it's for something else. I'll look into it more deeply if you don't have it handy.\n\nso it should be safe to use these as long as we check that they are not NaN.\n\nSorry to make sure I understand this correctly, sounds like only the three following cases will be valid:\n\nv1 table, no NaN counter, min/max could have NaN - use the existing logic, we can't do much about min/max==NaN\nv2 table, NaN counter exists, min/max will not be NaN - in this case metrics are produced by iceberg writer\nv2 table, no NaN counter, min/max will not be NaN - in this case the file is imported or from this fileMetrics\n\nThen to accommodate for (3) we will have to remember in evaluators that absence of NaN counter doesn't necessarily mean there's no NaN value in the column; but that might be fine since we will need this logic to accommodate (1) as well (unless we implement evaluators in a way that we can differentiate v1/2 table; not sure if we want that).", "author": "yyanyy", "createdAt": "2020-11-02T22:06:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mjg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY0NzE0Mw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517647143", "bodyText": "I don't have a reference for the Parquet fix. I think I was in a sync where it was discussed. Maybe we should generate our own lower/upper bounds for Parquet then.\nFloat.compare will sort NaN values last, so if we do get max=NaN from Parquet our evaluators should still work as expected. It will just include a much larger range than necessary. If we can generate better stats for table metadata, then that would be ideal.\nYour cases look correct, except that I would say that we expect only max=NaN or min=max=NaN from Parquet. Using the existing logic should be okay.\nI agree that the lack of a NaN counter means that the value is unknown. This is the case for all files written to v1 tables. V2 tables will generally have the NaN counter values, but not in all cases (imported files).", "author": "rdblue", "createdAt": "2020-11-04T21:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mjg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY3NzcwNQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517677705", "bodyText": "Thank you! I guess I'll revisit min/max status for Parquet after NaN related code are mostly done.", "author": "yyanyy", "createdAt": "2020-11-04T22:50:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0Mjg4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\nindex 20f18c2cb..9bc3404dc 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n\n@@ -74,19 +74,19 @@ public class ParquetUtil {\n \n   public static Metrics fileMetrics(InputFile file, MetricsConfig metricsConfig, NameMapping nameMapping) {\n     try (ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(file))) {\n-      return footerMetrics(reader.getFooter(), Stream.empty(), null, metricsConfig, nameMapping);\n+      return footerMetrics(reader.getFooter(), Stream.empty(), metricsConfig, nameMapping);\n     } catch (IOException e) {\n       throw new RuntimeIOException(e, \"Failed to read footer of file: %s\", file);\n     }\n   }\n \n   public static Metrics footerMetrics(ParquetMetadata metadata, Stream<FieldMetrics> fieldMetrics,\n-                                      Schema inputSchema, MetricsConfig metricsConfig) {\n-    return footerMetrics(metadata, fieldMetrics, inputSchema, metricsConfig, null);\n+                                      MetricsConfig metricsConfig) {\n+    return footerMetrics(metadata, fieldMetrics, metricsConfig, null);\n   }\n \n   public static Metrics footerMetrics(ParquetMetadata metadata, Stream<FieldMetrics> fieldMetrics,\n-                                      Schema inputSchema, MetricsConfig metricsConfig, NameMapping nameMapping) {\n+                                      MetricsConfig metricsConfig, NameMapping nameMapping) {\n     long rowCount = 0;\n     Map<Integer, Long> columnSizes = Maps.newHashMap();\n     Map<Integer, Long> valueCounts = Maps.newHashMap();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM3OTEwMA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r510379100", "bodyText": "Please add Java doc.", "author": "giovannifumarola", "createdAt": "2020-10-22T18:42:14Z", "path": "api/src/main/java/org/apache/iceberg/FieldMetrics.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+\n+import java.nio.ByteBuffer;\n+", "originalCommit": "a47cf2cfd8091ea98452c06ab77a9a69f09c2ec6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ5MTMwMg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r510491302", "bodyText": "\ud83d\udc4d  I will update this along with the test update I have.", "author": "yyanyy", "createdAt": "2020-10-22T22:21:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM3OTEwMA=="}], "type": "inlineReview", "revised_code": {"commit": "a128ae00949320577a279bb44d1245db5538db51", "chunk": "diff --git a/api/src/main/java/org/apache/iceberg/FieldMetrics.java b/api/src/main/java/org/apache/iceberg/FieldMetrics.java\nindex 991354811..d67e0dca6 100644\n--- a/api/src/main/java/org/apache/iceberg/FieldMetrics.java\n+++ b/api/src/main/java/org/apache/iceberg/FieldMetrics.java\n\n@@ -22,6 +22,9 @@ package org.apache.iceberg;\n \n import java.nio.ByteBuffer;\n \n+/**\n+ * Iceberg internally tracked field level metrics.\n+ */\n public class FieldMetrics {\n   private final int id;\n   private final long valueCount;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM4MTk1Mw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r510381953", "bodyText": "It is not clear the use of ID here.", "author": "giovannifumarola", "createdAt": "2020-10-22T18:47:19Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java", "diffHunk": "@@ -126,38 +126,39 @@ private SparkParquetWriters() {\n     @Override\n     public ParquetValueWriter<?> primitive(DataType sType, PrimitiveType primitive) {\n       ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+      Type.ID id = primitive.getId();", "originalCommit": "a47cf2cfd8091ea98452c06ab77a9a69f09c2ec6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ5MTY0NQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r510491645", "bodyText": "This is used to create FieldMetrics for individual writer so that we can later have a mapping between id and the new counter. Do you want me to add a comment in the code?", "author": "yyanyy", "createdAt": "2020-10-22T22:22:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM4MTk1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMzA5OQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514923099", "bodyText": "I think it would be more clear with more documentation for the FieldMetrics class and the Stream<FieldMetrics> metrics() method.", "author": "jackye1995", "createdAt": "2020-10-30T07:55:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM4MTk1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java\nindex 7c67a197e..ac345566d 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java\n\n@@ -126,39 +126,38 @@ public class SparkParquetWriters {\n     @Override\n     public ParquetValueWriter<?> primitive(DataType sType, PrimitiveType primitive) {\n       ColumnDescriptor desc = type.getColumnDescription(currentPath());\n-      Type.ID id = primitive.getId();\n \n       if (primitive.getOriginalType() != null) {\n         switch (primitive.getOriginalType()) {\n           case ENUM:\n           case JSON:\n           case UTF8:\n-            return utf8Strings(desc, id);\n+            return utf8Strings(desc);\n           case DATE:\n           case INT_8:\n           case INT_16:\n           case INT_32:\n-            return ints(sType, desc, id);\n+            return ints(sType, desc);\n           case INT_64:\n           case TIME_MICROS:\n           case TIMESTAMP_MICROS:\n-            return ParquetValueWriters.longs(desc, id);\n+            return ParquetValueWriters.longs(desc);\n           case DECIMAL:\n             DecimalMetadata decimal = primitive.getDecimalMetadata();\n             switch (primitive.getPrimitiveTypeName()) {\n               case INT32:\n-                return decimalAsInteger(desc, id, decimal.getPrecision(), decimal.getScale());\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n               case INT64:\n-                return decimalAsLong(desc, id, decimal.getPrecision(), decimal.getScale());\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n               case BINARY:\n               case FIXED_LEN_BYTE_ARRAY:\n-                return decimalAsFixed(desc, id, decimal.getPrecision(), decimal.getScale());\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n               default:\n                 throw new UnsupportedOperationException(\n                     \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n             }\n           case BSON:\n-            return byteArrays(desc, id);\n+            return byteArrays(desc);\n           default:\n             throw new UnsupportedOperationException(\n                 \"Unsupported logical type: \" + primitive.getOriginalType());\n"}}, {"oid": "a128ae00949320577a279bb44d1245db5538db51", "url": "https://github.com/apache/iceberg/commit/a128ae00949320577a279bb44d1245db5538db51", "message": "add flink and spark tests", "committedDate": "2020-10-28T23:18:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMjM1Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r513832352", "bodyText": "The change to this class was mostly trying to use it in TestSparkParquetMergingMetrics. Currently this class is only being used for reading rows for metadata tables (in RowDataReader, and I think only the metadata table will produce DataTask).\nIn the tests I wanted to convert Record to InternalRow for testing Spark appender. I was debating if I should expand this class beyond its current usage, or to write a new converter. Here are the two things I need to change to (partially) implement the former:\n\nnull handling which results to the change in get(): RowDataReader doesn't call get() directly (it uses Dyn reflections for reading individual attributes and skip nulls) for converting into other Spark internal row representation (UnsafeRow in this case), thus we didn't see issue. However, when spark parquet writer uses get(), without this change NPE will occur. Note that even after this change, other use cases of this class (e.g. getUTF8String() are still not null safe, and I wonder if people have opinion on if we want a full null-safe support update to all methods in this class.\nfor getBinary() change, currently we convert Fixed type to binary for Spark (link), and the method I used for creating random records generate fixed type with byte[] (link), and before this change getBinary() doesn't work for byte[] implementation of fixed type. Alternatively we could wrap fixed type in random record generator the same way we do for binary type. I decide to do the former to allow binary related types to be more flexible when wrapping them in this class, but I guess this comes back to the question to if we want to evolve this class or to create a separate converter.", "author": "yyanyy", "createdAt": "2020-10-29T00:03:17Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java", "diffHunk": "@@ -146,8 +146,13 @@ public UTF8String getUTF8String(int ordinal) {\n ", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MzY5OQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515673699", "bodyText": "For #1, I'm fine adding the null check since this isn't used in a high performance code path, but it would be better to have the code that uses this call isNullAt directly because that is the contract for Spark's InternalRow.\nFor #2, since getBinary is called for both fixed and binary types, I think we do need to check the type of the object. I'd much rather do that using instanceof rather than catching an exception. Can you update it to use struct.get(ordinal, Object.class) and then check the type of the object returned?", "author": "rdblue", "createdAt": "2020-11-01T21:23:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMjM1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM0MTM4MQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516341381", "bodyText": "Thanks for the information!\nFor 1, thanks for the info! I wasn't aware of the contract.\nI wasn't sure if we want to add isNullAt for this specific case though, as I guess the problem comes from a difference in behavior between InternalRow and StructInternalRow, and adding isNullAt might have a larger performance penalty in production.\nThe behavior difference comes from the ability of calling internalRow.get() that could return null. The NPE eventually comes from struct.get(index, types[index]) in SparkParquetWriters. While the actual InternalRow could return null for null column, for StructInternalRow it assumes the underlying get() returns non-null, and sometimes performs some actions to them that could lead to NPE. e.g. call toString for converting, or return as primitive type directly. Thus SparkParquetWriters was able to call get fine under normal circumstances, but it couldn't for the specific usage of StructInternalRow in this test.\nFor 2, that's a better idea, will do this instead.", "author": "yyanyy", "createdAt": "2020-11-02T23:50:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMjM1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY0Nzk1MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517647950", "bodyText": "For 1, let's just add the check. This isn't used in a performance-critical path.", "author": "rdblue", "createdAt": "2020-11-04T21:42:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMjM1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY1MDc1MQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517650751", "bodyText": "This looks good to me.", "author": "rdblue", "createdAt": "2020-11-04T21:48:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMjM1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java b/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java\nindex 3d20de7bf..5a0ea0e38 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java\n\n@@ -146,12 +146,15 @@ class StructInternalRow extends InternalRow {\n \n   @Override\n   public byte[] getBinary(int ordinal) {\n-    try {\n-      ByteBuffer bytes = struct.get(ordinal, ByteBuffer.class);\n-      return ByteBuffers.toByteArray(bytes);\n-    } catch (IllegalStateException e) {\n-      // fall back to use byte array for parsing\n-      return struct.get(ordinal, byte[].class);\n+    Object bytes = struct.get(ordinal, Object.class);\n+\n+    // should only be either ByteBuffer or byte[]\n+    if (bytes instanceof ByteBuffer) {\n+      return ByteBuffers.toByteArray((ByteBuffer) bytes);\n+    } else if (bytes instanceof byte[]) {\n+      return (byte[]) bytes;\n+    } else {\n+      throw new IllegalStateException(\"Unknown type for binary field. Type name: \" + bytes.getClass().getName());\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMzk2Ng==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r513833966", "bodyText": "I currently put it this way so that file types other than parquet could use it, however I've noticed that ORC reports metrics differently than parquet. I think ORC currently doesn't report anything for elements in list/map type, but report metrics for the list/map type themselves (i.e. root node), which is completely opposite to what parquet does today.\nTo me what parquet does seems more useful, and I wonder if people have opinion on if we want to record element metrics for ORC along with the new NaN counter. This is not a blocker for this specific pr, but the ORC implementation is on its way and I'd like to gather feedback sooner the better.", "author": "yyanyy", "createdAt": "2020-10-29T00:08:39Z", "path": "data/src/test/java/org/apache/iceberg/TestMergingMetrics.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public abstract class TestMergingMetrics<T> {\n+\n+  // all supported fields, except for UUID which is on deprecation path: see https://github.com/apache/iceberg/pull/1611\n+  // as well as Types.TimeType and Types.TimestampType.withoutZone as both are not supported by Spark\n+  protected static final Types.NestedField ID_FIELD = required(1, \"id\", Types.IntegerType.get());\n+  protected static final Types.NestedField DATA_FIELD = optional(2, \"data\", Types.StringType.get());\n+  protected static final Types.NestedField FLOAT_FIELD = required(3, \"float\", Types.FloatType.get());\n+  protected static final Types.NestedField DOUBLE_FIELD = optional(4, \"double\", Types.DoubleType.get());\n+  protected static final Types.NestedField DECIMAL_FIELD = optional(5, \"decimal\", Types.DecimalType.of(5, 3));\n+  protected static final Types.NestedField FIXED_FIELD = optional(7, \"fixed\", Types.FixedType.ofLength(4));\n+  protected static final Types.NestedField BINARY_FIELD = optional(8, \"binary\", Types.BinaryType.get());\n+  protected static final Types.NestedField FLOAT_LIST = optional(9, \"floatlist\",\n+      Types.ListType.ofRequired(10, Types.FloatType.get()));\n+  protected static final Types.NestedField LONG_FIELD = optional(11, \"long\", Types.LongType.get());\n+\n+  protected static final Types.NestedField MAP_FIELD_1 = optional(17, \"map1\",\n+      Types.MapType.ofOptional(18, 19, Types.FloatType.get(), Types.StringType.get())\n+  );\n+  protected static final Types.NestedField MAP_FIELD_2 = optional(20, \"map2\",\n+      Types.MapType.ofOptional(21, 22, Types.IntegerType.get(), Types.DoubleType.get())\n+  );\n+  protected static final Types.NestedField STRUCT_FIELD = optional(23, \"structField\", Types.StructType.of(\n+      required(24, \"booleanField\", Types.BooleanType.get()),\n+      optional(25, \"date\", Types.DateType.get()),\n+      optional(27, \"timestamp\", Types.TimestampType.withZone())\n+  ));\n+\n+  private static final Set<Integer> IDS_WITH_ZERO_NAN_COUNT = ImmutableSet.of(1, 2, 5, 7, 8, 11, 24, 25, 27);\n+\n+  private static final Map<Types.NestedField, Integer> FIELDS_WITH_NAN_COUNT_TO_ID = ImmutableMap.of(\n+      FLOAT_FIELD, 3, DOUBLE_FIELD, 4, FLOAT_LIST, 10, MAP_FIELD_1, 18, MAP_FIELD_2, 22\n+  );", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTYxMw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515669613", "bodyText": "I'm okay either way. Right now, we don't have any filter predicates that take advantage of nested stats, like containsNull or containsNaN for a column that is a list<float>. We will eventually want to support those predicates, but I don't think that we will add them in the short term since nothing really pushes them down for now. We can always add the stats for ORC when we start adding the predicates.", "author": "rdblue", "createdAt": "2020-11-01T20:43:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMzk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI4OTQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516289475", "bodyText": "Thank you for the explanation!", "author": "yyanyy", "createdAt": "2020-11-02T22:21:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMzk2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java b/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java\nindex f68688bf4..ef205d247 100644\n--- a/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java\n+++ b/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java\n\n@@ -21,14 +21,12 @@ package org.apache.iceberg;\n \n import java.util.List;\n import java.util.Map;\n-import java.util.Set;\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.RandomGenericData;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.types.Types;\n import org.junit.Assert;\n import org.junit.Rule;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNTQ2Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514915462", "bodyText": "Based on the behavior of footer metrics, it is not clear to me if we should have this metrics for Counts mode.", "author": "jackye1995", "createdAt": "2020-10-30T07:34:59Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java", "diffHunk": "@@ -149,9 +149,25 @@ public static Metrics footerMetrics(ParquetMetadata metadata, MetricsConfig metr\n     }\n \n     return new Metrics(rowCount, columnSizes, valueCounts, nullValueCounts,\n+        getNanValueCounts(fieldMetrics, metricsConfig, inputSchema),\n         toBufferMap(fileSchema, lowerBounds), toBufferMap(fileSchema, upperBounds));\n   }\n \n+  private static Map<Integer, Long> getNanValueCounts(\n+      Stream<FieldMetrics> fieldMetrics, MetricsConfig metricsConfig, Schema inputSchema) {\n+    if (fieldMetrics == null || inputSchema == null) {\n+      return Maps.newHashMap();\n+    }\n+\n+    return fieldMetrics\n+        .filter(metrics -> {\n+          String alias = inputSchema.idToAlias(metrics.getId());\n+          MetricsMode metricsMode = metricsConfig.columnMode(alias);\n+          return metricsMode != MetricsModes.None.get();", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDg2NA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515670864", "bodyText": "Yes, we counts mode should include NaN counts. Null counts are passed to increment before the mode check above.", "author": "rdblue", "createdAt": "2020-11-01T20:55:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNTQ2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\nindex 20f18c2cb..9bc3404dc 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n\n@@ -149,7 +149,7 @@ public class ParquetUtil {\n     }\n \n     return new Metrics(rowCount, columnSizes, valueCounts, nullValueCounts,\n-        getNanValueCounts(fieldMetrics, metricsConfig, inputSchema),\n+        getNanValueCounts(fieldMetrics, metricsConfig, fileSchema),\n         toBufferMap(fileSchema, lowerBounds), toBufferMap(fileSchema, upperBounds));\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNjg5MQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514916891", "bodyText": "Why we want to change the interface to include inputSchema? Line 99-100 below already creates the fieldSchema, are they different?", "author": "jackye1995", "createdAt": "2020-10-30T07:38:54Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java", "diffHunk": "@@ -65,28 +68,25 @@\n   private ParquetUtil() {\n   }\n \n-  // Access modifier is package-private, to only allow use from existing tests\n-  static Metrics fileMetrics(InputFile file) {\n-    return fileMetrics(file, MetricsConfig.getDefault());\n-  }\n-\n   public static Metrics fileMetrics(InputFile file, MetricsConfig metricsConfig) {\n     return fileMetrics(file, metricsConfig, null);\n   }\n \n   public static Metrics fileMetrics(InputFile file, MetricsConfig metricsConfig, NameMapping nameMapping) {\n     try (ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(file))) {\n-      return footerMetrics(reader.getFooter(), metricsConfig, nameMapping);\n+      return footerMetrics(reader.getFooter(), Stream.empty(), null, metricsConfig, nameMapping);\n     } catch (IOException e) {\n       throw new RuntimeIOException(e, \"Failed to read footer of file: %s\", file);\n     }\n   }\n \n-  public static Metrics footerMetrics(ParquetMetadata metadata, MetricsConfig metricsConfig) {\n-    return footerMetrics(metadata, metricsConfig, null);\n+  public static Metrics footerMetrics(ParquetMetadata metadata, Stream<FieldMetrics> fieldMetrics,\n+                                      Schema inputSchema, MetricsConfig metricsConfig) {\n+    return footerMetrics(metadata, fieldMetrics, inputSchema, metricsConfig, null);\n   }\n \n-  public static Metrics footerMetrics(ParquetMetadata metadata, MetricsConfig metricsConfig, NameMapping nameMapping) {\n+  public static Metrics footerMetrics(ParquetMetadata metadata, Stream<FieldMetrics> fieldMetrics,", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDEzOA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515670138", "bodyText": "I think this might be related to passing the Parquet field IDs. Since we don't need to do that, we can probably remove the new method here.", "author": "rdblue", "createdAt": "2020-11-01T20:48:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNjg5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDM3MQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515670371", "bodyText": "Oh, I see below that this is used to convert a metrics ID to a field alias. That looks fine to me.", "author": "rdblue", "createdAt": "2020-11-01T20:50:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNjg5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\nindex 20f18c2cb..9bc3404dc 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n\n@@ -74,19 +74,19 @@ public class ParquetUtil {\n \n   public static Metrics fileMetrics(InputFile file, MetricsConfig metricsConfig, NameMapping nameMapping) {\n     try (ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(file))) {\n-      return footerMetrics(reader.getFooter(), Stream.empty(), null, metricsConfig, nameMapping);\n+      return footerMetrics(reader.getFooter(), Stream.empty(), metricsConfig, nameMapping);\n     } catch (IOException e) {\n       throw new RuntimeIOException(e, \"Failed to read footer of file: %s\", file);\n     }\n   }\n \n   public static Metrics footerMetrics(ParquetMetadata metadata, Stream<FieldMetrics> fieldMetrics,\n-                                      Schema inputSchema, MetricsConfig metricsConfig) {\n-    return footerMetrics(metadata, fieldMetrics, inputSchema, metricsConfig, null);\n+                                      MetricsConfig metricsConfig) {\n+    return footerMetrics(metadata, fieldMetrics, metricsConfig, null);\n   }\n \n   public static Metrics footerMetrics(ParquetMetadata metadata, Stream<FieldMetrics> fieldMetrics,\n-                                      Schema inputSchema, MetricsConfig metricsConfig, NameMapping nameMapping) {\n+                                      MetricsConfig metricsConfig, NameMapping nameMapping) {\n     long rowCount = 0;\n     Map<Integer, Long> columnSizes = Maps.newHashMap();\n     Map<Integer, Long> valueCounts = Maps.newHashMap();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMjYwMA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514922600", "bodyText": "Documentation needed. And is the name a bit too generic?", "author": "jackye1995", "createdAt": "2020-10-30T07:54:03Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java", "diffHunk": "@@ -28,4 +30,7 @@\n   List<TripleWriter<?>> columns();\n \n   void setColumnStore(ColumnWriteStore columnStore);\n+\n+  Stream<FieldMetrics> metrics();", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NzI2MQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516357261", "bodyText": "I'll add documentations. For the name, I think ParquetValueWriter already implies that it's field specific, so I guess metrics itself would convey the idea that it's field specific metrics. Do you have better suggestions?", "author": "yyanyy", "createdAt": "2020-11-03T00:23:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMjYwMA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java\nindex 342636339..e060b86c4 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java\n\n@@ -31,6 +31,15 @@ public interface ParquetValueWriter<T> {\n \n   void setColumnStore(ColumnWriteStore columnStore);\n \n-  Stream<FieldMetrics> metrics();\n+  /**\n+   * Returns a stream of {@link FieldMetrics} that this ParquetValueWriter keeps track of.\n+   *\n+   * Since Parquet keeps track of most metrics in its footer, for now ParquetValueWriter only keeps track of NaN\n+   * counter, and only return non-empty stream if the writer writes double or float values either by itself or\n+   * transitively.\n+   */\n+  default Stream<FieldMetrics> metrics() {\n+    return Stream.empty();\n+  }\n }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMzU4MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r514923580", "bodyText": "Do we need all these metrics that can be obtained from the footer? To me it feels more reasonable for this class to only contain metrics that is not covered by the footer metrics.", "author": "jackye1995", "createdAt": "2020-10-30T07:56:21Z", "path": "api/src/main/java/org/apache/iceberg/FieldMetrics.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Iceberg internally tracked field level metrics.\n+ */\n+public class FieldMetrics {\n+  private final int id;\n+  private final long valueCount;", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1MDIyOQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516250229", "bodyText": "Since I'll implement the same for ORC and Avro, and Avro currently doesn't have metrics support, in Avro we will populate the other fields. Ryan brought up a good idea of having a ParquetFieldMetrics to limit the access to non-nan fields, and I'll extend this class to implement ParquetFieldMetrics, and only initialize ParquetFieldMetrics in parquet related writers.", "author": "yyanyy", "createdAt": "2020-11-02T20:58:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMzU4MA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/api/src/main/java/org/apache/iceberg/FieldMetrics.java b/api/src/main/java/org/apache/iceberg/FieldMetrics.java\nindex d67e0dca6..d67faa94f 100644\n--- a/api/src/main/java/org/apache/iceberg/FieldMetrics.java\n+++ b/api/src/main/java/org/apache/iceberg/FieldMetrics.java\n\n@@ -47,27 +47,45 @@ public class FieldMetrics {\n     this.upperBound = upperBound;\n   }\n \n-  public int getId() {\n+  /**\n+   * Returns the id of the field that the metrics within this class are associated with.\n+   */\n+  public int id() {\n     return id;\n   }\n \n-  public long getValueCount() {\n+  /**\n+   * Returns the number of all values, including nulls, NaN and repeated, for the given field.\n+   */\n+  public long valueCount() {\n     return valueCount;\n   }\n \n-  public long getNullValueCount() {\n+  /**\n+   * Returns the number of null values for this field.\n+   */\n+  public long nullValueCount() {\n     return nullValueCount;\n   }\n \n-  public long getNanValueCount() {\n+  /**\n+   * Returns the number of NaN values for this field. Will only be non-0 if this field is a double or float field.\n+   */\n+  public long nanValueCount() {\n     return nanValueCount;\n   }\n \n-  public ByteBuffer getLowerBound() {\n+  /**\n+   * Returns the lower bound value of this field.\n+   */\n+  public ByteBuffer lowerBound() {\n     return lowerBound;\n   }\n \n-  public ByteBuffer getUpperBound() {\n+  /**\n+   * Returns the upper bound value of this field.\n+   */\n+  public ByteBuffer upperBound() {\n     return upperBound;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NzQwOQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515667409", "bodyText": "Style: We avoid using get in method names because it adds no value. For a simple getter, the method should be named for the field: valueCount().", "author": "rdblue", "createdAt": "2020-11-01T20:23:19Z", "path": "api/src/main/java/org/apache/iceberg/FieldMetrics.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Iceberg internally tracked field level metrics.\n+ */\n+public class FieldMetrics {\n+  private final int id;\n+  private final long valueCount;\n+  private final long nullValueCount;\n+  private final long nanValueCount;\n+  private final ByteBuffer lowerBound;\n+  private final ByteBuffer upperBound;\n+\n+  public FieldMetrics(int id,\n+                      long valueCount,\n+                      long nullValueCount,\n+                      long nanValueCount,\n+                      ByteBuffer lowerBound,\n+                      ByteBuffer upperBound) {\n+    this.id = id;\n+    this.valueCount = valueCount;\n+    this.nullValueCount = nullValueCount;\n+    this.nanValueCount = nanValueCount;\n+    this.lowerBound = lowerBound;\n+    this.upperBound = upperBound;\n+  }\n+\n+  public int getId() {\n+    return id;\n+  }\n+\n+  public long getValueCount() {", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/api/src/main/java/org/apache/iceberg/FieldMetrics.java b/api/src/main/java/org/apache/iceberg/FieldMetrics.java\nindex d67e0dca6..d67faa94f 100644\n--- a/api/src/main/java/org/apache/iceberg/FieldMetrics.java\n+++ b/api/src/main/java/org/apache/iceberg/FieldMetrics.java\n\n@@ -47,27 +47,45 @@ public class FieldMetrics {\n     this.upperBound = upperBound;\n   }\n \n-  public int getId() {\n+  /**\n+   * Returns the id of the field that the metrics within this class are associated with.\n+   */\n+  public int id() {\n     return id;\n   }\n \n-  public long getValueCount() {\n+  /**\n+   * Returns the number of all values, including nulls, NaN and repeated, for the given field.\n+   */\n+  public long valueCount() {\n     return valueCount;\n   }\n \n-  public long getNullValueCount() {\n+  /**\n+   * Returns the number of null values for this field.\n+   */\n+  public long nullValueCount() {\n     return nullValueCount;\n   }\n \n-  public long getNanValueCount() {\n+  /**\n+   * Returns the number of NaN values for this field. Will only be non-0 if this field is a double or float field.\n+   */\n+  public long nanValueCount() {\n     return nanValueCount;\n   }\n \n-  public ByteBuffer getLowerBound() {\n+  /**\n+   * Returns the lower bound value of this field.\n+   */\n+  public ByteBuffer lowerBound() {\n     return lowerBound;\n   }\n \n-  public ByteBuffer getUpperBound() {\n+  /**\n+   * Returns the upper bound value of this field.\n+   */\n+  public ByteBuffer upperBound() {\n     return upperBound;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NzU4Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515667582", "bodyText": "If we intend to remove it, then we should add @Deprecated and Javadoc @deprecated will be removed in 0.12.0; use ... instead.", "author": "rdblue", "createdAt": "2020-11-01T20:24:45Z", "path": "api/src/main/java/org/apache/iceberg/Metrics.java", "diffHunk": "@@ -37,12 +37,14 @@\n   private Map<Integer, Long> columnSizes = null;\n   private Map<Integer, Long> valueCounts = null;\n   private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, Long> nanValueCounts = null;\n   private Map<Integer, ByteBuffer> lowerBounds = null;\n   private Map<Integer, ByteBuffer> upperBounds = null;\n \n   public Metrics() {\n   }\n \n+  // for temporary backward compatibility, will be removed when all writers support nanValueCounts", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/api/src/main/java/org/apache/iceberg/Metrics.java b/api/src/main/java/org/apache/iceberg/Metrics.java\nindex 5d5ce6f98..d5367c448 100644\n--- a/api/src/main/java/org/apache/iceberg/Metrics.java\n+++ b/api/src/main/java/org/apache/iceberg/Metrics.java\n\n@@ -44,7 +44,10 @@ public class Metrics implements Serializable {\n   public Metrics() {\n   }\n \n-  // for temporary backward compatibility, will be removed when all writers support nanValueCounts\n+  /**\n+   * @deprecated will be removed in 0.12.0; use {@link #Metrics(Long, Map, Map, Map, Map)} instead.\n+   */\n+  @Deprecated\n   public Metrics(Long rowCount,\n                  Map<Integer, Long> columnSizes,\n                  Map<Integer, Long> valueCounts,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NzY1NA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515667654", "bodyText": "For all float and double fields. Other entries should be omitted because they are 0.", "author": "rdblue", "createdAt": "2020-11-01T20:25:15Z", "path": "api/src/main/java/org/apache/iceberg/Metrics.java", "diffHunk": "@@ -103,6 +134,15 @@ public Long recordCount() {\n     return nullValueCounts;\n   }\n \n+  /**\n+   * Get the number of NaN values for all fields in a file.", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/api/src/main/java/org/apache/iceberg/Metrics.java b/api/src/main/java/org/apache/iceberg/Metrics.java\nindex 5d5ce6f98..d5367c448 100644\n--- a/api/src/main/java/org/apache/iceberg/Metrics.java\n+++ b/api/src/main/java/org/apache/iceberg/Metrics.java\n\n@@ -135,7 +141,7 @@ public class Metrics implements Serializable {\n   }\n \n   /**\n-   * Get the number of NaN values for all fields in a file.\n+   * Get the number of NaN values for all float and double fields in a file.\n    *\n    * @return a Map of fieldId to the number of NaN counts\n    */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Nzg3Ng==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515667876", "bodyText": "Rather than changing so many rows that call assertCounts, you may want to consider adding a second variant of the method that accepts the count. Then you'd only need to change the double and float cases.", "author": "rdblue", "createdAt": "2020-11-01T20:27:20Z", "path": "core/src/test/java/org/apache/iceberg/TestMetrics.java", "diffHunk": "@@ -257,31 +238,27 @@ public void testMetricsForDecimals() throws IOException {\n     record.setField(\"decimalAsInt64\", new BigDecimal(\"4.75\"));\n     record.setField(\"decimalAsFixed\", new BigDecimal(\"5.80\"));\n \n-    InputFile recordsFile = writeRecords(schema, record);\n-\n-    Metrics metrics = getMetrics(recordsFile);\n+    Metrics metrics = getMetrics(schema, record);\n     Assert.assertEquals(1L, (long) metrics.recordCount());\n-    assertCounts(1, 1L, 0L, metrics);\n+    assertCounts(1, 1L, 0L, 0L, metrics);", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2MTYyMQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516361621", "bodyText": "I was thinking to make NaN count explicitly required so that people adding new tests will remember to take it into consideration, but I guess it's not needed at least before NaN is fully supported, especially now that we don't want NaN count for non-double/float fields. Will update.", "author": "yyanyy", "createdAt": "2020-11-03T00:32:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2Nzg3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetrics.java b/core/src/test/java/org/apache/iceberg/TestMetrics.java\nindex e2c9256b7..0c5cf0cd2 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetrics.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetrics.java\n\n@@ -240,11 +245,11 @@ public abstract class TestMetrics {\n \n     Metrics metrics = getMetrics(schema, record);\n     Assert.assertEquals(1L, (long) metrics.recordCount());\n-    assertCounts(1, 1L, 0L, 0L, metrics);\n+    assertCounts(1, 1L, 0L, metrics);\n     assertBounds(1, DecimalType.of(4, 2), new BigDecimal(\"2.55\"), new BigDecimal(\"2.55\"), metrics);\n-    assertCounts(2, 1L, 0L, 0L, metrics);\n+    assertCounts(2, 1L, 0L, metrics);\n     assertBounds(2, DecimalType.of(14, 2), new BigDecimal(\"4.75\"), new BigDecimal(\"4.75\"), metrics);\n-    assertCounts(3, 1L, 0L, 0L, metrics);\n+    assertCounts(3, 1L, 0L, metrics);\n     assertBounds(3, DecimalType.of(22, 2), new BigDecimal(\"5.80\"), new BigDecimal(\"5.80\"), metrics);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTE0NA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515669144", "bodyText": "There are a few cases to consider with NaN values because comparison with NaN is always false.\nHere are a couple of implementations that have issues because they use comparison without checking for NaN:\n// max is NaN for values [NaN, 1.0, 1.1]\nFloat max = null;\nfor (value : values) {\n  if (max == null || max < value) {\n    max = value;\n  }\n}\n\n// max is NaN if any value is NaN\nFloat max = null;\nfor (value : values) {\n  max = (max != null && max >= value) ? max : value;\n}\nBecause the failure cases are different, I think we should test a few different cases:\n\nA column starts with NaN\nA column contains NaN in the middle\nA column ends with NaN", "author": "rdblue", "createdAt": "2020-11-01T20:38:59Z", "path": "core/src/test/java/org/apache/iceberg/TestMetrics.java", "diffHunk": "@@ -352,14 +327,34 @@ public void testMetricsForNullColumns() throws IOException {\n     Record secondRecord = GenericRecord.create(schema);\n     secondRecord.setField(\"intCol\", null);\n \n-    InputFile recordsFile = writeRecords(schema, firstRecord, secondRecord);\n-\n-    Metrics metrics = getMetrics(recordsFile);\n+    Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n-    assertCounts(1, 2L, 2L, metrics);\n+    assertCounts(1, 2L, 2L, 0L, metrics);\n     assertBounds(1, IntegerType.get(), null, null, metrics);\n   }\n \n+  @Test\n+  public void testMetricsForNaNColumns() throws IOException {", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3ODM4MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516378380", "bodyText": "Thanks for pointing this out! I guess this tests more about boundary than of NaN count, but it would be very helpful when we start to exclude NaN from upper/lower bounds. Will update.", "author": "yyanyy", "createdAt": "2020-11-03T01:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTE0NA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetrics.java b/core/src/test/java/org/apache/iceberg/TestMetrics.java\nindex e2c9256b7..0c5cf0cd2 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetrics.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetrics.java\n\n@@ -329,24 +334,20 @@ public abstract class TestMetrics {\n \n     Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n-    assertCounts(1, 2L, 2L, 0L, metrics);\n+    assertCounts(1, 2L, 2L, metrics);\n     assertBounds(1, IntegerType.get(), null, null, metrics);\n   }\n \n   @Test\n   public void testMetricsForNaNColumns() throws IOException {\n-    Schema schema = new Schema(\n-        optional(1, \"floatCol\", FloatType.get()),\n-        optional(2, \"doubleCol\", DoubleType.get())\n-    );\n-    Record firstRecord = GenericRecord.create(schema);\n+    Record firstRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n     firstRecord.setField(\"floatCol\", Float.NaN);\n     firstRecord.setField(\"doubleCol\", Double.NaN);\n-    Record secondRecord = GenericRecord.create(schema);\n+    Record secondRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n     secondRecord.setField(\"floatCol\", Float.NaN);\n     secondRecord.setField(\"doubleCol\", Double.NaN);\n \n-    Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n+    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, firstRecord, secondRecord);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n     assertCounts(1, 2L, 0L, 2L, metrics);\n     assertCounts(2, 2L, 0L, 2L, metrics);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTgxOQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515669819", "bodyText": "Instead of passing the id, you can get it from the column descriptor:\nint id = desc.getPrimitiveType().getId().intValue();\nThat will remove a lot of changes from this file.", "author": "rdblue", "createdAt": "2020-11-01T20:45:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -167,70 +168,70 @@ private FlinkParquetWriters() {\n       switch (primitive.getPrimitiveTypeName()) {\n         case FIXED_LEN_BYTE_ARRAY:\n         case BINARY:\n-          return byteArrays(desc);\n+          return byteArrays(desc, id);\n         case BOOLEAN:\n-          return ParquetValueWriters.booleans(desc);\n+          return ParquetValueWriters.booleans(desc, id);\n         case INT32:\n-          return ints(fType, desc);\n+          return ints(fType, desc, id);\n         case INT64:\n-          return ParquetValueWriters.longs(desc);\n+          return ParquetValueWriters.longs(desc, id);\n         case FLOAT:\n-          return ParquetValueWriters.floats(desc);\n+          return ParquetValueWriters.floats(desc, id);\n         case DOUBLE:\n-          return ParquetValueWriters.doubles(desc);\n+          return ParquetValueWriters.doubles(desc, id);\n         default:\n           throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n       }\n     }\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc, Type.ID id) {\n     if (type instanceof TinyIntType) {\n-      return ParquetValueWriters.tinyints(desc);\n+      return ParquetValueWriters.tinyints(desc, id);", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 5d7c1e3eb..c91b659b6 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -168,70 +167,70 @@ public class FlinkParquetWriters {\n       switch (primitive.getPrimitiveTypeName()) {\n         case FIXED_LEN_BYTE_ARRAY:\n         case BINARY:\n-          return byteArrays(desc, id);\n+          return byteArrays(desc);\n         case BOOLEAN:\n-          return ParquetValueWriters.booleans(desc, id);\n+          return ParquetValueWriters.booleans(desc);\n         case INT32:\n-          return ints(fType, desc, id);\n+          return ints(fType, desc);\n         case INT64:\n-          return ParquetValueWriters.longs(desc, id);\n+          return ParquetValueWriters.longs(desc);\n         case FLOAT:\n-          return ParquetValueWriters.floats(desc, id);\n+          return ParquetValueWriters.floats(desc);\n         case DOUBLE:\n-          return ParquetValueWriters.doubles(desc, id);\n+          return ParquetValueWriters.doubles(desc);\n         default:\n           throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n       }\n     }\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc, Type.ID id) {\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n     if (type instanceof TinyIntType) {\n-      return ParquetValueWriters.tinyints(desc, id);\n+      return ParquetValueWriters.tinyints(desc);\n     } else if (type instanceof SmallIntType) {\n-      return ParquetValueWriters.shorts(desc, id);\n+      return ParquetValueWriters.shorts(desc);\n     }\n-    return ParquetValueWriters.ints(desc, id);\n+    return ParquetValueWriters.ints(desc);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc, Type.ID id) {\n-    return new StringDataWriter(desc, id);\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc, Type.ID id) {\n-    return new TimeMicrosWriter(desc, id);\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc, Type.ID id,\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n                                                                                    int precision, int scale) {\n     Preconditions.checkArgument(precision <= 9, \"Cannot write decimal value as integer with precision larger than 9,\" +\n         \" wrong precision %s\", precision);\n-    return new IntegerDecimalWriter(desc, id, precision, scale);\n+    return new IntegerDecimalWriter(desc, precision, scale);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc, Type.ID id,\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n                                                                                 int precision, int scale) {\n     Preconditions.checkArgument(precision <= 18, \"Cannot write decimal value as long with precision larger than 18, \" +\n         \" wrong precision %s\", precision);\n-    return new LongDecimalWriter(desc, id, precision, scale);\n+    return new LongDecimalWriter(desc, precision, scale);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc, Type.ID id,\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n                                                                                  int precision, int scale) {\n-    return new FixedDecimalWriter(desc, id, precision, scale);\n+    return new FixedDecimalWriter(desc, precision, scale);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc, Type.ID id) {\n-    return new TimestampDataWriter(desc, id);\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n   }\n \n-  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc, Type.ID id) {\n-    return new ByteArrayWriter(desc, id);\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n   }\n \n   private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n-    private StringDataWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDY2Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515670662", "bodyText": "Alias isn't what you want to use here. An alias is the file schema's name when an Iceberg schema is converted from a file schema. Parquet and Avro don't allow special characters in field names, or a column's name may have changed after a file is written. In both cases, a file schema's names won't match the schema. The alias map exposes the original file field names for when we need to use them (e.g., get a page reader for a column from the file).\nIn this case, we want to use the table schema's name, not a file schema's name. That's why we use findColumnName above. That should work here as well.", "author": "rdblue", "createdAt": "2020-11-01T20:54:05Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java", "diffHunk": "@@ -149,9 +149,25 @@ public static Metrics footerMetrics(ParquetMetadata metadata, MetricsConfig metr\n     }\n \n     return new Metrics(rowCount, columnSizes, valueCounts, nullValueCounts,\n+        getNanValueCounts(fieldMetrics, metricsConfig, inputSchema),\n         toBufferMap(fileSchema, lowerBounds), toBufferMap(fileSchema, upperBounds));\n   }\n \n+  private static Map<Integer, Long> getNanValueCounts(\n+      Stream<FieldMetrics> fieldMetrics, MetricsConfig metricsConfig, Schema inputSchema) {\n+    if (fieldMetrics == null || inputSchema == null) {\n+      return Maps.newHashMap();\n+    }\n+\n+    return fieldMetrics\n+        .filter(metrics -> {\n+          String alias = inputSchema.idToAlias(metrics.getId());", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1MjMxOQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516252319", "bodyText": "Thank you for the explanation!", "author": "yyanyy", "createdAt": "2020-11-02T21:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDY2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\nindex 20f18c2cb..9bc3404dc 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java\n\n@@ -149,7 +149,7 @@ public class ParquetUtil {\n     }\n \n     return new Metrics(rowCount, columnSizes, valueCounts, nullValueCounts,\n-        getNanValueCounts(fieldMetrics, metricsConfig, inputSchema),\n+        getNanValueCounts(fieldMetrics, metricsConfig, fileSchema),\n         toBufferMap(fileSchema, lowerBounds), toBufferMap(fileSchema, upperBounds));\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTI1MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515671250", "bodyText": "Rather than passing incorrect metrics, I think it would be better to create a ParquetFieldMetrics class that throws exceptions when the other metrics are accessed. That will ensure that the other metrics values aren't used.", "author": "rdblue", "createdAt": "2020-11-01T20:59:33Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -165,9 +178,65 @@ public void writeDouble(int repetitionLevel, double value) {\n     }\n   }\n \n+  private static class FloatWriter extends UnboxedWriter<Float> {\n+    private final ColumnDescriptor desc;\n+    private long nanCount;\n+\n+    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n+      super(desc, id);\n+      this.desc = desc;\n+      this.nanCount = 0;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Float value) {\n+      super.write(repetitionLevel, value);\n+      if (Float.compare(Float.NaN, value) == 0) {\n+        nanCount++;\n+      }\n+    }\n+\n+    @Override\n+    public Stream<FieldMetrics> metrics() {\n+      if (id != null) {\n+        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1MjQ4Nw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516252487", "bodyText": "That's a good idea! Will update", "author": "yyanyy", "createdAt": "2020-11-02T21:03:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTI1MA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -179,64 +168,56 @@ public class ParquetValueWriters {\n   }\n \n   private static class FloatWriter extends UnboxedWriter<Float> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private FloatWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Float value) {\n-      super.write(repetitionLevel, value);\n-      if (Float.compare(Float.NaN, value) == 0) {\n+      writeFloat(repetitionLevel, value);\n+      if (Float.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class DoubleWriter extends UnboxedWriter<Double> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private DoubleWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private DoubleWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Double value) {\n-      super.write(repetitionLevel, value);\n-      if (Double.compare(Double.NaN, value) == 0) {\n+      writeDouble(repetitionLevel, value);\n+      if (Double.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class ByteWriter extends UnboxedWriter<Byte> {\n-    private ByteWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private ByteWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTQwMQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515671401", "bodyText": "I think this should always produce the metric. Field IDs are required to write, so we are guaranteed that they are always present (or should fail if one is not). And as long as this is always gathering the metric, we may as well return it.", "author": "rdblue", "createdAt": "2020-11-01T21:00:41Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -165,9 +178,65 @@ public void writeDouble(int repetitionLevel, double value) {\n     }\n   }\n \n+  private static class FloatWriter extends UnboxedWriter<Float> {\n+    private final ColumnDescriptor desc;\n+    private long nanCount;\n+\n+    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n+      super(desc, id);\n+      this.desc = desc;\n+      this.nanCount = 0;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Float value) {\n+      super.write(repetitionLevel, value);\n+      if (Float.compare(Float.NaN, value) == 0) {\n+        nanCount++;\n+      }\n+    }\n+\n+    @Override\n+    public Stream<FieldMetrics> metrics() {\n+      if (id != null) {", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -179,64 +168,56 @@ public class ParquetValueWriters {\n   }\n \n   private static class FloatWriter extends UnboxedWriter<Float> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private FloatWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Float value) {\n-      super.write(repetitionLevel, value);\n-      if (Float.compare(Float.NaN, value) == 0) {\n+      writeFloat(repetitionLevel, value);\n+      if (Float.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class DoubleWriter extends UnboxedWriter<Double> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private DoubleWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private DoubleWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Double value) {\n-      super.write(repetitionLevel, value);\n-      if (Double.compare(Double.NaN, value) == 0) {\n+      writeDouble(repetitionLevel, value);\n+      if (Double.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class ByteWriter extends UnboxedWriter<Byte> {\n-    private ByteWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private ByteWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTQ2Nw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515671467", "bodyText": "The descriptor isn't used, so I don't think it is needed. This class should keep the int id only, I think.", "author": "rdblue", "createdAt": "2020-11-01T21:01:15Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -165,9 +178,65 @@ public void writeDouble(int repetitionLevel, double value) {\n     }\n   }\n \n+  private static class FloatWriter extends UnboxedWriter<Float> {\n+    private final ColumnDescriptor desc;\n+    private long nanCount;\n+\n+    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n+      super(desc, id);\n+      this.desc = desc;", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1NjIxNw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516256217", "bodyText": "Not sure why I preserved it, thanks for pointing out!", "author": "yyanyy", "createdAt": "2020-11-02T21:11:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTQ2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -179,64 +168,56 @@ public class ParquetValueWriters {\n   }\n \n   private static class FloatWriter extends UnboxedWriter<Float> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private FloatWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Float value) {\n-      super.write(repetitionLevel, value);\n-      if (Float.compare(Float.NaN, value) == 0) {\n+      writeFloat(repetitionLevel, value);\n+      if (Float.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class DoubleWriter extends UnboxedWriter<Double> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private DoubleWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private DoubleWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Double value) {\n-      super.write(repetitionLevel, value);\n-      if (Double.compare(Double.NaN, value) == 0) {\n+      writeDouble(repetitionLevel, value);\n+      if (Double.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class ByteWriter extends UnboxedWriter<Byte> {\n-    private ByteWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private ByteWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTY4MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515671680", "bodyText": "This can call writeFloat directly instead of super.", "author": "rdblue", "createdAt": "2020-11-01T21:03:51Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -165,9 +178,65 @@ public void writeDouble(int repetitionLevel, double value) {\n     }\n   }\n \n+  private static class FloatWriter extends UnboxedWriter<Float> {\n+    private final ColumnDescriptor desc;\n+    private long nanCount;\n+\n+    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n+      super(desc, id);\n+      this.desc = desc;\n+      this.nanCount = 0;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Float value) {\n+      super.write(repetitionLevel, value);", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -179,64 +168,56 @@ public class ParquetValueWriters {\n   }\n \n   private static class FloatWriter extends UnboxedWriter<Float> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private FloatWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Float value) {\n-      super.write(repetitionLevel, value);\n-      if (Float.compare(Float.NaN, value) == 0) {\n+      writeFloat(repetitionLevel, value);\n+      if (Float.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class DoubleWriter extends UnboxedWriter<Double> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private DoubleWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private DoubleWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Double value) {\n-      super.write(repetitionLevel, value);\n-      if (Double.compare(Double.NaN, value) == 0) {\n+      writeDouble(repetitionLevel, value);\n+      if (Double.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class ByteWriter extends UnboxedWriter<Byte> {\n-    private ByteWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private ByteWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjI1NA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515672254", "bodyText": "I would prefer to use float.isNaN() instead because it is more direct. The implementation of compare does a few comparisons because it needs to work for normal values, but Float.isNaN just returns value != value.", "author": "rdblue", "createdAt": "2020-11-01T21:08:12Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -165,9 +178,65 @@ public void writeDouble(int repetitionLevel, double value) {\n     }\n   }\n \n+  private static class FloatWriter extends UnboxedWriter<Float> {\n+    private final ColumnDescriptor desc;\n+    private long nanCount;\n+\n+    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n+      super(desc, id);\n+      this.desc = desc;\n+      this.nanCount = 0;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Float value) {\n+      super.write(repetitionLevel, value);\n+      if (Float.compare(Float.NaN, value) == 0) {", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1NjM5Nw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r516256397", "bodyText": "Didn't notice that this method exists, thank you!", "author": "yyanyy", "createdAt": "2020-11-02T21:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjI1NA=="}], "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -179,64 +168,56 @@ public class ParquetValueWriters {\n   }\n \n   private static class FloatWriter extends UnboxedWriter<Float> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private FloatWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Float value) {\n-      super.write(repetitionLevel, value);\n-      if (Float.compare(Float.NaN, value) == 0) {\n+      writeFloat(repetitionLevel, value);\n+      if (Float.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class DoubleWriter extends UnboxedWriter<Double> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private DoubleWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private DoubleWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Double value) {\n-      super.write(repetitionLevel, value);\n-      if (Double.compare(Double.NaN, value) == 0) {\n+      writeDouble(repetitionLevel, value);\n+      if (Double.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class ByteWriter extends UnboxedWriter<Byte> {\n-    private ByteWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private ByteWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjI5Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515672292", "bodyText": "Same comments from the float case above.", "author": "rdblue", "createdAt": "2020-11-01T21:08:30Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -165,9 +178,65 @@ public void writeDouble(int repetitionLevel, double value) {\n     }\n   }\n \n+  private static class FloatWriter extends UnboxedWriter<Float> {\n+    private final ColumnDescriptor desc;\n+    private long nanCount;\n+\n+    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n+      super(desc, id);\n+      this.desc = desc;\n+      this.nanCount = 0;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Float value) {\n+      super.write(repetitionLevel, value);\n+      if (Float.compare(Float.NaN, value) == 0) {\n+        nanCount++;\n+      }\n+    }\n+\n+    @Override\n+    public Stream<FieldMetrics> metrics() {\n+      if (id != null) {\n+        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n+      } else {\n+        return Stream.empty();\n+      }\n+    }\n+  }\n+\n+  private static class DoubleWriter extends UnboxedWriter<Double> {", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -179,64 +168,56 @@ public class ParquetValueWriters {\n   }\n \n   private static class FloatWriter extends UnboxedWriter<Float> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private FloatWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private FloatWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Float value) {\n-      super.write(repetitionLevel, value);\n-      if (Float.compare(Float.NaN, value) == 0) {\n+      writeFloat(repetitionLevel, value);\n+      if (Float.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class DoubleWriter extends UnboxedWriter<Double> {\n-    private final ColumnDescriptor desc;\n+    private final int id;\n     private long nanCount;\n \n-    private DoubleWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n-      this.desc = desc;\n+    private DoubleWriter(ColumnDescriptor desc) {\n+      super(desc);\n+      this.id = desc.getPrimitiveType().getId().intValue();\n       this.nanCount = 0;\n     }\n \n     @Override\n     public void write(int repetitionLevel, Double value) {\n-      super.write(repetitionLevel, value);\n-      if (Double.compare(Double.NaN, value) == 0) {\n+      writeDouble(repetitionLevel, value);\n+      if (Double.isNaN(value)) {\n         nanCount++;\n       }\n     }\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      if (id != null) {\n-        return Stream.of(new FieldMetrics(id.intValue(), 0L, 0L, nanCount, null, null));\n-      } else {\n-        return Stream.empty();\n-      }\n+      return Stream.of(new ParquetFieldMetrics(id, nanCount));\n     }\n   }\n \n   private static class ByteWriter extends UnboxedWriter<Byte> {\n-    private ByteWriter(ColumnDescriptor desc, Type.ID id) {\n-      super(desc, id);\n+    private ByteWriter(ColumnDescriptor desc) {\n+      super(desc);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjQxOQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515672419", "bodyText": "Why not Stream.of(keyWriter.metrics(), valueWriter.metrics())? That seems easier than creating a stream only to create another stream.", "author": "rdblue", "createdAt": "2020-11-01T21:09:41Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -435,6 +514,11 @@ public void setColumnStore(ColumnWriteStore columnStore) {\n     }\n \n     protected abstract Iterator<Map.Entry<K, V>> pairs(M value);\n+\n+    @Override\n+    public Stream<FieldMetrics> metrics() {\n+      return Stream.of(keyWriter, valueWriter).flatMap(ParquetValueWriter::metrics);", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex f80d9401a..e971232ba 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -517,7 +498,7 @@ public class ParquetValueWriters {\n \n     @Override\n     public Stream<FieldMetrics> metrics() {\n-      return Stream.of(keyWriter, valueWriter).flatMap(ParquetValueWriter::metrics);\n+      return Stream.concat(keyWriter.metrics(), valueWriter.metrics());\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjQ4Mg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r515672482", "bodyText": "+1", "author": "rdblue", "createdAt": "2020-11-01T21:10:11Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriteAdapter.java", "diffHunk": "@@ -29,6 +30,12 @@\n import org.apache.parquet.hadoop.ParquetWriter;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n \n+/**\n+ * Parquet writer that wraps around hadoop's {@link ParquetWriter}.\n+ * It shouldn't be used in production; {@link org.apache.iceberg.parquet.ParquetWriter} is a better alternative.\n+ * @deprecated use {@link org.apache.iceberg.parquet.ParquetWriter}\n+ */\n+@Deprecated", "originalCommit": "a128ae00949320577a279bb44d1245db5538db51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "472c16dea561b5b42c17286d80697ef4c1afd3f5", "url": "https://github.com/apache/iceberg/commit/472c16dea561b5b42c17286d80697ef4c1afd3f5", "message": "update based on comments", "committedDate": "2020-11-03T01:54:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU0NTI1OQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517545259", "bodyText": "nit: start a sentence on a new line", "author": "jackye1995", "createdAt": "2020-11-04T18:25:00Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetFieldMetrics.java", "diffHunk": "@@ -23,32 +23,44 @@\n import org.apache.iceberg.FieldMetrics;\n \n /**\n- * TODO comments\n+ * Iceberg internally tracked field level metrics, used by Parquet writer only.\n+ * Parquet keeps track of most metrics in its footer, and only NaN counter is actually tracked by writers. This", "originalCommit": "7c4853dd8a4138a0a5c9cde3732ff1ce714f5c95", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetFieldMetrics.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetFieldMetrics.java\nindex 275dbec77..708785a5b 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetFieldMetrics.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetFieldMetrics.java\n\n@@ -24,8 +24,9 @@ import org.apache.iceberg.FieldMetrics;\n \n /**\n  * Iceberg internally tracked field level metrics, used by Parquet writer only.\n- * Parquet keeps track of most metrics in its footer, and only NaN counter is actually tracked by writers. This\n- * wrapper ensures that metrics not being updated by Parquet writers will not be incorrectly used, by throwing\n+ * <p>\n+ * Parquet keeps track of most metrics in its footer, and only NaN counter is actually tracked by writers.\n+ * This wrapper ensures that metrics not being updated by Parquet writers will not be incorrectly used, by throwing\n  * exceptions when they are accessed.\n  */\n public class ParquetFieldMetrics extends FieldMetrics {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU2NzczNA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517567734", "bodyText": "why is this suppressed?", "author": "jackye1995", "createdAt": "2020-11-04T19:04:29Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java", "diffHunk": "@@ -113,8 +115,8 @@ private ParquetValueWriters() {\n     return new MapWriter<>(dl, rl, keyWriter, valueWriter);\n   }\n \n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")", "originalCommit": "7ca9dee78b71436e77d92db0bf91de558edc05ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU5MDU5MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517590590", "bodyText": "This was due to column being directly accessed by its child classes; originally I added id in the same way so I moved this suppression out, but now since we don't need to make change to this class, I'll move it back.", "author": "yyanyy", "createdAt": "2020-11-04T19:46:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU2NzczNA=="}], "type": "inlineReview", "revised_code": {"commit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\nindex e971232ba..c17a2ffa1 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java\n\n@@ -115,8 +115,8 @@ public class ParquetValueWriters {\n     return new MapWriter<>(dl, rl, keyWriter, valueWriter);\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n   public abstract static class PrimitiveWriter<T> implements ParquetValueWriter<T> {\n+    @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n     protected final ColumnWriter<T> column;\n     private final List<TripleWriter<?>> children;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY1MDUzNA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r517650534", "bodyText": "Javadoc needs a <p> on the empty line above if you want to start a new paragraph.", "author": "rdblue", "createdAt": "2020-11-04T21:48:18Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -305,6 +306,9 @@ private SparkTableUtil() {\n    * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n    * metrics are set to null.\n    *\n+   * Note: certain metrics, like NaN counts, that are only supported by iceberg file writers but not file footers, will", "originalCommit": "7ca9dee78b71436e77d92db0bf91de558edc05ad", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java b/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\nindex 2de65a27f..22ee4ed51 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\n\n@@ -302,10 +302,10 @@ public class SparkTableUtil {\n \n   /**\n    * Returns the data files in a partition by listing the partition location.\n-   *\n+   * <p>\n    * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n    * metrics are set to null.\n-   *\n+   * <p>\n    * Note: certain metrics, like NaN counts, that are only supported by iceberg file writers but not file footers, will\n    * not be populated.\n    *\n"}}, {"oid": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "url": "https://github.com/apache/iceberg/commit/b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "message": "make StructInternalRow null safe", "committedDate": "2020-11-05T01:13:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NDMwMA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521644300", "bodyText": "You can also make these records static variables. That might reduce the size of these test cases.", "author": "rdblue", "createdAt": "2020-11-11T21:18:23Z", "path": "core/src/test/java/org/apache/iceberg/TestMetrics.java", "diffHunk": "@@ -347,14 +327,120 @@ public void testMetricsForNullColumns() throws IOException {\n     Record secondRecord = GenericRecord.create(schema);\n     secondRecord.setField(\"intCol\", null);\n \n-    InputFile recordsFile = writeRecords(schema, firstRecord, secondRecord);\n-\n-    Metrics metrics = getMetrics(recordsFile);\n+    Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n     assertCounts(1, 2L, 2L, metrics);\n     assertBounds(1, IntegerType.get(), null, null, metrics);\n   }\n \n+  @Test\n+  public void testMetricsForNaNColumns() throws IOException {\n+    Record firstRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    firstRecord.setField(\"floatCol\", Float.NaN);\n+    firstRecord.setField(\"doubleCol\", Double.NaN);\n+    Record secondRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    secondRecord.setField(\"floatCol\", Float.NaN);\n+    secondRecord.setField(\"doubleCol\", Double.NaN);\n+\n+    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, firstRecord, secondRecord);\n+    Assert.assertEquals(2L, (long) metrics.recordCount());\n+    assertCounts(1, 2L, 0L, 2L, metrics);\n+    assertCounts(2, 2L, 0L, 2L, metrics);\n+    // below: current behavior; will be null once NaN is excluded from upper/lower bound\n+    assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n+    assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n+  }\n+\n+  @Test\n+  public void testColumnBoundsWithNaNValueAtFront() throws IOException {\n+    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n+    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n+\n+    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n+    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n+\n+    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nanRecord.setField(\"floatCol\", Float.NaN);\n+    nanRecord.setField(\"doubleCol\", Double.NaN);\n+\n+    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nanRecord, nonNaNRecord1, nonNaNRecord2);\n+    Assert.assertEquals(3L, (long) metrics.recordCount());\n+    assertCounts(1, 3L, 0L, 1L, metrics);\n+    assertCounts(2, 3L, 0L, 1L, metrics);\n+\n+    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n+    // behaviors differ due to their implementation of comparison being different.\n+    if (fileFormat() == FileFormat.ORC) {\n+      assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n+      assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n+    } else {\n+      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n+      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n+    }\n+  }\n+\n+  @Test\n+  public void testColumnBoundsWithNaNValueInMiddle() throws IOException {\n+    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n+    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n+\n+    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n+    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n+\n+    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nanRecord.setField(\"floatCol\", Float.NaN);\n+    nanRecord.setField(\"doubleCol\", Double.NaN);\n+\n+    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nonNaNRecord1, nanRecord, nonNaNRecord2);\n+    Assert.assertEquals(3L, (long) metrics.recordCount());\n+    assertCounts(1, 3L, 0L, 1L, metrics);\n+    assertCounts(2, 3L, 0L, 1L, metrics);\n+\n+    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n+    // behaviors differ due to their implementation of comparison being different.\n+    if (fileFormat() == FileFormat.ORC) {\n+      assertBounds(1, FloatType.get(), 1.2F, 5.6F, metrics);\n+      assertBounds(2, DoubleType.get(), 3.4D, 7.8D, metrics);\n+    } else {\n+      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n+      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n+    }\n+  }\n+\n+  @Test\n+  public void testColumnBoundsWithNaNValueAtEnd() throws IOException {\n+    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n+    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n+\n+    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n+    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n+\n+    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nanRecord.setField(\"floatCol\", Float.NaN);\n+    nanRecord.setField(\"doubleCol\", Double.NaN);", "originalCommit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetrics.java b/core/src/test/java/org/apache/iceberg/TestMetrics.java\nindex 29ede30f2..984cc4287 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetrics.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetrics.java\n\n@@ -327,120 +347,14 @@ public abstract class TestMetrics {\n     Record secondRecord = GenericRecord.create(schema);\n     secondRecord.setField(\"intCol\", null);\n \n-    Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n+    InputFile recordsFile = writeRecords(schema, firstRecord, secondRecord);\n+\n+    Metrics metrics = getMetrics(recordsFile);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n     assertCounts(1, 2L, 2L, metrics);\n     assertBounds(1, IntegerType.get(), null, null, metrics);\n   }\n \n-  @Test\n-  public void testMetricsForNaNColumns() throws IOException {\n-    Record firstRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    firstRecord.setField(\"floatCol\", Float.NaN);\n-    firstRecord.setField(\"doubleCol\", Double.NaN);\n-    Record secondRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    secondRecord.setField(\"floatCol\", Float.NaN);\n-    secondRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, firstRecord, secondRecord);\n-    Assert.assertEquals(2L, (long) metrics.recordCount());\n-    assertCounts(1, 2L, 0L, 2L, metrics);\n-    assertCounts(2, 2L, 0L, 2L, metrics);\n-    // below: current behavior; will be null once NaN is excluded from upper/lower bound\n-    assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n-    assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n-  }\n-\n-  @Test\n-  public void testColumnBoundsWithNaNValueAtFront() throws IOException {\n-    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n-    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n-\n-    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n-    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n-\n-    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nanRecord.setField(\"floatCol\", Float.NaN);\n-    nanRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nanRecord, nonNaNRecord1, nonNaNRecord2);\n-    Assert.assertEquals(3L, (long) metrics.recordCount());\n-    assertCounts(1, 3L, 0L, 1L, metrics);\n-    assertCounts(2, 3L, 0L, 1L, metrics);\n-\n-    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n-    // behaviors differ due to their implementation of comparison being different.\n-    if (fileFormat() == FileFormat.ORC) {\n-      assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n-    } else {\n-      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n-    }\n-  }\n-\n-  @Test\n-  public void testColumnBoundsWithNaNValueInMiddle() throws IOException {\n-    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n-    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n-\n-    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n-    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n-\n-    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nanRecord.setField(\"floatCol\", Float.NaN);\n-    nanRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nonNaNRecord1, nanRecord, nonNaNRecord2);\n-    Assert.assertEquals(3L, (long) metrics.recordCount());\n-    assertCounts(1, 3L, 0L, 1L, metrics);\n-    assertCounts(2, 3L, 0L, 1L, metrics);\n-\n-    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n-    // behaviors differ due to their implementation of comparison being different.\n-    if (fileFormat() == FileFormat.ORC) {\n-      assertBounds(1, FloatType.get(), 1.2F, 5.6F, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, 7.8D, metrics);\n-    } else {\n-      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n-    }\n-  }\n-\n-  @Test\n-  public void testColumnBoundsWithNaNValueAtEnd() throws IOException {\n-    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n-    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n-\n-    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n-    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n-\n-    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nanRecord.setField(\"floatCol\", Float.NaN);\n-    nanRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nonNaNRecord1, nonNaNRecord2, nanRecord);\n-    Assert.assertEquals(3L, (long) metrics.recordCount());\n-    assertCounts(1, 3L, 0L, 1L, metrics);\n-    assertCounts(2, 3L, 0L, 1L, metrics);\n-\n-    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n-    // behaviors differ due to their implementation of comparison being different.\n-    if (fileFormat() == FileFormat.ORC) {\n-      assertBounds(1, FloatType.get(), 1.2F, 5.6F, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, 7.8D, metrics);\n-    } else {\n-      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n-    }\n-  }\n-\n   @Test\n   public void testMetricsForTopLevelWithMultipleRowGroup() throws Exception {\n     Assume.assumeTrue(\"Skip test for formats that do not support small row groups\", supportsSmallRowGroups());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NDg5Mw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521644893", "bodyText": "This refactor seems to have introduced a lot of changes. Is it needed? Seems like it may just introduce conflicts.", "author": "rdblue", "createdAt": "2020-11-11T21:19:38Z", "path": "core/src/test/java/org/apache/iceberg/TestMetrics.java", "diffHunk": "@@ -96,67 +97,54 @@\n       required(13, \"timestampColBelowEpoch\", TimestampType.withoutZone())\n   );\n \n+  private static final Schema FLOAT_DOUBLE_ONLY_SCHEMA = new Schema(\n+      optional(1, \"floatCol\", FloatType.get()),\n+      optional(2, \"doubleCol\", DoubleType.get())\n+  );\n+\n   private final byte[] fixed = \"abcd\".getBytes(StandardCharsets.UTF_8);\n \n   public abstract FileFormat fileFormat();\n \n-  public Metrics getMetrics(InputFile file) {\n-    return getMetrics(file, MetricsConfig.getDefault());\n-  }\n-\n-  public abstract Metrics getMetrics(InputFile file, MetricsConfig metricsConfig);\n+  public abstract Metrics getMetrics(Schema schema, MetricsConfig metricsConfig, Record... records) throws IOException;\n \n-  public abstract InputFile writeRecords(Schema schema, Record... records) throws IOException;\n+  public abstract Metrics getMetrics(Schema schema, Record... records) throws IOException;", "originalCommit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTczMzc2OQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521733769", "bodyText": "I think the main reason for refactoring is that, before this change we have writeRecords to create appender and return InputFile, and then in Parquet specific tests we use ParquetUtil.fileMetrics to read metrics from the file footer of InputFile directly. But now since NaN is tracked during writing, to test NaN we will need to test against appender.metrics().", "author": "yyanyy", "createdAt": "2020-11-12T00:32:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NDg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc2MTk2Mw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521761963", "bodyText": "Makes sense! Looks like we need to keep it then.", "author": "rdblue", "createdAt": "2020-11-12T01:33:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NDg5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetrics.java b/core/src/test/java/org/apache/iceberg/TestMetrics.java\nindex 29ede30f2..984cc4287 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetrics.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetrics.java\n\n@@ -97,21 +96,20 @@ public abstract class TestMetrics {\n       required(13, \"timestampColBelowEpoch\", TimestampType.withoutZone())\n   );\n \n-  private static final Schema FLOAT_DOUBLE_ONLY_SCHEMA = new Schema(\n-      optional(1, \"floatCol\", FloatType.get()),\n-      optional(2, \"doubleCol\", DoubleType.get())\n-  );\n-\n   private final byte[] fixed = \"abcd\".getBytes(StandardCharsets.UTF_8);\n \n   public abstract FileFormat fileFormat();\n \n-  public abstract Metrics getMetrics(Schema schema, MetricsConfig metricsConfig, Record... records) throws IOException;\n+  public Metrics getMetrics(InputFile file) {\n+    return getMetrics(file, MetricsConfig.getDefault());\n+  }\n+\n+  public abstract Metrics getMetrics(InputFile file, MetricsConfig metricsConfig);\n \n-  public abstract Metrics getMetrics(Schema schema, Record... records) throws IOException;\n+  public abstract InputFile writeRecords(Schema schema, Record... records) throws IOException;\n \n-  protected abstract Metrics getMetricsForRecordsWithSmallRowGroups(Schema schema, OutputFile outputFile,\n-                                                                    Record... records) throws IOException;\n+  public abstract InputFile writeRecordsWithSmallRowGroups(Schema schema, Record... records)\n+      throws IOException;\n \n   public abstract int splitCount(InputFile inputFile) throws IOException;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0NTAwMg==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521645002", "bodyText": "What about createOutputFile?", "author": "rdblue", "createdAt": "2020-11-11T21:19:51Z", "path": "core/src/test/java/org/apache/iceberg/TestMetrics.java", "diffHunk": "@@ -96,67 +97,54 @@\n       required(13, \"timestampColBelowEpoch\", TimestampType.withoutZone())\n   );\n \n+  private static final Schema FLOAT_DOUBLE_ONLY_SCHEMA = new Schema(\n+      optional(1, \"floatCol\", FloatType.get()),\n+      optional(2, \"doubleCol\", DoubleType.get())\n+  );\n+\n   private final byte[] fixed = \"abcd\".getBytes(StandardCharsets.UTF_8);\n \n   public abstract FileFormat fileFormat();\n \n-  public Metrics getMetrics(InputFile file) {\n-    return getMetrics(file, MetricsConfig.getDefault());\n-  }\n-\n-  public abstract Metrics getMetrics(InputFile file, MetricsConfig metricsConfig);\n+  public abstract Metrics getMetrics(Schema schema, MetricsConfig metricsConfig, Record... records) throws IOException;\n \n-  public abstract InputFile writeRecords(Schema schema, Record... records) throws IOException;\n+  public abstract Metrics getMetrics(Schema schema, Record... records) throws IOException;\n \n-  public abstract InputFile writeRecordsWithSmallRowGroups(Schema schema, Record... records)\n-      throws IOException;\n+  protected abstract Metrics getMetricsForRecordsWithSmallRowGroups(Schema schema, OutputFile outputFile,\n+                                                                    Record... records) throws IOException;\n \n   public abstract int splitCount(InputFile inputFile) throws IOException;\n \n   public boolean supportsSmallRowGroups() {\n     return false;\n   }\n \n+  protected abstract OutputFile createFileToWriteTo() throws IOException;", "originalCommit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetrics.java b/core/src/test/java/org/apache/iceberg/TestMetrics.java\nindex 29ede30f2..984cc4287 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetrics.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetrics.java\n\n@@ -97,21 +96,20 @@ public abstract class TestMetrics {\n       required(13, \"timestampColBelowEpoch\", TimestampType.withoutZone())\n   );\n \n-  private static final Schema FLOAT_DOUBLE_ONLY_SCHEMA = new Schema(\n-      optional(1, \"floatCol\", FloatType.get()),\n-      optional(2, \"doubleCol\", DoubleType.get())\n-  );\n-\n   private final byte[] fixed = \"abcd\".getBytes(StandardCharsets.UTF_8);\n \n   public abstract FileFormat fileFormat();\n \n-  public abstract Metrics getMetrics(Schema schema, MetricsConfig metricsConfig, Record... records) throws IOException;\n+  public Metrics getMetrics(InputFile file) {\n+    return getMetrics(file, MetricsConfig.getDefault());\n+  }\n+\n+  public abstract Metrics getMetrics(InputFile file, MetricsConfig metricsConfig);\n \n-  public abstract Metrics getMetrics(Schema schema, Record... records) throws IOException;\n+  public abstract InputFile writeRecords(Schema schema, Record... records) throws IOException;\n \n-  protected abstract Metrics getMetricsForRecordsWithSmallRowGroups(Schema schema, OutputFile outputFile,\n-                                                                    Record... records) throws IOException;\n+  public abstract InputFile writeRecordsWithSmallRowGroups(Schema schema, Record... records)\n+      throws IOException;\n \n   public abstract int splitCount(InputFile inputFile) throws IOException;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0Njg5MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521646890", "bodyText": "Could you avoid using equals? Primitive NaN is not equal to itself, so it's strange to see this here. What about casting and using Double.isNaN?", "author": "rdblue", "createdAt": "2020-11-11T21:23:57Z", "path": "data/src/test/java/org/apache/iceberg/TestMergingMetrics.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public abstract class TestMergingMetrics<T> {\n+\n+  // all supported fields, except for UUID which is on deprecation path: see https://github.com/apache/iceberg/pull/1611\n+  // as well as Types.TimeType and Types.TimestampType.withoutZone as both are not supported by Spark\n+  protected static final Types.NestedField ID_FIELD = required(1, \"id\", Types.IntegerType.get());\n+  protected static final Types.NestedField DATA_FIELD = optional(2, \"data\", Types.StringType.get());\n+  protected static final Types.NestedField FLOAT_FIELD = required(3, \"float\", Types.FloatType.get());\n+  protected static final Types.NestedField DOUBLE_FIELD = optional(4, \"double\", Types.DoubleType.get());\n+  protected static final Types.NestedField DECIMAL_FIELD = optional(5, \"decimal\", Types.DecimalType.of(5, 3));\n+  protected static final Types.NestedField FIXED_FIELD = optional(7, \"fixed\", Types.FixedType.ofLength(4));\n+  protected static final Types.NestedField BINARY_FIELD = optional(8, \"binary\", Types.BinaryType.get());\n+  protected static final Types.NestedField FLOAT_LIST = optional(9, \"floatlist\",\n+      Types.ListType.ofRequired(10, Types.FloatType.get()));\n+  protected static final Types.NestedField LONG_FIELD = optional(11, \"long\", Types.LongType.get());\n+\n+  protected static final Types.NestedField MAP_FIELD_1 = optional(17, \"map1\",\n+      Types.MapType.ofOptional(18, 19, Types.FloatType.get(), Types.StringType.get())\n+  );\n+  protected static final Types.NestedField MAP_FIELD_2 = optional(20, \"map2\",\n+      Types.MapType.ofOptional(21, 22, Types.IntegerType.get(), Types.DoubleType.get())\n+  );\n+  protected static final Types.NestedField STRUCT_FIELD = optional(23, \"structField\", Types.StructType.of(\n+      required(24, \"booleanField\", Types.BooleanType.get()),\n+      optional(25, \"date\", Types.DateType.get()),\n+      optional(27, \"timestamp\", Types.TimestampType.withZone())\n+  ));\n+\n+  private static final Map<Types.NestedField, Integer> FIELDS_WITH_NAN_COUNT_TO_ID = ImmutableMap.of(\n+      FLOAT_FIELD, 3, DOUBLE_FIELD, 4, FLOAT_LIST, 10, MAP_FIELD_1, 18, MAP_FIELD_2, 22\n+  );\n+\n+  // create a schema with all supported fields\n+  protected static final Schema SCHEMA = new Schema(\n+      ID_FIELD,\n+      DATA_FIELD,\n+      FLOAT_FIELD,\n+      DOUBLE_FIELD,\n+      DECIMAL_FIELD,\n+      FIXED_FIELD,\n+      BINARY_FIELD,\n+      FLOAT_LIST,\n+      LONG_FIELD,\n+      MAP_FIELD_1,\n+      MAP_FIELD_2,\n+      STRUCT_FIELD\n+  );\n+\n+  protected abstract FileAppender<T> writeAndGetAppender(List<Record> records) throws Exception;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Test\n+  public void verifySingleRecordMetric() throws Exception {\n+    Record record = GenericRecord.create(SCHEMA);\n+    record.setField(\"id\", 3);\n+    record.setField(\"float\", Float.NaN); // FLOAT_FIELD - 1\n+    record.setField(\"double\", Double.NaN); // DOUBLE_FIELD - 1\n+    record.setField(\"floatlist\", ImmutableList.of(3.3F, 2.8F, Float.NaN, -25.1F, Float.NaN)); // FLOAT_LIST - 2\n+    record.setField(\"map1\", ImmutableMap.of(Float.NaN, \"a\", 0F, \"b\")); // MAP_FIELD_1 - 1\n+    record.setField(\"map2\", ImmutableMap.of(\n+        0, 0D, 1, Double.NaN, 2, 2D, 3, Double.NaN, 4, Double.NaN)); // MAP_FIELD_2 - 3\n+\n+    FileAppender<T> appender = writeAndGetAppender(ImmutableList.of(record));\n+    Map<Integer, Long> nanValueCount = appender.metrics().nanValueCounts();\n+\n+    assertNaNCountMatch(1L, nanValueCount, FLOAT_FIELD);\n+    assertNaNCountMatch(1L, nanValueCount, DOUBLE_FIELD);\n+    assertNaNCountMatch(2L, nanValueCount, FLOAT_LIST);\n+    assertNaNCountMatch(1L, nanValueCount, MAP_FIELD_1);\n+    assertNaNCountMatch(3L, nanValueCount, MAP_FIELD_2);\n+  }\n+\n+  private void assertNaNCountMatch(Long expected, Map<Integer, Long> nanValueCount, Types.NestedField field) {\n+    Assert.assertEquals(\n+        String.format(\"NaN count for field %s does not match expected\", field.name()),\n+        expected, nanValueCount.get(FIELDS_WITH_NAN_COUNT_TO_ID.get(field)));\n+  }\n+\n+  @Test\n+  public void verifyRandomlyGeneratedRecordsMetric() throws Exception {\n+    List<Record> recordList = RandomGenericData.generate(SCHEMA, 50, 250L);\n+\n+    FileAppender<T> appender = writeAndGetAppender(recordList);\n+    Map<Integer, Long> nanValueCount = appender.metrics().nanValueCounts();\n+\n+    FIELDS_WITH_NAN_COUNT_TO_ID.forEach((key, value) -> Assert.assertEquals(\n+        String.format(\"NaN count for field %s does not match expected\", key.name()),\n+        getExpectedNaNCount(recordList, key),\n+        nanValueCount.get(value)));\n+\n+    SCHEMA.columns().stream()\n+        .filter(column -> !FIELDS_WITH_NAN_COUNT_TO_ID.containsKey(column))\n+        .map(Types.NestedField::fieldId)\n+        .forEach(id -> Assert.assertNull(\"NaN count for field %s should be null\", nanValueCount.get(id)));\n+  }\n+\n+  private Long getExpectedNaNCount(List<Record> expectedRecords, Types.NestedField field) {\n+    return expectedRecords.stream()\n+        .mapToLong(e -> {\n+          Object value = e.getField(field.name());\n+          if (value == null) {\n+            return 0;\n+          }\n+          if (FLOAT_FIELD.equals(field)) {\n+            return value.equals(Float.NaN) ? 1 : 0;", "originalCommit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "chunk": "diff --git a/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java b/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java\ndeleted file mode 100644\nindex ef205d247..000000000\n--- a/data/src/test/java/org/apache/iceberg/TestMergingMetrics.java\n+++ /dev/null\n\n@@ -1,164 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg;\n-\n-import java.util.List;\n-import java.util.Map;\n-import org.apache.iceberg.data.GenericRecord;\n-import org.apache.iceberg.data.RandomGenericData;\n-import org.apache.iceberg.data.Record;\n-import org.apache.iceberg.io.FileAppender;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-\n-import static org.apache.iceberg.types.Types.NestedField.optional;\n-import static org.apache.iceberg.types.Types.NestedField.required;\n-\n-public abstract class TestMergingMetrics<T> {\n-\n-  // all supported fields, except for UUID which is on deprecation path: see https://github.com/apache/iceberg/pull/1611\n-  // as well as Types.TimeType and Types.TimestampType.withoutZone as both are not supported by Spark\n-  protected static final Types.NestedField ID_FIELD = required(1, \"id\", Types.IntegerType.get());\n-  protected static final Types.NestedField DATA_FIELD = optional(2, \"data\", Types.StringType.get());\n-  protected static final Types.NestedField FLOAT_FIELD = required(3, \"float\", Types.FloatType.get());\n-  protected static final Types.NestedField DOUBLE_FIELD = optional(4, \"double\", Types.DoubleType.get());\n-  protected static final Types.NestedField DECIMAL_FIELD = optional(5, \"decimal\", Types.DecimalType.of(5, 3));\n-  protected static final Types.NestedField FIXED_FIELD = optional(7, \"fixed\", Types.FixedType.ofLength(4));\n-  protected static final Types.NestedField BINARY_FIELD = optional(8, \"binary\", Types.BinaryType.get());\n-  protected static final Types.NestedField FLOAT_LIST = optional(9, \"floatlist\",\n-      Types.ListType.ofRequired(10, Types.FloatType.get()));\n-  protected static final Types.NestedField LONG_FIELD = optional(11, \"long\", Types.LongType.get());\n-\n-  protected static final Types.NestedField MAP_FIELD_1 = optional(17, \"map1\",\n-      Types.MapType.ofOptional(18, 19, Types.FloatType.get(), Types.StringType.get())\n-  );\n-  protected static final Types.NestedField MAP_FIELD_2 = optional(20, \"map2\",\n-      Types.MapType.ofOptional(21, 22, Types.IntegerType.get(), Types.DoubleType.get())\n-  );\n-  protected static final Types.NestedField STRUCT_FIELD = optional(23, \"structField\", Types.StructType.of(\n-      required(24, \"booleanField\", Types.BooleanType.get()),\n-      optional(25, \"date\", Types.DateType.get()),\n-      optional(27, \"timestamp\", Types.TimestampType.withZone())\n-  ));\n-\n-  private static final Map<Types.NestedField, Integer> FIELDS_WITH_NAN_COUNT_TO_ID = ImmutableMap.of(\n-      FLOAT_FIELD, 3, DOUBLE_FIELD, 4, FLOAT_LIST, 10, MAP_FIELD_1, 18, MAP_FIELD_2, 22\n-  );\n-\n-  // create a schema with all supported fields\n-  protected static final Schema SCHEMA = new Schema(\n-      ID_FIELD,\n-      DATA_FIELD,\n-      FLOAT_FIELD,\n-      DOUBLE_FIELD,\n-      DECIMAL_FIELD,\n-      FIXED_FIELD,\n-      BINARY_FIELD,\n-      FLOAT_LIST,\n-      LONG_FIELD,\n-      MAP_FIELD_1,\n-      MAP_FIELD_2,\n-      STRUCT_FIELD\n-  );\n-\n-  protected abstract FileAppender<T> writeAndGetAppender(List<Record> records) throws Exception;\n-\n-  @Rule\n-  public TemporaryFolder temp = new TemporaryFolder();\n-\n-  @Test\n-  public void verifySingleRecordMetric() throws Exception {\n-    Record record = GenericRecord.create(SCHEMA);\n-    record.setField(\"id\", 3);\n-    record.setField(\"float\", Float.NaN); // FLOAT_FIELD - 1\n-    record.setField(\"double\", Double.NaN); // DOUBLE_FIELD - 1\n-    record.setField(\"floatlist\", ImmutableList.of(3.3F, 2.8F, Float.NaN, -25.1F, Float.NaN)); // FLOAT_LIST - 2\n-    record.setField(\"map1\", ImmutableMap.of(Float.NaN, \"a\", 0F, \"b\")); // MAP_FIELD_1 - 1\n-    record.setField(\"map2\", ImmutableMap.of(\n-        0, 0D, 1, Double.NaN, 2, 2D, 3, Double.NaN, 4, Double.NaN)); // MAP_FIELD_2 - 3\n-\n-    FileAppender<T> appender = writeAndGetAppender(ImmutableList.of(record));\n-    Map<Integer, Long> nanValueCount = appender.metrics().nanValueCounts();\n-\n-    assertNaNCountMatch(1L, nanValueCount, FLOAT_FIELD);\n-    assertNaNCountMatch(1L, nanValueCount, DOUBLE_FIELD);\n-    assertNaNCountMatch(2L, nanValueCount, FLOAT_LIST);\n-    assertNaNCountMatch(1L, nanValueCount, MAP_FIELD_1);\n-    assertNaNCountMatch(3L, nanValueCount, MAP_FIELD_2);\n-  }\n-\n-  private void assertNaNCountMatch(Long expected, Map<Integer, Long> nanValueCount, Types.NestedField field) {\n-    Assert.assertEquals(\n-        String.format(\"NaN count for field %s does not match expected\", field.name()),\n-        expected, nanValueCount.get(FIELDS_WITH_NAN_COUNT_TO_ID.get(field)));\n-  }\n-\n-  @Test\n-  public void verifyRandomlyGeneratedRecordsMetric() throws Exception {\n-    List<Record> recordList = RandomGenericData.generate(SCHEMA, 50, 250L);\n-\n-    FileAppender<T> appender = writeAndGetAppender(recordList);\n-    Map<Integer, Long> nanValueCount = appender.metrics().nanValueCounts();\n-\n-    FIELDS_WITH_NAN_COUNT_TO_ID.forEach((key, value) -> Assert.assertEquals(\n-        String.format(\"NaN count for field %s does not match expected\", key.name()),\n-        getExpectedNaNCount(recordList, key),\n-        nanValueCount.get(value)));\n-\n-    SCHEMA.columns().stream()\n-        .filter(column -> !FIELDS_WITH_NAN_COUNT_TO_ID.containsKey(column))\n-        .map(Types.NestedField::fieldId)\n-        .forEach(id -> Assert.assertNull(\"NaN count for field %s should be null\", nanValueCount.get(id)));\n-  }\n-\n-  private Long getExpectedNaNCount(List<Record> expectedRecords, Types.NestedField field) {\n-    return expectedRecords.stream()\n-        .mapToLong(e -> {\n-          Object value = e.getField(field.name());\n-          if (value == null) {\n-            return 0;\n-          }\n-          if (FLOAT_FIELD.equals(field)) {\n-            return value.equals(Float.NaN) ? 1 : 0;\n-          } else if  (DOUBLE_FIELD.equals(field)) {\n-            return value.equals(Double.NaN) ? 1 : 0;\n-          } else if  (FLOAT_LIST.equals(field)) {\n-            return ((List<Float>) value).stream()\n-                .filter(val -> val != null && val.equals(Float.NaN))\n-                .count();\n-          } else if  (MAP_FIELD_1.equals(field)) {\n-            return ((Map<Float, ?>) value).keySet().stream()\n-                .filter(key -> key.equals(Float.NaN))\n-                .count();\n-          } else if  (MAP_FIELD_2.equals(field)) {\n-            return ((Map<?, Double>) value).values().stream()\n-                .filter(val -> val != null && val.equals(Double.NaN))\n-                .count();\n-          } else {\n-            throw new RuntimeException(\"unknown field name for getting expected NaN count: \" + field.name());\n-          }\n-        }).sum();\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0ODg5MQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521648891", "bodyText": "Javadoc paragraphs need to be separated by <p> to display correctly.", "author": "rdblue", "createdAt": "2020-11-11T21:28:03Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java", "diffHunk": "@@ -28,4 +30,16 @@\n   List<TripleWriter<?>> columns();\n \n   void setColumnStore(ColumnWriteStore columnStore);\n+\n+  /**\n+   * Returns a stream of {@link FieldMetrics} that this ParquetValueWriter keeps track of.\n+   *", "originalCommit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTczNTY4OA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521735688", "bodyText": "Thanks for reminding! I'll make sure to double check this in later PRs.", "author": "yyanyy", "createdAt": "2020-11-12T00:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY0ODg5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "chunk": "diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java\nindex e060b86c4..342636339 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java\n\n@@ -31,15 +31,6 @@ public interface ParquetValueWriter<T> {\n \n   void setColumnStore(ColumnWriteStore columnStore);\n \n-  /**\n-   * Returns a stream of {@link FieldMetrics} that this ParquetValueWriter keeps track of.\n-   *\n-   * Since Parquet keeps track of most metrics in its footer, for now ParquetValueWriter only keeps track of NaN\n-   * counter, and only return non-empty stream if the writer writes double or float values either by itself or\n-   * transitively.\n-   */\n-  default Stream<FieldMetrics> metrics() {\n-    return Stream.empty();\n-  }\n+  Stream<FieldMetrics> metrics();\n }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY2MDI4Mw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521660283", "bodyText": "NaN as an upper bound should be safe, but NaN as a lower bound may not be. Does this mean we need to fix our evaluators to check for NaN?", "author": "rdblue", "createdAt": "2020-11-11T21:51:39Z", "path": "core/src/test/java/org/apache/iceberg/TestMetrics.java", "diffHunk": "@@ -347,14 +327,120 @@ public void testMetricsForNullColumns() throws IOException {\n     Record secondRecord = GenericRecord.create(schema);\n     secondRecord.setField(\"intCol\", null);\n \n-    InputFile recordsFile = writeRecords(schema, firstRecord, secondRecord);\n-\n-    Metrics metrics = getMetrics(recordsFile);\n+    Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n     assertCounts(1, 2L, 2L, metrics);\n     assertBounds(1, IntegerType.get(), null, null, metrics);\n   }\n \n+  @Test\n+  public void testMetricsForNaNColumns() throws IOException {\n+    Record firstRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    firstRecord.setField(\"floatCol\", Float.NaN);\n+    firstRecord.setField(\"doubleCol\", Double.NaN);\n+    Record secondRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    secondRecord.setField(\"floatCol\", Float.NaN);\n+    secondRecord.setField(\"doubleCol\", Double.NaN);\n+\n+    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, firstRecord, secondRecord);\n+    Assert.assertEquals(2L, (long) metrics.recordCount());\n+    assertCounts(1, 2L, 0L, 2L, metrics);\n+    assertCounts(2, 2L, 0L, 2L, metrics);\n+    // below: current behavior; will be null once NaN is excluded from upper/lower bound\n+    assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n+    assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n+  }\n+\n+  @Test\n+  public void testColumnBoundsWithNaNValueAtFront() throws IOException {\n+    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n+    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n+\n+    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n+    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n+\n+    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n+    nanRecord.setField(\"floatCol\", Float.NaN);\n+    nanRecord.setField(\"doubleCol\", Double.NaN);\n+\n+    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nanRecord, nonNaNRecord1, nonNaNRecord2);\n+    Assert.assertEquals(3L, (long) metrics.recordCount());\n+    assertCounts(1, 3L, 0L, 1L, metrics);\n+    assertCounts(2, 3L, 0L, 1L, metrics);\n+\n+    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n+    // behaviors differ due to their implementation of comparison being different.\n+    if (fileFormat() == FileFormat.ORC) {\n+      assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n+      assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);", "originalCommit": "b0e3e45395e0f995e66fa3915ebd7c57f889f78d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY2ODY3Nw==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521668677", "bodyText": "@omalley and @shardulm94, FYI. Looks like we are getting unexpected bounds for some ORC cases with NaN.", "author": "rdblue", "createdAt": "2020-11-11T22:10:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY2MDI4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc0Mzc2MA==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521743760", "bodyText": "I guess this means that today, we may skip including an ORC file for predicates that utilize bounds when the column to be evaluated contains non-NaN data but both upper and lower bound is NaN, which happens when the field of the first record in the file is NaN. Is my understanding correct? I can create an issue about this.", "author": "yyanyy", "createdAt": "2020-11-12T00:49:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY2MDI4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc2MjEwNQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r521762105", "bodyText": "Yeah, an issue with a test case would be great! Thank you!", "author": "rdblue", "createdAt": "2020-11-12T01:33:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY2MDI4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUyMzEzNQ==", "url": "https://github.com/apache/iceberg/pull/1641#discussion_r522523135", "bodyText": "Forgot to do that yesterday: #1761", "author": "yyanyy", "createdAt": "2020-11-13T00:18:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY2MDI4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetrics.java b/core/src/test/java/org/apache/iceberg/TestMetrics.java\nindex 29ede30f2..984cc4287 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetrics.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetrics.java\n\n@@ -327,120 +347,14 @@ public abstract class TestMetrics {\n     Record secondRecord = GenericRecord.create(schema);\n     secondRecord.setField(\"intCol\", null);\n \n-    Metrics metrics = getMetrics(schema, firstRecord, secondRecord);\n+    InputFile recordsFile = writeRecords(schema, firstRecord, secondRecord);\n+\n+    Metrics metrics = getMetrics(recordsFile);\n     Assert.assertEquals(2L, (long) metrics.recordCount());\n     assertCounts(1, 2L, 2L, metrics);\n     assertBounds(1, IntegerType.get(), null, null, metrics);\n   }\n \n-  @Test\n-  public void testMetricsForNaNColumns() throws IOException {\n-    Record firstRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    firstRecord.setField(\"floatCol\", Float.NaN);\n-    firstRecord.setField(\"doubleCol\", Double.NaN);\n-    Record secondRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    secondRecord.setField(\"floatCol\", Float.NaN);\n-    secondRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, firstRecord, secondRecord);\n-    Assert.assertEquals(2L, (long) metrics.recordCount());\n-    assertCounts(1, 2L, 0L, 2L, metrics);\n-    assertCounts(2, 2L, 0L, 2L, metrics);\n-    // below: current behavior; will be null once NaN is excluded from upper/lower bound\n-    assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n-    assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n-  }\n-\n-  @Test\n-  public void testColumnBoundsWithNaNValueAtFront() throws IOException {\n-    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n-    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n-\n-    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n-    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n-\n-    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nanRecord.setField(\"floatCol\", Float.NaN);\n-    nanRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nanRecord, nonNaNRecord1, nonNaNRecord2);\n-    Assert.assertEquals(3L, (long) metrics.recordCount());\n-    assertCounts(1, 3L, 0L, 1L, metrics);\n-    assertCounts(2, 3L, 0L, 1L, metrics);\n-\n-    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n-    // behaviors differ due to their implementation of comparison being different.\n-    if (fileFormat() == FileFormat.ORC) {\n-      assertBounds(1, FloatType.get(), Float.NaN, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), Double.NaN, Double.NaN, metrics);\n-    } else {\n-      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n-    }\n-  }\n-\n-  @Test\n-  public void testColumnBoundsWithNaNValueInMiddle() throws IOException {\n-    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n-    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n-\n-    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n-    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n-\n-    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nanRecord.setField(\"floatCol\", Float.NaN);\n-    nanRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nonNaNRecord1, nanRecord, nonNaNRecord2);\n-    Assert.assertEquals(3L, (long) metrics.recordCount());\n-    assertCounts(1, 3L, 0L, 1L, metrics);\n-    assertCounts(2, 3L, 0L, 1L, metrics);\n-\n-    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n-    // behaviors differ due to their implementation of comparison being different.\n-    if (fileFormat() == FileFormat.ORC) {\n-      assertBounds(1, FloatType.get(), 1.2F, 5.6F, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, 7.8D, metrics);\n-    } else {\n-      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n-    }\n-  }\n-\n-  @Test\n-  public void testColumnBoundsWithNaNValueAtEnd() throws IOException {\n-    Record nonNaNRecord1 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord1.setField(\"floatCol\", 1.2F);\n-    nonNaNRecord1.setField(\"doubleCol\", 3.4D);\n-\n-    Record nonNaNRecord2 = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nonNaNRecord2.setField(\"floatCol\", 5.6F);\n-    nonNaNRecord2.setField(\"doubleCol\", 7.8D);\n-\n-    Record nanRecord = GenericRecord.create(FLOAT_DOUBLE_ONLY_SCHEMA);\n-    nanRecord.setField(\"floatCol\", Float.NaN);\n-    nanRecord.setField(\"doubleCol\", Double.NaN);\n-\n-    Metrics metrics = getMetrics(FLOAT_DOUBLE_ONLY_SCHEMA, nonNaNRecord1, nonNaNRecord2, nanRecord);\n-    Assert.assertEquals(3L, (long) metrics.recordCount());\n-    assertCounts(1, 3L, 0L, 1L, metrics);\n-    assertCounts(2, 3L, 0L, 1L, metrics);\n-\n-    // below: current behavior; will be non-NaN values once NaN is excluded from upper/lower bound. ORC and Parquet's\n-    // behaviors differ due to their implementation of comparison being different.\n-    if (fileFormat() == FileFormat.ORC) {\n-      assertBounds(1, FloatType.get(), 1.2F, 5.6F, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, 7.8D, metrics);\n-    } else {\n-      assertBounds(1, FloatType.get(), 1.2F, Float.NaN, metrics);\n-      assertBounds(2, DoubleType.get(), 3.4D, Double.NaN, metrics);\n-    }\n-  }\n-\n   @Test\n   public void testMetricsForTopLevelWithMultipleRowGroup() throws Exception {\n     Assume.assumeTrue(\"Skip test for formats that do not support small row groups\", supportsSmallRowGroups());\n"}}, {"oid": "0aebbcd708832149b5b7b66b3481c7a8c267ff98", "url": "https://github.com/apache/iceberg/commit/0aebbcd708832149b5b7b66b3481c7a8c267ff98", "message": "Add NaN counter to Metrics and implement in Parquet writers", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "a1db924c1e080b192be084730758c844af3bdfc4", "url": "https://github.com/apache/iceberg/commit/a1db924c1e080b192be084730758c844af3bdfc4", "message": "update tests", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "1f7e4c7a568ab63bcf22d01470af84c232eed397", "url": "https://github.com/apache/iceberg/commit/1f7e4c7a568ab63bcf22d01470af84c232eed397", "message": "add flink and spark tests", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "8ddd25d6b83c47004bed3552bb091150b0af86af", "url": "https://github.com/apache/iceberg/commit/8ddd25d6b83c47004bed3552bb091150b0af86af", "message": "update based on comments", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "10220065611ca4486ff7aa7a38c3ccc10e8298e9", "url": "https://github.com/apache/iceberg/commit/10220065611ca4486ff7aa7a38c3ccc10e8298e9", "message": "update some comments", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "45474d75452aa48bf519bb8545337e3c81a925f7", "url": "https://github.com/apache/iceberg/commit/45474d75452aa48bf519bb8545337e3c81a925f7", "message": "clean up tests", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "2a16a59d0ff427e99e0bc77c1c70e97f0ea1ca0f", "url": "https://github.com/apache/iceberg/commit/2a16a59d0ff427e99e0bc77c1c70e97f0ea1ca0f", "message": "fix style", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "654021300393b5b3587aa127a92611b3812df0f0", "url": "https://github.com/apache/iceberg/commit/654021300393b5b3587aa127a92611b3812df0f0", "message": "make StructInternalRow null safe", "committedDate": "2020-11-12T00:37:57Z", "type": "commit"}, {"oid": "e8b0b3206d085c1b4f1adfb0ba9ae220c22be52c", "url": "https://github.com/apache/iceberg/commit/e8b0b3206d085c1b4f1adfb0ba9ae220c22be52c", "message": "update based on comments", "committedDate": "2020-11-12T01:11:54Z", "type": "commit"}, {"oid": "e8b0b3206d085c1b4f1adfb0ba9ae220c22be52c", "url": "https://github.com/apache/iceberg/commit/e8b0b3206d085c1b4f1adfb0ba9ae220c22be52c", "message": "update based on comments", "committedDate": "2020-11-12T01:11:54Z", "type": "forcePushed"}]}