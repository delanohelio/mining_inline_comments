{"pr_number": 827, "pr_title": "Refactor metadata Table tests to use a common parent", "pr_createdAt": "2020-03-05T02:46:51Z", "pr_url": "https://github.com/apache/iceberg/pull/827", "timeline": [{"oid": "3e4f731cfaaba7c7fa25d50cc3a6310d9a24793f", "url": "https://github.com/apache/iceberg/commit/3e4f731cfaaba7c7fa25d50cc3a6310d9a24793f", "message": "Refactored Table tests to use a common parent", "committedDate": "2020-03-05T08:12:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUyOTE2Ng==", "url": "https://github.com/apache/iceberg/pull/827#discussion_r388529166", "bodyText": "It isn't necessary to use super here. Could you remove those?", "author": "rdblue", "createdAt": "2020-03-05T19:57:05Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java", "diffHunk": "@@ -101,31 +79,7 @@ public void testEntriesTable() throws Exception {\n     System.out.println(tableLocation);\n     Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n \n-    List<SimpleRecord> records = Lists.newArrayList(new SimpleRecord(1, \"1\"));\n-\n-    Dataset<Row> inputDf = spark.createDataFrame(records, SimpleRecord.class);\n-    inputDf.select(\"id\", \"data\").write()\n-        .format(\"iceberg\")\n-        .mode(\"append\")\n-        .save(tableLocation);\n-\n-    table.refresh();\n-\n-    List<Row> actual = spark.read()\n-        .format(\"iceberg\")\n-        .load(tableLocation + \"#entries\")\n-        .collectAsList();\n-\n-    Assert.assertEquals(\"Should only contain one manifest\", 1, table.currentSnapshot().manifests().size());\n-    InputFile manifest = table.io().newInputFile(table.currentSnapshot().manifests().get(0).path());\n-    List<GenericData.Record> expected;\n-    try (CloseableIterable<GenericData.Record> rows = Avro.read(manifest).project(entriesTable.schema()).build()) {\n-      expected = Lists.newArrayList(rows);\n-    }\n-\n-    Assert.assertEquals(\"Entries table should have one row\", 1, expected.size());\n-    Assert.assertEquals(\"Actual results should have one row\", 1, actual.size());\n-    TestHelpers.assertEqualsSafe(entriesTable.schema().asStruct(), expected.get(0), actual.get(0));\n+    super.testEntriesTable(table, entriesTable, tableLocation, tableLocation + \"#entries\");", "originalCommit": "3e4f731cfaaba7c7fa25d50cc3a6310d9a24793f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6278eecf8cd10b07b1da93b94631c881d5563b65", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\nindex 30bb44632..e1674a13a 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n\n@@ -73,107 +64,26 @@ public class TestIcebergSourceHadoopTables extends TestIcebergSourceTablesBase {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n-  @Test\n-  public void testEntriesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    System.out.println(tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-\n-    super.testEntriesTable(table, entriesTable, tableLocation, tableLocation + \"#entries\");\n-  }\n-\n-  @Test\n-  public void testAllEntriesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    System.out.println(tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#all_entries\");\n-\n-    super.testAllEntriesTable(table, entriesTable, tableLocation, tableLocation + \"#all_entries\");\n-  }\n-\n-  @Test\n-  public void testFilesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#files\");\n-\n-    super.testFilesTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#files\");\n-  }\n-\n-  @Test\n-  public void testFilesTableWithSnapshotIdInheritance() throws Exception {\n-    try {\n-      Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-\n-      table.updateProperties()\n-              .set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, \"true\")\n-              .commit();\n-\n-      Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-      Table filesTable = TABLES.load(tableLocation + \"#files\");\n-\n-      super.testFilesTableWithSnapshotIdInheritance(table, entriesTable, filesTable,\n-              tableLocation, tableLocation + \"#files\");\n-    } finally {\n-      spark.sql(\"DROP TABLE parquet_table\");\n+  @Override\n+  public Table createTable(TableIdentifier ident, Schema schema, PartitionSpec spec) {\n+    if (spec.equals(PartitionSpec.unpartitioned())) {\n+      return TABLES.create(schema, tableLocation);\n     }\n+    return TABLES.create(schema, spec, tableLocation);\n   }\n \n-  @Test\n-  public void testFilesUnpartitionedTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#files\");\n-\n-    super.testFilesUnpartitionedTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#files\");\n+  @Override\n+  public Table loadTable(TableIdentifier ident, String entriesSuffix) {\n+    return TABLES.load(loadLocation(ident, entriesSuffix));\n   }\n \n-  @Test\n-  public void testAllDataFilesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#all_data_files\");\n-\n-    super.testAllDataFilesTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#all_data_files\");\n-  }\n-\n-  @Test\n-  public void testHistoryTable() {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    Table historyTable = TABLES.load(tableLocation + \"#history\");\n-\n-    super.testHistoryTable(table, historyTable, tableLocation, tableLocation + \"#history\");\n-  }\n-\n-  @Test\n-  public void testSnapshotsTable() {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    Table snapTable = TABLES.load(tableLocation + \"#snapshots\");\n-\n-    super.testSnapshotsTable(table, snapTable, tableLocation, tableLocation + \"#snapshots\");\n+  @Override\n+  public String loadLocation(TableIdentifier ident, String entriesSuffix) {\n+    return String.format(\"%s#%s\", loadLocation(ident), entriesSuffix);\n   }\n \n-  @Test\n-  public void testManifestsTable() {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table manifestTable = TABLES.load(tableLocation + \"#manifests\");\n-\n-    super.testManifestsTable(table, manifestTable, tableLocation, tableLocation + \"#manifests\");\n-  }\n-\n-  @Test\n-  public void testAllManifestsTable() {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table manifestTable = TABLES.load(tableLocation + \"#all_manifests\");\n-\n-    super.testAllManifestsTable(table, manifestTable, tableLocation, tableLocation + \"#all_manifests\");\n-  }\n-\n-  @Test\n-  public void testPartitionsTable() {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table partitionsTable = TABLES.load(tableLocation + \"#partitions\");\n-\n-    super.testPartitionsTable(table, partitionsTable, tableLocation, tableLocation + \"#partitions\");\n+  @Override\n+  public String loadLocation(TableIdentifier ident) {\n+    return tableLocation;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUyOTQ5Mg==", "url": "https://github.com/apache/iceberg/pull/827#discussion_r388529492", "bodyText": "Can you revert these changes? The indentation was correct.", "author": "rdblue", "createdAt": "2020-03-05T19:57:42Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java", "diffHunk": "@@ -695,63 +298,13 @@ public synchronized void testHiveAllManifestsTable() throws Exception {\n     TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"manifests_test\");\n     try {\n       Table table = catalog.createTable(\n-          tableIdentifier,\n-          SCHEMA,\n-          PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n+              tableIdentifier,", "originalCommit": "3e4f731cfaaba7c7fa25d50cc3a6310d9a24793f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6278eecf8cd10b07b1da93b94631c881d5563b65", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java\nindex 92742299c..669c87693 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java\n\n@@ -91,243 +77,34 @@ public class TestIcebergSourceHiveTables extends TestIcebergSourceTablesBase {\n     TestIcebergSourceHiveTables.spark = null;\n   }\n \n-  @Test\n-  public synchronized void testHiveTablesSupport() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"table\");\n-    try {\n-      catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n-\n-      List<SimpleRecord> expectedRecords = Lists.newArrayList(\n-          new SimpleRecord(1, \"1\"),\n-          new SimpleRecord(2, \"2\"),\n-          new SimpleRecord(3, \"3\"));\n-\n-      Dataset<Row> inputDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n-      inputDf.select(\"id\", \"data\").write()\n-          .format(\"iceberg\")\n-          .mode(SaveMode.Append)\n-          .save(tableIdentifier.toString());\n-\n-      Dataset<Row> resultDf = spark.read()\n-          .format(\"iceberg\")\n-          .load(tableIdentifier.toString());\n-      List<SimpleRecord> actualRecords = resultDf.orderBy(\"id\")\n-          .as(Encoders.bean(SimpleRecord.class))\n-          .collectAsList();\n-\n-      Assert.assertEquals(\"Records should match\", expectedRecords, actualRecords);\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveEntriesTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n-      Table entriesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"entries_test\", \"entries\"));\n-\n-      super.testEntriesTable(table, entriesTable, tableIdentifier.toString(), \"db.entries_test.entries\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveAllEntriesTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n-      Table entriesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"entries_test\", \"all_entries\"));\n-\n-      super.testAllEntriesTable(table, entriesTable, tableIdentifier.toString(),\n-              tableIdentifier.toString() + \".all_entries\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveFilesTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA,\n-              PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n-      Table entriesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"files_test\", \"entries\"));\n-      Table filesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"files_test\", \"files\"));\n-\n-      super.testFilesTable(table, entriesTable, filesTable,\n-              tableIdentifier.toString(), tableIdentifier.toString() + \".files\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveFilesTableWithSnapshotIdInheritance() throws Exception {\n-\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_inheritance_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA,\n-              PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n-\n-      table.updateProperties()\n-              .set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, \"true\")\n-              .commit();\n-\n-      Table entriesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"files_inheritance_test\", \"entries\"));\n-      Table filesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"files_inheritance_test\", \"files\"));\n-\n-      super.testFilesTableWithSnapshotIdInheritance(table, entriesTable, filesTable,\n-              tableIdentifier.toString(), tableIdentifier.toString() + \".files\");\n-    } finally {\n-      clients.run(client -> {\n-        spark.sql(\"DROP TABLE parquet_table\");\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveFilesUnpartitionedTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"unpartitioned_files_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA);\n-      Table entriesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"unpartitioned_files_test\", \"entries\"));\n-      Table filesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"unpartitioned_files_test\", \"files\"));\n-\n-      super.testFilesUnpartitionedTable(table, entriesTable, filesTable,\n-              tableIdentifier.toString(), tableIdentifier.toString() + \".files\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveAllDataFilesTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"files_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA,\n-              PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n-      Table entriesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"files_test\", \"entries\"));\n-      Table filesTable = catalog.loadTable(TableIdentifier.of(\"db\", \"files_test\", \"all_data_files\"));\n-\n-      super.testAllDataFilesTable(table, entriesTable, filesTable,\n-              tableIdentifier.toString(), tableIdentifier.toString() + \".all_data_files\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n-  }\n-\n-  @Test\n-  public synchronized void testHiveHistoryTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"history_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n-      Table historyTable = catalog.loadTable(TableIdentifier.of(\"db\", \"history_test\", \"history\"));\n-\n-      super.testHistoryTable(table, historyTable, tableIdentifier.toString(),\n-              tableIdentifier.toString() + \".history\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n+  @After\n+  public void dropTable() throws Exception {\n+    clients.run(client -> {\n+      client.dropTable(TestIcebergSourceHiveTables.currentIdentifier.namespace().level(0),\n+          TestIcebergSourceHiveTables.currentIdentifier.name());\n+      return null;\n+    });\n   }\n \n-  @Test\n-  public synchronized void testHiveSnapshotsTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"snapshots_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n-      Table snapTable = catalog.loadTable(TableIdentifier.of(\"db\", \"snapshots_test\", \"snapshots\"));\n-\n-      super.testSnapshotsTable(table, snapTable, tableIdentifier.toString(),\n-              tableIdentifier.toString() + \".snapshots\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n+  @Override\n+  public Table createTable(TableIdentifier ident, Schema schema, PartitionSpec spec) {\n+    TestIcebergSourceHiveTables.currentIdentifier = ident;\n+    return TestIcebergSourceHiveTables.catalog.createTable(ident, schema, spec);\n   }\n \n-  @Test\n-  public synchronized void testHiveManifestsTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"manifests_test\");\n-    try {\n-      Table table = catalog.createTable(\n-              tableIdentifier,\n-              SCHEMA,\n-              PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n-      Table manifestTable = catalog.loadTable(TableIdentifier.of(\"db\", \"manifests_test\", \"manifests\"));\n-\n-      super.testManifestsTable(table, manifestTable, tableIdentifier.toString(),\n-              tableIdentifier.toString() + \".manifests\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n+  @Override\n+  public Table loadTable(TableIdentifier ident, String entriesSuffix) {\n+    TableIdentifier identifier = TableIdentifier.of(ident.namespace().level(0), ident.name(), entriesSuffix);\n+    return TestIcebergSourceHiveTables.catalog.loadTable(identifier);\n   }\n \n-  @Test\n-  public synchronized void testHiveAllManifestsTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"manifests_test\");\n-    try {\n-      Table table = catalog.createTable(\n-              tableIdentifier,\n-              SCHEMA,\n-              PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n-      Table manifestTable = catalog.loadTable(TableIdentifier.of(\"db\", \"manifests_test\", \"all_manifests\"));\n-\n-      super.testAllManifestsTable(table, manifestTable, tableIdentifier.toString(),\n-              tableIdentifier.toString() + \".all_manifests\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n+  @Override\n+  public String loadLocation(TableIdentifier ident, String entriesSuffix) {\n+    return String.format(\"%s.%s\", loadLocation(ident), entriesSuffix);\n   }\n \n-  @Test\n-  public synchronized void testHivePartitionsTable() throws Exception {\n-    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"partitions_test\");\n-    try {\n-      Table table = catalog.createTable(tableIdentifier, SCHEMA,\n-              PartitionSpec.builderFor(SCHEMA).identity(\"id\").build());\n-      Table partitionsTable = catalog.loadTable(TableIdentifier.of(\"db\", \"partitions_test\", \"partitions\"));\n-\n-      super.testPartitionsTable(table, partitionsTable, tableIdentifier.toString(),\n-              tableIdentifier.toString() + \".partitions\");\n-    } finally {\n-      clients.run(client -> {\n-        client.dropTable(tableIdentifier.namespace().level(0), tableIdentifier.name());\n-        return null;\n-      });\n-    }\n+  @Override\n+  public String loadLocation(TableIdentifier ident) {\n+    return ident.toString();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUyOTc4Nw==", "url": "https://github.com/apache/iceberg/pull/827#discussion_r388529787", "bodyText": "Continuing indentation is 4 spaces, not 8. Could you update this?", "author": "rdblue", "createdAt": "2020-03-05T19:58:14Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java", "diffHunk": "@@ -185,92 +97,23 @@ public void testFilesTable() throws Exception {\n     Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n     Table filesTable = TABLES.load(tableLocation + \"#files\");\n \n-    Dataset<Row> df1 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(1, \"a\")), SimpleRecord.class);\n-    Dataset<Row> df2 = spark.createDataFrame(Lists.newArrayList(new SimpleRecord(2, \"b\")), SimpleRecord.class);\n-\n-    df1.select(\"id\", \"data\").write()\n-        .format(\"iceberg\")\n-        .mode(\"append\")\n-        .save(tableLocation);\n-\n-    // add a second file\n-    df2.select(\"id\", \"data\").write()\n-        .format(\"iceberg\")\n-        .mode(\"append\")\n-        .save(tableLocation);\n-\n-    // delete the first file to test that only live files are listed\n-    table.newDelete().deleteFromRowFilter(Expressions.equal(\"id\", 1)).commit();\n-\n-    List<Row> actual = spark.read()\n-        .format(\"iceberg\")\n-        .load(tableLocation + \"#files\")\n-        .collectAsList();\n-\n-    List<GenericData.Record> expected = Lists.newArrayList();\n-    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n-      InputFile in = table.io().newInputFile(manifest.path());\n-      try (CloseableIterable<GenericData.Record> rows = Avro.read(in).project(entriesTable.schema()).build()) {\n-        for (GenericData.Record record : rows) {\n-          if ((Integer) record.get(\"status\") < 2 /* added or existing */) {\n-            expected.add((GenericData.Record) record.get(\"data_file\"));\n-          }\n-        }\n-      }\n-    }\n-\n-    Assert.assertEquals(\"Files table should have one row\", 1, expected.size());\n-    Assert.assertEquals(\"Actual results should have one row\", 1, actual.size());\n-    TestHelpers.assertEqualsSafe(filesTable.schema().asStruct(), expected.get(0), actual.get(0));\n+    super.testFilesTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#files\");\n   }\n \n   @Test\n   public void testFilesTableWithSnapshotIdInheritance() throws Exception {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-\n-    table.updateProperties()\n-        .set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, \"true\")\n-        .commit();\n-\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#files\");\n+    try {\n+      Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n \n-    List<SimpleRecord> records = Lists.newArrayList(\n-        new SimpleRecord(1, \"a\"),\n-        new SimpleRecord(2, \"b\")\n-     );\n+      table.updateProperties()\n+              .set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, \"true\")", "originalCommit": "3e4f731cfaaba7c7fa25d50cc3a6310d9a24793f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6278eecf8cd10b07b1da93b94631c881d5563b65", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\nindex 30bb44632..e1674a13a 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n\n@@ -73,107 +64,26 @@ public class TestIcebergSourceHadoopTables extends TestIcebergSourceTablesBase {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n-  @Test\n-  public void testEntriesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    System.out.println(tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-\n-    super.testEntriesTable(table, entriesTable, tableLocation, tableLocation + \"#entries\");\n-  }\n-\n-  @Test\n-  public void testAllEntriesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    System.out.println(tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#all_entries\");\n-\n-    super.testAllEntriesTable(table, entriesTable, tableLocation, tableLocation + \"#all_entries\");\n-  }\n-\n-  @Test\n-  public void testFilesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#files\");\n-\n-    super.testFilesTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#files\");\n-  }\n-\n-  @Test\n-  public void testFilesTableWithSnapshotIdInheritance() throws Exception {\n-    try {\n-      Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-\n-      table.updateProperties()\n-              .set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, \"true\")\n-              .commit();\n-\n-      Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-      Table filesTable = TABLES.load(tableLocation + \"#files\");\n-\n-      super.testFilesTableWithSnapshotIdInheritance(table, entriesTable, filesTable,\n-              tableLocation, tableLocation + \"#files\");\n-    } finally {\n-      spark.sql(\"DROP TABLE parquet_table\");\n+  @Override\n+  public Table createTable(TableIdentifier ident, Schema schema, PartitionSpec spec) {\n+    if (spec.equals(PartitionSpec.unpartitioned())) {\n+      return TABLES.create(schema, tableLocation);\n     }\n+    return TABLES.create(schema, spec, tableLocation);\n   }\n \n-  @Test\n-  public void testFilesUnpartitionedTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#files\");\n-\n-    super.testFilesUnpartitionedTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#files\");\n+  @Override\n+  public Table loadTable(TableIdentifier ident, String entriesSuffix) {\n+    return TABLES.load(loadLocation(ident, entriesSuffix));\n   }\n \n-  @Test\n-  public void testAllDataFilesTable() throws Exception {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table entriesTable = TABLES.load(tableLocation + \"#entries\");\n-    Table filesTable = TABLES.load(tableLocation + \"#all_data_files\");\n-\n-    super.testAllDataFilesTable(table, entriesTable, filesTable, tableLocation, tableLocation + \"#all_data_files\");\n-  }\n-\n-  @Test\n-  public void testHistoryTable() {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    Table historyTable = TABLES.load(tableLocation + \"#history\");\n-\n-    super.testHistoryTable(table, historyTable, tableLocation, tableLocation + \"#history\");\n-  }\n-\n-  @Test\n-  public void testSnapshotsTable() {\n-    Table table = TABLES.create(SCHEMA, tableLocation);\n-    Table snapTable = TABLES.load(tableLocation + \"#snapshots\");\n-\n-    super.testSnapshotsTable(table, snapTable, tableLocation, tableLocation + \"#snapshots\");\n+  @Override\n+  public String loadLocation(TableIdentifier ident, String entriesSuffix) {\n+    return String.format(\"%s#%s\", loadLocation(ident), entriesSuffix);\n   }\n \n-  @Test\n-  public void testManifestsTable() {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table manifestTable = TABLES.load(tableLocation + \"#manifests\");\n-\n-    super.testManifestsTable(table, manifestTable, tableLocation, tableLocation + \"#manifests\");\n-  }\n-\n-  @Test\n-  public void testAllManifestsTable() {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table manifestTable = TABLES.load(tableLocation + \"#all_manifests\");\n-\n-    super.testAllManifestsTable(table, manifestTable, tableLocation, tableLocation + \"#all_manifests\");\n-  }\n-\n-  @Test\n-  public void testPartitionsTable() {\n-    Table table = TABLES.create(SCHEMA, PartitionSpec.builderFor(SCHEMA).identity(\"id\").build(), tableLocation);\n-    Table partitionsTable = TABLES.load(tableLocation + \"#partitions\");\n-\n-    super.testPartitionsTable(table, partitionsTable, tableLocation, tableLocation + \"#partitions\");\n+  @Override\n+  public String loadLocation(TableIdentifier ident) {\n+    return tableLocation;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUzNDM1Mg==", "url": "https://github.com/apache/iceberg/pull/827#discussion_r388534352", "bodyText": "The problem with passing in the table is that this creates a dependency between the caller and this code: this assumes that the caller has created the table with the correct schema, for example.\nInstead, what about creating abstract methods to create the table, drop the table if it exists, and load the entries table? That way, you could write these test cases as @Test methods that are inherited and the Hive and Hadoop subclasses would just need to implement a few methods.", "author": "rdblue", "createdAt": "2020-03-05T20:07:30Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java", "diffHunk": "@@ -0,0 +1,586 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.data.TestHelpers;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.junit.Assert;\n+\n+public abstract class TestIcebergSourceTablesBase {\n+\n+  protected static SparkSession spark;\n+\n+  public void testEntriesTable(Table table, Table entriesTable,", "originalCommit": "3e4f731cfaaba7c7fa25d50cc3a6310d9a24793f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA4MDExMA==", "url": "https://github.com/apache/iceberg/pull/827#discussion_r389080110", "bodyText": "Thanks for the feedback @rdblue! Just updated the PR to use the abstract methods you described, hopefully this is closer to what you are looking for.", "author": "golammott", "createdAt": "2020-03-06T18:51:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUzNDM1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "6278eecf8cd10b07b1da93b94631c881d5563b65", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\nindex 85c58787d..fc27ea887 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\n\n@@ -28,40 +28,91 @@ import org.apache.avro.generic.GenericData;\n import org.apache.avro.generic.GenericRecordBuilder;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.avro.Avro;\n import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.spark.SparkTableUtil;\n import org.apache.iceberg.spark.data.TestHelpers;\n+import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.catalyst.TableIdentifier;\n import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n \n public abstract class TestIcebergSourceTablesBase {\n \n   protected static SparkSession spark;\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get())\n+  );\n+\n+  public abstract Table createTable(TableIdentifier ident, Schema schema, PartitionSpec spec);\n+\n+  public abstract Table loadTable(TableIdentifier ident, String entriesSuffix);\n+\n+  public abstract String loadLocation(TableIdentifier ident, String entriesSuffix);\n+\n+  public abstract String loadLocation(TableIdentifier ident);\n+\n+  @Test\n+  public synchronized void testTablesSupport() {\n+    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"table\");\n+    createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"1\"),\n+        new SimpleRecord(2, \"2\"),\n+        new SimpleRecord(3, \"3\"));\n+\n+    Dataset<Row> inputDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    inputDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(SaveMode.Append)\n+        .save(loadLocation(tableIdentifier));\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .load(loadLocation(tableIdentifier));\n+    List<SimpleRecord> actualRecords = resultDf.orderBy(\"id\")\n+        .as(Encoders.bean(SimpleRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Records should match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testEntriesTable() throws Exception {\n+    TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"entries_test\");\n+    Table table = createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned());\n+    Table entriesTable = loadTable(tableIdentifier, \"entries\");\n \n-  public void testEntriesTable(Table table, Table entriesTable,\n-                               String tableLocation, String loadLocation) throws Exception {\n     List<SimpleRecord> records = Lists.newArrayList(new SimpleRecord(1, \"1\"));\n \n     Dataset<Row> inputDf = spark.createDataFrame(records, SimpleRecord.class);\n     inputDf.select(\"id\", \"data\").write()\n         .format(\"iceberg\")\n         .mode(\"append\")\n-        .save(tableLocation);\n+        .save(loadLocation(tableIdentifier));\n \n     table.refresh();\n \n     List<Row> actual = spark.read()\n         .format(\"iceberg\")\n-        .load(loadLocation)\n+        .load(loadLocation(tableIdentifier, \"entries\"))\n         .collectAsList();\n \n     Assert.assertEquals(\"Should only contain one manifest\", 1, table.currentSnapshot().manifests().size());\n"}}, {"oid": "6278eecf8cd10b07b1da93b94631c881d5563b65", "url": "https://github.com/apache/iceberg/commit/6278eecf8cd10b07b1da93b94631c881d5563b65", "message": "Make table tests have common abstract parent", "committedDate": "2020-03-06T18:36:48Z", "type": "commit"}, {"oid": "6278eecf8cd10b07b1da93b94631c881d5563b65", "url": "https://github.com/apache/iceberg/commit/6278eecf8cd10b07b1da93b94631c881d5563b65", "message": "Make table tests have common abstract parent", "committedDate": "2020-03-06T18:36:48Z", "type": "forcePushed"}]}