{"pr_number": 1783, "pr_title": "Custom catalogs from `IcebergSource`", "pr_createdAt": "2020-11-18T18:48:22Z", "pr_url": "https://github.com/apache/iceberg/pull/1783", "timeline": [{"oid": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "url": "https://github.com/apache/iceberg/commit/2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "message": "source fixes", "committedDate": "2020-11-30T17:30:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjQ0Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533022447", "bodyText": "I don't think catalogName is accurate because catalogs have names (this one is spark_catalog). It should be catalogClass to be more clear.", "author": "rdblue", "createdAt": "2020-12-01T01:57:49Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class SetupSourceCatalog {\n+\n+  private SetupSourceCatalog() {\n+\n+  }\n+\n+  public static void setupSparkCatalog(SparkSession spark) {\n+    setupSparkCatalog(spark, SparkSessionCatalog.class.getName());\n+  }\n+\n+  public static void setupSparkCatalog(SparkSession spark, String catalogName) {", "originalCommit": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM3MzcxMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533373710", "bodyText": "\ud83d\udc4d", "author": "rymurr", "createdAt": "2020-12-01T12:32:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjQ0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java b/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nsimilarity index 63%\nrename from spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nrename to spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nindex a941266c8..c8c7e4c83 100644\n--- a/spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\n\n@@ -12,10 +12,23 @@\n  * limitations under the License.\n  */\n \n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.iceberg.spark.source;\n \n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n \n public final class SetupSourceCatalog {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjg0Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533022842", "bodyText": "Why not add this to the iceberg-spark module? The only thing that isn't compatible is SparkSessionCatalog.class.getName(). The 2.4 code should ignore these settings, and this change would be a lot smaller.", "author": "rdblue", "createdAt": "2020-12-01T01:59:15Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class SetupSourceCatalog {", "originalCommit": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM3NDMzMg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533374332", "bodyText": "cool, I was thinking similar but wanted to get your opinion first", "author": "rymurr", "createdAt": "2020-12-01T12:33:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjg0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java b/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nsimilarity index 63%\nrename from spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nrename to spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nindex a941266c8..c8c7e4c83 100644\n--- a/spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\n\n@@ -12,10 +12,23 @@\n  * limitations under the License.\n  */\n \n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.iceberg.spark.source;\n \n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n \n public final class SetupSourceCatalog {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzUwNw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533023507", "bodyText": "@RussellSpitzer, I seem to remember looking at very similar logic recently. Did we commit that anywhere that we can reuse?", "author": "rdblue", "createdAt": "2020-12-01T02:01:00Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());", "originalCommit": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzkyNTYzMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r537925630", "bodyText": "The logic was added to Spark3Util: https://github.com/apache/iceberg/blob/master/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java#L607\nCan you update this to use those utils?", "author": "rdblue", "createdAt": "2020-12-07T23:56:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM1MTQ1OA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r538351458", "bodyText": "lovely, thats fixed", "author": "rymurr", "createdAt": "2020-12-08T13:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzUwNw=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex efaac0f4e..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -71,52 +85,32 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n         return ((TableCatalog) catalog).loadTable(ident);\n       }\n     } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n       throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n     }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      try {\n-        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n-      } catch (ParseException ignored) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-    if (ident.size() == 1) {\n-      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    String catalogName = tableIdentifier(options).first();\n-    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"oid": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "url": "https://github.com/apache/iceberg/commit/ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "message": "updates based on code review", "committedDate": "2020-12-01T13:00:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQxMjEyNg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533412126", "bodyText": "A potential complication: SupportsCatalogOptions doesn't allow for specifying the schema. I don't know how much people rely on this feature but it is a breaking change for the IcebergSource", "author": "rymurr", "createdAt": "2020-12-01T13:37:45Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema3.java", "diffHunk": "@@ -1,23 +0,0 @@\n-/*", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "409b1ad069f7723fd083d98c1bd6babe2f607395", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema3.java b/spark3/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema3.java\nnew file mode 100644\nindex 000000000..10a255e97\n--- /dev/null\n+++ b/spark3/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema3.java\n\n@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+public class TestSparkSchema3 extends TestSparkSchema {\n+}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDM4MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533614380", "bodyText": "Nit: unnecessary whitespace change.", "author": "rdblue", "createdAt": "2020-12-01T18:02:16Z", "path": "spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java", "diffHunk": "@@ -60,11 +62,17 @@\n       optional(3, \"c3\", Types.StringType.get())\n   );\n \n+", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzEyNw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534117127", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-02T12:07:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDM4MA=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java b/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\nindex 56e4df836..b59310040 100644\n--- a/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\n+++ b/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\n\n@@ -62,7 +62,6 @@ public abstract class TestScanTaskSerialization extends SparkTestBase {\n       optional(3, \"c3\", Types.StringType.get())\n   );\n \n-\n   @Rule\n   public TemporaryFolder temp = new TemporaryFolder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDYwNg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533614606", "bodyText": "Nit: can you remove this newline?", "author": "rdblue", "createdAt": "2020-12-01T18:02:38Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class SetupSourceCatalog {\n+\n+  private SetupSourceCatalog() {\n+", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzI5Ng==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534117296", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-02T12:07:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDYwNg=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java b/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\nindex c8c7e4c83..895689eb7 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java\n\n@@ -34,7 +34,6 @@ import org.apache.spark.sql.SparkSession;\n public final class SetupSourceCatalog {\n \n   private SetupSourceCatalog() {\n-\n   }\n \n   public static void setupSparkCatalog(SparkSession spark) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNjA4MQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533616081", "bodyText": "I think the correct exception is the Spark exception since this is going to be called from Spark code.", "author": "rdblue", "createdAt": "2020-12-01T18:05:03Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMDQxMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534120410", "bodyText": "The Spark NoSuchTableException is typed which would change the interface. It appears there is no well defined way to return from this function w/o a table being found. It NPEs if you return null so an untyped exception seems to be the expected way to denote no table found. I thought the Iceberg NoSuchTableException was the best compromise here.", "author": "rymurr", "createdAt": "2020-12-02T12:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNjA4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ3MjE3Mw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534472173", "bodyText": "Got it, I agree in that case. Thanks for explaining! You may want to add a comment here to explain in the code as well.", "author": "rdblue", "createdAt": "2020-12-02T20:50:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNjA4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex efaac0f4e..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -71,52 +85,32 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n         return ((TableCatalog) catalog).loadTable(ident);\n       }\n     } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n       throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n     }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      try {\n-        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n-      } catch (ParseException ignored) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-    if (ident.size() == 1) {\n-      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    String catalogName = tableIdentifier(options).first();\n-    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxODE0NQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533618145", "bodyText": "Why is this second attempt done?\nWrapping the identifier in ` should escape the entire string as a single identifier, so the parser would return the original path as one component.\nI think this would be equivalent to ident = path.", "author": "rdblue", "createdAt": "2020-12-01T18:08:28Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMjg2MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534122860", "bodyText": "you are correct. Fixed", "author": "rymurr", "createdAt": "2020-12-02T12:17:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxODE0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex efaac0f4e..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -71,52 +85,32 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n         return ((TableCatalog) catalog).loadTable(ident);\n       }\n     } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n       throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n     }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      try {\n-        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n-      } catch (ParseException ignored) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-    if (ident.size() == 1) {\n-      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    String catalogName = tableIdentifier(options).first();\n-    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxOTY2MQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533619661", "bodyText": "The default implementation always returns spark_catalog, not the current catalog. Since we want to use the current catalog when it isn't defined in the identifier, tableIdentifier(options) should fill it in. That would simplify this logic because it should always be non-null.", "author": "rdblue", "createdAt": "2020-12-01T18:10:55Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n+      } catch (ParseException ignored) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+    if (ident.size() == 1) {\n+      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n+    } else if (ident.size() == 2) {\n+      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n+        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n+      } else {\n+        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n+      }\n     } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n+        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n+      } else {\n+        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n+      }\n     }\n   }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    Table table = findTable(options, conf);\n-\n-    // Set confs from table properties\n-    mergeIcebergHadoopConfs(conf, table.properties());\n-\n-    // Re-overwrite values set in options and table properties but were not in the environment.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    return table;\n+  @Override\n+  public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n+    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n+    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n   }\n \n-  private static void mergeIcebergHadoopConfs(Configuration baseConf, Map<String, String> options) {\n-    options.keySet().stream()\n-        .filter(key -> key.startsWith(\"hadoop.\"))\n-        .forEach(key -> baseConf.set(key.replaceFirst(\"hadoop.\", \"\"), options.get(key)));\n+  @Override\n+  public String extractCatalog(CaseInsensitiveStringMap options) {\n+    String catalogName = tableIdentifier(options).first();\n+    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNDIwNA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534124204", "bodyText": "Ahh, yes. I just re-read the comments. Make sense, fixed.", "author": "rymurr", "createdAt": "2020-12-02T12:19:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxOTY2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex efaac0f4e..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -71,52 +85,32 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n         return ((TableCatalog) catalog).loadTable(ident);\n       }\n     } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n       throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n     }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      try {\n-        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n-      } catch (ParseException ignored) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-    if (ident.size() == 1) {\n-      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    String catalogName = tableIdentifier(options).first();\n-    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTM5NQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533621395", "bodyText": "This shouldn't fill in the default namespace. If the identifier was two parts, like prod.items, then it is not correct to modify that to be prod.default.items.\nI think that this should use the same logic as the else.", "author": "rdblue", "createdAt": "2020-12-01T18:13:57Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n+      } catch (ParseException ignored) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+    if (ident.size() == 1) {\n+      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n+    } else if (ident.size() == 2) {\n+      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n+        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNjAxMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534126013", "bodyText": "ok, makes sense. That also clarifies the todo i left there. Fixed to not add a default namespace. Does the same logic apply for the case above. If I pass items it will get re-written to default_catalog.default.items, should it only be default_catalog.items?", "author": "rymurr", "createdAt": "2020-12-02T12:23:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTM5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex efaac0f4e..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -71,52 +85,32 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n         return ((TableCatalog) catalog).loadTable(ident);\n       }\n     } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n       throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n     }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      try {\n-        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n-      } catch (ParseException ignored) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-    if (ident.size() == 1) {\n-      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    String catalogName = tableIdentifier(options).first();\n-    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTUyMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533621520", "bodyText": "Nit: could you separate the control flow statements with a newline?", "author": "rdblue", "createdAt": "2020-12-01T18:14:09Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n+      } catch (ParseException ignored) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+    if (ident.size() == 1) {", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNjE1Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534126152", "bodyText": "\ud83d\udc4d", "author": "rymurr", "createdAt": "2020-12-02T12:23:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTUyMA=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex efaac0f4e..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -71,52 +85,32 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n         return ((TableCatalog) catalog).loadTable(ident);\n       }\n     } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n       throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n     }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      try {\n-        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n-      } catch (ParseException ignored) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-    if (ident.size() == 1) {\n-      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    String catalogName = tableIdentifier(options).first();\n-    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTk0Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r535479942", "bodyText": "nit: should not change license", "author": "jackye1995", "createdAt": "2020-12-03T18:28:07Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/TestIcebergSource.java", "diffHunk": "@@ -1,27 +1,23 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one", "originalCommit": "033a36adbdb4ab57f50d70e5dfb369541ffd1c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEwOTcwMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r536109701", "bodyText": "fixed, dunno how that happened...", "author": "rymurr", "createdAt": "2020-12-04T13:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTk0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/spark/source/TestIcebergSource.java b/spark3/src/test/java/org/apache/iceberg/spark/source/TestIcebergSource.java\nindex a0b6ed9c9..ef63fbec1 100644\n--- a/spark3/src/test/java/org/apache/iceberg/spark/source/TestIcebergSource.java\n+++ b/spark3/src/test/java/org/apache/iceberg/spark/source/TestIcebergSource.java\n\n@@ -1,15 +1,20 @@\n /*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n  */\n \n package org.apache.iceberg.spark.source;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwMzk0Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r535503947", "bodyText": "Because we are now assigning an empty namespace for size==2 case,  I think it can be merged together with the else case. We only get ident.subList(1, ident.size()).toArray(new String[0]) simplified to ident.get(1), but the rest are all duplicates.", "author": "jackye1995", "createdAt": "2020-12-03T19:04:56Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +62,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    String currentCatalogName = catalogManager.currentCatalog().name();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      ident = new ArrayList<>();\n+      ident.add(path);\n+    }\n \n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    if (ident.size() == 1) {\n+      return Pair.of(currentCatalogName, TableIdentifier.of(defaultNamespace, ident.get(0)));\n+    } else if (ident.size() == 2) {", "originalCommit": "033a36adbdb4ab57f50d70e5dfb369541ffd1c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjExMDIwMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r536110201", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-04T13:46:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwMzk0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex 816dd21fe..00f618441 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -79,45 +92,25 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n     throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    String currentCatalogName = catalogManager.currentCatalog().name();\n-    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-    List<String> ident;\n     try {\n-      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), path);\n     } catch (ParseException e) {\n-      ident = new ArrayList<>();\n+      List<String> ident = new ArrayList<>();\n       ident.add(path);\n-    }\n-\n-    if (ident.size() == 1) {\n-      return Pair.of(currentCatalogName, TableIdentifier.of(defaultNamespace, ident.get(0)));\n-    } else if (ident.size() == 2) {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.get(1)));\n-      } else {\n-        return Pair.of(currentCatalogName, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n-    } else {\n-      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n-        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n-      } else {\n-        return Pair.of(currentCatalogName, TableIdentifier.of(ident.toArray(new String[0])));\n-      }\n+      return Spark3Util.catalogAndIdentifier(SparkSession.active(), ident);\n     }\n   }\n \n   @Override\n   public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n-    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n-    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n+    return catalogAndIdentifier(options).identifier();\n   }\n \n   @Override\n   public String extractCatalog(CaseInsensitiveStringMap options) {\n-    return tableIdentifier(options).first();\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n }\n"}}, {"oid": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "url": "https://github.com/apache/iceberg/commit/b96e39268dfe840446f260fa8de6eba2b39ac7b3", "message": "use util function to get catalog and identifier", "committedDate": "2020-12-08T13:14:13Z", "type": "forcePushed"}, {"oid": "d4bb4b7eb12626bf224ce6aa95aef07bd37f1b93", "url": "https://github.com/apache/iceberg/commit/d4bb4b7eb12626bf224ce6aa95aef07bd37f1b93", "message": "rebase to pick up #1843 and fix build", "committedDate": "2020-12-09T13:40:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4OTI3MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539489270", "bodyText": "What does \"using settings from 'catalog'\" mean in practice? Hadoop tables are loaded using a generic HadoopTables, so the catalog doesn't really affect it at all, I think. In that case, this is promising something that it doesn't need to and I'd prefer to avoid making claims about behavior like that.", "author": "rdblue", "createdAt": "2020-12-09T17:10:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -19,22 +19,49 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> loads a HadoopTable at given path using settings from 'catalog'", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUyNDgzMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539524831", "bodyText": "Yes you are correct. I was thinking of a catalog having different hadoop Configuration but I don't think that makes any sense. Fixed now.", "author": "rymurr", "createdAt": "2020-12-09T17:58:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4OTI3MA=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex ebd69622d..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -19,15 +19,11 @@\n \n package org.apache.iceberg.spark.source;\n \n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.PathIdentifier;\n import org.apache.iceberg.spark.Spark3Util;\n-import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MDE5NQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539490195", "bodyText": "In these examples, I think it is easier to understand if you separate the cases and use catalog and namespace, rather than \"if xxx is a catalog . . . otherwise . . .\".", "author": "rdblue", "createdAt": "2020-12-09T17:12:11Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -19,22 +19,49 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> loads a HadoopTable at given path using settings from 'catalog'\n+ *  table = \"catalog.namespace.`file:/path/to/table`\" -> fails. Namespace doesn't exist for paths\n+ *  table = \"tablename\" -> loads currentCatalog.currentNamespace.tablename\n+ *  table = \"xxx.tablename\" -> if xxx is a catalog load \"tablename\" from the specified catalog. Otherwise\n+ *          load \"xxx.tablename\" from current catalog\n+ *  table = \"xxx.yyy.tablename\" -> if xxx is a catalog load \"yyy.tablename\" from the specified catalog. Otherwise\n+ *          load \"xxx.yyy.tablename\" from current catalog", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUyNDg4OA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539524888", "bodyText": "agreed, fixed.", "author": "rymurr", "createdAt": "2020-12-09T17:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MDE5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex ebd69622d..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -19,15 +19,11 @@\n \n package org.apache.iceberg.spark.source;\n \n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.PathIdentifier;\n import org.apache.iceberg.spark.Spark3Util;\n-import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MjQzMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539492433", "bodyText": "When implementing SupportsCatalogOptions, I don't think this will ever be called. Should we remove it and throw UnsupportedOperationException instead?", "author": "rdblue", "createdAt": "2020-12-09T17:15:13Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUyNzc4MQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539527781", "bodyText": "inferPartitioning method uses it :-( Can we return Transform[0] and have Spark do the work?", "author": "rymurr", "createdAt": "2020-12-09T18:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MjQzMw=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex ebd69622d..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -84,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539495163", "bodyText": "Is this try/catch to avoid the / check? I think that's a pretty reasonable check. I'm not sure that this is a good idea because the cases you listed above would pass this.\nFor example, this uses a valid identifier, but is clearly a path reference:\nspark.format(\"iceberg\").load(\"catalog.`file:/path/to/table`\");\nAs I mentioned above, I don't think that we should support the mixed path and catalog identifiers, but I still think it is probably a more predictable check to just look for /.", "author": "rdblue", "createdAt": "2020-12-09T17:18:49Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    SparkSession spark = SparkSession.active();\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzMTExMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539531111", "bodyText": "the try component takes care of catalog.table, catalog.namespace.table and catalog.`file://path/to/table`  but doens't take care of file://path/to/table which throws a parse exception. The catch part deals w/ file://path/to/table.\nI suppose we can check\n\nif there is a ` character and a / and reject (don't mix catalogs and paths)\nIf just / then treat as path\nassume its tables and parse it", "author": "rymurr", "createdAt": "2020-12-09T18:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzNzgyMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539537823", "bodyText": "Yeah, I think that the problem is that catalog.`file://path/to/table`  is ambiguous and should not be supported. But parsing it successfully means that it would be translated to a path-based table with a catalog.\nI don't think a more complicated check is needed, to see if the / is probably escaped. If there is a /, then it's a path. That's a really simple rule to follow.", "author": "rdblue", "createdAt": "2020-12-09T18:17:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0NDkwMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539544903", "bodyText": "After thnking more I have thrown out catalog.`file:/path/to/table` and simplified the method", "author": "rymurr", "createdAt": "2020-12-09T18:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex ebd69622d..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -84,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTgwNw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539495807", "bodyText": "Nit: Using ImmutableList makes this more concise: Spark3Util.catalogAndIdentifier(spark, ImmutableList.of(path))", "author": "rdblue", "createdAt": "2020-12-09T17:19:33Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    SparkSession spark = SparkSession.active();\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, ident);", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzMTE4OQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539531189", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-09T18:07:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTgwNw=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex ebd69622d..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -84,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5ODk4Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539498982", "bodyText": "I think it would be simpler just to start this method with a check for the path:\nif (path.contains(\"/\")) {\n  Identifier ident = new PathIdentifer(path);\n  // do catalog resolution\n  return catalog, ident;\n}\n\nIt's easier to document and understand if the rules are simple.", "author": "rdblue", "createdAt": "2020-12-09T17:23:57Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    SparkSession spark = SparkSession.active();\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, ident);\n+    }\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n+    String[] currentNamespace = catalogManager.currentNamespace();\n+    // we have to check for paths but want to re-use the exiting utils to extract catalog/identifier\n+    if (checkPathIdentifier(catalogAndIdentifier.identifier(), currentNamespace)) {", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0NTE0OQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539545149", "bodyText": "agreed, cleaned up", "author": "rymurr", "createdAt": "2020-12-09T18:28:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5ODk4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex ebd69622d..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -84,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODAxOA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539748018", "bodyText": "Same here, no need for the catalog/path lines.", "author": "rdblue", "createdAt": "2020-12-10T00:20:50Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -20,21 +20,45 @@\n package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> fails. Don't set a catalog for paths\n+ *  table = \"catalog.namespace.`file:/path/to/table`\" -> fails. Namespace doesn't exist for paths", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzA2Mw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133063", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-10T12:34:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODAxOA=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex bcb884344..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -21,13 +21,13 @@ package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.PathIdentifier;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n import org.apache.spark.sql.connector.catalog.Identifier;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODA4Ng==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539748086", "bodyText": "Nit: starts with \"otherwise\" still.", "author": "rdblue", "createdAt": "2020-12-10T00:21:02Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -20,21 +20,45 @@\n package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> fails. Don't set a catalog for paths\n+ *  table = \"catalog.namespace.`file:/path/to/table`\" -> fails. Namespace doesn't exist for paths\n+ *  table = \"tablename\" -> loads currentCatalog.currentNamespace.tablename\n+ *  table = \"catalog.tablename\" -> load \"tablename\" from the specified catalog.\n+ *  table = \"namespace.tablename\" -> Otherwise load \"namespace.tablename\" from current catalog", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzA5Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133092", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-10T12:34:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODA4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex bcb884344..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -21,13 +21,13 @@ package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.PathIdentifier;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n import org.apache.spark.sql.connector.catalog.Identifier;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODIxMg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539748212", "bodyText": "Nit: looks like newlines between control flow are missing in a lot of these changes.", "author": "rdblue", "createdAt": "2020-12-10T00:21:23Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +80,70 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzEyMg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133122", "bodyText": "done.", "author": "rymurr", "createdAt": "2020-12-10T12:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODIxMg=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex bcb884344..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -81,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTIxMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539749213", "bodyText": "This is doing a lot of extra work by not calling CatalogAndIdentifier directly. There are two maps created, two identical catalog/table resolutions, and then this needs to get the active session and look up the catalog that was already loaded. Is it possible to refactor so that the \"extract\" functions use a common method that can be used here?", "author": "rdblue", "createdAt": "2020-12-10T00:23:47Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +80,70 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzE4Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133187", "bodyText": "yup, this wasn't refactored when I removed some code from the extract methods. Now that they don't do anything I have cleaned up the getTable method.", "author": "rymurr", "createdAt": "2020-12-10T12:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTIxMw=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex bcb884344..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -81,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTU4NA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539749584", "bodyText": "I think this should parse the identifier using Spark rather than DOT.splitToList. It should be spark.sessionState().sqlParser().parseMultipartIdentifier(path).", "author": "rdblue", "createdAt": "2020-12-10T00:24:39Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +80,70 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    setupDefaultSparkCatalog();\n     String path = options.get(\"path\");\n+    SparkSession spark = SparkSession.active();\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n+    }\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark,\n+        DOT.splitToList(path));", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzMxNg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133316", "bodyText": "done. I have thrown an IllegalArgumentException in place of the Spark ParseException. This will eventually be caught and turned back into an AnalysisException.", "author": "rymurr", "createdAt": "2020-12-10T12:34:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTU4NA=="}], "type": "inlineReview", "revised_code": {"commit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex bcb884344..e43b53570 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -81,10 +78,10 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   @Override\n   public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n-    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n-    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n-    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n     try {\n       if (catalog instanceof TableCatalog) {\n         return ((TableCatalog) catalog).loadTable(ident);\n"}}, {"oid": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "url": "https://github.com/apache/iceberg/commit/3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "message": "clean up based on code review", "committedDate": "2020-12-10T12:52:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMDI3MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540600270", "bodyText": "Sorry for not catching this before, but I think this should be passed in. This should avoid extra calls to SparkSession.active().", "author": "rdblue", "createdAt": "2020-12-11T00:27:02Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +77,77 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    setupDefaultSparkCatalog();\n     String path = options.get(\"path\");\n+    SparkSession spark = SparkSession.active();\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n-    } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n     }\n-  }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    Table table = findTable(options, conf);\n+    final Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot parse path %s. It is not a valid SQL table\", path), e);\n+    }\n \n-    // Set confs from table properties\n-    mergeIcebergHadoopConfs(conf, table.properties());\n+    if (catalogAndIdentifier.catalog().name().equals(\"spark_catalog\") &&\n+        !(catalogAndIdentifier.catalog() instanceof SparkSessionCatalog)) {\n+      // catalog is a session catalog but does not support Iceberg. Use Iceberg instead.\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          catalogAndIdentifier.identifier());\n+    } else {\n+      return catalogAndIdentifier;\n+    }\n+  }\n \n-    // Re-overwrite values set in options and table properties but were not in the environment.\n-    mergeIcebergHadoopConfs(conf, options);\n+  @Override\n+  public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n+    return catalogAndIdentifier(options).identifier();\n+  }\n \n-    return table;\n+  @Override\n+  public String extractCatalog(CaseInsensitiveStringMap options) {\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n \n-  private static void mergeIcebergHadoopConfs(Configuration baseConf, Map<String, String> options) {\n-    options.keySet().stream()\n-        .filter(key -> key.startsWith(\"hadoop.\"))\n-        .forEach(key -> baseConf.set(key.replaceFirst(\"hadoop.\", \"\"), options.get(key)));\n+  private static void setupDefaultSparkCatalog() {\n+    SparkSession spark = SparkSession.active();", "originalCommit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgzMDg4Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540830882", "bodyText": "I should have too. Fixed :-)", "author": "rymurr", "createdAt": "2020-12-11T10:03:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMDI3MA=="}], "type": "inlineReview", "revised_code": {"commit": "ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex e43b53570..e9d609438 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -97,9 +96,9 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n-    setupDefaultSparkCatalog();\n-    String path = options.get(\"path\");\n     SparkSession spark = SparkSession.active();\n+    setupDefaultSparkCatalog(spark);\n+    String path = options.get(\"path\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMjIwMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540602203", "bodyText": "There's no need to catch ParseException any more. A new version of catalogAndIdentifer was added with context to form the error message. This could be:\nSpark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\"identifier\", spark, path);", "author": "rdblue", "createdAt": "2020-12-11T00:32:20Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +77,77 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    setupDefaultSparkCatalog();\n     String path = options.get(\"path\");\n+    SparkSession spark = SparkSession.active();\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n-    } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n     }\n-  }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    Table table = findTable(options, conf);\n+    final Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot parse path %s. It is not a valid SQL table\", path), e);", "originalCommit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgzMzU4NA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540833584", "bodyText": "nice, thanks to @RussellSpitzer! I added another method there to extract the default catalog w/o passing it in.", "author": "rymurr", "createdAt": "2020-12-11T10:07:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMjIwMw=="}], "type": "inlineReview", "revised_code": {"commit": "ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex e43b53570..e9d609438 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -97,9 +96,9 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n \n   private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n-    setupDefaultSparkCatalog();\n-    String path = options.get(\"path\");\n     SparkSession spark = SparkSession.active();\n+    setupDefaultSparkCatalog(spark);\n+    String path = options.get(\"path\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n"}}, {"oid": "ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "url": "https://github.com/apache/iceberg/commit/ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "message": "minor fixes and simplification", "committedDate": "2020-12-11T10:10:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExNTkwMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541115903", "bodyText": "The \"Cannot parse\" part is filled in automatically, so you just need to pass the name of the arg. Probably \"path or identifier\".", "author": "rdblue", "createdAt": "2020-12-11T17:40:31Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +76,72 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    SparkSession spark = SparkSession.active();\n+    setupDefaultSparkCatalog(spark);\n     String path = options.get(\"path\");\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n-    } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n     }\n-  }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n+    final Spark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n+        \"Cannot parse path %s. It is not a valid SQL table\", spark, path);", "originalCommit": "ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExODI5OQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541118299", "bodyText": "\ud83e\udd26 fixed", "author": "rymurr", "createdAt": "2020-12-11T17:44:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExNTkwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTEyMTI5Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541121297", "bodyText": "That was fast!", "author": "rdblue", "createdAt": "2020-12-11T17:49:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExNTkwMw=="}], "type": "inlineReview", "revised_code": {"commit": "d8aada53d5b15833d49fe452021215b7e35ec0e3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex e9d609438..4ef5eee7c 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -108,7 +108,7 @@ public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions\n     }\n \n     final Spark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"Cannot parse path %s. It is not a valid SQL table\", spark, path);\n+        \"path or identifier\", spark, path);\n \n     if (catalogAndIdentifier.catalog().name().equals(\"spark_catalog\") &&\n         !(catalogAndIdentifier.catalog() instanceof SparkSessionCatalog)) {\n"}}, {"oid": "d8aada53d5b15833d49fe452021215b7e35ec0e3", "url": "https://github.com/apache/iceberg/commit/d8aada53d5b15833d49fe452021215b7e35ec0e3", "message": "log line fix", "committedDate": "2020-12-11T17:45:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI5MjYzMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541292631", "bodyText": "Maybe important to note that these are used with this priority as well, ie\nif \"catalog.namespace.table\" is valid it will be read and \"namespace.namespace.table\" will be ignored ?", "author": "RussellSpitzer", "createdAt": "2020-12-11T21:07:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -20,21 +20,41 @@\n package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path", "originalCommit": "d8aada53d5b15833d49fe452021215b7e35ec0e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQyODc0Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r542428742", "bodyText": "very good point @RussellSpitzer, I have included a note to that effect", "author": "rymurr", "createdAt": "2020-12-14T14:31:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI5MjYzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "409b1ad069f7723fd083d98c1bd6babe2f607395", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\nindex 4ef5eee7c..4a9fce7a0 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java\n\n@@ -19,42 +19,28 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.util.Arrays;\n import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.spark.PathIdentifier;\n-import org.apache.iceberg.spark.Spark3Util;\n-import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n import org.apache.spark.sql.connector.catalog.Identifier;\n-import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n-import org.apache.spark.sql.connector.catalog.Table;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.internal.StaticSQLConf;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-/**\n- * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n- *\n- * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n- *\n- *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n- *  table = \"tablename\" -> loads currentCatalog.currentNamespace.tablename\n- *  table = \"catalog.tablename\" -> load \"tablename\" from the specified catalog.\n- *  table = \"namespace.tablename\" -> load \"namespace.tablename\" from current catalog\n- *  table = \"catalog.namespace.tablename\" -> \"namespace.tablename\" from the specified catalog.\n- *  table = \"namespace1.namespace2.tablename\" -> load \"namespace1.namespace2.tablename\" from current catalog\n- *\n- */\n-public class IcebergSource implements DataSourceRegister, SupportsCatalogOptions {\n-  private static final String DEFAULT_CATALOG_NAME = \"default_iceberg\";\n-  private static final String DEFAULT_CATALOG = \"spark.sql.catalog.\" + DEFAULT_CATALOG_NAME;\n-\n+public class IcebergSource implements DataSourceRegister, TableProvider {\n   @Override\n   public String shortName() {\n     return \"iceberg\";\n"}}, {"oid": "409b1ad069f7723fd083d98c1bd6babe2f607395", "url": "https://github.com/apache/iceberg/commit/409b1ad069f7723fd083d98c1bd6babe2f607395", "message": "straw man custom catalog from `IcebergSource`", "committedDate": "2020-12-22T16:45:38Z", "type": "commit"}, {"oid": "d45ee4cc6f0e2c198e192d33e2bf85c87c584de1", "url": "https://github.com/apache/iceberg/commit/d45ee4cc6f0e2c198e192d33e2bf85c87c584de1", "message": "IcebergSource now uses SupportsCatalogOptions\n\nIt compiles and mostly works but SessionCatalog isn't set anywhere.\n\nHave to change a lot of the Spark3 tests to register a catalog now", "committedDate": "2020-12-22T16:45:39Z", "type": "commit"}, {"oid": "239c2e72f7424fed4c77c66e82ac8ee86aaff3d3", "url": "https://github.com/apache/iceberg/commit/239c2e72f7424fed4c77c66e82ac8ee86aaff3d3", "message": "start fixing tests", "committedDate": "2020-12-22T16:45:40Z", "type": "commit"}, {"oid": "581638cded5a197447024eecf2970f2418b60eb2", "url": "https://github.com/apache/iceberg/commit/581638cded5a197447024eecf2970f2418b60eb2", "message": "tests all fixed", "committedDate": "2020-12-22T16:45:41Z", "type": "commit"}, {"oid": "ef262111680150957ce4d24f67690045df53ddf4", "url": "https://github.com/apache/iceberg/commit/ef262111680150957ce4d24f67690045df53ddf4", "message": "source fixes", "committedDate": "2020-12-22T16:45:41Z", "type": "commit"}, {"oid": "2a8bac3dfcfa01b0eeed7cb260bfc21653caaf2f", "url": "https://github.com/apache/iceberg/commit/2a8bac3dfcfa01b0eeed7cb260bfc21653caaf2f", "message": "updates based on code review", "committedDate": "2020-12-22T16:45:42Z", "type": "commit"}, {"oid": "c39f108ed03c06f8381c2569ec9e05f20e1643df", "url": "https://github.com/apache/iceberg/commit/c39f108ed03c06f8381c2569ec9e05f20e1643df", "message": "clean up based on code review", "committedDate": "2020-12-22T16:45:43Z", "type": "commit"}, {"oid": "50808b7e5f3bdd4e16a2a630e68647c4beee9394", "url": "https://github.com/apache/iceberg/commit/50808b7e5f3bdd4e16a2a630e68647c4beee9394", "message": "comment explaining exception type", "committedDate": "2020-12-22T16:45:44Z", "type": "commit"}, {"oid": "8ebe3c9db9c9efdb188a2b8fbb1644e9629564c9", "url": "https://github.com/apache/iceberg/commit/8ebe3c9db9c9efdb188a2b8fbb1644e9629564c9", "message": "clean up and docs", "committedDate": "2020-12-22T16:45:45Z", "type": "commit"}, {"oid": "20e9d218fb43411fa5766a26281bffdad8435ea3", "url": "https://github.com/apache/iceberg/commit/20e9d218fb43411fa5766a26281bffdad8435ea3", "message": "use util function to get catalog and identifier", "committedDate": "2020-12-22T16:45:46Z", "type": "commit"}, {"oid": "806e92ac97ef7ecbb002647d176b0434609cc8be", "url": "https://github.com/apache/iceberg/commit/806e92ac97ef7ecbb002647d176b0434609cc8be", "message": "rebase to pick up #1843 and fix build", "committedDate": "2020-12-22T16:45:47Z", "type": "commit"}, {"oid": "4821282b25229bf82445a8b01c7bca92ebe1acaf", "url": "https://github.com/apache/iceberg/commit/4821282b25229bf82445a8b01c7bca92ebe1acaf", "message": "start a custom catalog if no iceberg catalogs are available", "committedDate": "2020-12-22T16:45:48Z", "type": "commit"}, {"oid": "04ce0069aeb9ce0e88f4e9e9fa5fc9560ad6cc12", "url": "https://github.com/apache/iceberg/commit/04ce0069aeb9ce0e88f4e9e9fa5fc9560ad6cc12", "message": "clean up", "committedDate": "2020-12-22T16:45:49Z", "type": "commit"}, {"oid": "ca249bc2770c997df29f322cc544085cf1e221a6", "url": "https://github.com/apache/iceberg/commit/ca249bc2770c997df29f322cc544085cf1e221a6", "message": "address code review and simplify default catalog logic", "committedDate": "2020-12-22T16:45:50Z", "type": "commit"}, {"oid": "4a38352a144ee806cb3118e27fb4a353b3f76ca0", "url": "https://github.com/apache/iceberg/commit/4a38352a144ee806cb3118e27fb4a353b3f76ca0", "message": "clean up based on code review", "committedDate": "2020-12-22T16:45:51Z", "type": "commit"}, {"oid": "c9b6483d078188a7993fbcb0f7d3be9d5dbe20f7", "url": "https://github.com/apache/iceberg/commit/c9b6483d078188a7993fbcb0f7d3be9d5dbe20f7", "message": "minor fixes and simplification", "committedDate": "2020-12-22T16:45:52Z", "type": "commit"}, {"oid": "b8c2320225fec647416249fed3dcc6bef788bc1e", "url": "https://github.com/apache/iceberg/commit/b8c2320225fec647416249fed3dcc6bef788bc1e", "message": "log line fix", "committedDate": "2020-12-22T16:45:53Z", "type": "commit"}, {"oid": "a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "url": "https://github.com/apache/iceberg/commit/a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "message": "log line fix", "committedDate": "2020-12-22T16:45:53Z", "type": "commit"}, {"oid": "a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "url": "https://github.com/apache/iceberg/commit/a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "message": "log line fix", "committedDate": "2020-12-22T16:45:53Z", "type": "forcePushed"}]}