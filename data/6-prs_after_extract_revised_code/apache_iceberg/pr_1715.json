{"pr_number": 1715, "pr_title": "Fix Actions to be Compatible with V2 Catalogs", "pr_createdAt": "2020-11-04T02:11:04Z", "pr_url": "https://github.com/apache/iceberg/pull/1715", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA3MTUzNA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517071534", "bodyText": "Maybe this method can not handle some corner cases.\nFor example, in the spark2 action, the file_path is very special (file://level1.level2.level3.level4/database/table).", "author": "liukun4515", "createdAt": "2020-11-04T03:00:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -144,6 +123,32 @@ protected String metadataTableName(String tableName, MetadataTableType type) {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    String metadataTableName = metadataTableName(tableName, type);", "originalCommit": "9ab39c789cf014a8b27749e8736e914c4d882b15", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nsimilarity index 87%\nrename from spark/src/main/java/org/apache/iceberg/actions/BaseAction.java\nrename to spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 78f626a3d..49625e2f6 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -123,10 +125,11 @@ abstract class BaseAction<R> implements Action<R> {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n-  protected Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n-    String metadataTableName = metadataTableName(tableName, type);\n-    if (metadataTableName.split(\"\\\\.\").length > 3) {\n-      // This table has a three or more components, use the V2Catalog read path\n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    String metadataTableName = metadataTableName(tableName, tableLocation, type);\n+    if (!metadataTableName.contains(\"/\") && metadataTableName.split(\"\\\\.\").length > 3) {\n+      // This table has a three or more components and has no slashes, use the V2Catalog read path\n       return spark.table(metadataTableName);\n     } else {\n       // This table is file based or has no catalog (db.tableName), read via non-catalog path\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA3Nzk2NQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517077965", "bodyText": "can we add some examples for explaining the case?", "author": "liukun4515", "createdAt": "2020-11-04T03:26:49Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -144,6 +123,32 @@ protected String metadataTableName(String tableName, MetadataTableType type) {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    String metadataTableName = metadataTableName(tableName, type);\n+    if (metadataTableName.split(\"\\\\.\").length > 3) {", "originalCommit": "9ab39c789cf014a8b27749e8736e914c4d882b15", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nsimilarity index 87%\nrename from spark/src/main/java/org/apache/iceberg/actions/BaseAction.java\nrename to spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 78f626a3d..49625e2f6 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -123,10 +125,11 @@ abstract class BaseAction<R> implements Action<R> {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n-  protected Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n-    String metadataTableName = metadataTableName(tableName, type);\n-    if (metadataTableName.split(\"\\\\.\").length > 3) {\n-      // This table has a three or more components, use the V2Catalog read path\n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    String metadataTableName = metadataTableName(tableName, tableLocation, type);\n+    if (!metadataTableName.contains(\"/\") && metadataTableName.split(\"\\\\.\").length > 3) {\n+      // This table has a three or more components and has no slashes, use the V2Catalog read path\n       return spark.table(metadataTableName);\n     } else {\n       // This table is file based or has no catalog (db.tableName), read via non-catalog path\n"}}, {"oid": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "url": "https://github.com/apache/iceberg/commit/2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "message": "Fix Actions to be Compatible with V2 Catalogs\n\nPreviously the Action code would use the table name when attempting to look\nup MetadataTable names. This is an issue in Spark3 with V2 catalogs because\nwe strip off the catalog information if the catalog is named \"hadoop\" or \"hive\"\nand include it in all other cases. When we include the catalog name this breaks\nthe lookup code which uses the DataSourceRegistry pathway and not the V2 Table\npathway. To fix this we introduce a method for using the V2 pathway when a catalog\nname is present.", "committedDate": "2020-11-04T03:29:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NzgyMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517087823", "bodyText": "I only added this, (and the test inside orphan files) because we'll have additional coverage for the use cases once the SQL api is introduced and vectorizing the entire suite of actions was getting a bit onerous.", "author": "RussellSpitzer", "createdAt": "2020-11-04T04:12:54Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -19,5 +19,46 @@\n \n package org.apache.iceberg.actions;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n+  @Test", "originalCommit": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java b/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\nindex 622f28823..d25a4c0a9 100644\n--- a/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\n+++ b/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\n\n@@ -60,5 +60,54 @@ public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkCatalogHadoopTable() throws TableAlreadyExistsException, NoSuchTableException, IOException {\n+    spark.conf().set(\"spark.sql.catalog.hadoop\", \"org.apache.iceberg.spark.SparkCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.hadoop.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.hadoop.warehouse\", tableLocation);\n+    SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"hadoop\");\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO hadoop.default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSuchTableException, IOException {\n+    spark.conf().set(\"spark.sql.catalog.hive\", \"org.apache.iceberg.spark.SparkCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.hive.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.hive.warehouse\", tableLocation);\n+    SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"hive\");\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO hive.default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n \n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4ODEzMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r517088133", "bodyText": "I didn't want to make these static, but because two of the actions no longer extend BaseSparkActions they cannot access these methods anymore. I think in the future we should try our best to keep from having Actions implementing  \"BaseAction\" if we can help it so that the implementations of frame work specific code can use their shared methods.", "author": "RussellSpitzer", "createdAt": "2020-11-04T04:14:22Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +125,33 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,", "originalCommit": "2d7acebf5da8c73d72caedebf6dfb94d36cb4534", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 49625e2f6..04cfcb988 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -129,10 +129,10 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                   MetadataTableType type) {\n     String metadataTableName = metadataTableName(tableName, tableLocation, type);\n     if (!metadataTableName.contains(\"/\") && metadataTableName.split(\"\\\\.\").length > 3) {\n-      // This table has a three or more components and has no slashes, use the V2Catalog read path\n+      // This table has a three or more components and is not a file (catalog.db.table.meta), use the V2 read path\n       return spark.table(metadataTableName);\n     } else {\n-      // This table is file based or has no catalog (db.tableName), read via non-catalog path\n+      // This table is file based or has no catalog (db.tableName.meta) or (/some/path#meta), read via non-catalog path\n       return spark.read().format(\"iceberg\").load(metadataTableName);\n     }\n   }\n"}}, {"oid": "7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "url": "https://github.com/apache/iceberg/commit/7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "message": "Fix Actions to be Compatible with V2 Catalogs\n\nPreviously the Action code would use the table name when attempting to look\nup MetadataTable names. This is an issue in Spark3 with V2 catalogs because\nwe strip off the catalog information if the catalog is named \"hadoop\" or \"hive\"\nand include it in all other cases. When we include the catalog name this breaks\nthe lookup code which uses the DataSourceRegistry pathway and not the V2 Table\npathway. To fix this we introduce a method for using the V2 pathway when a catalog\nname is present.", "committedDate": "2020-11-04T04:50:58Z", "type": "commit"}, {"oid": "7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "url": "https://github.com/apache/iceberg/commit/7a788d9c002bf56d32e91e8a2ba6fad713eba45b", "message": "Fix Actions to be Compatible with V2 Catalogs\n\nPreviously the Action code would use the table name when attempting to look\nup MetadataTable names. This is an issue in Spark3 with V2 catalogs because\nwe strip off the catalog information if the catalog is named \"hadoop\" or \"hive\"\nand include it in all other cases. When we include the catalog name this breaks\nthe lookup code which uses the DataSourceRegistry pathway and not the V2 Table\npathway. To fix this we introduce a method for using the V2 pathway when a catalog\nname is present.", "committedDate": "2020-11-04T04:50:58Z", "type": "forcePushed"}, {"oid": "7594cc7d9bde91ccb2795b300960a927c648f83f", "url": "https://github.com/apache/iceberg/commit/7594cc7d9bde91ccb2795b300960a927c648f83f", "message": "Fix Hadoop/Hive Catalog Naming Issue\n\nPreviously there was a conflict between manually non-Spark-catalogs\nand Spark Catalogs in the metadata table name resoultion code. Because\nof this ambiguity we will attempt to load all tables as if their catalog\nis a named SparkCatalog, if this fails we will fall back to the old\npath of removing the \"hadoop\" or \"hive\" catalog names and resolving.", "committedDate": "2020-11-17T22:03:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NTQ4OA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525595488", "bodyText": "nit: It may not necessarily be a metadata location. It can be a valid Hadoop table location too.", "author": "aokolnychyi", "createdAt": "2020-11-17T23:33:13Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 4851c71a0..98e8d5a57 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -130,24 +134,27 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                   MetadataTableType type) {\n     DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n-      // Metadata location passed, load without a catalog\n+      // Hadoop Table or Metadata location passed, load without a catalog\n       return noCatalogReader.load(tableName + \"#\" + type);\n-    } else {\n-      // Try catalog based name based resolution\n-      try {\n-        return spark.table(tableName + \".\" + type);\n-      } catch (Exception e) {\n-        // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-        if (tableName.startsWith(\"hadoop.\")) {\n-          // Try loading by location as Hadoop table without Catalog\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        } else if (tableName.startsWith(\"hive\")) {\n-          // Try loading by name as a Hive table without Catalog\n-          return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-        } else {\n-          throw new IllegalArgumentException(String.format(\n-              \"Cannot find the metadata table for %s of type %s\", tableName, type));\n-        }\n+    }\n+    // Try catalog based name based resolution\n+    try {\n+      return spark.table(tableName + \".\" + type);\n+    } catch (Exception e) {\n+      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n+        // Rethrow unexpected exceptions\n+        throw e;\n+      }\n+      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n+      if (tableName.startsWith(\"hadoop.\")) {\n+        // Try loading by location as Hadoop table without Catalog\n+        return noCatalogReader.load(tableLocation + \"#\" + type);\n+      } else if (tableName.startsWith(\"hive\")) {\n+        // Try loading by name as a Hive table without Catalog\n+        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n+      } else {\n+        throw new IllegalArgumentException(String.format(\n+            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n       }\n     }\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NjU2OQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525596569", "bodyText": "nit: do we think this else helps clarity? We have a return statement anyway. Without this else, we could reduce the indentation of the block below.", "author": "aokolnychyi", "createdAt": "2020-11-17T23:36:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog\n+      return noCatalogReader.load(tableName + \"#\" + type);\n+    } else {", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYwMzc0Ng==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525603746", "bodyText": "It helps for my brain,  but I can drop the if else if you like. I tend to prefer nesting over implied control flow breaks but I think I'm not in the majority here. I'll change it since its what we did on the other pr as well", "author": "RussellSpitzer", "createdAt": "2020-11-17T23:56:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NjU2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 4851c71a0..98e8d5a57 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -130,24 +134,27 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                   MetadataTableType type) {\n     DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n-      // Metadata location passed, load without a catalog\n+      // Hadoop Table or Metadata location passed, load without a catalog\n       return noCatalogReader.load(tableName + \"#\" + type);\n-    } else {\n-      // Try catalog based name based resolution\n-      try {\n-        return spark.table(tableName + \".\" + type);\n-      } catch (Exception e) {\n-        // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-        if (tableName.startsWith(\"hadoop.\")) {\n-          // Try loading by location as Hadoop table without Catalog\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        } else if (tableName.startsWith(\"hive\")) {\n-          // Try loading by name as a Hive table without Catalog\n-          return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-        } else {\n-          throw new IllegalArgumentException(String.format(\n-              \"Cannot find the metadata table for %s of type %s\", tableName, type));\n-        }\n+    }\n+    // Try catalog based name based resolution\n+    try {\n+      return spark.table(tableName + \".\" + type);\n+    } catch (Exception e) {\n+      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n+        // Rethrow unexpected exceptions\n+        throw e;\n+      }\n+      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n+      if (tableName.startsWith(\"hadoop.\")) {\n+        // Try loading by location as Hadoop table without Catalog\n+        return noCatalogReader.load(tableLocation + \"#\" + type);\n+      } else if (tableName.startsWith(\"hive\")) {\n+        // Try loading by name as a Hive table without Catalog\n+        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n+      } else {\n+        throw new IllegalArgumentException(String.format(\n+            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n       }\n     }\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5ODgwMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525598803", "bodyText": "If we do a static import for ALL_MANIFESTS, will it fit on one line?", "author": "aokolnychyi", "createdAt": "2020-11-17T23:42:23Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -47,9 +87,9 @@\n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n     JavaSparkContext context = new JavaSparkContext(spark.sparkContext());\n     Broadcast<FileIO> ioBroadcast = context.broadcast(SparkUtil.serializableFileIO(table()));\n-    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n \n-    Dataset<ManifestFileBean> allManifests = spark.read().format(\"iceberg\").load(allManifestsMetadataTable)\n+    Dataset<ManifestFileBean> allManifests = loadMetadataTable(spark, tableName, table().location(),\n+        MetadataTableType.ALL_MANIFESTS)", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5OTA4OQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525599089", "bodyText": "If not, we can make a var String tableLocation = table().location() if we need to reduce the length further.", "author": "aokolnychyi", "createdAt": "2020-11-17T23:43:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5ODgwMw=="}], "type": "inlineReview", "revised_code": {"commit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 4851c71a0..98e8d5a57 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -88,8 +94,7 @@ abstract class BaseSparkAction<R> implements Action<R> {\n     JavaSparkContext context = new JavaSparkContext(spark.sparkContext());\n     Broadcast<FileIO> ioBroadcast = context.broadcast(SparkUtil.serializableFileIO(table()));\n \n-    Dataset<ManifestFileBean> allManifests = loadMetadataTable(spark, tableName, table().location(),\n-        MetadataTableType.ALL_MANIFESTS)\n+    Dataset<ManifestFileBean> allManifests = loadMetadataTable(spark, tableName, table().location(), ALL_MANIFESTS)\n         .selectExpr(\"path\", \"length\", \"partition_spec_id as partitionSpecId\", \"added_snapshot_id as addedSnapshotId\")\n         .dropDuplicates(\"path\")\n         .repartition(spark.sessionState().conf().numShufflePartitions()) // avoid adaptive execution combining tasks\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5OTM5Nw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525599397", "bodyText": "Same here. Any way to fit on one line?", "author": "aokolnychyi", "createdAt": "2020-11-17T23:44:04Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -202,9 +202,8 @@ public RewriteManifestsActionResult execute() {\n         .createDataset(Lists.transform(manifests, ManifestFile::path), Encoders.STRING())\n         .toDF(\"manifest\");\n \n-    String entriesMetadataTable = metadataTableName(MetadataTableType.ENTRIES);\n-    Dataset<Row> manifestEntryDF = spark.read().format(\"iceberg\")\n-        .load(entriesMetadataTable)\n+    Dataset<Row> manifestEntryDF = BaseSparkAction.loadMetadataTable(spark, table.name(), table().location(),", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java\nindex 42e1366cd..32d74d058 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java\n\n@@ -202,8 +205,7 @@ public class RewriteManifestsAction\n         .createDataset(Lists.transform(manifests, ManifestFile::path), Encoders.STRING())\n         .toDF(\"manifest\");\n \n-    Dataset<Row> manifestEntryDF = BaseSparkAction.loadMetadataTable(spark, table.name(), table().location(),\n-        MetadataTableType.ENTRIES)\n+    Dataset<Row> manifestEntryDF = BaseSparkAction.loadMetadataTable(spark, table.name(), table().location(), ENTRIES)\n         .filter(\"status < 2\") // select only live entries\n         .selectExpr(\"input_file_name() as manifest\", \"snapshot_id\", \"sequence_number\", \"data_file\");\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTIxMQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525621211", "bodyText": "Can this be a more specific exception? Seems dangerous to catch 'em all.", "author": "rdblue", "createdAt": "2020-11-18T00:46:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog\n+      return noCatalogReader.load(tableName + \"#\" + type);\n+    } else {\n+      // Try catalog based name based resolution\n+      try {\n+        return spark.table(tableName + \".\" + type);\n+      } catch (Exception e) {", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzNTI1Mg==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525635252", "bodyText": ":pokemon: - Let me check what gets thrown and I\"ll try to narrow it down to reasonable classes of exception", "author": "RussellSpitzer", "createdAt": "2020-11-18T01:28:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjIwOTU5OA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r526209598", "bodyText": "Unfortunately these are unchecked from Scala so we do need to catch all runtime exceptions, but I can do a check after the exception is caught if it is a ParseException (2.4) or AnalysisException(3.0) and rethrow if it isn't.", "author": "RussellSpitzer", "createdAt": "2020-11-18T16:06:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTIxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 4851c71a0..98e8d5a57 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -130,24 +134,27 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                   MetadataTableType type) {\n     DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n-      // Metadata location passed, load without a catalog\n+      // Hadoop Table or Metadata location passed, load without a catalog\n       return noCatalogReader.load(tableName + \"#\" + type);\n-    } else {\n-      // Try catalog based name based resolution\n-      try {\n-        return spark.table(tableName + \".\" + type);\n-      } catch (Exception e) {\n-        // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-        if (tableName.startsWith(\"hadoop.\")) {\n-          // Try loading by location as Hadoop table without Catalog\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        } else if (tableName.startsWith(\"hive\")) {\n-          // Try loading by name as a Hive table without Catalog\n-          return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-        } else {\n-          throw new IllegalArgumentException(String.format(\n-              \"Cannot find the metadata table for %s of type %s\", tableName, type));\n-        }\n+    }\n+    // Try catalog based name based resolution\n+    try {\n+      return spark.table(tableName + \".\" + type);\n+    } catch (Exception e) {\n+      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n+        // Rethrow unexpected exceptions\n+        throw e;\n+      }\n+      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n+      if (tableName.startsWith(\"hadoop.\")) {\n+        // Try loading by location as Hadoop table without Catalog\n+        return noCatalogReader.load(tableLocation + \"#\" + type);\n+      } else if (tableName.startsWith(\"hive\")) {\n+        // Try loading by name as a Hive table without Catalog\n+        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n+      } else {\n+        throw new IllegalArgumentException(String.format(\n+            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n       }\n     }\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkyNg==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525622926", "bodyText": "This logic is just for Spark 2.x right? If so then maybe we can add a comment explaining it and noting when we can remove it.", "author": "rdblue", "createdAt": "2020-11-18T00:51:03Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -86,6 +126,32 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n+                                                  MetadataTableType type) {\n+    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    if (tableName.contains(\"/\")) {\n+      // Metadata location passed, load without a catalog\n+      return noCatalogReader.load(tableName + \"#\" + type);\n+    } else {\n+      // Try catalog based name based resolution\n+      try {\n+        return spark.table(tableName + \".\" + type);\n+      } catch (Exception e) {\n+        // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n+        if (tableName.startsWith(\"hadoop.\")) {", "originalCommit": "7594cc7d9bde91ccb2795b300960a927c648f83f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMjEyMw==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525632123", "bodyText": "Anton I were talking about this, the issue is that it isn't just for SparkV2 as it's quite possible for a user to be using manual catalogs in a Spark3 application (or ported Spark2 application.) If we remove this we break anyone who is accessing tables by this method.\nI think that's actually something that's fine to break eventually but I think it would be pretty big breaking change.\nExample\nActions.forTable(tableCreatedFromHadoopOrHiveCatalog)", "author": "RussellSpitzer", "createdAt": "2020-11-18T01:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzNDIyMQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r525634221", "bodyText": "Sounds reasonable to me.", "author": "rdblue", "createdAt": "2020-11-18T01:25:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkyNg=="}], "type": "inlineReview", "revised_code": {"commit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 4851c71a0..98e8d5a57 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -130,24 +134,27 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                   MetadataTableType type) {\n     DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n-      // Metadata location passed, load without a catalog\n+      // Hadoop Table or Metadata location passed, load without a catalog\n       return noCatalogReader.load(tableName + \"#\" + type);\n-    } else {\n-      // Try catalog based name based resolution\n-      try {\n-        return spark.table(tableName + \".\" + type);\n-      } catch (Exception e) {\n-        // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-        if (tableName.startsWith(\"hadoop.\")) {\n-          // Try loading by location as Hadoop table without Catalog\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        } else if (tableName.startsWith(\"hive\")) {\n-          // Try loading by name as a Hive table without Catalog\n-          return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-        } else {\n-          throw new IllegalArgumentException(String.format(\n-              \"Cannot find the metadata table for %s of type %s\", tableName, type));\n-        }\n+    }\n+    // Try catalog based name based resolution\n+    try {\n+      return spark.table(tableName + \".\" + type);\n+    } catch (Exception e) {\n+      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n+        // Rethrow unexpected exceptions\n+        throw e;\n+      }\n+      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n+      if (tableName.startsWith(\"hadoop.\")) {\n+        // Try loading by location as Hadoop table without Catalog\n+        return noCatalogReader.load(tableLocation + \"#\" + type);\n+      } else if (tableName.startsWith(\"hive\")) {\n+        // Try loading by name as a Hive table without Catalog\n+        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n+      } else {\n+        throw new IllegalArgumentException(String.format(\n+            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n       }\n     }\n   }\n"}}, {"oid": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "url": "https://github.com/apache/iceberg/commit/1e989b78a3f284415de9cacf8b95bb7e8da248e7", "message": "Minor changes based on reviewer comments", "committedDate": "2020-11-18T16:41:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NDA2MA==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r526294060", "bodyText": "I think it should be fine to add it to our exceptions in checkstyle.xml. I thought it was already there.", "author": "aokolnychyi", "createdAt": "2020-11-18T17:42:08Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -24,21 +24,67 @@\n import org.apache.iceberg.BaseTable;\n import org.apache.iceberg.ManifestFiles;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.StaticTableOperations;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.io.ClosingIterator;\n import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkUtil;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.FlatMapFunction;\n import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.DataFrameReader;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+\n+// CHECKSTYLE:OFF\n+import static org.apache.iceberg.MetadataTableType.ALL_MANIFESTS;", "originalCommit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff4f8fb8e8d3cbfd7406e650f1b0a1ea4253e269", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 98e8d5a57..367fa2870 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -44,9 +44,7 @@ import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n \n-// CHECKSTYLE:OFF\n import static org.apache.iceberg.MetadataTableType.ALL_MANIFESTS;\n-// CHECKSTYLE:ON\n \n abstract class BaseSparkAction<R> implements Action<R> {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NDc0NQ==", "url": "https://github.com/apache/iceberg/pull/1715#discussion_r526294745", "bodyText": "same here", "author": "aokolnychyi", "createdAt": "2020-11-18T17:43:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -67,6 +66,10 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+// CHECKSTYLE:OFF\n+import static org.apache.iceberg.MetadataTableType.ENTRIES;", "originalCommit": "1e989b78a3f284415de9cacf8b95bb7e8da248e7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff4f8fb8e8d3cbfd7406e650f1b0a1ea4253e269", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java\nindex 32d74d058..4402d2e07 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java\n\n@@ -66,9 +66,7 @@ import org.apache.spark.sql.types.StructType;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-// CHECKSTYLE:OFF\n import static org.apache.iceberg.MetadataTableType.ENTRIES;\n-// CHECKSTYLE:ON\n \n /**\n  * An action that rewrites manifests in a distributed manner and co-locates metadata for partitions.\n"}}, {"oid": "ff4f8fb8e8d3cbfd7406e650f1b0a1ea4253e269", "url": "https://github.com/apache/iceberg/commit/ff4f8fb8e8d3cbfd7406e650f1b0a1ea4253e269", "message": "Sort StaticImport Exclusions and Add MetadataTableType.*", "committedDate": "2020-11-18T18:05:59Z", "type": "commit"}]}