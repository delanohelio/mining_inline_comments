{"pr_number": 1862, "pr_title": "Spark: Implement copy-on-write DELETE", "pr_createdAt": "2020-12-02T16:40:49Z", "pr_url": "https://github.com/apache/iceberg/pull/1862", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNTgxNw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534315817", "bodyText": "Well, I wanted it to be write.delete.isolation-level but we have no way to find out which operation is being performed in the merge builder.", "author": "aokolnychyi", "createdAt": "2020-12-02T16:41:47Z", "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -140,4 +140,10 @@ private TableProperties() {\n \n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n+\n+  public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";", "originalCommit": "225167896ff7c3b7c4c59ff7998306673dbf9904", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQyMTMyMA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535421320", "bodyText": "It might be better to use write.row-level.isolation-level so it matches the mode property. This doesn't apply to INSERT OVERWRITE so it would be nice to be clear about it.\nNot sure if we can make that property name better. Seems odd to have row-level and isolation-level next to one another since \"row-level\" is specifying the granularity of the changes and \"isolation-level\" is actually choosing an option.", "author": "rdblue", "createdAt": "2020-12-03T17:07:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNTgxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMjk4Nw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536132987", "bodyText": "Fixed.", "author": "aokolnychyi", "createdAt": "2020-12-04T14:21:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNTgxNw=="}], "type": "inlineReview", "revised_code": {"commit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/TableProperties.java b/core/src/main/java/org/apache/iceberg/TableProperties.java\nindex f5703b024..5a5e08251 100644\n--- a/core/src/main/java/org/apache/iceberg/TableProperties.java\n+++ b/core/src/main/java/org/apache/iceberg/TableProperties.java\n\n@@ -144,6 +144,6 @@ public class TableProperties {\n   public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";\n   public static final String WRITE_ISOLATION_LEVEL_DEFAULT = \"serializable\";\n \n-  public static final String WRITE_ROW_LEVEL_MODE = \"write.delete.mode\";\n+  public static final String WRITE_ROW_LEVEL_MODE = \"write.row-level.mode\";\n   public static final String WRITE_ROW_LEVEL_MODE_DEFAULT = \"copy-on-write\";\n }\n"}}, {"oid": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "url": "https://github.com/apache/iceberg/commit/8ecea59d1035bcc73931cb923314bf7db4392b9c", "message": "Spark: Implement copy-on-write DELETE", "committedDate": "2020-12-02T16:42:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxNjYwNA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534316604", "bodyText": "Same here. I wanted it to be write.delete.mode.", "author": "aokolnychyi", "createdAt": "2020-12-02T16:42:52Z", "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -140,4 +140,10 @@ private TableProperties() {\n \n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n+\n+  public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";\n+  public static final String WRITE_ISOLATION_LEVEL_DEFAULT = \"serializable\";\n+\n+  public static final String WRITE_ROW_LEVEL_MODE = \"write.row-level.mode\";", "originalCommit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9731a2fe7fde5ccccf1c1f18b8b9018763060e50", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/TableProperties.java b/core/src/main/java/org/apache/iceberg/TableProperties.java\nindex 5a5e08251..fad990372 100644\n--- a/core/src/main/java/org/apache/iceberg/TableProperties.java\n+++ b/core/src/main/java/org/apache/iceberg/TableProperties.java\n\n@@ -141,6 +141,12 @@ public class TableProperties {\n   public static final String GC_ENABLED = \"gc.enabled\";\n   public static final boolean GC_ENABLED_DEFAULT = true;\n \n+  public static final String MAX_SNAPSHOT_AGE_MS = \"history.expire.max-snapshot-age-ms\";\n+  public static final long MAX_SNAPSHOT_AGE_MS_DEFAULT = 5 * 24 * 60 * 60 * 1000; // 5 days\n+\n+  public static final String MIN_SNAPSHOTS_TO_KEEP = \"history.expire.min-snapshots-to-keep\";\n+  public static final int MIN_SNAPSHOTS_TO_KEEP_DEFAULT = 1;\n+\n   public static final String WRITE_ISOLATION_LEVEL = \"write.isolation-level\";\n   public static final String WRITE_ISOLATION_LEVEL_DEFAULT = \"serializable\";\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTk2NQ==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534485965", "bodyText": "Does this need to be in api? I think it is only used to parse a string and hold the result. For now, I would move it to core to avoid exposing it too widely.\nFYI @jacques-n: isolation level for a single table.", "author": "rdblue", "createdAt": "2020-12-02T21:15:21Z", "path": "api/src/main/java/org/apache/iceberg/IsolationLevel.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+/**\n+ * An isolation level in a table.\n+ * <p>\n+ * Two isolation levels are supported: serializable and snapshot isolation. Both of them provide\n+ * a read consistent view of the table to all operations and allow readers to see only already\n+ * committed data. While serializable is the strongest isolation level in databases,\n+ * snapshot isolation is beneficial for environments with many concurrent writers.\n+ * <p>\n+ * The serializable isolation level guarantees that an ongoing UPDATE/DELETE/MERGE operation\n+ * fails if a concurrent transaction commits a new file that might contain rows matching\n+ * the condition used in UPDATE/DELETE/MERGE. For example, if there is an ongoing update\n+ * on a subset of rows and a concurrent transaction adds a new file with records\n+ * that potentially match the update condition, the update operation must fail under\n+ * the serializable isolation but can still commit under the snapshot isolation.\n+ */\n+public enum IsolationLevel {", "originalCommit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MzExNg==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535043116", "bodyText": "Sound good to me.", "author": "aokolnychyi", "createdAt": "2020-12-03T10:05:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzE3Nw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133177", "bodyText": "Moved.", "author": "aokolnychyi", "createdAt": "2020-12-04T14:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTk2NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534633796", "bodyText": "Shouldn't this return writeInfo.options()?", "author": "rdblue", "createdAt": "2020-12-03T03:07:26Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.iceberg.IsolationLevel;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder;\n+import org.apache.spark.sql.connector.read.Scan;\n+import org.apache.spark.sql.connector.read.ScanBuilder;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+class SparkMergeBuilder implements MergeBuilder {\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final LogicalWriteInfo writeInfo;\n+  private final IsolationLevel isolationLevel;\n+\n+  // lazy vars\n+  private ScanBuilder lazyScanBuilder;\n+  private Scan configuredScan;\n+  private WriteBuilder lazyWriteBuilder;\n+\n+  SparkMergeBuilder(SparkSession spark, Table table, LogicalWriteInfo writeInfo) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.writeInfo = writeInfo;\n+\n+    String isolationLevelAsString = table.properties().getOrDefault(\n+        TableProperties.WRITE_ISOLATION_LEVEL,\n+        TableProperties.WRITE_ISOLATION_LEVEL_DEFAULT\n+    ).toUpperCase(Locale.ROOT);\n+    this.isolationLevel = IsolationLevel.valueOf(isolationLevelAsString);\n+  }\n+\n+  @Override\n+  public ScanBuilder asScanBuilder() {\n+    return scanBuilder();\n+  }\n+\n+  private ScanBuilder scanBuilder() {\n+    if (lazyScanBuilder == null) {\n+      SparkScanBuilder scanBuilder = new SparkScanBuilder(spark, table, scanOptions()) {\n+        public Scan build() {\n+          Scan scan = super.buildMergeScan();\n+          SparkMergeBuilder.this.configuredScan = scan;\n+          return scan;\n+        }\n+      };\n+      // ignore residuals to ensure we read full files\n+      lazyScanBuilder = scanBuilder.ignoreResiduals();\n+    }\n+\n+    return lazyScanBuilder;\n+  }\n+\n+  private CaseInsensitiveStringMap scanOptions() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return CaseInsensitiveStringMap.empty();", "originalCommit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0NDA3Mw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535044073", "bodyText": "It should, plus we need to validate no snapshot-id is set in options. Good catch.", "author": "aokolnychyi", "createdAt": "2020-12-03T10:06:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU0MDg3MQ==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535540871", "bodyText": "Now that I'm thinking about this more, why choose a snapshot here and pass it to the scan? The scan could determine the snapshot itself without modifying these options. The writer gets the snapshot ID from the scan anyway, so it doesn't matter whether it is determined here or in the scan.", "author": "rdblue", "createdAt": "2020-12-03T19:53:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzI4MA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133280", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-04T14:21:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzMzc5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "96f9001a60091e2c8601d3d17ad81469fa01f138", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java\nindex 8692d5653..11ad2e967 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java\n\n@@ -19,21 +19,20 @@\n \n package org.apache.iceberg.spark.source;\n \n-import java.util.HashMap;\n import java.util.Locale;\n import java.util.Map;\n import org.apache.arrow.util.Preconditions;\n import org.apache.iceberg.IsolationLevel;\n-import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.TableProperties;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.connector.iceberg.write.MergeBuilder;\n import org.apache.spark.sql.connector.read.Scan;\n import org.apache.spark.sql.connector.read.ScanBuilder;\n import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n import org.apache.spark.sql.connector.write.WriteBuilder;\n-import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL;\n+import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL_DEFAULT;\n \n class SparkMergeBuilder implements MergeBuilder {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNDk3NQ==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534634975", "bodyText": "Are v2 references committed in Spark?", "author": "rdblue", "createdAt": "2020-12-03T03:11:05Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "originalCommit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0NjYyMw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535046623", "bodyText": "Yeah, they are already supported.", "author": "aokolnychyi", "createdAt": "2020-12-03T10:08:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNDk3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "96f9001a60091e2c8601d3d17ad81469fa01f138", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\nindex 704a38cd7..b9aeba70f 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\n\n@@ -168,10 +168,19 @@ public class SparkTable implements org.apache.spark.sql.connector.catalog.Table,\n   }\n \n   @Override\n-  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n-    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n-    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n-    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  public MergeBuilder newMergeBuilder(String operation, LogicalWriteInfo info) {\n+    String mode = getRowLevelOperationMode(operation);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported mode for %s: %s\", operation, mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, operation, info);\n+  }\n+\n+  private String getRowLevelOperationMode(String operation) {\n+    Map<String, String> props = icebergTable.properties();\n+    if (operation.equalsIgnoreCase(\"delete\")) {\n+      return props.getOrDefault(DELETE_MODE, DELETE_MODE_DEFAULT);\n+    } else {\n+      throw new IllegalArgumentException(\"Unsupported operation: \" + operation);\n+    }\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNjMzNQ==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r534636335", "bodyText": "We may want to add another TODO to detect more cases that don't require rewrites.", "author": "rdblue", "createdAt": "2020-12-03T03:15:16Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n+    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  }\n+\n+  @Override\n+  public boolean canDeleteWhere(Filter[] filters) {\n+    if (table().specs().size() > 1) {\n+      // cannot guarantee a metadata delete will be successful if we have multiple specs\n+      return false;\n+    }\n+\n+    Set<Integer> identitySourceIds = table().spec().identitySourceIds();\n+    Schema schema = table().schema();\n+\n+    for (Filter filter : filters) {\n+      // return false if the filter requires rewrite or if we cannot translate the filter\n+      if (requiresRewrite(filter, schema, identitySourceIds) || SparkFilters.convert(filter) == null) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private boolean requiresRewrite(Filter filter, Schema schema, Set<Integer> identitySourceIds) {\n+    // TODO: handle dots correctly via v2references", "originalCommit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0NTA1Nw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535045057", "bodyText": "Will do.", "author": "aokolnychyi", "createdAt": "2020-12-03T10:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNjMzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzU4Mg==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133582", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-04T14:22:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYzNjMzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "96f9001a60091e2c8601d3d17ad81469fa01f138", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\nindex 704a38cd7..b9aeba70f 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\n\n@@ -168,10 +168,19 @@ public class SparkTable implements org.apache.spark.sql.connector.catalog.Table,\n   }\n \n   @Override\n-  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n-    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n-    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n-    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  public MergeBuilder newMergeBuilder(String operation, LogicalWriteInfo info) {\n+    String mode = getRowLevelOperationMode(operation);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported mode for %s: %s\", operation, mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, operation, info);\n+  }\n+\n+  private String getRowLevelOperationMode(String operation) {\n+    Map<String, String> props = icebergTable.properties();\n+    if (operation.equalsIgnoreCase(\"delete\")) {\n+      return props.getOrDefault(DELETE_MODE, DELETE_MODE_DEFAULT);\n+    } else {\n+      throw new IllegalArgumentException(\"Unsupported operation: \" + operation);\n+    }\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535041450", "bodyText": "We could set operation inside info.options() in RewriteDelete and pass it here so that we can support table properties like write.delete.mode instead of write.row-level.mode. Thoughts, @rdblue?", "author": "aokolnychyi", "createdAt": "2020-12-03T10:04:00Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java", "diffHunk": "@@ -160,6 +167,43 @@ public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n     return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n   }\n \n+  @Override\n+  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {", "originalCommit": "8ecea59d1035bcc73931cb923314bf7db4392b9c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ0NTQ3MA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535445470", "bodyText": "I'm not a fan of passing things through Spark's options. If we have the information in the writer, can we just call a setter to set it on the reader?", "author": "rdblue", "createdAt": "2020-12-03T17:41:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTA0NA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535479044", "bodyText": "We have this info only in RewriteDelete rule that calls newMergeBuilder. After that, we have no way to know whether it is a delete or merge.", "author": "aokolnychyi", "createdAt": "2020-12-03T18:26:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTgzMw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535479833", "bodyText": "So if we pass something that tells us what logical operation it is, we can have write.delete.isolation-level.", "author": "aokolnychyi", "createdAt": "2020-12-03T18:27:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ4MDUwMw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535480503", "bodyText": "Unless we want to have newMergeBuilder, newDeleteBuilder, etc.", "author": "aokolnychyi", "createdAt": "2020-12-03T18:29:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUzOTQwNA==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535539404", "bodyText": "I like that this is used for both MERGE and DELETE without needing to customize, so I'm reluctant to do too much here. It is close, but I think I would opt to have a single config property.\nOperation is something that we could pass through LogicalWriteInfo in the future for better logging and purposes like this one. For now, we could add the operation as a String passed to newMergeBuilder, but there is no guarantee that we would add this to LogicalWriteInfo later. I think that means that we should go for a single property and we can customize later if we do get the operation when the builder is created.", "author": "rdblue", "createdAt": "2020-12-03T19:51:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU1MzI4Nw==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r535553287", "bodyText": "After discussing this directly, I think the right way to go is to add the operation name. That fixes the isolation level and mode property problem. And it is reasonable to add the operation to LogicalWriteInfo.", "author": "rdblue", "createdAt": "2020-12-03T20:11:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzg2MQ==", "url": "https://github.com/apache/iceberg/pull/1862#discussion_r536133861", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-04T14:22:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA0MTQ1MA=="}], "type": "inlineReview", "revised_code": {"commit": "96f9001a60091e2c8601d3d17ad81469fa01f138", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\nindex 704a38cd7..b9aeba70f 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/source/SparkTable.java\n\n@@ -168,10 +168,19 @@ public class SparkTable implements org.apache.spark.sql.connector.catalog.Table,\n   }\n \n   @Override\n-  public MergeBuilder newMergeBuilder(LogicalWriteInfo info) {\n-    String mode = icebergTable.properties().getOrDefault(WRITE_ROW_LEVEL_MODE, WRITE_ROW_LEVEL_MODE_DEFAULT);\n-    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported row operations mode: %s\", mode);\n-    return new SparkMergeBuilder(sparkSession(), icebergTable, info);\n+  public MergeBuilder newMergeBuilder(String operation, LogicalWriteInfo info) {\n+    String mode = getRowLevelOperationMode(operation);\n+    ValidationException.check(mode.equals(\"copy-on-write\"), \"Unsupported mode for %s: %s\", operation, mode);\n+    return new SparkMergeBuilder(sparkSession(), icebergTable, operation, info);\n+  }\n+\n+  private String getRowLevelOperationMode(String operation) {\n+    Map<String, String> props = icebergTable.properties();\n+    if (operation.equalsIgnoreCase(\"delete\")) {\n+      return props.getOrDefault(DELETE_MODE, DELETE_MODE_DEFAULT);\n+    } else {\n+      throw new IllegalArgumentException(\"Unsupported operation: \" + operation);\n+    }\n   }\n \n   @Override\n"}}, {"oid": "9731a2fe7fde5ccccf1c1f18b8b9018763060e50", "url": "https://github.com/apache/iceberg/commit/9731a2fe7fde5ccccf1c1f18b8b9018763060e50", "message": "Spark: Implement copy-on-write DELETE", "committedDate": "2020-12-04T14:19:09Z", "type": "commit"}, {"oid": "96f9001a60091e2c8601d3d17ad81469fa01f138", "url": "https://github.com/apache/iceberg/commit/96f9001a60091e2c8601d3d17ad81469fa01f138", "message": "Review comments and more tests", "committedDate": "2020-12-04T14:19:49Z", "type": "commit"}, {"oid": "96f9001a60091e2c8601d3d17ad81469fa01f138", "url": "https://github.com/apache/iceberg/commit/96f9001a60091e2c8601d3d17ad81469fa01f138", "message": "Review comments and more tests", "committedDate": "2020-12-04T14:19:49Z", "type": "forcePushed"}, {"oid": "912a761447e4c4d139e922ce679a4836dbc1b57d", "url": "https://github.com/apache/iceberg/commit/912a761447e4c4d139e922ce679a4836dbc1b57d", "message": "Refine test", "committedDate": "2020-12-04T16:11:43Z", "type": "commit"}, {"oid": "2baa4af4e9ab1ff2fbcdaf5a7996989795e7f624", "url": "https://github.com/apache/iceberg/commit/2baa4af4e9ab1ff2fbcdaf5a7996989795e7f624", "message": "Fix style", "committedDate": "2020-12-04T16:13:36Z", "type": "commit"}, {"oid": "bd9a9b33d721c6400f418d74e1d78796f6b4c7f9", "url": "https://github.com/apache/iceberg/commit/bd9a9b33d721c6400f418d74e1d78796f6b4c7f9", "message": "Ignore concurrency tests for Hadoop", "committedDate": "2020-12-04T18:01:08Z", "type": "commit"}]}