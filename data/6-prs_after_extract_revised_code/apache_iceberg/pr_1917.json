{"pr_number": 1917, "pr_title": "Hive: Convert the CREATE TABLE ... PARTITIONED BY to Iceberg identity partitions", "pr_createdAt": "2020-12-11T14:23:03Z", "pr_url": "https://github.com/apache/iceberg/pull/1917", "timeline": [{"oid": "f9ccf40c143745250ab6846e4c46f1e4b4c58ac6", "url": "https://github.com/apache/iceberg/commit/f9ccf40c143745250ab6846e4c46f1e4b4c58ac6", "message": "Hive: Convert the CREATE TABLE ... PARTITIONED BY to Iceberg identity partitions", "committedDate": "2020-12-11T14:13:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU2Mjk1MQ==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541562951", "bodyText": "nit: can use !hmsTable.getPartitionKeys().isEmpty()", "author": "marton-bod", "createdAt": "2020-12-12T12:01:10Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && hmsTable.getPartitionKeys().size() != 0) {", "originalCommit": "f9ccf40c143745250ab6846e4c46f1e4b4c58ac6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU3MTQwNQ==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541571405", "bodyText": "Done", "author": "pvary", "createdAt": "2020-12-12T12:57:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU2Mjk1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "04073e2ca7626be583375c66c38be12b29e9a1cd", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java b/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\nindex 9b20113c0..c0e6a2438 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\n\n@@ -205,7 +205,7 @@ public class HiveIcebergMetaHook implements HiveMetaHook {\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      if (hmsTable.isSetPartitionKeys() && hmsTable.getPartitionKeys().size() != 0) {\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {\n         // Add partitioning columns to the original column list before creating the Iceberg Schema\n         List<FieldSchema> cols = new ArrayList<>(hmsTable.getSd().getCols());\n         cols.addAll(hmsTable.getPartitionKeys());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU2OTU0MA==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541569540", "bodyText": "Can we make runPartitionedWriteTest generic enough to reuse it here too? Would be great to have an execution test for multi-level partition as well.", "author": "marton-bod", "createdAt": "2020-12-12T12:45:12Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java", "diffHunk": "@@ -666,45 +666,52 @@ public void testCreateTableWithColumnSpecification() throws IOException {\n   }\n \n   @Test\n-  public void testCreateTableWithColumnSpecificationPartitioned() {\n+  public void testCreateTableWithColumnSpecificationPartitioned() throws IOException {\n+    TableIdentifier identifier = TableIdentifier.of(\"default\", \"customers\");\n+    runPartitionedWriteTest(identifier, createSQLForPartitionedTableBySchema(testTables, identifier));\n+  }\n+\n+  @Test\n+  public void testCreatePartitionedTableByProperty() throws IOException {\n+    TableIdentifier identifier = TableIdentifier.of(\"default\", \"customers\");\n+    runPartitionedWriteTest(identifier, createSQLForPartitionedTableByProperty(testTables, identifier));\n+  }\n+\n+  @Test\n+  public void testCreatePartitionedTableWithPropertiesAndWithColumnSpecification() {\n+    PartitionSpec spec = PartitionSpec.builderFor(CUSTOMER_SCHEMA).identity(\"last_name\").build();\n+\n     AssertHelpers.assertThrows(\"should throw exception\", IllegalArgumentException.class,\n-        \"currently not supported\", () -> {\n+        \"Provide only one of the following\", () -> {\n           shell.executeStatement(\"CREATE EXTERNAL TABLE customers (customer_id BIGINT) \" +\n               \"PARTITIONED BY (first_name STRING) \" +\n               \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n-              testTables.locationForCreateTableSQL(TableIdentifier.of(\"default\", \"customers\")));\n+              testTables.locationForCreateTableSQL(TableIdentifier.of(\"default\", \"customers\")) +\n+              \" TBLPROPERTIES ('\" + InputFormatConfig.PARTITION_SPEC + \"'='\" +\n+              PartitionSpecParser.toJson(spec) + \"')\");\n         }\n     );\n   }\n \n   @Test\n-  public void testCreatePartitionedTable() throws IOException {\n+  public void testCreateTableWithColumnSpecificationMultilevelPartitioned() {", "originalCommit": "f9ccf40c143745250ab6846e4c46f1e4b4c58ac6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTc3NjM2MQ==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541776361", "bodyText": "done", "author": "pvary", "createdAt": "2020-12-12T20:38:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU2OTU0MA=="}], "type": "inlineReview", "revised_code": {"commit": "b362083163f4052e8aa124d5b3d5c3ed2577d785", "chunk": "diff --git a/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java\nindex 6c355773e..0a75cd6c9 100644\n--- a/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java\n+++ b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java\n\n@@ -645,36 +647,44 @@ public class TestHiveIcebergStorageHandler {\n   @Test\n   public void testCreateTableWithColumnSpecification() throws IOException {\n     TableIdentifier identifier = TableIdentifier.of(\"default\", \"customers\");\n-\n-    shell.executeStatement(\"CREATE EXTERNAL TABLE customers (customer_id BIGINT, first_name STRING, last_name STRING)\" +\n+    Map<StructLike, List<Record>> data = new HashMap<>(1);\n+    data.put(null, CUSTOMER_RECORDS);\n+    String createSql = \"CREATE EXTERNAL TABLE \" + identifier +\n+        \" (customer_id BIGINT, first_name STRING, last_name STRING)\" +\n         \" STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n-        testTables.locationForCreateTableSQL(identifier));\n-\n-    // Check the Iceberg table data\n-    org.apache.iceberg.Table icebergTable = testTables.loadTable(identifier);\n-    Assert.assertEquals(CUSTOMER_SCHEMA.asStruct(), icebergTable.schema().asStruct());\n-    Assert.assertEquals(SPEC, icebergTable.spec());\n-\n-    testTables.appendIcebergTable(shell.getHiveConf(), icebergTable, fileFormat, null, CUSTOMER_RECORDS);\n-\n-    List<Object[]> descRows = shell.executeStatement(\"SELECT * FROM default.customers ORDER BY customer_id DESC\");\n-\n-    Assert.assertEquals(3, descRows.size());\n-    Assert.assertArrayEquals(new Object[] {2L, \"Trudy\", \"Pink\"}, descRows.get(0));\n-    Assert.assertArrayEquals(new Object[] {1L, \"Bob\", \"Green\"}, descRows.get(1));\n-    Assert.assertArrayEquals(new Object[] {0L, \"Alice\", \"Brown\"}, descRows.get(2));\n+        testTables.locationForCreateTableSQL(identifier);\n+    runCreateAndReadTest(identifier, createSql, CUSTOMER_SCHEMA, PartitionSpec.unpartitioned(), data);\n   }\n \n   @Test\n   public void testCreateTableWithColumnSpecificationPartitioned() throws IOException {\n     TableIdentifier identifier = TableIdentifier.of(\"default\", \"customers\");\n-    runPartitionedWriteTest(identifier, createSQLForPartitionedTableBySchema(testTables, identifier));\n+    PartitionSpec spec = PartitionSpec.builderFor(CUSTOMER_SCHEMA).identity(\"last_name\").build();\n+    Map<StructLike, List<Record>> data = ImmutableMap.of(\n+        Row.of(\"Brown\"), Collections.singletonList(CUSTOMER_RECORDS.get(0)),\n+        Row.of(\"Green\"), Collections.singletonList(CUSTOMER_RECORDS.get(1)),\n+        Row.of(\"Pink\"), Collections.singletonList(CUSTOMER_RECORDS.get(2)));\n+    String createSql = \"CREATE EXTERNAL TABLE \" + identifier +\n+        \" (customer_id BIGINT, first_name STRING) PARTITIONED BY (last_name STRING) \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        testTables.locationForCreateTableSQL(identifier);\n+    runCreateAndReadTest(identifier, createSql, CUSTOMER_SCHEMA, spec, data);\n   }\n \n   @Test\n   public void testCreatePartitionedTableByProperty() throws IOException {\n     TableIdentifier identifier = TableIdentifier.of(\"default\", \"customers\");\n-    runPartitionedWriteTest(identifier, createSQLForPartitionedTableByProperty(testTables, identifier));\n+    PartitionSpec spec = PartitionSpec.builderFor(CUSTOMER_SCHEMA).identity(\"last_name\").build();\n+    Map<StructLike, List<Record>> data = ImmutableMap.of(\n+        Row.of(\"Brown\"), Collections.singletonList(CUSTOMER_RECORDS.get(0)),\n+        Row.of(\"Green\"), Collections.singletonList(CUSTOMER_RECORDS.get(1)),\n+        Row.of(\"Pink\"), Collections.singletonList(CUSTOMER_RECORDS.get(2)));\n+    String createSql = \"CREATE EXTERNAL TABLE \" + identifier +\n+        \" STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        testTables.locationForCreateTableSQL(identifier) +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(spec) + \"', \" +\n+        \"'\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\";\n+    runCreateAndReadTest(identifier, createSql, CUSTOMER_SCHEMA, spec, data);\n   }\n \n   @Test\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU3MDA2Mg==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541570062", "bodyText": "nit: shall we check hmsTable.isSetPartitionKeys() here? in other places we seem to use that instead of the null check", "author": "marton-bod", "createdAt": "2020-12-12T12:48:44Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && hmsTable.getPartitionKeys().size() != 0) {\n+        // Add partitioning columns to the original column list before creating the Iceberg Schema\n+        List<FieldSchema> cols = new ArrayList<>(hmsTable.getSd().getCols());\n+        cols.addAll(hmsTable.getPartitionKeys());\n+        return HiveSchemaUtil.convert(cols);\n+      } else {\n+        return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      }\n     }\n   }\n \n   private static PartitionSpec spec(Schema schema, Properties properties,\n       org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n \n-    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),\n-        \"Partitioned Hive tables are currently not supported\");\n-\n     if (properties.getProperty(InputFormatConfig.PARTITION_SPEC) != null) {\n+      Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),", "originalCommit": "f9ccf40c143745250ab6846e4c46f1e4b4c58ac6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU3MTQ1Ng==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541571456", "bodyText": "Done", "author": "pvary", "createdAt": "2020-12-12T12:57:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU3MDA2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "04073e2ca7626be583375c66c38be12b29e9a1cd", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java b/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\nindex 9b20113c0..c0e6a2438 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\n\n@@ -205,7 +205,7 @@ public class HiveIcebergMetaHook implements HiveMetaHook {\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      if (hmsTable.isSetPartitionKeys() && hmsTable.getPartitionKeys().size() != 0) {\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {\n         // Add partitioning columns to the original column list before creating the Iceberg Schema\n         List<FieldSchema> cols = new ArrayList<>(hmsTable.getSd().getCols());\n         cols.addAll(hmsTable.getPartitionKeys());\n"}}, {"oid": "04073e2ca7626be583375c66c38be12b29e9a1cd", "url": "https://github.com/apache/iceberg/commit/04073e2ca7626be583375c66c38be12b29e9a1cd", "message": "Review comments", "committedDate": "2020-12-12T12:56:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU3NDk5NA==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541574994", "bodyText": "shouldn't this be !hmsTable.isSetPartitionKeys()?", "author": "marton-bod", "createdAt": "2020-12-12T13:20:45Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {\n+        // Add partitioning columns to the original column list before creating the Iceberg Schema\n+        List<FieldSchema> cols = new ArrayList<>(hmsTable.getSd().getCols());\n+        cols.addAll(hmsTable.getPartitionKeys());\n+        return HiveSchemaUtil.convert(cols);\n+      } else {\n+        return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      }\n     }\n   }\n \n   private static PartitionSpec spec(Schema schema, Properties properties,\n       org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n \n-    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),\n-        \"Partitioned Hive tables are currently not supported\");\n-\n     if (properties.getProperty(InputFormatConfig.PARTITION_SPEC) != null) {\n+      Preconditions.checkArgument(hmsTable.isSetPartitionKeys() || hmsTable.getPartitionKeys().isEmpty(),", "originalCommit": "04073e2ca7626be583375c66c38be12b29e9a1cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTc3NjM3Ng==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r541776376", "bodyText": "done", "author": "pvary", "createdAt": "2020-12-12T20:38:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU3NDk5NA=="}], "type": "inlineReview", "revised_code": {"commit": "b362083163f4052e8aa124d5b3d5c3ed2577d785", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java b/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\nindex c0e6a2438..011178a99 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java\n\n@@ -220,7 +220,7 @@ public class HiveIcebergMetaHook implements HiveMetaHook {\n       org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n \n     if (properties.getProperty(InputFormatConfig.PARTITION_SPEC) != null) {\n-      Preconditions.checkArgument(hmsTable.isSetPartitionKeys() || hmsTable.getPartitionKeys().isEmpty(),\n+      Preconditions.checkArgument(!hmsTable.isSetPartitionKeys() || hmsTable.getPartitionKeys().isEmpty(),\n           \"Provide only one of the following: Hive partition specification, or the \" +\n               InputFormatConfig.PARTITION_SPEC + \" property\");\n       return PartitionSpecParser.fromJson(schema, properties.getProperty(InputFormatConfig.PARTITION_SPEC));\n"}}, {"oid": "b362083163f4052e8aa124d5b3d5c3ed2577d785", "url": "https://github.com/apache/iceberg/commit/b362083163f4052e8aa124d5b3d5c3ed2577d785", "message": "Refactored test so we try out the insert path for multi level partitions as well", "committedDate": "2020-12-12T17:01:25Z", "type": "commit"}, {"oid": "ff585b45f95684a236aacd7fd8f1dbc177ab0765", "url": "https://github.com/apache/iceberg/commit/ff585b45f95684a236aacd7fd8f1dbc177ab0765", "message": "Inceased heap size", "committedDate": "2020-12-12T19:04:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NDYwOA==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r544744608", "bodyText": "Nit: generally prefer Lists.newArrayList() to specific class constructors.", "author": "rdblue", "createdAt": "2020-12-17T01:32:00Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {\n+        // Add partitioning columns to the original column list before creating the Iceberg Schema\n+        List<FieldSchema> cols = new ArrayList<>(hmsTable.getSd().getCols());", "originalCommit": "ff585b45f95684a236aacd7fd8f1dbc177ab0765", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1NjMyNg==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r551856326", "bodyText": "Fix in #2029", "author": "pvary", "createdAt": "2021-01-05T10:50:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NDYwOA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NDg5Nw==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r544744897", "bodyText": "Nit: this could be an else if to remove a level of indentation.", "author": "rdblue", "createdAt": "2020-12-17T01:32:41Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {", "originalCommit": "ff585b45f95684a236aacd7fd8f1dbc177ab0765", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1NjM4OA==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r551856388", "bodyText": "Fix in #2029", "author": "pvary", "createdAt": "2021-01-05T10:50:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NDg5Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NTA4Mw==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r544745083", "bodyText": "When would isSetPartitionKeys() be true and getPartitionKeys() empty?", "author": "rdblue", "createdAt": "2020-12-17T01:33:28Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {", "originalCommit": "ff585b45f95684a236aacd7fd8f1dbc177ab0765", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1Njc3OA==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r551856778", "bodyText": "Theoretically it could be set to an empty list. Checked this way to keep on the safe side.", "author": "pvary", "createdAt": "2021-01-05T10:51:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NTA4Mw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NTMwMg==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r544745302", "bodyText": "Nit: could be else if case.", "author": "rdblue", "createdAt": "2020-12-17T01:34:07Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -197,20 +205,32 @@ private static Schema schema(Properties properties, org.apache.hadoop.hive.metas\n     if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n       return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {\n+        // Add partitioning columns to the original column list before creating the Iceberg Schema\n+        List<FieldSchema> cols = new ArrayList<>(hmsTable.getSd().getCols());\n+        cols.addAll(hmsTable.getPartitionKeys());\n+        return HiveSchemaUtil.convert(cols);\n+      } else {\n+        return HiveSchemaUtil.convert(hmsTable.getSd().getCols());\n+      }\n     }\n   }\n \n   private static PartitionSpec spec(Schema schema, Properties properties,\n       org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n \n-    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),\n-        \"Partitioned Hive tables are currently not supported\");\n-\n     if (properties.getProperty(InputFormatConfig.PARTITION_SPEC) != null) {\n+      Preconditions.checkArgument(!hmsTable.isSetPartitionKeys() || hmsTable.getPartitionKeys().isEmpty(),\n+          \"Provide only one of the following: Hive partition specification, or the \" +\n+              InputFormatConfig.PARTITION_SPEC + \" property\");\n       return PartitionSpecParser.fromJson(schema, properties.getProperty(InputFormatConfig.PARTITION_SPEC));\n     } else {\n-      return PartitionSpec.unpartitioned();\n+      if (hmsTable.isSetPartitionKeys() && !hmsTable.getPartitionKeys().isEmpty()) {", "originalCommit": "ff585b45f95684a236aacd7fd8f1dbc177ab0765", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1Njg4Mg==", "url": "https://github.com/apache/iceberg/pull/1917#discussion_r551856882", "bodyText": "Fix in #2029", "author": "pvary", "createdAt": "2021-01-05T10:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0NTMwMg=="}], "type": "inlineReview", "revised_code": null}]}