{"pr_number": 1539, "pr_title": "Add names to parameterized tests and simplify the parameters list", "pr_createdAt": "2020-10-01T02:21:28Z", "pr_url": "https://github.com/apache/iceberg/pull/1539", "timeline": [{"oid": "9cd9ec33e58a150657023119af91821edaa62127", "url": "https://github.com/apache/iceberg/commit/9cd9ec33e58a150657023119af91821edaa62127", "message": "Add parameter name notation and simplify for about half of tests", "committedDate": "2020-10-01T02:05:31Z", "type": "commit"}, {"oid": "e9798bdfa0e3a98d4af283eec4fd82084b61afa0", "url": "https://github.com/apache/iceberg/commit/e9798bdfa0e3a98d4af283eec4fd82084b61afa0", "message": "Update another to cherry pick over to this branch", "committedDate": "2020-10-01T02:13:01Z", "type": "commit"}, {"oid": "e8a27d38c759b780c2a61546804c81e120c4cb31", "url": "https://github.com/apache/iceberg/commit/e8a27d38c759b780c2a61546804c81e120c4cb31", "message": "Update more patameterized tests with names", "committedDate": "2020-10-01T02:17:12Z", "type": "commit"}, {"oid": "3d3c13a48dc33a283cc875eb7fd99386dd70afc8", "url": "https://github.com/apache/iceberg/commit/3d3c13a48dc33a283cc875eb7fd99386dd70afc8", "message": "Handle more changes to parameterized names. Will come back to baseNamespace string array problem.", "committedDate": "2020-10-01T02:51:39Z", "type": "commit"}, {"oid": "d616f0942a811b736083151da17fcab248af982b", "url": "https://github.com/apache/iceberg/commit/d616f0942a811b736083151da17fcab248af982b", "message": "Remove attempt to see if the name field of the annotation was a metatemplate that one could code in", "committedDate": "2020-10-01T02:53:45Z", "type": "commit"}, {"oid": "103c994ad1e5e21a8d0d8213e2288c2f9e0b8864", "url": "https://github.com/apache/iceberg/commit/103c994ad1e5e21a8d0d8213e2288c2f9e0b8864", "message": "WIP on a BaseNamespace class that will have a meaningful toString method", "committedDate": "2020-10-01T03:16:20Z", "type": "commit"}, {"oid": "409955c2e38902e03bae158245dc0e986b4862bb", "url": "https://github.com/apache/iceberg/commit/409955c2e38902e03bae158245dc0e986b4862bb", "message": "Add note about plans to handle the String[] baseNamespace", "committedDate": "2020-10-01T03:18:40Z", "type": "commit"}, {"oid": "663eb0eed667c4dbf6c7854aa8e57441cd4ee12e", "url": "https://github.com/apache/iceberg/commit/663eb0eed667c4dbf6c7854aa8e57441cd4ee12e", "message": "Found another case that requires a valid toString method, will handle in another PR", "committedDate": "2020-10-01T03:30:40Z", "type": "commit"}, {"oid": "f424ceff88717c731709b8505731abaca955bfb4", "url": "https://github.com/apache/iceberg/commit/f424ceff88717c731709b8505731abaca955bfb4", "message": "Found a class that does not run with all of its parameters - need to find out why", "committedDate": "2020-10-01T03:34:50Z", "type": "commit"}, {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187", "url": "https://github.com/apache/iceberg/commit/3f8f2a5827054008833c514d920d5fbd82ff5187", "message": "Simplify array initializer for Object[][]", "committedDate": "2020-10-01T03:43:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497977246", "bodyText": "Can we remove these lines then?", "author": "rdblue", "createdAt": "2020-10-01T04:29:38Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,24 @@ public void close() throws CatalogException {\n     }\n   }\n \n+  // Going to save this change for another PR as this PR is already really big.", "originalCommit": "3f8f2a5827054008833c514d920d5fbd82ff5187", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3OTk2NA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497979964", "bodyText": "Yeah. All of the classes that show up kind of wonky, I'm going to just treat the same in this PR (let them show up as (new String[2]{\"hello\", \"world\"}).toString like the JUnit runner wants them to), and then I'll open an issue to follow up on them in a separate PR.\nSo this PR can be a straight addition of just names to the tests.", "author": "kbendick", "createdAt": "2020-10-01T04:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MjAyMg==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497982022", "bodyText": "I have removed this and will open an issue to track all of the places where the test parameters show up as string array references or other wonky toString values.", "author": "kbendick", "createdAt": "2020-10-01T04:50:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4NDg0Nw==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497984847", "bodyText": "Ahhh I see your comment about reusing namespace. That makes the most sense to me as well. Given that I'll have to change the non-test code, I'll do i in a separate PR. I'll create an issue for it now and get to it sometime this week.", "author": "kbendick", "createdAt": "2020-10-01T05:02:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NDcwOA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497994708", "bodyText": "I've removed those lines and created an issue to update baseNamespace to use the Namespace class for its human-readable toString implementation and its preexisting convenient constructors.\nHere's the issue: #1541. I'll pick it up sometime this week if nobody else does.", "author": "kbendick", "createdAt": "2020-10-01T05:40:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "71fe0bbda8dbb52d54e4fc9bc88b2195a2ae14bb", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex a4b7de9f5..e014cc2cd 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -130,23 +130,6 @@ public class FlinkCatalog extends AbstractCatalog {\n     }\n   }\n \n-  // Going to save this change for another PR as this PR is already really big.\n-  /**\n-  private static class BaseNamespace {\n-    private final String[] baseNamespace;\n-\n-    public BaseNamespace(String[] baseNamespace) {\n-      this.baseNamespace = baseNamespace;\n-    }\n-\n-    @Override\n-    public boolean equals(Object obj) {\n-      if (!(obj instanceof BaseNamespace) || obj == null) {\n-        return false;\n-      }\n-      BaseNamespace that = (BaseNamespace) obj;\n-    }\n-  }*/\n \n   private Namespace toNamespace(String database) {\n     String[] namespace = new String[baseNamespace.length + 1];\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497977641", "bodyText": "I don't think that metadata columns have been implemented for vectorized Parquet yet? Does it pass if you uncomment it?", "author": "rdblue", "createdAt": "2020-10-01T04:31:12Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "originalCommit": "3f8f2a5827054008833c514d920d5fbd82ff5187", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MDg4OA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497980888", "bodyText": "Nope. None of the three tests pass with true.\nHere's a stack trace from testReadRowNumbersWithFilter:\nActual should be an InternalRow: schema\njava.lang.AssertionError: Actual should be an InternalRow: schema\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.iceberg.spark.data.TestHelpers.assertEquals(TestHelpers.java:611)\n\tat org.apache.iceberg.spark.data.TestHelpers.assertEquals(TestHelpers.java:600)\n\tat org.apache.iceberg.spark.data.TestSparkParquetReadMetadataColumns.readAndValidate(TestSparkParquetReadMetadataColumns.java:212)\n\tat org.apache.iceberg.spark.data.TestSparkParquetReadMetadataColumns.testReadRowNumbersWithFilter(TestSparkParquetReadMetadataColumns.java:165)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n...", "author": "kbendick", "createdAt": "2020-10-01T04:45:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzI4OA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497983288", "bodyText": "I can open an issue for this if you'd like. I know there's been a lot of discussion around the time and cost of reading tables with very large numbers of metadata files, and possibly this would help?", "author": "kbendick", "createdAt": "2020-10-01T04:55:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4NTU3MA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497985570", "bodyText": "I opened an issue to add support: #1540\nPossibly we can remove the discussion of true and just add a link to the issue? Is this even something we care about implementing (I believe likely yes if we allow metadata files to reach the GB size). Especially given that we had a discussion today in the meeting and later on slack about distributed query planning for metadata and file pruning started by @aokolnychyi. However, I would defer to your judgement about whether or not to keep the issue open.", "author": "kbendick", "createdAt": "2020-10-01T05:05:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM4OTYxMw==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498389613", "bodyText": "Yes, we should point to the issue. Thanks for updating it.\nTo clarify, this is for vectorized data reads with the _pos metadata column to get row position, not for reading metadata files. I think that this is being implemented in #1356.", "author": "rdblue", "createdAt": "2020-10-01T16:58:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4ODA0Ng==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498488046", "bodyText": "Thanks for the clarification. I changed the issue title to Add in support for vectorized parquet reads with filters on row position in metadata. How does that sound? I also updated the comment.", "author": "kbendick", "createdAt": "2020-10-01T20:11:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "71fe0bbda8dbb52d54e4fc9bc88b2195a2ae14bb", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java\nindex 4d2ff6921..2c1d09659 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java\n\n@@ -108,7 +108,7 @@ public class TestSparkParquetReadMetadataColumns {\n   @Parameterized.Parameters(name =  \"vectorized = {0}\")\n   // Note - Does not currently run with the `true` parameter. Why is that?\n   public static Object[] parameters() {\n-    return new Object[] { false }; // new Object[] { true }\n+    return new Object[] { false, true };\n   }\n \n   @Rule\n"}}, {"oid": "71fe0bbda8dbb52d54e4fc9bc88b2195a2ae14bb", "url": "https://github.com/apache/iceberg/commit/71fe0bbda8dbb52d54e4fc9bc88b2195a2ae14bb", "message": "Fix more template strings and remove commented out code", "committedDate": "2020-10-01T04:48:56Z", "type": "commit"}, {"oid": "0042199763aa5a6dd0ff59119e705562829be901", "url": "https://github.com/apache/iceberg/commit/0042199763aa5a6dd0ff59119e705562829be901", "message": "Add in note about adding in true if / when parquet vectorized metadata column support is added", "committedDate": "2020-10-01T04:53:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzYzOQ==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497983639", "bodyText": "@rdblue should I remove this one and instead grab it in a follow up PR? Its just part of the test suites, but the template parameter T could be grabbed via java reflection - and it is different in some of the tests.", "author": "kbendick", "createdAt": "2020-10-01T04:56:56Z", "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java", "diffHunk": "@@ -408,6 +408,11 @@ public String name() {\n         public TestInputFormat<T> create(Configuration conf) {\n           return function.apply(conf);\n         }\n+\n+        @Override\n+        public String toString() {\n+          return String.format(\"Test%s<T>\", name());", "originalCommit": "0042199763aa5a6dd0ff59119e705562829be901", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5ODIwNg==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497998206", "bodyText": "I'll leave the toString method and then open an issue for getting the proper name of the template parameter, given that we test with multiple types substituted for T. It can be obtained via java reflection and since this is limited to the test files, there's no real overhead outside of potentially test time (but this isn't called so often as to be a concern).", "author": "kbendick", "createdAt": "2020-10-01T05:52:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwMDUxOA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498000518", "bodyText": "Ok. I've decided to leave it in as stands and I've created an issue to see if I can follow up on it so that we have the different types used in the parameter groups properly displayed in the tests: #1542", "author": "kbendick", "createdAt": "2020-10-01T06:00:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzYzOQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7", "url": "https://github.com/apache/iceberg/commit/fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7", "message": "Update TODO to be a link to the open issue", "committedDate": "2020-10-01T05:44:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498008283", "bodyText": "I checked on this, and ORC-611 is closed and the fix versions are 1.6.4 and 1.7.0.\nI'm going to see if I can get these tests to pass or open a ticket about possibly upgrading our ORC version if not.", "author": "kbendick", "createdAt": "2020-10-01T06:23:46Z", "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "diffHunk": "@@ -222,52 +222,48 @@ public void createParquetInputFile(List<Record> records) throws IOException {\n   private final Object readValue;\n   private final Object skipValue;\n \n-  @Parameterized.Parameters\n+  @Parameterized.Parameters(name = \"format = {0} column = {1} readValue = {2} skipValue = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        new Object[] { \"parquet\", \"boolean\", false, true },\n-        new Object[] { \"parquet\", \"int\", 5, 55 },\n-        new Object[] { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"parquet\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"parquet\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n-        new Object[] { \"parquet\", \"timestamp\",\n-            \"2018-06-29T10:02:34.000000\",\n-            \"2018-06-29T15:02:34.000000\" },\n-        new Object[] { \"parquet\", \"timestamptz\",\n-            \"2018-06-29T10:02:34.000000+00:00\",\n-            \"2018-06-29T10:02:34.000000-07:00\" },\n-        new Object[] { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n-        // new Object[] { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n-        new Object[] { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n-        new Object[] { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n-        new Object[] { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n-        new Object[] { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n-        new Object[] { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n+        { \"parquet\", \"boolean\", false, true },\n+        { \"parquet\", \"int\", 5, 55 },\n+        { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"parquet\", \"float\", 1.97f, 2.11f },\n+        { \"parquet\", \"double\", 2.11d, 1.97d },\n+        { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"parquet\", \"timestamp\", \"2018-06-29T10:02:34.000000\", \"2018-06-29T15:02:34.000000\" },\n+        { \"parquet\", \"timestamptz\", \"2018-06-29T10:02:34.000000+00:00\", \"2018-06-29T10:02:34.000000-07:00\" },\n+        { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n+        // { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n+        { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n+        { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n+        { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n+        { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n+        { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n \n-        new Object[] { \"orc\", \"boolean\", false, true },\n-        new Object[] { \"orc\", \"int\", 5, 55 },\n-        new Object[] { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"orc\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"orc\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"orc\", \"boolean\", false, true },\n+        { \"orc\", \"int\", 5, 55 },\n+        { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"orc\", \"float\", 1.97f, 2.11f },\n+        { \"orc\", \"double\", 2.11d, 1.97d },\n+        { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n         // Temporarily disable filters on Timestamp columns due to ORC-611\n-        // new Object[] { \"orc\", \"timestamp\",\n+        // { \"orc\", \"timestamp\",", "originalCommit": "fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM2NTQ5NA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498365494", "bodyText": "I tried updating but it did not affect the tests. There were several orc related files on the compile time path, but I'm not sure what ran during the tests.\nIt looks like the issue ORC-611 was closed, but it still exists in https://issues.apache.org/jira/browse/HIVE-23036", "author": "kbendick", "createdAt": "2020-10-01T16:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM3NjU1NA==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498376554", "bodyText": "I'm going to also try 1.7.0, but I'm wondering if we aren't fully excluding some ORC dependencies from other dependencies.", "author": "kbendick", "createdAt": "2020-10-01T16:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4Mjc1OQ==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498482759", "bodyText": "Turns out 1.7.0 is not published to maven. I am keeping these tests commented out with a mention of the corresponding still open HIVE ticket, as it indicates where in the OrcInputFormat for Hive is the issue. Might be of use to us.", "author": "kbendick", "createdAt": "2020-10-01T20:00:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "d4cedb50bacdcf858506bf7defbe9682e1d6c0b4", "chunk": "diff --git a/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java b/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java\nindex ce1eabc70..0a29c4fef 100644\n--- a/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java\n+++ b/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java\n\n@@ -250,6 +250,10 @@ public class TestMetricsRowGroupFilterTypes {\n         { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n         { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n         // Temporarily disable filters on Timestamp columns due to ORC-611\n+        //\n+        // ORC-611 is closed with fix versions of ORC 1.6.4 and 1.7.0.\n+        // We should upgrade so that we can properly handle ORC row group\n+        // filtering on timestamp/tz types.\n         // { \"orc\", \"timestamp\",\n         //     \"2018-06-29T10:02:34.000000\",\n         //     \"2018-06-29T15:02:34.000000\" },\n"}}, {"oid": "d4cedb50bacdcf858506bf7defbe9682e1d6c0b4", "url": "https://github.com/apache/iceberg/commit/d4cedb50bacdcf858506bf7defbe9682e1d6c0b4", "message": "Add a note about ORC-611 being fixed in newer versions for sub-millisecond min/max stats", "committedDate": "2020-10-01T06:29:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM4NTY1OQ==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498385659", "bodyText": "Can we remove this since we don't need to change this file?", "author": "rdblue", "createdAt": "2020-10-01T16:51:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,7 @@ public void close() throws CatalogException {\n     }\n   }\n \n+", "originalCommit": "d4cedb50bacdcf858506bf7defbe9682e1d6c0b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4MzIxMg==", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498483212", "bodyText": "Removed!", "author": "kbendick", "createdAt": "2020-10-01T20:01:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM4NTY1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "0f7aeab1ccfb73f5bf1ada6559da4ffc0dd4dc04", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex e014cc2cd..02b4d1ecd 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -130,7 +130,6 @@ public class FlinkCatalog extends AbstractCatalog {\n     }\n   }\n \n-\n   private Namespace toNamespace(String database) {\n     String[] namespace = new String[baseNamespace.length + 1];\n     System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n"}}, {"oid": "5c1be7b5327e00d0456e92c939bb2e6e5bfd0218", "url": "https://github.com/apache/iceberg/commit/5c1be7b5327e00d0456e92c939bb2e6e5bfd0218", "message": "Update note about failing row group filter metrics test for orc timestamp types", "committedDate": "2020-10-01T19:53:05Z", "type": "commit"}, {"oid": "0f7aeab1ccfb73f5bf1ada6559da4ffc0dd4dc04", "url": "https://github.com/apache/iceberg/commit/0f7aeab1ccfb73f5bf1ada6559da4ffc0dd4dc04", "message": "Remove unnecessary white space addition", "committedDate": "2020-10-01T20:00:50Z", "type": "commit"}, {"oid": "16bd11a59d1c7ce52e27ef7dc057abc8ef251579", "url": "https://github.com/apache/iceberg/commit/16bd11a59d1c7ce52e27ef7dc057abc8ef251579", "message": "Update comment about vectorized parquet reads not supported when filtering by row position from table metadata", "committedDate": "2020-10-01T20:13:26Z", "type": "commit"}, {"oid": "2939a3e9bbc1e8789b51d87c7b8328cab7296f6a", "url": "https://github.com/apache/iceberg/commit/2939a3e9bbc1e8789b51d87c7b8328cab7296f6a", "message": "Further clarify comment about what is not implemented", "committedDate": "2020-10-01T20:18:08Z", "type": "commit"}, {"oid": "ff5e81de5d74f6f1e1b14c3c922392058b5c3736", "url": "https://github.com/apache/iceberg/commit/ff5e81de5d74f6f1e1b14c3c922392058b5c3736", "message": "Point to the issue for baseNamespace in the code vs having a TODO", "committedDate": "2020-10-01T22:08:14Z", "type": "commit"}, {"oid": "1be400cef4e7d13dd05bf0b6e3b51708666549e5", "url": "https://github.com/apache/iceberg/commit/1be400cef4e7d13dd05bf0b6e3b51708666549e5", "message": "Add a note about the issue for baseNamespace in the FlinkCatalog", "committedDate": "2020-10-01T22:35:14Z", "type": "commit"}, {"oid": "12cdf3a0003a2a6466c0af76c4982fdc652e1175", "url": "https://github.com/apache/iceberg/commit/12cdf3a0003a2a6466c0af76c4982fdc652e1175", "message": "Remove todo about baseNamespace in flink table sink tests as the issue is now referenced in FlinkCatalog directly", "committedDate": "2020-10-01T22:38:40Z", "type": "commit"}, {"oid": "2d3a10a220fab4d26ce1b628fc7cc8bb64293761", "url": "https://github.com/apache/iceberg/commit/2d3a10a220fab4d26ce1b628fc7cc8bb64293761", "message": "Mention reference to issue for capturing template paramater type for test output", "committedDate": "2020-10-01T22:47:34Z", "type": "commit"}]}