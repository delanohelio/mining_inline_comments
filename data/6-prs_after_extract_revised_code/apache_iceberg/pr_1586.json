{"pr_number": 1586, "pr_title": "Flink: support specifying user-provided hive-site.xml for hive catalog.", "pr_createdAt": "2020-10-12T14:07:27Z", "pr_url": "https://github.com/apache/iceberg/pull/1586", "timeline": [{"oid": "8664b9f8c5ea4d2054592f33f8d37efe21fad22f", "url": "https://github.com/apache/iceberg/commit/8664b9f8c5ea4d2054592f33f8d37efe21fad22f", "message": "Flink: support specifying user-provided hive-site.xml for hive catalog.", "committedDate": "2020-10-13T13:38:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk2NTQwOQ==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r503965409", "bodyText": "The user can choose to set warehouse property or load hive-site.xml when creating the iceberg catalog.  If use the former,  the warehouse path may be different with the hive metastore, that means it will create files under the user specified directory.\nIf use the hive-site.xml,  then it should be the same configuration as the metastore.", "author": "openinx", "createdAt": "2020-10-13T13:46:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java", "diffHunk": "@@ -38,8 +40,13 @@ static CatalogLoader hadoop(String name, Configuration hadoopConf, String wareho\n     return new HadoopCatalogLoader(name, hadoopConf, warehouseLocation);\n   }\n \n-  static CatalogLoader hive(String name, Configuration hadoopConf, String uri, int clientPoolSize) {\n-    return new HiveCatalogLoader(name, hadoopConf, uri, clientPoolSize);\n+  static CatalogLoader hive(String name, Configuration hadoopConf, String uri, String warehouse, int clientPoolSize) {", "originalCommit": "8664b9f8c5ea4d2054592f33f8d37efe21fad22f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "247f5d769b6dde6f26f2a361ce8507e87b91934e", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java b/flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java\nindex 386b0fc7c..4d0670cff 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java\n\n@@ -21,19 +21,24 @@ package org.apache.iceberg.flink;\n \n import java.io.Serializable;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.hadoop.SerializableConfiguration;\n import org.apache.iceberg.hive.HiveCatalog;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n-import org.apache.iceberg.relocated.com.google.common.base.Strings;\n \n /**\n  * Serializable loader to load an Iceberg {@link Catalog}.\n  */\n public interface CatalogLoader extends Serializable {\n \n+  /**\n+   * Create a new catalog with the provided properties. NOTICE: for flink, we may initialize the {@link CatalogLoader}\n+   * at flink sql client side or job manager side, and then serialize this catalog loader to task manager, finally\n+   * deserialize it and create a new catalog at task manager side.\n+   *\n+   * @return a newly created {@link Catalog}\n+   */\n   Catalog loadCatalog();\n \n   static CatalogLoader hadoop(String name, Configuration hadoopConf, String warehouseLocation) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQ1MDI4Ng==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r504450286", "bodyText": "Moving those tow method inside this super class, because the newly introduced unit test TestFlinkHiveCatalog  would use them.", "author": "openinx", "createdAt": "2020-10-14T07:11:23Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -53,4 +64,36 @@ public static void stopMetastore() {\n     flinkCatalogs.values().forEach(Catalog::close);\n     flinkCatalogs.clear();\n   }\n+\n+  protected TableEnvironment getTableEnv() {", "originalCommit": "eb7fe6f4b1dcf891952181d985d4fabdd5df71b3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dd9d1c84ea4b26d04ca039013984bbe59dd2a17f", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\nindex 01d6e389e..b680af792 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n\n@@ -61,8 +56,6 @@ public abstract class FlinkTestBase extends AbstractTestBase {\n     metastore.stop();\n     catalog.close();\n     FlinkTestBase.catalog = null;\n-    flinkCatalogs.values().forEach(Catalog::close);\n-    flinkCatalogs.clear();\n   }\n \n   protected TableEnvironment getTableEnv() {\n"}}, {"oid": "12f0a5b0b5488f17e94e3217dd9b8410ca1cbff0", "url": "https://github.com/apache/iceberg/commit/12f0a5b0b5488f17e94e3217dd9b8410ca1cbff0", "message": "Rebase to master", "committedDate": "2020-10-15T10:22:43Z", "type": "forcePushed"}, {"oid": "423eecbdac3840c14a8a9114c4f61996f7f0865a", "url": "https://github.com/apache/iceberg/commit/423eecbdac3840c14a8a9114c4f61996f7f0865a", "message": "Flink: support specifying user-provided hive-site.xml for hive catalog.", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "2aba22991fc8f4c3e0f8112ef27ed8fb6e8df813", "url": "https://github.com/apache/iceberg/commit/2aba22991fc8f4c3e0f8112ef27ed8fb6e8df813", "message": "Loading hive config from classpath if neccessary.", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "dd9d1c84ea4b26d04ca039013984bbe59dd2a17f", "url": "https://github.com/apache/iceberg/commit/dd9d1c84ea4b26d04ca039013984bbe59dd2a17f", "message": "Fix the broken unit tests.", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "2602e4882c9a830db75bf4397b940dae3e6d9c02", "url": "https://github.com/apache/iceberg/commit/2602e4882c9a830db75bf4397b940dae3e6d9c02", "message": "Revert \"Fix the broken unit tests.\"\n\nThis reverts commit ef87f5378f8e4e3982f25ae5a74e83eb4b3938de.", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "6dec7b10ef8187ba404aaa4b1328775c7bd8c12c", "url": "https://github.com/apache/iceberg/commit/6dec7b10ef8187ba404aaa4b1328775c7bd8c12c", "message": "Fix the broken unit tests", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "342c3eca8f4cfd1230b0e1f23d36edc97569dceb", "url": "https://github.com/apache/iceberg/commit/342c3eca8f4cfd1230b0e1f23d36edc97569dceb", "message": "Checkstyle issues", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "2166bdfde3920afec8002d25351ac3056e57ff85", "url": "https://github.com/apache/iceberg/commit/2166bdfde3920afec8002d25351ac3056e57ff85", "message": "Add document", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "7c3c59410620295e3b99edfc66c8c81ae19cb185", "url": "https://github.com/apache/iceberg/commit/7c3c59410620295e3b99edfc66c8c81ae19cb185", "message": "Rebase to master", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "12b742cc44a8c63c4b3451d61c6f5b2508288014", "url": "https://github.com/apache/iceberg/commit/12b742cc44a8c63c4b3451d61c6f5b2508288014", "message": "Fix the broken unit tests", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "51b6abb4bc7ff664bd0f9725ffc0808a363bc402", "url": "https://github.com/apache/iceberg/commit/51b6abb4bc7ff664bd0f9725ffc0808a363bc402", "message": "Checkstyle issues", "committedDate": "2020-10-19T10:09:43Z", "type": "commit"}, {"oid": "de034032848848aaafdab978c251d7a7024a7b11", "url": "https://github.com/apache/iceberg/commit/de034032848848aaafdab978c251d7a7024a7b11", "message": "Set the thrift pool size.", "committedDate": "2020-10-19T10:12:56Z", "type": "commit"}, {"oid": "de034032848848aaafdab978c251d7a7024a7b11", "url": "https://github.com/apache/iceberg/commit/de034032848848aaafdab978c251d7a7024a7b11", "message": "Set the thrift pool size.", "committedDate": "2020-10-19T10:12:56Z", "type": "forcePushed"}, {"oid": "c15399ff0553f4ce374550e6c7e08195870eb667", "url": "https://github.com/apache/iceberg/commit/c15399ff0553f4ce374550e6c7e08195870eb667", "message": "Update site/docs/flink.md\n\nCo-authored-by: Adrian Woodhead <massdosage@gmail.com>", "committedDate": "2020-10-21T02:13:36Z", "type": "commit"}, {"oid": "c23e16fd3f6548d358fb4e8c378f7cd96b2c8886", "url": "https://github.com/apache/iceberg/commit/c23e16fd3f6548d358fb4e8c378f7cd96b2c8886", "message": "Addressing the comments.", "committedDate": "2020-10-21T02:31:42Z", "type": "commit"}, {"oid": "38b83b2654a87a61bb5fccb14eca0667d08bdf83", "url": "https://github.com/apache/iceberg/commit/38b83b2654a87a61bb5fccb14eca0667d08bdf83", "message": "Precheck for the existence of hive-conf-dir", "committedDate": "2020-10-21T02:42:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1MzQ2MQ==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509453461", "bodyText": "Why did this change? Needing to increase the number of threads used by the Hive MetaStore is a red flag that this may be leaking Hive connections. In cases where all of the connections are handled by a ClientPool, we should not need to increase the number of handler threads. This helps us avoid leaks because we catch them in testing rather than at runtime.", "author": "rdblue", "createdAt": "2020-10-21T17:03:17Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -34,23 +40,53 @@\n   private static TestHiveMetastore metastore = null;\n   protected static HiveConf hiveConf = null;\n   protected static HiveCatalog catalog = null;\n-  protected static ConcurrentMap<String, Catalog> flinkCatalogs;\n+\n+  private volatile TableEnvironment tEnv = null;\n \n   @BeforeClass\n   public static void startMetastore() {\n     FlinkTestBase.metastore = new TestHiveMetastore();\n-    metastore.start();\n+    metastore.start(15);", "originalCommit": "38b83b2654a87a61bb5fccb14eca0667d08bdf83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1NDI2Nw==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509854267", "bodyText": "I thought the hive handlers were not enough in the TestFlinkCatalogDatabase  test suite, but today I checked the logs carefully, it's indeed a connection leak issue.   The catalog connection would be closed only when unregistering the catalog, means executing the sql DROP CATALOG test_catalog.  I found we did not release the connection after a test case finished.   No need to increase the handler number here, just need to provide Before/After to create/release connection, pls see f2740c6.", "author": "openinx", "createdAt": "2020-10-22T03:13:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1MzQ2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "f2740c6cf4f7aa5e090a4b3df60e853aaa140e38", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\nindex a39c9566a..b680af792 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n\n@@ -46,7 +46,7 @@ public abstract class FlinkTestBase extends AbstractTestBase {\n   @BeforeClass\n   public static void startMetastore() {\n     FlinkTestBase.metastore = new TestHiveMetastore();\n-    metastore.start(15);\n+    metastore.start();\n     FlinkTestBase.hiveConf = metastore.hiveConf();\n     FlinkTestBase.catalog = new HiveCatalog(metastore.hiveConf());\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1Njc4MQ==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509456781", "bodyText": "Style nit: we typically add an empty line after control flow statements. This removes one and doesn't add one after the new if.", "author": "rdblue", "createdAt": "2020-10-21T17:08:37Z", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java", "diffHunk": "@@ -72,13 +72,19 @@ public HiveCatalog(Configuration conf) {\n   }\n \n   public HiveCatalog(String name, String uri, int clientPoolSize, Configuration conf) {\n+    this(name, uri, null, clientPoolSize, conf);\n+  }\n+\n+  public HiveCatalog(String name, String uri, String warehouse, int clientPoolSize, Configuration conf) {\n     this.name = name;\n     this.conf = new Configuration(conf);\n     // before building the client pool, overwrite the configuration's URIs if the argument is non-null\n     if (uri != null) {\n       this.conf.set(HiveConf.ConfVars.METASTOREURIS.varname, uri);\n     }\n-\n+    if (warehouse != null) {\n+      this.conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouse);\n+    }", "originalCommit": "38b83b2654a87a61bb5fccb14eca0667d08bdf83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7fdda77ee9942d3a26239498741a12b078c004df", "chunk": "diff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\nindex 0ed104bd9..d188da5d3 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\n\n@@ -82,9 +82,11 @@ public class HiveCatalog extends BaseMetastoreCatalog implements Closeable, Supp\n     if (uri != null) {\n       this.conf.set(HiveConf.ConfVars.METASTOREURIS.varname, uri);\n     }\n+\n     if (warehouse != null) {\n       this.conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouse);\n     }\n+\n     this.clients = new HiveClientPool(clientPoolSize, this.conf);\n     this.createStack = Thread.currentThread().getStackTrace();\n     this.closed = false;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1Nzc1MQ==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509457751", "bodyText": "Why not keep this location as a field in the HiveCatalog instead of storing it in conf? Then the default logic would only need to be run once.", "author": "rdblue", "createdAt": "2020-10-21T17:10:07Z", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java", "diffHunk": "@@ -72,13 +72,19 @@ public HiveCatalog(Configuration conf) {\n   }\n \n   public HiveCatalog(String name, String uri, int clientPoolSize, Configuration conf) {\n+    this(name, uri, null, clientPoolSize, conf);\n+  }\n+\n+  public HiveCatalog(String name, String uri, String warehouse, int clientPoolSize, Configuration conf) {\n     this.name = name;\n     this.conf = new Configuration(conf);\n     // before building the client pool, overwrite the configuration's URIs if the argument is non-null\n     if (uri != null) {\n       this.conf.set(HiveConf.ConfVars.METASTOREURIS.varname, uri);\n     }\n-\n+    if (warehouse != null) {\n+      this.conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouse);", "originalCommit": "38b83b2654a87a61bb5fccb14eca0667d08bdf83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MzY1Mg==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509843652", "bodyText": "As we discussed in this comment,   we could set warehouse string or hive-conf-dir to get the hive warehouse.   If use a local field,  then in getWarehouseLocation,  we would check the local field first and then read the hiveConf ?  I want to unify the parse path so I  put this key-value into hiveConf (It's a deep-cloned conf, so should not affect the origin conf).", "author": "openinx", "createdAt": "2020-10-22T02:33:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1Nzc1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1MDQyNw==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509850427", "bodyText": "Yes, It can be handled in the same way as URIs.", "author": "JingsongLi", "createdAt": "2020-10-22T02:58:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1Nzc1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE2MjEzMg==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r511162132", "bodyText": "I think handling this the same way that we handle the URI makes sense. We set the URI because we need to pass it back into Hive to connect, which I don't think applies to the warehouse path, but handling both the same way is reasonable.", "author": "rdblue", "createdAt": "2020-10-23T21:25:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ1Nzc1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "7fdda77ee9942d3a26239498741a12b078c004df", "chunk": "diff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\nindex 0ed104bd9..d188da5d3 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\n\n@@ -82,9 +82,11 @@ public class HiveCatalog extends BaseMetastoreCatalog implements Closeable, Supp\n     if (uri != null) {\n       this.conf.set(HiveConf.ConfVars.METASTOREURIS.varname, uri);\n     }\n+\n     if (warehouse != null) {\n       this.conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, warehouse);\n     }\n+\n     this.clients = new HiveClientPool(clientPoolSize, this.conf);\n     this.createStack = Thread.currentThread().getStackTrace();\n     this.closed = false;\n"}}, {"oid": "7fdda77ee9942d3a26239498741a12b078c004df", "url": "https://github.com/apache/iceberg/commit/7fdda77ee9942d3a26239498741a12b078c004df", "message": "Addressing the comment from Ryan", "committedDate": "2020-10-22T02:40:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1MDg0OA==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509850848", "bodyText": "Maybe I missed something. Why not load hive-conf into hadoopConf here?", "author": "JingsongLi", "createdAt": "2020-10-22T03:00:28Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java", "diffHunk": "@@ -71,12 +72,16 @@ protected CatalogLoader createCatalogLoader(String name, Map<String, String> pro\n     String catalogType = properties.getOrDefault(ICEBERG_CATALOG_TYPE, \"hive\");\n     switch (catalogType) {\n       case \"hive\":\n-        int clientPoolSize = Integer.parseInt(properties.getOrDefault(HIVE_CLIENT_POOL_SIZE, \"2\"));\n+        // The values of properties 'uri', 'warehouse', 'hive-conf-dir' are allowed to be null, in that case it will\n+        // fallback to parse those values from hadoop configuration which is loaded from classpath.\n         String uri = properties.get(HIVE_URI);\n-        return CatalogLoader.hive(name, hadoopConf, uri, clientPoolSize);\n+        String warehouse = properties.get(WAREHOUSE_LOCATION);\n+        int clientPoolSize = Integer.parseInt(properties.getOrDefault(HIVE_CLIENT_POOL_SIZE, \"2\"));\n+        String hiveConfDir = properties.get(HIVE_CONF_DIR);\n+        return CatalogLoader.hive(name, hadoopConf, uri, warehouse, clientPoolSize, hiveConfDir);", "originalCommit": "7fdda77ee9942d3a26239498741a12b078c004df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2MzM3MA==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509863370", "bodyText": "Oh, I almost forgot that the loadCatalog would be executed at task manager side for flink.   we should merge hive conf into hadoop conf before initializing the catalog loader, so that we won't miss to load a non-existed path in task manager node.", "author": "openinx", "createdAt": "2020-10-22T03:50:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1MDg0OA=="}], "type": "inlineReview", "revised_code": {"commit": "247f5d769b6dde6f26f2a361ce8507e87b91934e", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java\nindex 9fa06c78a..9484a9664 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java\n\n@@ -78,7 +84,8 @@ public class FlinkCatalogFactory implements CatalogFactory {\n         String warehouse = properties.get(WAREHOUSE_LOCATION);\n         int clientPoolSize = Integer.parseInt(properties.getOrDefault(HIVE_CLIENT_POOL_SIZE, \"2\"));\n         String hiveConfDir = properties.get(HIVE_CONF_DIR);\n-        return CatalogLoader.hive(name, hadoopConf, uri, warehouse, clientPoolSize, hiveConfDir);\n+        Configuration newHadoopConf = mergeHiveConf(hadoopConf, hiveConfDir);\n+        return CatalogLoader.hive(name, newHadoopConf, uri, warehouse, clientPoolSize);\n \n       case \"hadoop\":\n         String warehouseLocation = properties.get(WAREHOUSE_LOCATION);\n"}}, {"oid": "f2740c6cf4f7aa5e090a4b3df60e853aaa140e38", "url": "https://github.com/apache/iceberg/commit/f2740c6cf4f7aa5e090a4b3df60e853aaa140e38", "message": "Fix the hive connection leak issues", "committedDate": "2020-10-22T03:04:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1MzQ5NA==", "url": "https://github.com/apache/iceberg/pull/1586#discussion_r509853494", "bodyText": "NIT: use hiveConf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname) instead of null.", "author": "JingsongLi", "createdAt": "2020-10-22T03:10:25Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -107,7 +107,7 @@ protected StructLikeSet rowSet(String name, Table testTable, String... columns)\n     RowType rowType = FlinkSchemaUtil.convert(projected);\n     CatalogLoader hiveCatalogLoader = CatalogLoader.hive(catalog.name(),\n         hiveConf,\n-        hiveConf.get(HiveConf.ConfVars.METASTOREURIS.varname),\n+        hiveConf.get(HiveConf.ConfVars.METASTOREURIS.varname), null,", "originalCommit": "7fdda77ee9942d3a26239498741a12b078c004df", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ad9aafa55f80dd74abd3d65c2adf4ca42b6b242", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java\nindex 42c2a1cef..c24a887cd 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java\n\n@@ -107,7 +107,8 @@ public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n     RowType rowType = FlinkSchemaUtil.convert(projected);\n     CatalogLoader hiveCatalogLoader = CatalogLoader.hive(catalog.name(),\n         hiveConf,\n-        hiveConf.get(HiveConf.ConfVars.METASTOREURIS.varname), null,\n+        hiveConf.get(HiveConf.ConfVars.METASTOREURIS.varname),\n+        hiveConf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname),\n         hiveConf.getInt(\"iceberg.hive.client-pool-size\", 5)\n     );\n     FlinkInputFormat inputFormat = FlinkSource.forRowData()\n"}}, {"oid": "247f5d769b6dde6f26f2a361ce8507e87b91934e", "url": "https://github.com/apache/iceberg/commit/247f5d769b6dde6f26f2a361ce8507e87b91934e", "message": "Loading hive-site configuration when initialize the Iceberg catalog loader.", "committedDate": "2020-10-22T03:46:33Z", "type": "commit"}, {"oid": "4ad9aafa55f80dd74abd3d65c2adf4ca42b6b242", "url": "https://github.com/apache/iceberg/commit/4ad9aafa55f80dd74abd3d65c2adf4ca42b6b242", "message": "Make the document more clear", "committedDate": "2020-10-22T04:08:39Z", "type": "commit"}]}