{"pr_number": 1784, "pr_title": "Fix Resolving of SparkSession Table's Metadata Tables", "pr_createdAt": "2020-11-18T20:30:26Z", "pr_url": "https://github.com/apache/iceberg/pull/1784", "timeline": [{"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "url": "https://github.com/apache/iceberg/commit/703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "message": "Fix Resolving of SparkSession Table's Metadata Tables\n\nDo to an issue within Spark's Resolution rules we cannot acces a table in the session\ncatalog with a multipart identifier. Because we also cannot determine whether the underlying\nIceberg table is Hadoop or Hive based we also cannot know the right method for reading the\ntable. To work around this for now we attempt to first load the table as a HiveTable, if\nthe table isn't found we fall back and attempt to load it as a Hadoop table.", "committedDate": "2020-11-18T20:29:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526401342", "bodyText": "One huge drawback to this is we can't actually use any Catalog Options that have been specified, so if we have options in the future than alter the way a Metadata table is read we won't have them here.", "author": "RussellSpitzer", "createdAt": "2020-11-18T20:31:29Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "originalCommit": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQxMTE0MA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526411140", "bodyText": "If we could extend out into BaseSpark3Actions this would be a good place to diverge, since I think we could load the metadata table directly from the catalog, and bypass the name resolution code. But we can't do that without redoing the hierarchy or having the Rewrite Actions use reflection to determine their \"loadMetadataTable\" Method", "author": "RussellSpitzer", "createdAt": "2020-11-18T20:49:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4NTM1Mw==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526585353", "bodyText": "Still to figure out is how we do this, since the current method is static and it must be because the hierarchy is currently setup such that some spark actions do not extend from this class", "author": "RussellSpitzer", "createdAt": "2020-11-19T04:23:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex bd726c352..1ada6cc50 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -129,28 +129,35 @@ abstract class BaseSparkAction<R> implements Action<R> {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      if (tableName.startsWith(\"spark_catalog\")) {\n-        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n-        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n-        // in the hive manner first, then fall back and try the location if we have completely run out of options\n-        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n-        try {\n-          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n-        } catch (NoSuchTableException noSuchTableException) {\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        }\n-      } else {\n-        return spark.table(tableName + \".\" + type);\n-      }\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);\n     } catch (Exception e) {\n       if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n         // Rethrow unexpected exceptions\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0ODk5OQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526448999", "bodyText": "Would it make sense to call it dataFrameReader instead of noCatalogReader everywhere?", "author": "aokolnychyi", "createdAt": "2020-11-18T21:56:29Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "originalCommit": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ2Nzg3Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526467872", "bodyText": "\ud83e\udd37 It's a no catalog reader whenever it's used :) But sure I can rename it.", "author": "RussellSpitzer", "createdAt": "2020-11-18T22:32:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0ODk5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex bd726c352..1ada6cc50 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -129,28 +129,35 @@ abstract class BaseSparkAction<R> implements Action<R> {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      if (tableName.startsWith(\"spark_catalog\")) {\n-        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n-        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n-        // in the hive manner first, then fall back and try the location if we have completely run out of options\n-        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n-        try {\n-          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n-        } catch (NoSuchTableException noSuchTableException) {\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        }\n-      } else {\n-        return spark.table(tableName + \".\" + type);\n-      }\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);\n     } catch (Exception e) {\n       if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n         // Rethrow unexpected exceptions\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0OTc0Mw==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526449743", "bodyText": "Do we want to put this logic into a separate method like loadMetadataUsingCatalog or something?", "author": "aokolnychyi", "createdAt": "2020-11-18T21:57:50Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {", "originalCommit": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MDY0Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526470642", "bodyText": "My only thought on why not to do this, is that hopefully in the future we get to remove the \"startsWith(spark_catalog)\" branch, and then the method is just spark.table(). But I can change it for now.", "author": "RussellSpitzer", "createdAt": "2020-11-18T22:38:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0OTc0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex bd726c352..1ada6cc50 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -129,28 +129,35 @@ abstract class BaseSparkAction<R> implements Action<R> {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      if (tableName.startsWith(\"spark_catalog\")) {\n-        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n-        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n-        // in the hive manner first, then fall back and try the location if we have completely run out of options\n-        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n-        try {\n-          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n-        } catch (NoSuchTableException noSuchTableException) {\n-          return noCatalogReader.load(tableLocation + \"#\" + type);\n-        }\n-      } else {\n-        return spark.table(tableName + \".\" + type);\n-      }\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);\n     } catch (Exception e) {\n       if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n         // Rethrow unexpected exceptions\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1MDU3OA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526450578", "bodyText": "nit: throws Exception?", "author": "aokolnychyi", "createdAt": "2020-11-18T21:59:23Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,56 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {", "originalCommit": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java b/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\nindex 0bf69695a..aa92330a6 100644\n--- a/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\n+++ b/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\n\n@@ -113,8 +113,7 @@ public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n   }\n \n   @Test\n-  public void testSparkSessionCatalogHadoopTable()\n-      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n     spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n     spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1MDYzMg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526450632", "bodyText": "nit: same here", "author": "aokolnychyi", "createdAt": "2020-11-18T21:59:31Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,56 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {", "originalCommit": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "chunk": "diff --git a/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java b/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\nindex 0bf69695a..aa92330a6 100644\n--- a/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\n+++ b/spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java\n\n@@ -113,8 +113,7 @@ public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n   }\n \n   @Test\n-  public void testSparkSessionCatalogHadoopTable()\n-      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n     spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n     spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n"}}, {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "url": "https://github.com/apache/iceberg/commit/6e621037c6d46acb3f96370e3fd752eff8be1ecb", "message": "Reviewer Comments", "committedDate": "2020-11-18T22:40:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNDcyMw==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526504723", "bodyText": "Nit: typo: \"due\" not \"do\".", "author": "rdblue", "createdAt": "2020-11-19T00:05:54Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.", "originalCommit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c3588beb33bfcca24ed2be0394da7316356631b8", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 1ada6cc50..78f7b6963 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -133,7 +133,7 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                            MetadataTableType type) {\n     DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.startsWith(\"spark_catalog\")) {\n-      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // Due to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n       // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n       // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n       // hadoop location method if that fails\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNTAxOA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526505018", "bodyText": "I think this is a bug in Spark. There isn't a work-around that I know of.", "author": "rdblue", "createdAt": "2020-11-19T00:06:45Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog", "originalCommit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c3588beb33bfcca24ed2be0394da7316356631b8", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 1ada6cc50..78f7b6963 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -133,7 +133,7 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                            MetadataTableType type) {\n     DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.startsWith(\"spark_catalog\")) {\n-      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // Due to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n       // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n       // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n       // hadoop location method if that fails\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjE3OQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526506179", "bodyText": "If we know that the catalog is spark_catalog, then we should just try to load without removing the catalog name. If we remove the catalog name, then we don't know that the right table will be loaded because the Spark catalog may not be the session's current catalog.\nAnd, if the metadata table type works then so would using the prefix spark_catalog. Names like spark_catalog.db.table work, it is just spark_catalog.db.table.meta that does not. If meta is added and the current catalog is spark_catalog, then I think it will fail no matter what.", "author": "rdblue", "createdAt": "2020-11-19T00:10:10Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "originalCommit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU5MTU4OA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526591588", "bodyText": "I don't think I follow.  Spark checks\ndef isSessionCatalog(catalog: CatalogPlugin): Boolean = {\n    catalog.name().equalsIgnoreCase(CatalogManager.SESSION_CATALOG_NAME)\n  }\nTo decide if the catalog is the session catalog and fail the parsing. If it does then lookup table matches this pattern\nobject SessionCatalogAndIdentifier {\n    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n\n    def unapply(parts: Seq[String]): Option[(CatalogPlugin, Identifier)] = parts match {\n      case CatalogAndIdentifier(catalog, ident) if CatalogV2Util.isSessionCatalog(catalog) =>\n        if (ident.namespace.length != 1) {\n          throw new AnalysisException(\n            s\"The namespace in session catalog must have exactly one name part: ${parts.quoted}\")\n        }\n        Some(catalog, ident)\n      case _ => None\n    }\n  }\nSo it doesn't matter if the spark-catalog is the current Catalog or not, we can never load a table by name with more than 3 pieces if it starts with spark-catalog.\nHere we are falling back to looking into the default hive catalog, which is all we can do without having direct access to Spark3 CatalogPlugins.", "author": "RussellSpitzer", "createdAt": "2020-11-19T04:47:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjE3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "c3588beb33bfcca24ed2be0394da7316356631b8", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 1ada6cc50..78f7b6963 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -133,7 +133,7 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                            MetadataTableType type) {\n     DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.startsWith(\"spark_catalog\")) {\n-      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // Due to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n       // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n       // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n       // hadoop location method if that fails\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjY0OA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526506648", "bodyText": "Missing return?", "author": "rdblue", "createdAt": "2020-11-19T00:11:30Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);", "originalCommit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4OTQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526589475", "bodyText": "yep sorry, when I renamed this I forgot to add the return", "author": "RussellSpitzer", "createdAt": "2020-11-19T04:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjY0OA=="}], "type": "inlineReview", "revised_code": {"commit": "c3588beb33bfcca24ed2be0394da7316356631b8", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 1ada6cc50..78f7b6963 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -133,7 +133,7 @@ abstract class BaseSparkAction<R> implements Action<R> {\n                                                            MetadataTableType type) {\n     DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.startsWith(\"spark_catalog\")) {\n-      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // Due to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n       // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n       // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n       // hadoop location method if that fails\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526511929", "bodyText": "I think this test case only works because HiveCatalogs uses the value of hive.metastore.uris from the environment's hive-site.xml. By removing spark_catalog and then using the DataFrameReader, the Hive catalog from HiveCatalogs is used, which has the same URI.\nI think this is actually the right thing to do, but I would do it more directly and obviously so that it is clear what is happening:\n\nGet the session catalog from the catalog manager\nIf the session catalog is a SparkSessionCatalog, get the underlying Iceberg catalog\nUse the Iceberg catalog to load the metadata table, because it accepts the full table identifier", "author": "rdblue", "createdAt": "2020-11-19T00:27:16Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,54 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"sessioncattest\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.dropTable(id);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();", "originalCommit": "6e621037c6d46acb3f96370e3fd752eff8be1ecb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4NDY4Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526584682", "bodyText": "Discussed on slack: Notes here\nMain reason I didn't do this originally is we then need to break the code in to a Spark3 and Spark2 versions. But if we could, we wouldn't have to do this at all since we could just in the Spark 3 mode load the table by getting the Catalog directly and loading from there, instead of trying to fall back to the non-catalog path. Thinking we are going to go down the path of attempting to get Spark Version specific code in here.", "author": "RussellSpitzer", "createdAt": "2020-11-19T04:20:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjk5MzM0Ng==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526993346", "bodyText": "More notes for myself, Once we have direct access to to catalog manager, let's just do\nString[] ns = {\"default\", \"sessioncattest\"};\nDataset.ofRows(spark, DataSourceV2Relation.create(cat.loadTable(Identifier.of(ns, \"entries\")), Option.apply(cat), Option.apply(id)));\nManually resolving the relation and ignoring the misfeature of the session catalog name resolution", "author": "RussellSpitzer", "createdAt": "2020-11-19T15:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "c3588beb33bfcca24ed2be0394da7316356631b8", "url": "https://github.com/apache/iceberg/commit/c3588beb33bfcca24ed2be0394da7316356631b8", "message": "Few Reviewer Comments", "committedDate": "2020-11-19T05:16:25Z", "type": "commit"}, {"oid": "c3588beb33bfcca24ed2be0394da7316356631b8", "url": "https://github.com/apache/iceberg/commit/c3588beb33bfcca24ed2be0394da7316356631b8", "message": "Few Reviewer Comments", "committedDate": "2020-11-19T05:16:25Z", "type": "forcePushed"}, {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "url": "https://github.com/apache/iceberg/commit/e2b762efb8ed72928fc62e873779bd1abe11bfcb", "message": "Change to use Spark3 Specific pathway for resolution", "committedDate": "2020-11-19T20:30:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTI5NQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527185295", "bodyText": "Everything below here was pulled from the Create PR and is basically a copy of the Catalog And Identifier Resolution methods from Spark.", "author": "RussellSpitzer", "createdAt": "2020-11-19T20:42:40Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTY4MA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527185680", "bodyText": "This is how Spark would have made the relation from our metadata table if it didn't think multiple pieces in the Namespace was a dealbreaker.", "author": "RussellSpitzer", "createdAt": "2020-11-19T20:43:29Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE0MA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203140", "bodyText": "How stable do we expect implicits to be?", "author": "aokolnychyi", "createdAt": "2020-11-19T21:15:10Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());\n+      } catch (Exception e) {\n+        return new CatalogAndIdentifier(currentCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper(namePartsSeq).asIdentifier());\n+      }\n+    }\n+  }\n+\n+  public static TableIdentifier toTableIdentifier(Identifier table) {\n+    return new CatalogV2Implicits.IdentifierHelper(table).asTableIdentifier();", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyODI3Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527228272", "bodyText": "This isn't used here, it's only in the Create Action. I'll remove it for now and we can decide whether we need it later.", "author": "RussellSpitzer", "createdAt": "2020-11-19T21:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE0MA=="}], "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203444", "bodyText": "Can we factor this out into a separate method like toIdentifier?", "author": "aokolnychyi", "createdAt": "2020-11-19T21:15:37Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzYzOQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203639", "bodyText": "I mean the construction of MultipartIdentifierHelper.", "author": "aokolnychyi", "createdAt": "2020-11-19T21:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyODA0OA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527228048", "bodyText": "I am just dropping this whole section, so we just use Identifier.of directly. We won't use the implicit then.", "author": "RussellSpitzer", "createdAt": "2020-11-19T21:55:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA=="}], "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzgyNg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203826", "bodyText": "Can we do the precondition first in the method?", "author": "aokolnychyi", "createdAt": "2020-11-19T21:16:18Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNzg1OA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527207858", "bodyText": "Yeah I just like nonEmpty more than !(isEmpty)", "author": "RussellSpitzer", "createdAt": "2020-11-19T21:23:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzgyNg=="}], "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNDE1MA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527204150", "bodyText": "Can we introduce CatalogManager catalogManager variable and reuse it in all lines below?", "author": "aokolnychyi", "createdAt": "2020-11-19T21:16:54Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxOTc3OA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527219778", "bodyText": "Yeah, mostly this was just a direct translation of the scala code with explicit implicit, we can change it to be more idiomatic java", "author": "RussellSpitzer", "createdAt": "2020-11-19T21:40:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNDE1MA=="}], "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTMxNQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527205315", "bodyText": "Can we split it into multiple lines?\n... parser = spark.sessionState().sqlParser();\n... nameParts = parser.parseMultipartIdentifier(name);\nreturn catalogAndIdentifier(spark, JavaConverters.seqAsJavaList(nameParts));", "author": "aokolnychyi", "createdAt": "2020-11-19T21:19:04Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex c48c6f281..f00a2faa4 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -588,8 +590,10 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n-    return catalogAndIdentifier(spark,\n-          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+    ParserInterface parser = spark.sessionState().sqlParser();\n+    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n+    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527205882", "bodyText": "Can we invert the condition and check if it starts with 3?", "author": "aokolnychyi", "createdAt": "2020-11-19T21:20:01Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||\n+        exception.getMessage().contains(\"SparkException\") ||\n+        exception.getMessage().contains(\"NoSuchTableException\") ||\n+        exception.getMessage().contains(\"CatalogNotFoundException\");\n+  }\n+\n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type)\n+      throws NoSuchMethodException {\n+    if (loadCatalogImpl == null) {\n+      loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+          .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+              SparkSession.class, String.class, MetadataTableType.class)\n+          .buildStaticChecked();\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n+      // Try DSV2 catalog based name based resolution\n+      if (!spark.version().startsWith(\"2\")) {", "originalCommit": "e2b762efb8ed72928fc62e873779bd1abe11bfcb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNzA3NA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527207074", "bodyText": "but what about Spark 4 :) , let me change it to a numeral check > 2", "author": "RussellSpitzer", "createdAt": "2020-11-19T21:22:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwOTI4Mg==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527209282", "bodyText": "actually I guess since we have an explicit spark3 module we can check just for 3 ...", "author": "RussellSpitzer", "createdAt": "2020-11-19T21:26:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex 64813a5ac..b6f1e069b 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -157,7 +157,7 @@ abstract class BaseSparkAction<R> implements Action<R> {\n     }\n     try {\n       // Try DSV2 catalog based name based resolution\n-      if (!spark.version().startsWith(\"2\")) {\n+      if (spark.version().startsWith(\"3\")) {\n         return loadCatalogMetadataTable(spark, tableName, type);\n       }\n     } catch (Exception e) {\n"}}, {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "url": "https://github.com/apache/iceberg/commit/43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "message": "Remove Scalaisms for pure Java Code", "committedDate": "2020-11-19T22:05:16Z", "type": "commit"}, {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "url": "https://github.com/apache/iceberg/commit/43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "message": "Remove Scalaisms for pure Java Code", "committedDate": "2020-11-19T22:05:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI3Mjk2OQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527272969", "bodyText": "Just realized I can move all of this into the Spark3Util class and just return null if we can't find the table via that method", "author": "RussellSpitzer", "createdAt": "2020-11-19T23:30:03Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||", "originalCommit": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex b6f1e069b..d783de62d 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -127,23 +127,19 @@ abstract class BaseSparkAction<R> implements Action<R> {\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n-  private static boolean isExpectedCatalogLookupException(Exception exception) {\n-    return exception.getMessage().contains(\"AnalysisException\") ||\n-        exception.getMessage().contains(\"SparkException\") ||\n-        exception.getMessage().contains(\"NoSuchTableException\") ||\n-        exception.getMessage().contains(\"CatalogNotFoundException\");\n-  }\n-\n   // Attempt to use Spark3 Catalog resolution if available on the path\n   private static DynMethods.StaticMethod loadCatalogImpl = null;\n \n-  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type)\n-      throws NoSuchMethodException {\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n     if (loadCatalogImpl == null) {\n-      loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n-          .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n-              SparkSession.class, String.class, MetadataTableType.class)\n-          .buildStaticChecked();\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();\n+      } catch (NoSuchMethodException e) {\n+        throw new IllegalArgumentException(\"Cannot find Spark3Util class but Spark 3 is being used.\", e);\n+      }\n     }\n     return loadCatalogImpl.invoke(spark, tableName, type);\n   }\n"}}, {"oid": "a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "url": "https://github.com/apache/iceberg/commit/a0bbdcd72ff4127cc79df827a6fbdaa3d41d338b", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table.", "committedDate": "2020-11-20T00:25:26Z", "type": "forcePushed"}, {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "url": "https://github.com/apache/iceberg/commit/ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table.", "committedDate": "2020-11-20T01:02:20Z", "type": "commit"}, {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "url": "https://github.com/apache/iceberg/commit/ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "message": "Move Exception handling from loadCatalogMetadataTable into Spark3\n\nThis way we can handle the exceptions with the proper classes. Now instead we\nreturn null if we can't find the correct table.", "committedDate": "2020-11-20T01:02:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzM3MTg0NQ==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527371845", "bodyText": "nit: I'd put the relation into a separate var to keep this on one line.", "author": "aokolnychyi", "createdAt": "2020-11-20T03:33:01Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +564,97 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /**\n+   * Returns a Metadata Table Dataset if it can be loaded from a Spark V2 Catalog\n+   *\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   *\n+   * @param spark SparkSession used for looking up catalog references and tables\n+   * @param name The multipart identifier of the base Iceberg table\n+   * @param type The type of metadata table to load\n+   * @return null if we cannot find the Metadata Table, a Dataset of rows otherwise\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type) {\n+    try {\n+      CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+      if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+        BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+        Identifier baseIdent = catalogAndIdentifier.identifier;\n+        Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+        Table metaTable = catalog.loadTable(metaIdent);\n+        return Dataset\n+            .ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));", "originalCommit": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex add61ea81..10523d439 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -582,11 +582,10 @@ public class Spark3Util {\n       CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n       if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n         BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n-        Identifier baseIdent = catalogAndIdentifier.identifier;\n-        Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n-        Table metaTable = catalog.loadTable(metaIdent);\n-        return Dataset\n-            .ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+        Identifier baseId = catalogAndIdentifier.identifier;\n+        Identifier metaId = Identifier.of(ArrayUtils.add(baseId.namespace(), baseId.name()), type.name());\n+        Table metaTable = catalog.loadTable(metaId);\n+        return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaId)));\n       }\n     } catch (NoSuchTableException | ParseException e) {\n       // Could not find table\n"}}, {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "url": "https://github.com/apache/iceberg/commit/59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "message": "Reviewer Comments", "committedDate": "2020-11-20T05:00:03Z", "type": "commit"}, {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "url": "https://github.com/apache/iceberg/commit/59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "message": "Reviewer Comments", "committedDate": "2020-11-20T05:00:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzQ4MA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527957480", "bodyText": "This doesn't need to load the method each time it is called. Usually, I would add orNoop and call build to construct a static field. Then in this method, you'd just need to check whether you have the method or noop:\n  Preconditions.checkArgument(!LOAD_CATALOG.isNoop(), \"Cannot find Spark3Util class ...\");\n  LOAD_CATALOG.invoke(spark, tableName, type);", "author": "rdblue", "createdAt": "2020-11-20T20:37:41Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();", "originalCommit": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MTI4NA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527961284", "bodyText": "That's nifty, I originally had a second variable to check if it was failed to load but thought that was a bit of a waste. This seems pretty clean though", "author": "RussellSpitzer", "createdAt": "2020-11-20T20:46:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzQ4MA=="}], "type": "inlineReview", "revised_code": {"commit": "52e4d4ef1eefafb23acdc0f7523c055a1366be87", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex d783de62d..d976b769a 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -128,20 +129,15 @@ abstract class BaseSparkAction<R> implements Action<R> {\n   }\n \n   // Attempt to use Spark3 Catalog resolution if available on the path\n-  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+  private static final DynMethods.UnboundMethod LOAD_CATALOG = DynMethods.builder(\"loadCatalogMetadataTable\")\n+      .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+          SparkSession.class, String.class, MetadataTableType.class)\n+      .orNoop()\n+      .build();\n \n   private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n-    if (loadCatalogImpl == null) {\n-      try {\n-        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n-            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n-                SparkSession.class, String.class, MetadataTableType.class)\n-            .buildStaticChecked();\n-      } catch (NoSuchMethodException e) {\n-        throw new IllegalArgumentException(\"Cannot find Spark3Util class but Spark 3 is being used.\", e);\n-      }\n-    }\n-    return loadCatalogImpl.invoke(spark, tableName, type);\n+    Preconditions.checkArgument(!LOAD_CATALOG.isNoop(), \"Cannot find Spark3Util class but Spark3 is in use\");\n+    return LOAD_CATALOG.invoke(spark, tableName, type);\n   }\n \n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzkxNA==", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527957914", "bodyText": "Nit: should have an empty line after the last block.", "author": "rdblue", "createdAt": "2020-11-20T20:38:51Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();\n+      } catch (NoSuchMethodException e) {\n+        throw new IllegalArgumentException(\"Cannot find Spark3Util class but Spark 3 is being used.\", e);\n+      }\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n-    try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n-      }\n-      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-      if (tableName.startsWith(\"hadoop.\")) {\n-        // Try loading by location as Hadoop table without Catalog\n-        return noCatalogReader.load(tableLocation + \"#\" + type);\n-      } else if (tableName.startsWith(\"hive\")) {\n-        // Try loading by name as a Hive table without Catalog\n-        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-      } else {\n-        throw new IllegalArgumentException(String.format(\n-            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n+    // Try DSV2 catalog based name based resolution\n+    if (spark.version().startsWith(\"3\")) {\n+      Dataset<Row> catalogMetadataTable = loadCatalogMetadataTable(spark, tableName, type);\n+      if (catalogMetadataTable != null) {\n+        return catalogMetadataTable;\n       }\n     }\n+    // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog", "originalCommit": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "52e4d4ef1eefafb23acdc0f7523c055a1366be87", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\nindex d783de62d..d976b769a 100644\n--- a/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java\n\n@@ -128,20 +129,15 @@ abstract class BaseSparkAction<R> implements Action<R> {\n   }\n \n   // Attempt to use Spark3 Catalog resolution if available on the path\n-  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+  private static final DynMethods.UnboundMethod LOAD_CATALOG = DynMethods.builder(\"loadCatalogMetadataTable\")\n+      .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+          SparkSession.class, String.class, MetadataTableType.class)\n+      .orNoop()\n+      .build();\n \n   private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n-    if (loadCatalogImpl == null) {\n-      try {\n-        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n-            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n-                SparkSession.class, String.class, MetadataTableType.class)\n-            .buildStaticChecked();\n-      } catch (NoSuchMethodException e) {\n-        throw new IllegalArgumentException(\"Cannot find Spark3Util class but Spark 3 is being used.\", e);\n-      }\n-    }\n-    return loadCatalogImpl.invoke(spark, tableName, type);\n+    Preconditions.checkArgument(!LOAD_CATALOG.isNoop(), \"Cannot find Spark3Util class but Spark3 is in use\");\n+    return LOAD_CATALOG.invoke(spark, tableName, type);\n   }\n \n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n"}}, {"oid": "52e4d4ef1eefafb23acdc0f7523c055a1366be87", "url": "https://github.com/apache/iceberg/commit/52e4d4ef1eefafb23acdc0f7523c055a1366be87", "message": "More Reviewer Comments\n\nSwitch to static field for reflected Spark3Util method call", "committedDate": "2020-11-20T21:09:09Z", "type": "commit"}, {"oid": "52e4d4ef1eefafb23acdc0f7523c055a1366be87", "url": "https://github.com/apache/iceberg/commit/52e4d4ef1eefafb23acdc0f7523c055a1366be87", "message": "More Reviewer Comments\n\nSwitch to static field for reflected Spark3Util method call", "committedDate": "2020-11-20T21:09:09Z", "type": "forcePushed"}, {"oid": "927e44d8623d3879893a2bd07d475c7baf585cb5", "url": "https://github.com/apache/iceberg/commit/927e44d8623d3879893a2bd07d475c7baf585cb5", "message": "Fix Reflective Invocation, Needs Static Context", "committedDate": "2020-11-20T21:58:59Z", "type": "commit"}]}