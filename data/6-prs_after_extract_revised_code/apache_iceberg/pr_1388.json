{"pr_number": 1388, "pr_title": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups", "pr_createdAt": "2020-08-26T21:03:13Z", "pr_url": "https://github.com/apache/iceberg/pull/1388", "timeline": [{"oid": "fedca8fdf3e7d6b963006714038b20916cbd13b0", "url": "https://github.com/apache/iceberg/commit/fedca8fdf3e7d6b963006714038b20916cbd13b0", "message": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups", "committedDate": "2020-08-26T21:20:21Z", "type": "commit"}, {"oid": "fedca8fdf3e7d6b963006714038b20916cbd13b0", "url": "https://github.com/apache/iceberg/commit/fedca8fdf3e7d6b963006714038b20916cbd13b0", "message": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups", "committedDate": "2020-08-26T21:20:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2ODIyMA==", "url": "https://github.com/apache/iceberg/pull/1388#discussion_r478768220", "bodyText": "What about adding a Parquet.concat util method? I don't think it is a good idea to make ParquetIO public just for this test case. But it would be nice to have a concat method somewhere that could concatenate Parquet files.", "author": "rdblue", "createdAt": "2020-08-28T00:38:04Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java", "diffHunk": "@@ -39,4 +51,48 @@\n   public void testVectorizedReadsWithNewContainers() throws IOException {\n \n   }\n+\n+  @Test\n+  public void testMixedDictionaryNonDictionaryReads() throws IOException {\n+    Schema schema = new Schema(SUPPORTED_PRIMITIVES.fields());\n+\n+    File dictionaryEncodedFile = temp.newFile();\n+    Assert.assertTrue(\"Delete should succeed\", dictionaryEncodedFile.delete());\n+    Iterable<GenericData.Record> dictionaryEncodableData = RandomData.generateDictionaryEncodableData(\n+        schema,\n+        10000,\n+        0L,\n+        RandomData.DEFAULT_NULL_PERCENTAGE);\n+    try (FileAppender<GenericData.Record> writer = getParquetWriter(schema, dictionaryEncodedFile)) {\n+      writer.addAll(dictionaryEncodableData);\n+    }\n+\n+    File plainEncodingFile = temp.newFile();\n+    Assert.assertTrue(\"Delete should succeed\", plainEncodingFile.delete());\n+    Iterable<GenericData.Record> nonDictionaryData = RandomData.generate(schema, 10000, 0L,\n+        RandomData.DEFAULT_NULL_PERCENTAGE);\n+    try (FileAppender<GenericData.Record> writer = getParquetWriter(schema, plainEncodingFile)) {\n+      writer.addAll(nonDictionaryData);\n+    }\n+\n+    File mixedFile = temp.newFile();\n+    Assert.assertTrue(\"Delete should succeed\", mixedFile.delete());\n+    OutputFile outputFile = Files.localOutput(mixedFile);\n+    int rowGroupSize = Integer.parseInt(PARQUET_ROW_GROUP_SIZE_BYTES_DEFAULT);\n+    ParquetFileWriter writer = new ParquetFileWriter(", "originalCommit": "fedca8fdf3e7d6b963006714038b20916cbd13b0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java b/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\nindex 96561a54c..bd5c53d7e 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\n\n@@ -55,14 +52,13 @@ public class TestParquetDictionaryEncodedVectorizedReads extends TestParquetVect\n   @Test\n   public void testMixedDictionaryNonDictionaryReads() throws IOException {\n     Schema schema = new Schema(SUPPORTED_PRIMITIVES.fields());\n-\n     File dictionaryEncodedFile = temp.newFile();\n     Assert.assertTrue(\"Delete should succeed\", dictionaryEncodedFile.delete());\n     Iterable<GenericData.Record> dictionaryEncodableData = RandomData.generateDictionaryEncodableData(\n-        schema,\n-        10000,\n-        0L,\n-        RandomData.DEFAULT_NULL_PERCENTAGE);\n+            schema,\n+            10000,\n+            0L,\n+            RandomData.DEFAULT_NULL_PERCENTAGE);\n     try (FileAppender<GenericData.Record> writer = getParquetWriter(schema, dictionaryEncodedFile)) {\n       writer.addAll(dictionaryEncodableData);\n     }\n"}}, {"oid": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "url": "https://github.com/apache/iceberg/commit/41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "message": "Add a utility to concatenate parquet files", "committedDate": "2020-08-28T09:28:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQyNzY2Mg==", "url": "https://github.com/apache/iceberg/pull/1388#discussion_r479427662", "bodyText": "We can use the default row group size from table properties here. It will be ignored when appending files because row groups are appended directly and not rewritten.", "author": "rdblue", "createdAt": "2020-08-28T17:00:30Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -690,4 +692,25 @@ private ParquetReadBuilder(org.apache.parquet.io.InputFile file) {\n       return new ParquetReadSupport<>(schema, readSupport, callInit, nameMapping);\n     }\n   }\n+\n+  /**\n+   * @param inputFiles   an {@link Iterable} of parquet files. The order of iteration determines the order in which\n+   *                     content of files are read and written to the @param outputFile\n+   * @param outputFile   the output parquet file containing all the data from @param inputFiles\n+   * @param rowGroupSize the row group size to use when writing the @param outputFile\n+   * @param schema       the schema of the data\n+   * @param metadata     extraMetadata to write at the footer of the @param outputFile\n+   */\n+  public static void concat(Iterable<File> inputFiles, File outputFile, int rowGroupSize, Schema schema,\n+                            Map<String, String> metadata) throws IOException {\n+    OutputFile file = Files.localOutput(outputFile);\n+    ParquetFileWriter writer = new ParquetFileWriter(\n+            ParquetIO.file(file), ParquetSchemaUtil.convert(schema, \"table\"),\n+            ParquetFileWriter.Mode.CREATE, rowGroupSize, 0);", "originalCommit": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQyODIxNA==", "url": "https://github.com/apache/iceberg/pull/1388#discussion_r479428214", "bodyText": "I think the input files and output file should use InputFile and OutputFile. That way this isn't limited to just the local FS.", "author": "rdblue", "createdAt": "2020-08-28T17:01:40Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -690,4 +692,25 @@ private ParquetReadBuilder(org.apache.parquet.io.InputFile file) {\n       return new ParquetReadSupport<>(schema, readSupport, callInit, nameMapping);\n     }\n   }\n+\n+  /**\n+   * @param inputFiles   an {@link Iterable} of parquet files. The order of iteration determines the order in which\n+   *                     content of files are read and written to the @param outputFile\n+   * @param outputFile   the output parquet file containing all the data from @param inputFiles\n+   * @param rowGroupSize the row group size to use when writing the @param outputFile\n+   * @param schema       the schema of the data\n+   * @param metadata     extraMetadata to write at the footer of the @param outputFile\n+   */\n+  public static void concat(Iterable<File> inputFiles, File outputFile, int rowGroupSize, Schema schema,", "originalCommit": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}