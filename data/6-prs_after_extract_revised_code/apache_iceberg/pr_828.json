{"pr_number": 828, "pr_title": "iceberg-spark changes for vectorized reads", "pr_createdAt": "2020-03-05T19:16:45Z", "pr_url": "https://github.com/apache/iceberg/pull/828", "timeline": [{"oid": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "url": "https://github.com/apache/iceberg/commit/be26a697c78dd47963c03dd3d363a40bb34ff14e", "message": "Cleanup and address code review comments.\n\nSummary of changes:\n1) Below new test cases added:\n   - Test for code path when optional values are mostly null\n   - Test for case when containers are not reused for every batch\n   - Test for case to verify arrow's validity vector is set correctly when setArrowValidityVector = true\n2) Reuse container logic is now similar to row based read path\n3) We now always set the nullability holder. Arrow validity vector is set only for purpose of supplying complete arrow vectors when requested to do so.", "committedDate": "2020-06-12T19:41:32Z", "type": "forcePushed"}, {"oid": "a3dd1ce3818801c051df88e1557af8de3f5e42d0", "url": "https://github.com/apache/iceberg/commit/a3dd1ce3818801c051df88e1557af8de3f5e42d0", "message": "Run gradle build instead of check since build runs assembly + check", "committedDate": "2020-06-12T23:00:40Z", "type": "forcePushed"}, {"oid": "3c9cc0667b97e18a00c7fcb2c0324cb1ec529d56", "url": "https://github.com/apache/iceberg/commit/3c9cc0667b97e18a00c7fcb2c0324cb1ec529d56", "message": "Run gradle build instead of check since build runs assembly + check", "committedDate": "2020-06-12T23:14:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4MDgxMg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r439680812", "bodyText": "Why make this a method? So it can be overridden?", "author": "rdblue", "createdAt": "2020-06-12T23:39:45Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "diffHunk": "@@ -48,11 +48,15 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n \n public class TestParquetVectorizedReads extends AvroDataTest {\n-  private static final int NUM_ROWS = 1_000_000;\n+  private static final int NUM_ROWS = 200_000;\n \n   @Override\n   protected void writeAndValidate(Schema schema) throws IOException {\n-    writeAndValidate(schema, NUM_ROWS, 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+    writeAndValidate(schema, getNumRows(), 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+  }\n+\n+  protected int getNumRows() {\n+    return NUM_ROWS;", "originalCommit": "8893379821bf6532a2b7001b574e65a98f8a699a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4MTYyMg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r439681622", "bodyText": "Yes", "author": "samarthjain", "createdAt": "2020-06-12T23:44:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4MDgxMg=="}], "type": "inlineReview", "revised_code": {"commit": "066c1ed7c1512225a05548a6152cf81df35f1dc6", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java b/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\nindex 58cd5b023..3e4f5f95c 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n\n@@ -55,10 +55,6 @@ public class TestParquetVectorizedReads extends AvroDataTest {\n     writeAndValidate(schema, getNumRows(), 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n   }\n \n-  protected int getNumRows() {\n-    return NUM_ROWS;\n-  }\n-\n   private void writeAndValidate(\n       Schema schema, int numRecords, long seed, float nullPercentage,\n       boolean setAndCheckArrowValidityVector, boolean reuseContainers)\n"}}, {"oid": "066c1ed7c1512225a05548a6152cf81df35f1dc6", "url": "https://github.com/apache/iceberg/commit/066c1ed7c1512225a05548a6152cf81df35f1dc6", "message": "Adjust tests, increase memory and increase travis build timeout", "committedDate": "2020-06-12T23:56:08Z", "type": "forcePushed"}, {"oid": "39a8550e4d041c18c538af6e9245ea7e0218f823", "url": "https://github.com/apache/iceberg/commit/39a8550e4d041c18c538af6e9245ea7e0218f823", "message": "Revert travis_wait since tests are not taking a long time now", "committedDate": "2020-06-13T00:25:00Z", "type": "forcePushed"}, {"oid": "951812f364f6f0c8b3b9aacf57994b7b58f896d6", "url": "https://github.com/apache/iceberg/commit/951812f364f6f0c8b3b9aacf57994b7b58f896d6", "message": "Adjust tests, increase memory and increase travis build timeout", "committedDate": "2020-06-13T01:05:24Z", "type": "forcePushed"}, {"oid": "2fed8256caaa08fbf25b985f9a7b15678968e59a", "url": "https://github.com/apache/iceberg/commit/2fed8256caaa08fbf25b985f9a7b15678968e59a", "message": "Cleanup and address code review comments.\n\nSummary of changes:\n1) Below new test cases added:\n   - Test for code path when optional values are mostly null\n   - Test for case when containers are not reused for every batch\n   - Test for case to verify arrow's validity vector is set correctly when setArrowValidityVector = true\n2) Reuse container logic is now similar to row based read path\n3) We now always set the nullability holder. Arrow validity vector is set only for purpose of supplying complete arrow vectors when requested to do so.", "committedDate": "2020-06-13T07:51:20Z", "type": "forcePushed"}, {"oid": "24161c76e6aa54394c00739dbf31084ccdbf550f", "url": "https://github.com/apache/iceberg/commit/24161c76e6aa54394c00739dbf31084ccdbf550f", "message": "Cleanup and address code review comments.\n\nSummary of changes:\n1) Below new test cases added:\n   - Test for code path when optional values are mostly null\n   - Test for case when containers are not reused for every batch\n   - Test for case to verify arrow's validity vector is set correctly when setArrowValidityVector = true\n2) Reuse container logic is now similar to row based read path\n3) We now always set the nullability holder. Arrow validity vector is set only for purpose of supplying complete arrow vectors when requested to do so.", "committedDate": "2020-06-15T07:29:43Z", "type": "commit"}, {"oid": "24161c76e6aa54394c00739dbf31084ccdbf550f", "url": "https://github.com/apache/iceberg/commit/24161c76e6aa54394c00739dbf31084ccdbf550f", "message": "Cleanup and address code review comments.\n\nSummary of changes:\n1) Below new test cases added:\n   - Test for code path when optional values are mostly null\n   - Test for case when containers are not reused for every batch\n   - Test for case to verify arrow's validity vector is set correctly when setArrowValidityVector = true\n2) Reuse container logic is now similar to row based read path\n3) We now always set the nullability holder. Arrow validity vector is set only for purpose of supplying complete arrow vectors when requested to do so.", "committedDate": "2020-06-15T07:29:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExMzM2Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389113363", "bodyText": "Why convert directly from Arrow to Spark? Shouldn't this make guarantees about Arrow storage fields that are used for a given Iceberg type instead?", "author": "rdblue", "createdAt": "2020-03-06T19:54:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class ArrowUtils {\n+\n+  private static ArrowUtils instance;\n+  private RootAllocator rootAllocator;\n+\n+  private ArrowUtils() {\n+    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n+  }\n+\n+  public static ArrowUtils instance() {\n+    if (instance == null) {\n+      instance = new ArrowUtils();\n+    }\n+    return instance;\n+  }\n+\n+  public RootAllocator rootAllocator() {\n+    return rootAllocator;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  public DataType fromArrowType(ArrowType data) {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java b/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java\ndeleted file mode 100644\nindex 02fbc435d..000000000\n--- a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java\n+++ /dev/null\n\n@@ -1,113 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg.spark.arrow;\n-\n-import org.apache.arrow.memory.RootAllocator;\n-import org.apache.arrow.vector.types.DateUnit;\n-import org.apache.arrow.vector.types.FloatingPointPrecision;\n-import org.apache.arrow.vector.types.TimeUnit;\n-import org.apache.arrow.vector.types.pojo.ArrowType;\n-import org.apache.arrow.vector.types.pojo.Field;\n-import org.apache.spark.sql.types.ArrayType;\n-import org.apache.spark.sql.types.DataType;\n-import org.apache.spark.sql.types.DataTypes;\n-import org.apache.spark.sql.types.DecimalType;\n-import org.apache.spark.sql.types.Metadata;\n-import org.apache.spark.sql.types.StructField;\n-import org.apache.spark.sql.types.StructType;\n-\n-public class ArrowUtils {\n-\n-  private static ArrowUtils instance;\n-  private RootAllocator rootAllocator;\n-\n-  private ArrowUtils() {\n-    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n-  }\n-\n-  public static ArrowUtils instance() {\n-    if (instance == null) {\n-      instance = new ArrowUtils();\n-    }\n-    return instance;\n-  }\n-\n-  public RootAllocator rootAllocator() {\n-    return rootAllocator;\n-  }\n-\n-  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-  public DataType fromArrowType(ArrowType data) {\n-\n-    if (data instanceof ArrowType.Bool) {\n-      return DataTypes.BooleanType;\n-    } else if (data instanceof ArrowType.Int) {\n-      ArrowType.Int intData = (ArrowType.Int) data;\n-      if (intData.getIsSigned() && intData.getBitWidth() == 8) {\n-        return DataTypes.ByteType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 2) {\n-        return DataTypes.ShortType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 4) {\n-        return DataTypes.IntegerType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 8) {\n-        return DataTypes.LongType;\n-      }\n-    } else if (data instanceof ArrowType.FloatingPoint) {\n-      ArrowType.FloatingPoint floatData = (ArrowType.FloatingPoint) data;\n-      if (floatData.getPrecision() == FloatingPointPrecision.SINGLE) {\n-        return DataTypes.FloatType;\n-      } else if (floatData.getPrecision() == FloatingPointPrecision.DOUBLE) {\n-        return DataTypes.DoubleType;\n-      }\n-    } else if (data instanceof ArrowType.Utf8) {\n-      return DataTypes.StringType;\n-    } else if (data instanceof ArrowType.Binary) {\n-      return DataTypes.BinaryType;\n-    } else if (data instanceof ArrowType.Decimal) {\n-      ArrowType.Decimal decimalData = (ArrowType.Decimal) data;\n-      return new DecimalType(decimalData.getPrecision(), decimalData.getScale());\n-    } else if (data instanceof ArrowType.Date && ((ArrowType.Date) data).getUnit() == DateUnit.DAY) {\n-      return DataTypes.DateType;\n-    } else if (data instanceof ArrowType.Timestamp && ((ArrowType.Timestamp) data).getUnit() == TimeUnit.MICROSECOND) {\n-      return DataTypes.TimestampType;\n-    }\n-\n-    throw new UnsupportedOperationException(\"Unsupported data type: \" + data);\n-  }\n-\n-  public DataType fromArrowField(Field field) {\n-    ArrowType arrowType = field.getType();\n-    if (arrowType instanceof ArrowType.List) {\n-      Field elementField = field.getChildren().get(0);\n-      DataType elementType = fromArrowField(elementField);\n-      return new ArrayType(elementType, elementField.isNullable());\n-    } else if (arrowType instanceof ArrowType.Struct) {\n-      StructField[] fields = new StructField[field.getChildren().size()];\n-      int index = 0;\n-      for (Field f : field.getChildren()) {\n-        DataType dt = fromArrowField(f);\n-        fields[index++] = new StructField(f.getName(), dt, f.isNullable(), Metadata.empty());\n-      }\n-      return new StructType(fields);\n-    } else {\n-      return fromArrowType(arrowType);\n-    }\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExMzc4Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389113787", "bodyText": "bSize is batch size? If so, batchSize is more clear.", "author": "rdblue", "createdAt": "2020-03-06T19:55:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+  private final int batchSize;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers, int bSize) {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 52%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 478b5a9b8..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,13 +19,12 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n-import org.apache.arrow.vector.FieldVector;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n import org.apache.parquet.column.page.PageReadStore;\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n import org.apache.parquet.hadoop.metadata.ColumnPath;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNjA5MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389116091", "bodyText": "Should there be more checking that numRows is valid? It should be the same for all vectors, right?\nWhat happens when all of the result vectors are null? Looks like this will set the batch length to 0, but I think we want to set it to something valid.", "author": "rdblue", "createdAt": "2020-03-06T20:00:55Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+  private final int batchSize;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers, int bSize) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+    this.batchSize = bSize;\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numValsToRead) {\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int numRows = 0;", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 52%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 478b5a9b8..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,13 +19,12 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n-import org.apache.arrow.vector.FieldVector;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n import org.apache.parquet.column.page.PageReadStore;\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n import org.apache.parquet.hadoop.metadata.ColumnPath;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNjg4Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389116883", "bodyText": "You might want to move this into a factory method for IcebergArrowColumnVector instead of embedding it here. This logic looks more related to how column vectors work than to the batch. Here, you could just call IcebergArrowColumnVector.forHolder(holder) and that would return either the null vector or a real one.", "author": "rdblue", "createdAt": "2020-03-06T20:02:46Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+  private final int batchSize;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers, int bSize) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+    this.batchSize = bSize;\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numValsToRead) {\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int numRows = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numValsToRead);", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 52%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 478b5a9b8..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,13 +19,12 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n-import org.apache.arrow.vector.FieldVector;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n import org.apache.parquet.column.page.PageReadStore;\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n import org.apache.parquet.hadoop.metadata.ColumnPath;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNzAwMw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389117003", "bodyText": "Nit: no need for this empty line.", "author": "rdblue", "createdAt": "2020-03-06T20:03:02Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNzk0NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389117944", "bodyText": "The convention for singletons used elsewhere is to create a static field named INSTANCE and name this method get:\npublic class SomeSingleton {\n  private static final SomeSingleton INSTANCE = new SomeSingleton();\n  public static SomeSingleton get() {\n    return INSTANCE;\n  }\n}", "author": "rdblue", "createdAt": "2020-03-06T20:05:14Z", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class ArrowUtils {\n+\n+  private static ArrowUtils instance;\n+  private RootAllocator rootAllocator;\n+\n+  private ArrowUtils() {\n+    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n+  }\n+\n+  public static ArrowUtils instance() {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java b/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java\ndeleted file mode 100644\nindex 02fbc435d..000000000\n--- a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java\n+++ /dev/null\n\n@@ -1,113 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg.spark.arrow;\n-\n-import org.apache.arrow.memory.RootAllocator;\n-import org.apache.arrow.vector.types.DateUnit;\n-import org.apache.arrow.vector.types.FloatingPointPrecision;\n-import org.apache.arrow.vector.types.TimeUnit;\n-import org.apache.arrow.vector.types.pojo.ArrowType;\n-import org.apache.arrow.vector.types.pojo.Field;\n-import org.apache.spark.sql.types.ArrayType;\n-import org.apache.spark.sql.types.DataType;\n-import org.apache.spark.sql.types.DataTypes;\n-import org.apache.spark.sql.types.DecimalType;\n-import org.apache.spark.sql.types.Metadata;\n-import org.apache.spark.sql.types.StructField;\n-import org.apache.spark.sql.types.StructType;\n-\n-public class ArrowUtils {\n-\n-  private static ArrowUtils instance;\n-  private RootAllocator rootAllocator;\n-\n-  private ArrowUtils() {\n-    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n-  }\n-\n-  public static ArrowUtils instance() {\n-    if (instance == null) {\n-      instance = new ArrowUtils();\n-    }\n-    return instance;\n-  }\n-\n-  public RootAllocator rootAllocator() {\n-    return rootAllocator;\n-  }\n-\n-  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-  public DataType fromArrowType(ArrowType data) {\n-\n-    if (data instanceof ArrowType.Bool) {\n-      return DataTypes.BooleanType;\n-    } else if (data instanceof ArrowType.Int) {\n-      ArrowType.Int intData = (ArrowType.Int) data;\n-      if (intData.getIsSigned() && intData.getBitWidth() == 8) {\n-        return DataTypes.ByteType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 2) {\n-        return DataTypes.ShortType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 4) {\n-        return DataTypes.IntegerType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 8) {\n-        return DataTypes.LongType;\n-      }\n-    } else if (data instanceof ArrowType.FloatingPoint) {\n-      ArrowType.FloatingPoint floatData = (ArrowType.FloatingPoint) data;\n-      if (floatData.getPrecision() == FloatingPointPrecision.SINGLE) {\n-        return DataTypes.FloatType;\n-      } else if (floatData.getPrecision() == FloatingPointPrecision.DOUBLE) {\n-        return DataTypes.DoubleType;\n-      }\n-    } else if (data instanceof ArrowType.Utf8) {\n-      return DataTypes.StringType;\n-    } else if (data instanceof ArrowType.Binary) {\n-      return DataTypes.BinaryType;\n-    } else if (data instanceof ArrowType.Decimal) {\n-      ArrowType.Decimal decimalData = (ArrowType.Decimal) data;\n-      return new DecimalType(decimalData.getPrecision(), decimalData.getScale());\n-    } else if (data instanceof ArrowType.Date && ((ArrowType.Date) data).getUnit() == DateUnit.DAY) {\n-      return DataTypes.DateType;\n-    } else if (data instanceof ArrowType.Timestamp && ((ArrowType.Timestamp) data).getUnit() == TimeUnit.MICROSECOND) {\n-      return DataTypes.TimestampType;\n-    }\n-\n-    throw new UnsupportedOperationException(\"Unsupported data type: \" + data);\n-  }\n-\n-  public DataType fromArrowField(Field field) {\n-    ArrowType arrowType = field.getType();\n-    if (arrowType instanceof ArrowType.List) {\n-      Field elementField = field.getChildren().get(0);\n-      DataType elementType = fromArrowField(elementField);\n-      return new ArrayType(elementType, elementField.isNullable());\n-    } else if (arrowType instanceof ArrowType.Struct) {\n-      StructField[] fields = new StructField[field.getChildren().size()];\n-      int index = 0;\n-      for (Field f : field.getChildren()) {\n-        DataType dt = fromArrowField(f);\n-        fields[index++] = new StructField(f.getName(), dt, f.isNullable(), Metadata.empty());\n-      }\n-      return new StructType(fields);\n-    } else {\n-      return fromArrowType(arrowType);\n-    }\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExOTA5NQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389119095", "bodyText": "The data type methods could be static instead. Could you break this class into SparkArrowTypeUtil and something like ArrowAllocation?", "author": "rdblue", "createdAt": "2020-03-06T20:07:50Z", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class ArrowUtils {\n+\n+  private static ArrowUtils instance;\n+  private RootAllocator rootAllocator;\n+\n+  private ArrowUtils() {\n+    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n+  }\n+\n+  public static ArrowUtils instance() {\n+    if (instance == null) {\n+      instance = new ArrowUtils();\n+    }\n+    return instance;\n+  }\n+\n+  public RootAllocator rootAllocator() {\n+    return rootAllocator;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  public DataType fromArrowType(ArrowType data) {\n+\n+    if (data instanceof ArrowType.Bool) {\n+      return DataTypes.BooleanType;\n+    } else if (data instanceof ArrowType.Int) {\n+      ArrowType.Int intData = (ArrowType.Int) data;\n+      if (intData.getIsSigned() && intData.getBitWidth() == 8) {\n+        return DataTypes.ByteType;\n+      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 2) {\n+        return DataTypes.ShortType;\n+      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 4) {\n+        return DataTypes.IntegerType;\n+      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 8) {\n+        return DataTypes.LongType;\n+      }\n+    } else if (data instanceof ArrowType.FloatingPoint) {\n+      ArrowType.FloatingPoint floatData = (ArrowType.FloatingPoint) data;\n+      if (floatData.getPrecision() == FloatingPointPrecision.SINGLE) {\n+        return DataTypes.FloatType;\n+      } else if (floatData.getPrecision() == FloatingPointPrecision.DOUBLE) {\n+        return DataTypes.DoubleType;\n+      }\n+    } else if (data instanceof ArrowType.Utf8) {\n+      return DataTypes.StringType;\n+    } else if (data instanceof ArrowType.Binary) {\n+      return DataTypes.BinaryType;\n+    } else if (data instanceof ArrowType.Decimal) {\n+      ArrowType.Decimal decimalData = (ArrowType.Decimal) data;\n+      return new DecimalType(decimalData.getPrecision(), decimalData.getScale());\n+    } else if (data instanceof ArrowType.Date && ((ArrowType.Date) data).getUnit() == DateUnit.DAY) {\n+      return DataTypes.DateType;\n+    } else if (data instanceof ArrowType.Timestamp && ((ArrowType.Timestamp) data).getUnit() == TimeUnit.MICROSECOND) {\n+      return DataTypes.TimestampType;\n+    }\n+\n+    throw new UnsupportedOperationException(\"Unsupported data type: \" + data);\n+  }\n+\n+  public DataType fromArrowField(Field field) {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java b/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java\ndeleted file mode 100644\nindex 02fbc435d..000000000\n--- a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java\n+++ /dev/null\n\n@@ -1,113 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg.spark.arrow;\n-\n-import org.apache.arrow.memory.RootAllocator;\n-import org.apache.arrow.vector.types.DateUnit;\n-import org.apache.arrow.vector.types.FloatingPointPrecision;\n-import org.apache.arrow.vector.types.TimeUnit;\n-import org.apache.arrow.vector.types.pojo.ArrowType;\n-import org.apache.arrow.vector.types.pojo.Field;\n-import org.apache.spark.sql.types.ArrayType;\n-import org.apache.spark.sql.types.DataType;\n-import org.apache.spark.sql.types.DataTypes;\n-import org.apache.spark.sql.types.DecimalType;\n-import org.apache.spark.sql.types.Metadata;\n-import org.apache.spark.sql.types.StructField;\n-import org.apache.spark.sql.types.StructType;\n-\n-public class ArrowUtils {\n-\n-  private static ArrowUtils instance;\n-  private RootAllocator rootAllocator;\n-\n-  private ArrowUtils() {\n-    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n-  }\n-\n-  public static ArrowUtils instance() {\n-    if (instance == null) {\n-      instance = new ArrowUtils();\n-    }\n-    return instance;\n-  }\n-\n-  public RootAllocator rootAllocator() {\n-    return rootAllocator;\n-  }\n-\n-  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-  public DataType fromArrowType(ArrowType data) {\n-\n-    if (data instanceof ArrowType.Bool) {\n-      return DataTypes.BooleanType;\n-    } else if (data instanceof ArrowType.Int) {\n-      ArrowType.Int intData = (ArrowType.Int) data;\n-      if (intData.getIsSigned() && intData.getBitWidth() == 8) {\n-        return DataTypes.ByteType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 2) {\n-        return DataTypes.ShortType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 4) {\n-        return DataTypes.IntegerType;\n-      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 8) {\n-        return DataTypes.LongType;\n-      }\n-    } else if (data instanceof ArrowType.FloatingPoint) {\n-      ArrowType.FloatingPoint floatData = (ArrowType.FloatingPoint) data;\n-      if (floatData.getPrecision() == FloatingPointPrecision.SINGLE) {\n-        return DataTypes.FloatType;\n-      } else if (floatData.getPrecision() == FloatingPointPrecision.DOUBLE) {\n-        return DataTypes.DoubleType;\n-      }\n-    } else if (data instanceof ArrowType.Utf8) {\n-      return DataTypes.StringType;\n-    } else if (data instanceof ArrowType.Binary) {\n-      return DataTypes.BinaryType;\n-    } else if (data instanceof ArrowType.Decimal) {\n-      ArrowType.Decimal decimalData = (ArrowType.Decimal) data;\n-      return new DecimalType(decimalData.getPrecision(), decimalData.getScale());\n-    } else if (data instanceof ArrowType.Date && ((ArrowType.Date) data).getUnit() == DateUnit.DAY) {\n-      return DataTypes.DateType;\n-    } else if (data instanceof ArrowType.Timestamp && ((ArrowType.Timestamp) data).getUnit() == TimeUnit.MICROSECOND) {\n-      return DataTypes.TimestampType;\n-    }\n-\n-    throw new UnsupportedOperationException(\"Unsupported data type: \" + data);\n-  }\n-\n-  public DataType fromArrowField(Field field) {\n-    ArrowType arrowType = field.getType();\n-    if (arrowType instanceof ArrowType.List) {\n-      Field elementField = field.getChildren().get(0);\n-      DataType elementType = fromArrowField(elementField);\n-      return new ArrayType(elementType, elementField.isNullable());\n-    } else if (arrowType instanceof ArrowType.Struct) {\n-      StructField[] fields = new StructField[field.getChildren().size()];\n-      int index = 0;\n-      for (Field f : field.getChildren()) {\n-        DataType dt = fromArrowField(f);\n-        fields[index++] = new StructField(f.getName(), dt, f.isNullable(), Metadata.empty());\n-      }\n-      return new StructType(fields);\n-    } else {\n-      return fromArrowType(arrowType);\n-    }\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMDI0NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389120244", "bodyText": "I thought we supported maps?", "author": "rdblue", "createdAt": "2020-03-06T20:10:29Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY2NTk5Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390665992", "bodyText": "Not currently, no.", "author": "samarthjain", "createdAt": "2020-03-10T23:19:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMDI0NA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMDg0Ng==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389120846", "bodyText": "Doesn't Spark check nullability and only call these methods if isNullAt is false? I thought that was why it isn't necessary to check this in the primitive methods, like getLong.", "author": "rdblue", "createdAt": "2020-03-06T20:11:42Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMjg1OA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389122858", "bodyText": "I think this should be moved out of this method. There's no need for this method to have side-effects, and this makes it hard to understand how this class works because childColumns appears to be uninitialized when reading the constructor. I'd much rather add childColumns() to the ArrowVectorAccessor interface and initialize like this:\n  this.accessor = VectorAccessors.get(vector);\n  this.childColumns = accessor.childColumns();\n\nYou could also remove the childColumns field and always call accessor.childColumns().", "author": "rdblue", "createdAt": "2020-03-06T20:16:31Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNDQ0Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389124447", "bodyText": "It would be nice to have an error message with the type that was accessed.", "author": "rdblue", "createdAt": "2020-03-06T20:20:26Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNjI3NQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389126275", "bodyText": "Can we move these to a separate file? The convention used elsewhere is to put ArrowVectorAccessor in a file and the implementations in a ArrowVectorAccessors class with a private constructor.", "author": "rdblue", "createdAt": "2020-03-06T20:24:36Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNjYzMQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389126631", "bodyText": "Isn't the use of final in Arrow causing us trouble? Why use final here? Does it have a performance benefit?", "author": "rdblue", "createdAt": "2020-03-06T20:25:27Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNzEzMQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389127131", "bodyText": "ArrowVectorAccessor doesn't define getStruct. Do you mean getChild?", "author": "rdblue", "createdAt": "2020-03-06T20:26:33Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector vector;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDateAccessor extends DictionaryIntAccessor {\n+    DictionaryDateAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class TimestampAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector vector;\n+\n+    TimestampAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryTimestampAccessor extends DictionaryLongAccessor {\n+    DictionaryTimestampAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class ArrayAccessor extends ArrowVectorAccessor {\n+\n+    private final ListVector vector;\n+    private final ArrowColumnVector arrayData;\n+\n+    ArrayAccessor(ListVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+      this.arrayData = new ArrowColumnVector(vector.getDataVector());\n+    }\n+\n+    @Override\n+    final ColumnarArray getArray(int rowId) {\n+      ArrowBuf offsets = vector.getOffsetBuffer();\n+      int index = rowId * ListVector.OFFSET_WIDTH;\n+      int start = offsets.getInt(index);\n+      int end = offsets.getInt(index + ListVector.OFFSET_WIDTH);\n+      return new ColumnarArray(arrayData, start, end - start);\n+    }\n+  }\n+\n+  /**\n+   * Any call to \"get\" method will throw UnsupportedOperationException.\n+   * <p>\n+   * Access struct values in a ArrowColumnVector doesn't use this vector. Instead, it uses getStruct() method defined in\n+   * the parent class. Any call to \"get\" method in this class is a bug in the code.", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNzY2NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389127664", "bodyText": "I think these should process the dictionary first to transform each value to a Decimal and then use that table instead of the dictionary at read time.", "author": "rdblue", "createdAt": "2020-03-06T20:27:46Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector vector;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDateAccessor extends DictionaryIntAccessor {\n+    DictionaryDateAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class TimestampAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector vector;\n+\n+    TimestampAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryTimestampAccessor extends DictionaryLongAccessor {\n+    DictionaryTimestampAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class ArrayAccessor extends ArrowVectorAccessor {\n+\n+    private final ListVector vector;\n+    private final ArrowColumnVector arrayData;\n+\n+    ArrayAccessor(ListVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+      this.arrayData = new ArrowColumnVector(vector.getDataVector());\n+    }\n+\n+    @Override\n+    final ColumnarArray getArray(int rowId) {\n+      ArrowBuf offsets = vector.getOffsetBuffer();\n+      int index = rowId * ListVector.OFFSET_WIDTH;\n+      int start = offsets.getInt(index);\n+      int end = offsets.getInt(index + ListVector.OFFSET_WIDTH);\n+      return new ColumnarArray(arrayData, start, end - start);\n+    }\n+  }\n+\n+  /**\n+   * Any call to \"get\" method will throw UnsupportedOperationException.\n+   * <p>\n+   * Access struct values in a ArrowColumnVector doesn't use this vector. Instead, it uses getStruct() method defined in\n+   * the parent class. Any call to \"get\" method in this class is a bug in the code.\n+   */\n+  private class StructAccessor extends ArrowVectorAccessor {\n+\n+    StructAccessor(StructVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class DictionaryDecimalBinaryAccessor extends ArrowVectorAccessor {\n+    private final IntVector vector;\n+\n+    DictionaryDecimalBinaryAccessor(IntVector vector, int precision, int scale) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    //TODO: still need to evaluate if this is the most efficient way\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary value = dictionary.decodeToBinary(vector.get(rowId));\n+      BigInteger unscaledValue = new BigInteger(value.getBytesUnsafe());\n+      return Decimal.apply(unscaledValue.longValue(), precision, scale);\n+    }\n+  }\n+\n+  private class DictionaryDecimalLongAccessor extends ArrowVectorAccessor {\n+    private final IntVector vector;\n+\n+    DictionaryDecimalLongAccessor(IntVector vector, int precision, int scale) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    //TODO: still need to evaluate if this is the most efficient way\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      long unscaledValue = dictionary.decodeToLong(vector.get(rowId));\n+      return Decimal.apply(unscaledValue, precision, scale);", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyODQwMA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389128400", "bodyText": "Nit: the indentation here was correct before.", "author": "rdblue", "createdAt": "2020-03-06T20:29:26Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -91,20 +64,16 @@\n import org.apache.spark.sql.sources.v2.reader.SupportsPushDownFilters;\n import org.apache.spark.sql.sources.v2.reader.SupportsPushDownRequiredColumns;\n import org.apache.spark.sql.sources.v2.reader.SupportsReportStatistics;\n-import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.sources.v2.reader.SupportsScanColumnarBatch;\n import org.apache.spark.sql.types.DataType;\n-import org.apache.spark.sql.types.Decimal;\n-import org.apache.spark.sql.types.DecimalType;\n-import org.apache.spark.sql.types.StringType;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n-import org.apache.spark.unsafe.types.UTF8String;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-import scala.collection.JavaConverters;\n \n-class Reader implements DataSourceReader, SupportsPushDownFilters, SupportsPushDownRequiredColumns,\n-    SupportsReportStatistics {\n+class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPushDownFilters,\n+        SupportsPushDownRequiredColumns, SupportsReportStatistics {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -73,7 +71,7 @@ import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPushDownFilters,\n-        SupportsPushDownRequiredColumns, SupportsReportStatistics {\n+    SupportsPushDownRequiredColumns, SupportsReportStatistics {\n   private static final Logger LOG = LoggerFactory.getLogger(Reader.class);\n \n   private static final Filter[] NO_FILTERS = new Filter[0];\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyODQ1OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389128459", "bodyText": "Nit: no need to remove this line.", "author": "rdblue", "createdAt": "2020-03-06T20:29:35Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -158,11 +129,19 @@\n     } else {\n       this.localityPreferred = false;\n     }\n-", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -129,25 +148,25 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n     } else {\n       this.localityPreferred = false;\n     }\n+\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n-    // override logic to check when batched reads is enabled by turning off batched reads\n-    boolean disableBatchedReads =\n-            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);\n-    if (disableBatchedReads) {\n-      enableBatchRead = Boolean.FALSE;\n-    }\n-    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.numrecordsperbatch\");\n-    this.batchSize =\n-        numRecordsPerBatchOpt.map(Integer::parseInt).orElse(VectorizedArrowReader.DEFAULT_BATCH_SIZE);\n+\n+    this.batchReadsEnabled = options.get(\"vectorization-enabled\").map(Boolean::parseBoolean).orElse(\n+        PropertyUtil.propertyAsBoolean(table.properties(),\n+            TableProperties.PARQUET_VECTORIZATION_ENABLED, TableProperties.PARQUET_VECTORIZATION_ENABLED_DEFAULT));\n+    this.batchSize = options.get(\"batch-size\").map(Integer::parseInt).orElse(\n+        PropertyUtil.propertyAsInt(table.properties(),\n+          TableProperties.PARQUET_BATCH_SIZE, TableProperties.PARQUET_BATCH_SIZE_DEFAULT));\n   }\n \n   private Schema lazySchema() {\n     if (schema == null) {\n       if (requestedSchema != null) {\n-        this.schema = SparkSchemaUtil.prune(table.schema(), requestedSchema);\n+        // the projection should include all columns that will be returned, including those only used in filters\n+        this.schema = SparkSchemaUtil.prune(table.schema(), requestedSchema, filterExpression(), caseSensitive);\n       } else {\n         this.schema = table.schema();\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTEwMQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389129101", "bodyText": "Options that enable/disable should use \"enabled\". That avoids users needing to think about whether options enable or disable - true always enables and false always disables.\nI recommend using iceberg.read.parquet-vectorization.enabled.", "author": "rdblue", "createdAt": "2020-03-06T20:31:20Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -158,11 +129,19 @@\n     } else {\n       this.localityPreferred = false;\n     }\n-\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+    // override logic to check when batched reads is enabled by turning off batched reads\n+    boolean disableBatchedReads =\n+            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -129,25 +148,25 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n     } else {\n       this.localityPreferred = false;\n     }\n+\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n-    // override logic to check when batched reads is enabled by turning off batched reads\n-    boolean disableBatchedReads =\n-            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);\n-    if (disableBatchedReads) {\n-      enableBatchRead = Boolean.FALSE;\n-    }\n-    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.numrecordsperbatch\");\n-    this.batchSize =\n-        numRecordsPerBatchOpt.map(Integer::parseInt).orElse(VectorizedArrowReader.DEFAULT_BATCH_SIZE);\n+\n+    this.batchReadsEnabled = options.get(\"vectorization-enabled\").map(Boolean::parseBoolean).orElse(\n+        PropertyUtil.propertyAsBoolean(table.properties(),\n+            TableProperties.PARQUET_VECTORIZATION_ENABLED, TableProperties.PARQUET_VECTORIZATION_ENABLED_DEFAULT));\n+    this.batchSize = options.get(\"batch-size\").map(Integer::parseInt).orElse(\n+        PropertyUtil.propertyAsInt(table.properties(),\n+          TableProperties.PARQUET_BATCH_SIZE, TableProperties.PARQUET_BATCH_SIZE_DEFAULT));\n   }\n \n   private Schema lazySchema() {\n     if (schema == null) {\n       if (requestedSchema != null) {\n-        this.schema = SparkSchemaUtil.prune(table.schema(), requestedSchema);\n+        // the projection should include all columns that will be returned, including those only used in filters\n+        this.schema = SparkSchemaUtil.prune(table.schema(), requestedSchema, filterExpression(), caseSensitive);\n       } else {\n         this.schema = table.schema();\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTQ0Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389129442", "bodyText": "How about iceberg.read.parquet-vectorization.batch-size?", "author": "rdblue", "createdAt": "2020-03-06T20:32:17Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -158,11 +129,19 @@\n     } else {\n       this.localityPreferred = false;\n     }\n-\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+    // override logic to check when batched reads is enabled by turning off batched reads\n+    boolean disableBatchedReads =\n+            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);\n+    if (disableBatchedReads) {\n+      enableBatchRead = Boolean.FALSE;\n+    }\n+    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.numrecordsperbatch\");", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -129,25 +148,25 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n     } else {\n       this.localityPreferred = false;\n     }\n+\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n-    // override logic to check when batched reads is enabled by turning off batched reads\n-    boolean disableBatchedReads =\n-            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);\n-    if (disableBatchedReads) {\n-      enableBatchRead = Boolean.FALSE;\n-    }\n-    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.numrecordsperbatch\");\n-    this.batchSize =\n-        numRecordsPerBatchOpt.map(Integer::parseInt).orElse(VectorizedArrowReader.DEFAULT_BATCH_SIZE);\n+\n+    this.batchReadsEnabled = options.get(\"vectorization-enabled\").map(Boolean::parseBoolean).orElse(\n+        PropertyUtil.propertyAsBoolean(table.properties(),\n+            TableProperties.PARQUET_VECTORIZATION_ENABLED, TableProperties.PARQUET_VECTORIZATION_ENABLED_DEFAULT));\n+    this.batchSize = options.get(\"batch-size\").map(Integer::parseInt).orElse(\n+        PropertyUtil.propertyAsInt(table.properties(),\n+          TableProperties.PARQUET_BATCH_SIZE, TableProperties.PARQUET_BATCH_SIZE_DEFAULT));\n   }\n \n   private Schema lazySchema() {\n     if (schema == null) {\n       if (requestedSchema != null) {\n-        this.schema = SparkSchemaUtil.prune(table.schema(), requestedSchema);\n+        // the projection should include all columns that will be returned, including those only used in filters\n+        this.schema = SparkSchemaUtil.prune(table.schema(), requestedSchema, filterExpression(), caseSensitive);\n       } else {\n         this.schema = table.schema();\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDI2OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389130269", "bodyText": "Javadoc: Instead of [...], did you mean to use either {@link ...} to link to the class, or {@code ...} to use fixed-width font?", "author": "rdblue", "createdAt": "2020-03-06T20:34:17Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -168,26 +194,29 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   }\n \n   /**\n-   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   * This is called in the Spark Driver when data is to be materialized into {@link ColumnarBatch}\n    */\n   @Override\n   public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n-    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");\n+    Preconditions.checkState(enableBatchRead(), \"Batched reads not enabled\");\n     Preconditions.checkState(batchSize > 0, \"Invalid batch size\");\n     String tableSchemaString = SchemaParser.toJson(table.schema());\n     String expectedSchemaString = SchemaParser.toJson(lazySchema());\n \n     List<InputPartition<ColumnarBatch>> readTasks = Lists.newArrayList();\n     for (CombinedScanTask task : tasks()) {\n-      readTasks.add(\n-              new ColumnarBatchReadTask(task, tableSchemaString, expectedSchemaString,\n-                      io, encryptionManager, caseSensitive, batchSize));\n+      readTasks.add(new ReadTask<>(\n+          task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive, localityPreferred,\n+          new BatchReaderFactory(batchSize)));\n     }\n-    LOG.info(\"=> Batching input partitions with {} tasks.\", readTasks.size());\n+    LOG.info(\"Batching input partitions with {} tasks.\", readTasks.size());\n \n     return readTasks;\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into {@link InternalRow}\n+   */\n   @Override\n   public List<InputPartition<InternalRow>> planInputPartitions() {\n     String tableSchemaString = SchemaParser.toJson(table.schema());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDY5Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389130692", "bodyText": "Can we move this refactor into a separate PR?", "author": "rdblue", "createdAt": "2020-03-06T20:35:26Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class InternalRowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3NjExMg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r394576112", "bodyText": "I am going to leave these changes in this PR and start a new PR for the refactor. Once the refactor PR is approved, I will merge those changes into this PR.", "author": "samarthjain", "createdAt": "2020-03-18T19:03:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDY5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDY2MTkzOQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r394661939", "bodyText": "Opened #853", "author": "samarthjain", "createdAt": "2020-03-18T21:55:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDY5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java b/spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java\nsimilarity index 50%\nrename from spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java\nrename to spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java\nindex 65563c16b..c0e46ebd0 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java\n\n@@ -19,137 +19,109 @@\n \n package org.apache.iceberg.spark.source;\n \n-import com.google.common.base.Preconditions;\n-import com.google.common.collect.Iterators;\n-import com.google.common.collect.Lists;\n+import java.math.BigDecimal;\n import java.nio.ByteBuffer;\n-import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n import java.util.Set;\n-import java.util.function.Function;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataTask;\n import org.apache.iceberg.FileScanTask;\n-import org.apache.iceberg.PartitionField;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.StructLike;\n import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.common.DynMethods;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.orc.ORC;\n import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.data.SparkAvroReader;\n import org.apache.iceberg.spark.data.SparkOrcReader;\n import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.TypeUtil;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.PartitionUtil;\n import org.apache.spark.rdd.InputFileBlockHolder;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.catalyst.expressions.Attribute;\n import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n-import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n-import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n-import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n-import org.apache.spark.sql.types.BinaryType;\n-import org.apache.spark.sql.types.DataType;\n import org.apache.spark.sql.types.Decimal;\n-import org.apache.spark.sql.types.DecimalType;\n-import org.apache.spark.sql.types.StringType;\n-import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.unsafe.types.UTF8String;\n import scala.collection.JavaConverters;\n \n-class InternalRowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {\n+class RowDataReader extends BaseDataReader<InternalRow> {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  private static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n+      .impl(UnsafeProjection.class, InternalRow.class)\n+      .build();\n \n-  InternalRowTaskDataReader(\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+\n+  RowDataReader(\n       CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n       EncryptionManager encryptionManager, boolean caseSensitive) {\n-    super(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive);\n-  }\n-\n-  @Override\n-  public InternalRow get() {\n-    return current;\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n   }\n \n   @Override\n-  Iterator<InternalRow> open(FileScanTask task) {\n+  CloseableIterator<InternalRow> open(FileScanTask task) {\n     DataFile file = task.file();\n \n     // update the current file for Spark's filename() function\n     InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n \n     // schema or rows returned by readers\n-    Schema finalSchema = expectedSchema;\n     PartitionSpec spec = task.spec();\n     Set<Integer> idColumns = spec.identitySourceIds();\n+    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n-    // schema needed for the projection and filtering\n-    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n-    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n-    boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n-    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n-\n-    Schema iterSchema;\n-    Iterator<InternalRow> iter;\n-\n-    if (hasJoinedPartitionColumns) {\n-      // schema used to read data files\n-      Schema readSchema = TypeUtil.selectNot(requiredSchema, idColumns);\n-      Schema partitionSchema = TypeUtil.select(requiredSchema, idColumns);\n-      PartitionRowConverter convertToRow = new PartitionRowConverter(partitionSchema, spec);\n-      JoinedRow joined = new JoinedRow();\n-\n-      InternalRow partition = convertToRow.apply(file.partition());\n-      joined.withRight(partition);\n-\n-      // create joined rows and project from the joined schema to the final schema\n-      iterSchema = TypeUtil.join(readSchema, partitionSchema);\n-      iter = Iterators.transform(open(task, readSchema), joined::withLeft);\n-    } else if (hasExtraFilterColumns) {\n-      // add projection to the final schema\n-      iterSchema = requiredSchema;\n-      iter = open(task, requiredSchema);\n-    } else {\n-      // return the base iterator\n-      iterSchema = finalSchema;\n-      iter = open(task, finalSchema);\n+    if (projectsIdentityPartitionColumns) {\n+      return open(task, expectedSchema, PartitionUtil.constantsMap(task, RowDataReader::convertConstant))\n+          .iterator();\n     }\n-\n-    // TODO: remove the projection by reporting the iterator's schema back to Spark\n-    return Iterators.transform(\n-        iter,\n-        APPLY_PROJECTION.bind(projection(finalSchema, iterSchema))::invoke);\n+    // return the base iterator\n+    return open(task, expectedSchema, ImmutableMap.of()).iterator();\n   }\n \n-  private Iterator<InternalRow> open(FileScanTask task, Schema readSchema) {\n+  private CloseableIterable<InternalRow> open(FileScanTask task, Schema readSchema, Map<Integer, ?> idToConstant) {\n     CloseableIterable<InternalRow> iter;\n-    //TODO: samarth can there be a data task for columnar batch counterpart?\n     if (task.isDataTask()) {\n       iter = newDataIterable(task.asDataTask(), readSchema);\n     } else {\n-      InputFile location = inputFiles.get(task.file().path().toString());\n+      InputFile location = getInputFile(task);\n       Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n \n       switch (task.file().format()) {\n         case PARQUET:\n-          iter = newParquetIterable(location, task, readSchema);\n+          iter = newParquetIterable(location, task, readSchema, idToConstant);\n           break;\n \n         case AVRO:\n-          iter = newAvroIterable(location, task, readSchema);\n+          iter = newAvroIterable(location, task, readSchema, idToConstant);\n           break;\n \n         case ORC:\n-          iter = newOrcIterable(location, task, readSchema);\n+          iter = newOrcIterable(location, task, readSchema, idToConstant);\n           break;\n \n         default:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTE0Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389131147", "bodyText": "It would be nice to make the names shorter. What about BaseTaskReader, BatchTaskReader, and RowTaskReader?", "author": "rdblue", "createdAt": "2020-03-06T20:36:26Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ColumnarBatchTaskDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class ColumnarBatchTaskDataReader extends BaseTaskDataReader<ColumnarBatch>", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/ColumnarBatchTaskDataReader.java b/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/source/ColumnarBatchTaskDataReader.java\nrename to spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\nindex aa470759c..eeb3ad559 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/ColumnarBatchTaskDataReader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\n\n@@ -19,68 +19,50 @@\n \n package org.apache.iceberg.spark.source;\n \n-import com.google.common.base.Preconditions;\n-import java.util.Iterator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.parquet.Parquet;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n-import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n-import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n \n-class ColumnarBatchTaskDataReader extends BaseTaskDataReader<ColumnarBatch>\n-    implements InputPartitionReader<ColumnarBatch> {\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n \n-  ColumnarBatchTaskDataReader(\n-      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n-      EncryptionManager encryptionManager, boolean caseSensitive, int bSize) {\n-    super(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive, bSize);\n+  BatchDataReader(\n+      CombinedScanTask task, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n   }\n \n   @Override\n-  public ColumnarBatch get() {\n-    return current;\n-  }\n-\n-  @Override\n-  Iterator<ColumnarBatch> open(FileScanTask task) {\n-    // schema or rows returned by readers\n-    Schema finalSchema = expectedSchema;\n-    // schema needed for the projection and filtering\n-    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n-    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n-    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n-    Iterator<ColumnarBatch> iter;\n-    if (hasExtraFilterColumns) {\n-      iter = open(task, requiredSchema);\n-    } else {\n-      iter = open(task, finalSchema);\n-    }\n-    return iter;\n-  }\n-\n-  private Iterator<ColumnarBatch> open(FileScanTask task, Schema readSchema) {\n+  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n     CloseableIterable<ColumnarBatch> iter;\n-    InputFile location = inputFiles.get(task.file().path().toString());\n+    InputFile location = getInputFile(task);\n     Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n     if (task.file().format() == FileFormat.PARQUET) {\n       iter = Parquet.read(location)\n-          .project(readSchema)\n+          .project(expectedSchema)\n           .split(task.start(), task.length())\n-          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(tableSchema, readSchema,\n-              fileSchema, batchSize))\n+          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n+              fileSchema, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED))\n+          .recordsPerBatch(batchSize)\n           .filter(task.residual())\n           .caseSensitive(caseSensitive)\n-          .recordsPerBatch(batchSize)\n-          // Spark eagerly consumes the batches so the underlying memory allocated could be reused\n+          // Spark eagerly consumes the batches. So the underlying memory allocated could be reused\n           // without worrying about subsequent reads clobbering over each other. This improves\n           // read performance as every batch read doesn't have to pay the cost of allocating memory.\n           .reuseContainers()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ3MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389131471", "bodyText": "Nit: Can you remove =>?", "author": "rdblue", "createdAt": "2020-03-06T20:37:08Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n+    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");\n+    Preconditions.checkState(batchSize > 0, \"Invalid batch size\");\n+    String tableSchemaString = SchemaParser.toJson(table.schema());\n+    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n+\n+    List<InputPartition<ColumnarBatch>> readTasks = Lists.newArrayList();\n+    for (CombinedScanTask task : tasks()) {\n+      readTasks.add(\n+              new ColumnarBatchReadTask(task, tableSchemaString, expectedSchemaString,\n+                      io, encryptionManager, caseSensitive, batchSize));\n+    }\n+    LOG.info(\"=> Batching input partitions with {} tasks.\", readTasks.size());", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -168,26 +194,29 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   }\n \n   /**\n-   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   * This is called in the Spark Driver when data is to be materialized into {@link ColumnarBatch}\n    */\n   @Override\n   public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n-    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");\n+    Preconditions.checkState(enableBatchRead(), \"Batched reads not enabled\");\n     Preconditions.checkState(batchSize > 0, \"Invalid batch size\");\n     String tableSchemaString = SchemaParser.toJson(table.schema());\n     String expectedSchemaString = SchemaParser.toJson(lazySchema());\n \n     List<InputPartition<ColumnarBatch>> readTasks = Lists.newArrayList();\n     for (CombinedScanTask task : tasks()) {\n-      readTasks.add(\n-              new ColumnarBatchReadTask(task, tableSchemaString, expectedSchemaString,\n-                      io, encryptionManager, caseSensitive, batchSize));\n+      readTasks.add(new ReadTask<>(\n+          task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive, localityPreferred,\n+          new BatchReaderFactory(batchSize)));\n     }\n-    LOG.info(\"=> Batching input partitions with {} tasks.\", readTasks.size());\n+    LOG.info(\"Batching input partitions with {} tasks.\", readTasks.size());\n \n     return readTasks;\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into {@link InternalRow}\n+   */\n   @Override\n   public List<InputPartition<InternalRow>> planInputPartitions() {\n     String tableSchemaString = SchemaParser.toJson(table.schema());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTgxNQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389131815", "bodyText": "Why would enableBatchRead be null? Why not use a boolean instead?", "author": "rdblue", "createdAt": "2020-03-06T20:37:53Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n+    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzODE2OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390638169", "bodyText": "enableBatchRead is of type Boolean. It is lazily initialized in enableBatchRead().", "author": "samarthjain", "createdAt": "2020-03-10T22:05:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzc4Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410527787", "bodyText": "Accessing the enableBatchRead that is lazily computed creates an order dependency between this and enableBatchRead(). I'd rather simplify this and use the method call directly.", "author": "rdblue", "createdAt": "2020-04-18T00:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTgxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -168,26 +194,29 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   }\n \n   /**\n-   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   * This is called in the Spark Driver when data is to be materialized into {@link ColumnarBatch}\n    */\n   @Override\n   public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n-    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");\n+    Preconditions.checkState(enableBatchRead(), \"Batched reads not enabled\");\n     Preconditions.checkState(batchSize > 0, \"Invalid batch size\");\n     String tableSchemaString = SchemaParser.toJson(table.schema());\n     String expectedSchemaString = SchemaParser.toJson(lazySchema());\n \n     List<InputPartition<ColumnarBatch>> readTasks = Lists.newArrayList();\n     for (CombinedScanTask task : tasks()) {\n-      readTasks.add(\n-              new ColumnarBatchReadTask(task, tableSchemaString, expectedSchemaString,\n-                      io, encryptionManager, caseSensitive, batchSize));\n+      readTasks.add(new ReadTask<>(\n+          task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive, localityPreferred,\n+          new BatchReaderFactory(batchSize)));\n     }\n-    LOG.info(\"=> Batching input partitions with {} tasks.\", readTasks.size());\n+    LOG.info(\"Batching input partitions with {} tasks.\", readTasks.size());\n \n     return readTasks;\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into {@link InternalRow}\n+   */\n   @Override\n   public List<InputPartition<InternalRow>> planInputPartitions() {\n     String tableSchemaString = SchemaParser.toJson(table.schema());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjEzNQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389132135", "bodyText": "Style: control flow statements should be followed by empty lines.", "author": "rdblue", "createdAt": "2020-03-06T20:38:35Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -259,6 +259,43 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();\n+  }\n+\n+  private boolean lazyCheckEnableBatchRead() {\n+    boolean allParquetFiles =\n+            tasks().stream()\n+                    .allMatch(combinedScanTask -> combinedScanTask.files()\n+                            .stream()\n+                            .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                                    FileFormat.PARQUET)));\n+    if (!allParquetFiles) {\n+      this.enableBatchRead = false;\n+      return false;\n+    }", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -261,39 +290,34 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n \n   @Override\n   public boolean enableBatchRead() {\n-    return lazyCheckEnableBatchRead();\n-  }\n+    if (readUsingBatch == null) {\n+      boolean allParquetFileScanTasks =\n+          tasks().stream()\n+              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n+                  .stream()\n+                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                      FileFormat.PARQUET)));\n \n-  private boolean lazyCheckEnableBatchRead() {\n-    boolean allParquetFiles =\n-            tasks().stream()\n-                    .allMatch(combinedScanTask -> combinedScanTask.files()\n-                            .stream()\n-                            .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n-                                    FileFormat.PARQUET)));\n-    if (!allParquetFiles) {\n-      this.enableBatchRead = false;\n-      return false;\n-    }\n-    int numColumns = lazySchema().columns().size();\n-    if (numColumns == 0) {\n-      this.enableBatchRead = false;\n-      return false;\n-    }\n-    boolean projectIdentityPartitionColumn =\n-            tasks().stream()\n-                    .anyMatch(combinedScanTask -> combinedScanTask.files()\n-                            .stream()\n-                            .anyMatch(fileScanTask -> !fileScanTask.spec().identitySourceIds().isEmpty()));\n-    if (projectIdentityPartitionColumn) {\n-      this.enableBatchRead = false;\n-      return false;\n-    }\n-    if (enableBatchRead == null) {\n-      // Enable batched reads only if all requested columns are primitive otherwise revert to row-based reads\n-      this.enableBatchRead = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n+      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n+\n+      boolean hasNoIdentityProjections = tasks().stream()\n+          .allMatch(combinedScanTask -> combinedScanTask.files()\n+              .stream()\n+              .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n+\n+      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n+\n+      this.readUsingBatch = batchReadsEnabled && allParquetFileScanTasks && atLeastOneColumn &&\n+          hasNoIdentityProjections && onlyPrimitives;\n     }\n-    return enableBatchRead;\n+    return readUsingBatch;\n+  }\n+\n+  private static void mergeIcebergHadoopConfs(\n+      Configuration baseConf, Map<String, String> options) {\n+    options.keySet().stream()\n+        .filter(key -> key.startsWith(\"hadoop.\"))\n+        .forEach(key -> baseConf.set(key.replaceFirst(\"hadoop.\", \"\"), options.get(key)));\n   }\n \n   private List<CombinedScanTask> tasks() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjkzNw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389132937", "bodyText": "Style: We typically indent method args from the same location. That would be either like it was before, or moving all of the method args to use the 4-space continuation indent.", "author": "rdblue", "createdAt": "2020-03-06T20:40:32Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -343,113 +375,103 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   public String toString() {\n     return String.format(\n         \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-  private static class BaseReadTask implements Serializable {\n-    final CombinedScanTask task;\n+  private static class ReadTask<T> implements Serializable, InputPartition<T> {\n+    private final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    final Broadcast<FileIO> io;\n-    final Broadcast<EncryptionManager> encryptionManager;\n-    final boolean caseSensitive;\n+    private final Broadcast<FileIO> io;\n+    private final Broadcast<EncryptionManager> encryptionManager;\n+    private final boolean caseSensitive;\n+    private final boolean localityPreferred;\n+    private final ReaderFactory<T> readerFactory;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n+    private transient String[] preferredLocations;\n \n-    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive) {\n+    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+                     boolean caseSensitive, boolean localityPreferred, ReaderFactory<T> readerFactory) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n+      this.localityPreferred = localityPreferred;\n+      this.preferredLocations = getPreferredLocations();\n+      this.readerFactory = readerFactory;\n+    }\n+\n+    @Override\n+    public InputPartitionReader<T> createPartitionReader() {\n+      return readerFactory.create(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n+          encryptionManager.value(), caseSensitive);\n+    }\n+\n+    @Override\n+    public String[] preferredLocations() {\n+      return preferredLocations;\n     }\n \n-    Schema lazyTableSchema() {\n+    private Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    Schema lazyExpectedSchema() {\n+    private Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+    private String[] getPreferredLocations() {\n+      if (!localityPreferred) {\n+        return new String[0];\n+      }\n \n+      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+      return Util.blockLocations(task, conf);\n+    }\n   }\n \n-  private static class InternalRowReadTask extends BaseReadTask implements InputPartition<InternalRow> {\n-    private final boolean localityPreferred;\n-    private transient String[] preferredLocations;\n+  private interface ReaderFactory<T> extends Serializable {\n+    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n+                                   EncryptionManager encryptionManager, boolean caseSensitive);\n+  }\n \n-    private InternalRowReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive, boolean localityPref) {\n-      super(task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive);\n-      this.localityPreferred = localityPref;\n-      this.preferredLocations = getPreferredLocations();\n-    }\n+  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n+    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new InternalRowTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive);\n+    private InternalRowReaderFactory() {\n     }\n \n     @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private String[] getPreferredLocations() {\n-      if (!localityPreferred) {\n-        return new String[0];\n-      }\n-\n-      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n-      Set<String> locations = Sets.newHashSet();\n-      for (FileScanTask f : task.files()) {\n-        Path path = new Path(f.file().path().toString());\n-        try {\n-          FileSystem fs = path.getFileSystem(conf);\n-          for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n-            locations.addAll(Arrays.asList(b.getHosts()));\n-          }\n-        } catch (IOException ioe) {\n-          LOG.warn(\"Failed to get block locations for path {}\", path, ioe);\n-        }\n-      }\n-\n-      return locations.toArray(new String[0]);\n+    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n     }\n   }\n \n-  /**\n-   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n-   */\n-  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {\n+  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n     private final int batchSize;\n \n-    ColumnarBatchReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString, Broadcast<FileIO> fileIo,\n-        Broadcast<EncryptionManager> encryptionManager, boolean caseSensitive, int numRecordsPerBatch) {\n-      super(task, tableSchemaString, expectedSchemaString, fileIo, encryptionManager, caseSensitive);\n-      this.batchSize = numRecordsPerBatch;\n+    BatchReaderFactory(int batchSize) {\n+      this.batchSize = batchSize;\n     }\n \n     @Override\n-    public InputPartitionReader<ColumnarBatch> createPartitionReader() {\n-      return new ColumnarBatchTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive, batchSize);\n+    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMzIzNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389133234", "bodyText": "Nit: unnecessary newline.", "author": "rdblue", "createdAt": "2020-03-06T20:41:16Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+        boolean caseSensitive) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n-      this.localityPreferred = localityPreferred;\n-      this.preferredLocations = getPreferredLocations();\n     }\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new TaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-        encryptionManager.value(), caseSensitive);\n-    }\n-\n-    @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private Schema lazyTableSchema() {\n+    Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    private Schema lazyExpectedSchema() {\n+    Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -343,113 +375,103 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   public String toString() {\n     return String.format(\n         \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-  private static class BaseReadTask implements Serializable {\n-    final CombinedScanTask task;\n+  private static class ReadTask<T> implements Serializable, InputPartition<T> {\n+    private final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    final Broadcast<FileIO> io;\n-    final Broadcast<EncryptionManager> encryptionManager;\n-    final boolean caseSensitive;\n+    private final Broadcast<FileIO> io;\n+    private final Broadcast<EncryptionManager> encryptionManager;\n+    private final boolean caseSensitive;\n+    private final boolean localityPreferred;\n+    private final ReaderFactory<T> readerFactory;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n+    private transient String[] preferredLocations;\n \n-    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive) {\n+    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+                     boolean caseSensitive, boolean localityPreferred, ReaderFactory<T> readerFactory) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n+      this.localityPreferred = localityPreferred;\n+      this.preferredLocations = getPreferredLocations();\n+      this.readerFactory = readerFactory;\n+    }\n+\n+    @Override\n+    public InputPartitionReader<T> createPartitionReader() {\n+      return readerFactory.create(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n+          encryptionManager.value(), caseSensitive);\n+    }\n+\n+    @Override\n+    public String[] preferredLocations() {\n+      return preferredLocations;\n     }\n \n-    Schema lazyTableSchema() {\n+    private Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    Schema lazyExpectedSchema() {\n+    private Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+    private String[] getPreferredLocations() {\n+      if (!localityPreferred) {\n+        return new String[0];\n+      }\n \n+      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+      return Util.blockLocations(task, conf);\n+    }\n   }\n \n-  private static class InternalRowReadTask extends BaseReadTask implements InputPartition<InternalRow> {\n-    private final boolean localityPreferred;\n-    private transient String[] preferredLocations;\n+  private interface ReaderFactory<T> extends Serializable {\n+    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n+                                   EncryptionManager encryptionManager, boolean caseSensitive);\n+  }\n \n-    private InternalRowReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive, boolean localityPref) {\n-      super(task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive);\n-      this.localityPreferred = localityPref;\n-      this.preferredLocations = getPreferredLocations();\n-    }\n+  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n+    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new InternalRowTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive);\n+    private InternalRowReaderFactory() {\n     }\n \n     @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private String[] getPreferredLocations() {\n-      if (!localityPreferred) {\n-        return new String[0];\n-      }\n-\n-      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n-      Set<String> locations = Sets.newHashSet();\n-      for (FileScanTask f : task.files()) {\n-        Path path = new Path(f.file().path().toString());\n-        try {\n-          FileSystem fs = path.getFileSystem(conf);\n-          for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n-            locations.addAll(Arrays.asList(b.getHosts()));\n-          }\n-        } catch (IOException ioe) {\n-          LOG.warn(\"Failed to get block locations for path {}\", path, ioe);\n-        }\n-      }\n-\n-      return locations.toArray(new String[0]);\n+    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n     }\n   }\n \n-  /**\n-   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n-   */\n-  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {\n+  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n     private final int batchSize;\n \n-    ColumnarBatchReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString, Broadcast<FileIO> fileIo,\n-        Broadcast<EncryptionManager> encryptionManager, boolean caseSensitive, int numRecordsPerBatch) {\n-      super(task, tableSchemaString, expectedSchemaString, fileIo, encryptionManager, caseSensitive);\n-      this.batchSize = numRecordsPerBatch;\n+    BatchReaderFactory(int batchSize) {\n+      this.batchSize = batchSize;\n     }\n \n     @Override\n-    public InputPartitionReader<ColumnarBatch> createPartitionReader() {\n-      return new ColumnarBatchTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive, batchSize);\n+    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMzgwNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389133804", "bodyText": "Nit: extra newline.", "author": "rdblue", "createdAt": "2020-03-06T20:42:33Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -699,4 +486,5 @@ public int size() {\n       throw new UnsupportedOperationException(\"Not implemented: set\");\n     }\n   }\n+", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -486,5 +508,4 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n       throw new UnsupportedOperationException(\"Not implemented: set\");\n     }\n   }\n-\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNDA1MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389134051", "bodyText": "Why doesn't this support preferred locations?", "author": "rdblue", "createdAt": "2020-03-06T20:43:09Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -383,286 +433,23 @@ private Schema lazyExpectedSchema() {\n     }\n   }\n \n-  private static class TaskDataReader implements InputPartitionReader<InternalRow> {\n-    // for some reason, the apply method can't be called from Java without reflection\n-    private static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n-        .impl(UnsafeProjection.class, InternalRow.class)\n-        .build();\n-\n-    private final Iterator<FileScanTask> tasks;\n-    private final Schema tableSchema;\n-    private final Schema expectedSchema;\n-    private final FileIO fileIo;\n-    private final Map<String, InputFile> inputFiles;\n-    private final boolean caseSensitive;\n-\n-    private Iterator<InternalRow> currentIterator = null;\n-    private Closeable currentCloseable = null;\n-    private InternalRow current = null;\n-\n-    TaskDataReader(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n-                   EncryptionManager encryptionManager, boolean caseSensitive) {\n-      this.fileIo = fileIo;\n-      this.tasks = task.files().iterator();\n-      this.tableSchema = tableSchema;\n-      this.expectedSchema = expectedSchema;\n-      Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(Iterables.transform(task.files(),\n-          fileScanTask ->\n-              EncryptedFiles.encryptedInput(\n-                  this.fileIo.newInputFile(fileScanTask.file().path().toString()),\n-                  fileScanTask.file().keyMetadata())));\n-      ImmutableMap.Builder<String, InputFile> inputFileBuilder = ImmutableMap.builder();\n-      decryptedFiles.forEach(decrypted -> inputFileBuilder.put(decrypted.location(), decrypted));\n-      this.inputFiles = inputFileBuilder.build();\n-      // open last because the schemas and fileIo must be set\n-      this.currentIterator = open(tasks.next());\n-      this.caseSensitive = caseSensitive;\n-    }\n+  /**\n+   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n+   */\n+  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -343,113 +375,103 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   public String toString() {\n     return String.format(\n         \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-  private static class BaseReadTask implements Serializable {\n-    final CombinedScanTask task;\n+  private static class ReadTask<T> implements Serializable, InputPartition<T> {\n+    private final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    final Broadcast<FileIO> io;\n-    final Broadcast<EncryptionManager> encryptionManager;\n-    final boolean caseSensitive;\n+    private final Broadcast<FileIO> io;\n+    private final Broadcast<EncryptionManager> encryptionManager;\n+    private final boolean caseSensitive;\n+    private final boolean localityPreferred;\n+    private final ReaderFactory<T> readerFactory;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n+    private transient String[] preferredLocations;\n \n-    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive) {\n+    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+                     boolean caseSensitive, boolean localityPreferred, ReaderFactory<T> readerFactory) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n+      this.localityPreferred = localityPreferred;\n+      this.preferredLocations = getPreferredLocations();\n+      this.readerFactory = readerFactory;\n+    }\n+\n+    @Override\n+    public InputPartitionReader<T> createPartitionReader() {\n+      return readerFactory.create(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n+          encryptionManager.value(), caseSensitive);\n+    }\n+\n+    @Override\n+    public String[] preferredLocations() {\n+      return preferredLocations;\n     }\n \n-    Schema lazyTableSchema() {\n+    private Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    Schema lazyExpectedSchema() {\n+    private Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+    private String[] getPreferredLocations() {\n+      if (!localityPreferred) {\n+        return new String[0];\n+      }\n \n+      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+      return Util.blockLocations(task, conf);\n+    }\n   }\n \n-  private static class InternalRowReadTask extends BaseReadTask implements InputPartition<InternalRow> {\n-    private final boolean localityPreferred;\n-    private transient String[] preferredLocations;\n+  private interface ReaderFactory<T> extends Serializable {\n+    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n+                                   EncryptionManager encryptionManager, boolean caseSensitive);\n+  }\n \n-    private InternalRowReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive, boolean localityPref) {\n-      super(task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive);\n-      this.localityPreferred = localityPref;\n-      this.preferredLocations = getPreferredLocations();\n-    }\n+  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n+    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new InternalRowTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive);\n+    private InternalRowReaderFactory() {\n     }\n \n     @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private String[] getPreferredLocations() {\n-      if (!localityPreferred) {\n-        return new String[0];\n-      }\n-\n-      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n-      Set<String> locations = Sets.newHashSet();\n-      for (FileScanTask f : task.files()) {\n-        Path path = new Path(f.file().path().toString());\n-        try {\n-          FileSystem fs = path.getFileSystem(conf);\n-          for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n-            locations.addAll(Arrays.asList(b.getHosts()));\n-          }\n-        } catch (IOException ioe) {\n-          LOG.warn(\"Failed to get block locations for path {}\", path, ioe);\n-        }\n-      }\n-\n-      return locations.toArray(new String[0]);\n+    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n     }\n   }\n \n-  /**\n-   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n-   */\n-  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {\n+  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n     private final int batchSize;\n \n-    ColumnarBatchReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString, Broadcast<FileIO> fileIo,\n-        Broadcast<EncryptionManager> encryptionManager, boolean caseSensitive, int numRecordsPerBatch) {\n-      super(task, tableSchemaString, expectedSchemaString, fileIo, encryptionManager, caseSensitive);\n-      this.batchSize = numRecordsPerBatch;\n+    BatchReaderFactory(int batchSize) {\n+      this.batchSize = batchSize;\n     }\n \n     @Override\n-    public InputPartitionReader<ColumnarBatch> createPartitionReader() {\n-      return new ColumnarBatchTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive, batchSize);\n+    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNDYwMA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389134600", "bodyText": "This should be enableBatchRead().", "author": "rdblue", "createdAt": "2020-03-06T20:44:20Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -343,113 +375,103 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   public String toString() {\n     return String.format(\n         \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-  private static class BaseReadTask implements Serializable {\n-    final CombinedScanTask task;\n+  private static class ReadTask<T> implements Serializable, InputPartition<T> {\n+    private final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    final Broadcast<FileIO> io;\n-    final Broadcast<EncryptionManager> encryptionManager;\n-    final boolean caseSensitive;\n+    private final Broadcast<FileIO> io;\n+    private final Broadcast<EncryptionManager> encryptionManager;\n+    private final boolean caseSensitive;\n+    private final boolean localityPreferred;\n+    private final ReaderFactory<T> readerFactory;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n+    private transient String[] preferredLocations;\n \n-    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive) {\n+    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+                     boolean caseSensitive, boolean localityPreferred, ReaderFactory<T> readerFactory) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n+      this.localityPreferred = localityPreferred;\n+      this.preferredLocations = getPreferredLocations();\n+      this.readerFactory = readerFactory;\n+    }\n+\n+    @Override\n+    public InputPartitionReader<T> createPartitionReader() {\n+      return readerFactory.create(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n+          encryptionManager.value(), caseSensitive);\n+    }\n+\n+    @Override\n+    public String[] preferredLocations() {\n+      return preferredLocations;\n     }\n \n-    Schema lazyTableSchema() {\n+    private Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    Schema lazyExpectedSchema() {\n+    private Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+    private String[] getPreferredLocations() {\n+      if (!localityPreferred) {\n+        return new String[0];\n+      }\n \n+      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+      return Util.blockLocations(task, conf);\n+    }\n   }\n \n-  private static class InternalRowReadTask extends BaseReadTask implements InputPartition<InternalRow> {\n-    private final boolean localityPreferred;\n-    private transient String[] preferredLocations;\n+  private interface ReaderFactory<T> extends Serializable {\n+    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n+                                   EncryptionManager encryptionManager, boolean caseSensitive);\n+  }\n \n-    private InternalRowReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive, boolean localityPref) {\n-      super(task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive);\n-      this.localityPreferred = localityPref;\n-      this.preferredLocations = getPreferredLocations();\n-    }\n+  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n+    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new InternalRowTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive);\n+    private InternalRowReaderFactory() {\n     }\n \n     @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private String[] getPreferredLocations() {\n-      if (!localityPreferred) {\n-        return new String[0];\n-      }\n-\n-      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n-      Set<String> locations = Sets.newHashSet();\n-      for (FileScanTask f : task.files()) {\n-        Path path = new Path(f.file().path().toString());\n-        try {\n-          FileSystem fs = path.getFileSystem(conf);\n-          for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n-            locations.addAll(Arrays.asList(b.getHosts()));\n-          }\n-        } catch (IOException ioe) {\n-          LOG.warn(\"Failed to get block locations for path {}\", path, ioe);\n-        }\n-      }\n-\n-      return locations.toArray(new String[0]);\n+    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n     }\n   }\n \n-  /**\n-   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n-   */\n-  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {\n+  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n     private final int batchSize;\n \n-    ColumnarBatchReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString, Broadcast<FileIO> fileIo,\n-        Broadcast<EncryptionManager> encryptionManager, boolean caseSensitive, int numRecordsPerBatch) {\n-      super(task, tableSchemaString, expectedSchemaString, fileIo, encryptionManager, caseSensitive);\n-      this.batchSize = numRecordsPerBatch;\n+    BatchReaderFactory(int batchSize) {\n+      this.batchSize = batchSize;\n     }\n \n     @Override\n-    public InputPartitionReader<ColumnarBatch> createPartitionReader() {\n-      return new ColumnarBatchTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive, batchSize);\n+    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNzQ0NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389137444", "bodyText": "Looks like the only differences between the batch and row tasks is the locality and batch size. Locality should probably be supported by both. If that's done, then do we need 3 classes? What about using one and using batchSize=1 for row-based tasks? Then we'd be able to have just one class for both cases.", "author": "rdblue", "createdAt": "2020-03-06T20:51:01Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNTAyNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390635024", "bodyText": "I think 3 classes are still needed since we create different InputPartitionReader for each version.", "author": "samarthjain", "createdAt": "2020-03-10T21:57:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNzQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY2ODM1OA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390668358", "bodyText": "You're right. That was changed in a later version of the API. What about using anonymous classes and an abstract BaseReadTask? That might be clean, but up to you.", "author": "rdblue", "createdAt": "2020-03-10T23:26:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNzQ0NA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 4bae134bd..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -343,113 +375,103 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   public String toString() {\n     return String.format(\n         \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-  private static class BaseReadTask implements Serializable {\n-    final CombinedScanTask task;\n+  private static class ReadTask<T> implements Serializable, InputPartition<T> {\n+    private final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    final Broadcast<FileIO> io;\n-    final Broadcast<EncryptionManager> encryptionManager;\n-    final boolean caseSensitive;\n+    private final Broadcast<FileIO> io;\n+    private final Broadcast<EncryptionManager> encryptionManager;\n+    private final boolean caseSensitive;\n+    private final boolean localityPreferred;\n+    private final ReaderFactory<T> readerFactory;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n+    private transient String[] preferredLocations;\n \n-    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive) {\n+    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+                     boolean caseSensitive, boolean localityPreferred, ReaderFactory<T> readerFactory) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n+      this.localityPreferred = localityPreferred;\n+      this.preferredLocations = getPreferredLocations();\n+      this.readerFactory = readerFactory;\n+    }\n+\n+    @Override\n+    public InputPartitionReader<T> createPartitionReader() {\n+      return readerFactory.create(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n+          encryptionManager.value(), caseSensitive);\n+    }\n+\n+    @Override\n+    public String[] preferredLocations() {\n+      return preferredLocations;\n     }\n \n-    Schema lazyTableSchema() {\n+    private Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    Schema lazyExpectedSchema() {\n+    private Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+    private String[] getPreferredLocations() {\n+      if (!localityPreferred) {\n+        return new String[0];\n+      }\n \n+      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+      return Util.blockLocations(task, conf);\n+    }\n   }\n \n-  private static class InternalRowReadTask extends BaseReadTask implements InputPartition<InternalRow> {\n-    private final boolean localityPreferred;\n-    private transient String[] preferredLocations;\n+  private interface ReaderFactory<T> extends Serializable {\n+    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n+                                   EncryptionManager encryptionManager, boolean caseSensitive);\n+  }\n \n-    private InternalRowReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive, boolean localityPref) {\n-      super(task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive);\n-      this.localityPreferred = localityPref;\n-      this.preferredLocations = getPreferredLocations();\n-    }\n+  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n+    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new InternalRowTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive);\n+    private InternalRowReaderFactory() {\n     }\n \n     @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private String[] getPreferredLocations() {\n-      if (!localityPreferred) {\n-        return new String[0];\n-      }\n-\n-      Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n-      Set<String> locations = Sets.newHashSet();\n-      for (FileScanTask f : task.files()) {\n-        Path path = new Path(f.file().path().toString());\n-        try {\n-          FileSystem fs = path.getFileSystem(conf);\n-          for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n-            locations.addAll(Arrays.asList(b.getHosts()));\n-          }\n-        } catch (IOException ioe) {\n-          LOG.warn(\"Failed to get block locations for path {}\", path, ioe);\n-        }\n-      }\n-\n-      return locations.toArray(new String[0]);\n+    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n     }\n   }\n \n-  /**\n-   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n-   */\n-  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {\n+  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n     private final int batchSize;\n \n-    ColumnarBatchReadTask(\n-        CombinedScanTask task, String tableSchemaString, String expectedSchemaString, Broadcast<FileIO> fileIo,\n-        Broadcast<EncryptionManager> encryptionManager, boolean caseSensitive, int numRecordsPerBatch) {\n-      super(task, tableSchemaString, expectedSchemaString, fileIo, encryptionManager, caseSensitive);\n-      this.batchSize = numRecordsPerBatch;\n+    BatchReaderFactory(int batchSize) {\n+      this.batchSize = batchSize;\n     }\n \n     @Override\n-    public InputPartitionReader<ColumnarBatch> createPartitionReader() {\n-      return new ColumnarBatchTaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-          encryptionManager.value(), caseSensitive, batchSize);\n+    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n+                                                    FileIO io, EncryptionManager encryptionManager,\n+                                                    boolean caseSensitive) {\n+      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3NjE0NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389176144", "bodyText": "Could this implement VectorHolder and get passed into the read method, rather than allocating a VectorHolder and copying its contents?", "author": "rdblue", "createdAt": "2020-03-06T22:32:44Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMzNzgxNQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r395337815", "bodyText": "Not sure I follow, @rdblue.", "author": "samarthjain", "createdAt": "2020-03-19T21:46:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3NjE0NA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3NjU1Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389176553", "bodyText": "Is this necessary if we always eagerly decode ints?", "author": "rdblue", "createdAt": "2020-03-06T22:33:51Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3ODEzMg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389178132", "bodyText": "Have we tested using IntVector instead of DateDayVector?\nIt would simplify the decoding logic to always use an IntVector, and I'm not sure what the benefit of using a DateDayVector is. Isn't the underlying buffer identical?\nSame logic applies for TimeStampMicroTZVector and LongVector.", "author": "rdblue", "createdAt": "2020-03-06T22:38:44Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY3MDQ3OA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390670478", "bodyText": "Filed #834 to follow up on this.", "author": "samarthjain", "createdAt": "2020-03-10T23:33:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3ODEzMg=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE4MDA2MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389180060", "bodyText": "Does NullableVarCharHolder make a copy?", "author": "rdblue", "createdAt": "2020-03-06T22:44:54Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY3MTk5NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390671994", "bodyText": "No, it uses the underlying buffer and sets the start and end positions to get the variable width data out of it.\nhttps://github.com/apache/arrow/blob/maint-0.14.x/java/vector/src/main/java/org/apache/arrow/vector/VarCharVector.java#L134", "author": "samarthjain", "createdAt": "2020-03-10T23:38:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE4MDA2MA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE4MTM0NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389181344", "bodyText": "This looks concerning to me. This variant of UTF8String uses Java's Unsafe to work with bytes underneath an object. By setting this to null, this is using an absolute address, which I assume should be off-heap. Does our allocator use off-heap memory?", "author": "rdblue", "createdAt": "2020-03-06T22:48:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);", "originalCommit": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex ac4002a34..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -19,33 +19,9 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import io.netty.buffer.ArrowBuf;\n-import java.math.BigInteger;\n-import org.apache.arrow.vector.BigIntVector;\n-import org.apache.arrow.vector.BitVector;\n-import org.apache.arrow.vector.DateDayVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n-import org.apache.arrow.vector.Float4Vector;\n-import org.apache.arrow.vector.Float8Vector;\n-import org.apache.arrow.vector.IntVector;\n-import org.apache.arrow.vector.SmallIntVector;\n-import org.apache.arrow.vector.TimeStampMicroTZVector;\n-import org.apache.arrow.vector.TinyIntVector;\n-import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.VarBinaryVector;\n-import org.apache.arrow.vector.complex.ListVector;\n-import org.apache.arrow.vector.complex.StructVector;\n-import org.apache.arrow.vector.holders.NullableVarCharHolder;\n-import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n-import org.apache.iceberg.spark.arrow.ArrowUtils;\n-import org.apache.parquet.Preconditions;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.DecimalMetadata;\n-import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb", "url": "https://github.com/apache/iceberg/commit/714c943b3812c8c65309e8161e2801cecafb4dbb", "message": "Merge reader changes from master. Rebase branch to master", "committedDate": "2020-03-23T19:21:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwNzU5Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410507597", "bodyText": "Nit: indentation is off.", "author": "rdblue", "createdAt": "2020-04-17T23:01:34Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -111,7 +111,8 @@ void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int\n   }\n \n   void readBatchOfDictionaryEncodedTimestampMillis(FieldVector vector, int startOffset, int numValuesToRead,\n-                                                   Dictionary dict, NullabilityHolder nullabilityHolder) {\n+                                                   Dictionary dict, NullabilityHolder nullabilityHolder,\n+      int typeWidth) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java\nindex 43d6a50e5..52e389ece 100644\n--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java\n\n@@ -110,9 +100,9 @@ public class VectorizedDictionaryEncodedParquetValuesReader extends BaseVectoriz\n     }\n   }\n \n-  void readBatchOfDictionaryEncodedTimestampMillis(FieldVector vector, int startOffset, int numValuesToRead,\n-                                                   Dictionary dict, NullabilityHolder nullabilityHolder,\n-      int typeWidth) {\n+  void readBatchOfDictionaryEncodedTimestampMillis(\n+      FieldVector vector, int startOffset, int numValuesToRead,\n+      Dictionary dict, NullabilityHolder nullabilityHolder, int typeWidth) {\n     int left = numValuesToRead;\n     int idx = startOffset;\n     while (left > 0) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDAxNw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410510017", "bodyText": "Does this make Spark start faster? Should we use it in other tests?", "author": "rdblue", "createdAt": "2020-04-17T23:12:16Z", "path": "spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java", "diffHunk": "@@ -92,15 +93,24 @@ protected void cleanupFiles() throws IOException {\n     }\n   }\n \n-  protected void setupSpark() {\n-    spark = SparkSession.builder()\n-        .config(\"spark.ui.enabled\", false)\n-        .master(\"local\")\n-        .getOrCreate();\n+  protected void setupSpark(boolean enableDictionaryEncoding) {\n+    SparkSession.Builder builder = SparkSession.builder()\n+            .config(\"spark.ui.enabled\", false);", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY0MTExNg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420641116", "bodyText": "I am actually not sure. But it does look like a lot of tests within Spark turn off the UI.\nFor ex- https://github.com/apache/spark/blob/a222644e1df907d0aba19634a166e146dfb4f551/core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala#L268", "author": "samarthjain", "createdAt": "2020-05-06T08:58:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDAxNw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDQyMg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410510422", "bodyText": "What about the call in case PACKED just below? Does that also need to use the typeWidth?", "author": "rdblue", "createdAt": "2020-04-17T23:13:57Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java", "diffHunk": "@@ -193,7 +193,7 @@ public void readBatchOfDictionaryEncodedLongs(\n         case RLE:\n           if (currentValue == maxDefLevel) {\n             dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedLongs(vector,\n-                idx, numValues, dict, nullabilityHolder);\n+                idx, numValues, dict, nullabilityHolder, typeWidth);", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODY4OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r417678689", "bodyText": "Good catch, looks like I missed it. Will fix here and other places.", "author": "samarthjain", "createdAt": "2020-04-29T23:54:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDQyMg=="}], "type": "inlineReview", "revised_code": {"commit": "e3f72438b503419a1a02c76a98f1d179babaf712", "chunk": "diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java\nindex ae9879e56..86918f7de 100644\n--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java\n\n@@ -193,7 +193,7 @@ public final class VectorizedParquetDefinitionLevelReader extends BaseVectorized\n         case RLE:\n           if (currentValue == maxDefLevel) {\n             dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedLongs(vector,\n-                idx, numValues, dict, nullabilityHolder, typeWidth);\n+                idx, numValues, dict, nullabilityHolder);\n           } else {\n             setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n           }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDkzOA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410510938", "bodyText": "These changes look concerning. It looks like the old offset (only index) must not have been correct. If so, there are places where getDataBuffer().setLong(...) and similar methods are called but aren't updated like these. Are those cases bugs as well?", "author": "rdblue", "createdAt": "2020-04-17T23:16:33Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -72,7 +72,7 @@ void readBatchOfDictionaryIds(IntVector intVector, int startOffset, int numValue\n   }\n \n   void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int numValuesToRead, Dictionary dict,\n-                                         NullabilityHolder nullabilityHolder) {\n+                                         NullabilityHolder nullabilityHolder, int typeWidth) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODg3Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r417678873", "bodyText": "Good catch, looks like I missed it. Will fix here and other places.", "author": "samarthjain", "createdAt": "2020-04-29T23:54:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDkzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNzU5Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420637597", "bodyText": "Looks like the test TestSparkParquetFallbackToDictionaryEncodingForVectorizedReader wasn't adequately testing the fallback to plain encoding behavior for all the datatypes. Will tweak it.", "author": "samarthjain", "createdAt": "2020-05-06T08:52:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDkzOA=="}], "type": "inlineReview", "revised_code": {"commit": "e3f72438b503419a1a02c76a98f1d179babaf712", "chunk": "diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java\nindex 43d6a50e5..e71d61aa6 100644\n--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java\n\n@@ -72,7 +72,7 @@ public class VectorizedDictionaryEncodedParquetValuesReader extends BaseVectoriz\n   }\n \n   void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int numValuesToRead, Dictionary dict,\n-                                         NullabilityHolder nullabilityHolder, int typeWidth) {\n+                                         NullabilityHolder nullabilityHolder) {\n     int left = numValuesToRead;\n     int idx = startOffset;\n     while (left > 0) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTA1NQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410511055", "bodyText": "Why is this in the Spark package? Is it specific to Spark?", "author": "rdblue", "createdAt": "2020-04-17T23:17:11Z", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+\n+public class ArrowAllocation {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3OTQxNg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r417679416", "bodyText": "This can be moved to iceberg-arrow.", "author": "samarthjain", "createdAt": "2020-04-29T23:56:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTA1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java b/arrow/src/main/java/org/apache/iceberg/arrow/ArrowAllocation.java\nsimilarity index 96%\nrename from spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java\nrename to arrow/src/main/java/org/apache/iceberg/arrow/ArrowAllocation.java\nindex c1a38a7b7..49882ce90 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/ArrowAllocation.java\n\n@@ -17,7 +17,7 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.arrow;\n+package org.apache.iceberg.arrow;\n \n import org.apache.arrow.memory.RootAllocator;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTUyMA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410511520", "bodyText": "It looks like this could easily be refactored into separate methods: getDictionaryVectorAccessor(VectorHolder) and getPlainVectorAccessor(VectorHolder).", "author": "rdblue", "createdAt": "2020-04-17T23:19:11Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxNDU2Ng==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410514566", "bodyText": "This would probably address the complexity problem above without suppressing it.", "author": "rdblue", "createdAt": "2020-04-17T23:33:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTUyMA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\nindex ba1d3eb95..74732a3e4 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n\n@@ -21,11 +21,11 @@ package org.apache.iceberg.spark.data.vectorized;\n \n import io.netty.buffer.ArrowBuf;\n import java.math.BigInteger;\n+import java.util.stream.IntStream;\n import org.apache.arrow.vector.BigIntVector;\n import org.apache.arrow.vector.BitVector;\n import org.apache.arrow.vector.DateDayVector;\n import org.apache.arrow.vector.FieldVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n import org.apache.arrow.vector.Float4Vector;\n import org.apache.arrow.vector.Float8Vector;\n import org.apache.arrow.vector.IntVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxNzIyMA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410517220", "bodyText": "I think that the dictionary should be eagerly decoded, for a few reasons:\n\ngetBytesUnsafe will make a copy of the bytes when the binary is backed by a ByteBuffer or an array slice. It may also cache values and perform checks.\ngetBytesUnsafe should only be used when the bytes are immediately consumed, but UTF8String keeps a reference, so even if there isn't a copy this could result in a correctness problem\ndecodeToBinary should incur a dynamic dispatch cost (this also applies to the other dictionary readers)\n\nIt's fairly easy to eagerly decode:\n  private static class DictionaryStringAccessor extends ArrowVectorAccessor {\n    private IntVector offsetVector;\n    private UTF8String[] decodedDictionary;\n\n    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n      super(vector);\n      this.offsetVector = vector;\n      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n          .mapToObj(dictionary::decodeToBinary)\n          .map(binary -> UTF8String.fromBytes(binary.getBytes()))\n          .toArray(UTF8String[]::new);\n    }\n\n    @Override\n    final UTF8String getUTF8String(int rowId) {\n      int offset = offsetVector.get(rowId);\n      return decodedDictionary[offset];\n    }\n  }\nAlso, there's no need for the DictionaryArrowVectorAccessor, since it only tracks one column that is also tracked by its parent.", "author": "rdblue", "createdAt": "2020-04-17T23:46:13Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DECIMAL:\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        return new StructAccessor(structVector);\n+      }\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return parquetDictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return parquetDictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private static class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private abstract static class DictionaryArrowVectorAccessor extends ArrowVectorAccessor {\n+    final Dictionary parquetDictionary;\n+    final IntVector dictionaryVector;\n+\n+    private DictionaryArrowVectorAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.dictionaryVector = vector;\n+      this.parquetDictionary = dictionary;\n+    }\n+  }\n+\n+  private static class DictionaryStringAccessor extends DictionaryArrowVectorAccessor {\n+\n+    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\nindex ba1d3eb95..74732a3e4 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n\n@@ -21,11 +21,11 @@ package org.apache.iceberg.spark.data.vectorized;\n \n import io.netty.buffer.ArrowBuf;\n import java.math.BigInteger;\n+import java.util.stream.IntStream;\n import org.apache.arrow.vector.BigIntVector;\n import org.apache.arrow.vector.BitVector;\n import org.apache.arrow.vector.DateDayVector;\n import org.apache.arrow.vector.FieldVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n import org.apache.arrow.vector.Float4Vector;\n import org.apache.arrow.vector.Float8Vector;\n import org.apache.arrow.vector.IntVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxODI0OA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410518248", "bodyText": "Similar to the string dictionary accessor, I think we can avoid extra work and make this simpler by eagerly decoding here:\n  private static class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n    private final IntVector vector;\n    private final double[] decodedDictionary;\n\n    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n      super(vector);\n      this.vector = vector;\n      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n          .mapToDouble(dictionary::decodeToDouble)\n          .toArray();\n    }\n\n    @Override\n    final double getDouble(int rowId) {\n      return decodedDictionary[vector.get(rowId)];\n    }\n  }\nParquet decodes into a double[] as well, but this avoids dynamic dispatch costs for decodeToDouble and is easier to see what's happening.", "author": "rdblue", "createdAt": "2020-04-17T23:52:23Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DECIMAL:\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        return new StructAccessor(structVector);\n+      }\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return parquetDictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends DictionaryArrowVectorAccessor {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\nindex ba1d3eb95..74732a3e4 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n\n@@ -21,11 +21,11 @@ package org.apache.iceberg.spark.data.vectorized;\n \n import io.netty.buffer.ArrowBuf;\n import java.math.BigInteger;\n+import java.util.stream.IntStream;\n import org.apache.arrow.vector.BigIntVector;\n import org.apache.arrow.vector.BitVector;\n import org.apache.arrow.vector.DateDayVector;\n import org.apache.arrow.vector.FieldVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n import org.apache.arrow.vector.Float4Vector;\n import org.apache.arrow.vector.Float8Vector;\n import org.apache.arrow.vector.IntVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTA1Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410519053", "bodyText": "Nit: why is the class name plural? Shouldn't it be ColumnarBatchReader?", "author": "rdblue", "createdAt": "2020-04-17T23:56:51Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTE2NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410519164", "bodyText": "Why not new VectorizedArrowReader[readers.size()]?", "author": "rdblue", "createdAt": "2020-04-17T23:57:32Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTU3Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410519577", "bodyText": "Or better, this is where streams are really useful:\n    this.readers = readers.stream()\n        .map(VectorizedArrowReader.class::cast)\n        .toArray(VectorizedArrowReader[]::new);", "author": "rdblue", "createdAt": "2020-04-17T23:59:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ0Mzg5Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418443897", "bodyText": "Good tip on using streams. Makes it so much more concise.", "author": "samarthjain", "createdAt": "2020-05-01T07:17:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTE2NA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMjk0Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410522943", "bodyText": "This doesn't use projectedIcebergSchema. Can you remove it?\nAlso, I think it would be possible to use just the expected schema instead of the table schema. In primitive, the table schema is used to find the Iceberg type. But if the Iceberg type is missing from the projection, then it can't be projected and will be skipped later. So it should be safe to do this:\n      ...\n      ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n      // Nested types not yet supported for vectorized reads\n      if (desc.getMaxRepetitionLevel() > 0) {\n        return null;\n      }\n\n      Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n      if (icebergField == null) {\n        return null;\n      }\n\n      return new VectorizedArrowReader(\n          desc, icebergField, rootAllocator, batchSize, false /* setArrowValidityVector */);\nIf that works, then you wouldn't need to pass the file schema all the way in.", "author": "rdblue", "createdAt": "2020-04-18T00:19:55Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReaders buildReader(\n+      Schema tableSchema,\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReaders)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(tableSchema, expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader> {\n+    private final MessageType parquetSchema;\n+    private final Schema tableIcebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema tableSchema,\n+        Schema projectedIcebergSchema,", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc2NzU2OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r419767569", "bodyText": "Will change. However, we still need the file schema(parquetSchema) as it is used to get hold of the column descriptor.", "author": "samarthjain", "createdAt": "2020-05-04T22:31:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMjk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MjYwNg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420292606", "bodyText": "I think I meant table schema, not file schema.", "author": "rdblue", "createdAt": "2020-05-05T17:44:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMjk0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\nindex cbb1fb864..01cbe6f28 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n\n@@ -19,18 +19,18 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.stream.IntStream;\n import org.apache.arrow.memory.BufferAllocator;\n import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n import org.apache.iceberg.parquet.VectorizedReader;\n-import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.schema.GroupType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMzA0Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410523047", "bodyText": "This is another case where IntStream can make this a bit simpler:\n      IntStream.range(0, fields.size())\n          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));", "author": "rdblue", "createdAt": "2020-04-18T00:20:28Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReaders buildReader(\n+      Schema tableSchema,\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReaders)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(tableSchema, expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader> {\n+    private final MessageType parquetSchema;\n+    private final Schema tableIcebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema tableSchema,\n+        Schema projectedIcebergSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.tableIcebergSchema = tableSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      for (int i = 0; i < fields.size(); i += 1) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\nindex cbb1fb864..01cbe6f28 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n\n@@ -19,18 +19,18 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.stream.IntStream;\n import org.apache.arrow.memory.BufferAllocator;\n import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n import org.apache.iceberg.parquet.VectorizedReader;\n-import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.schema.GroupType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDExNw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524117", "bodyText": "Preconditions accept a format string and arguments to avoid string creation every time the method is called, which is what happens when you use concatenation like this. Can you update the check like this?\nPreconditions.checkArgument(numRowsToRead > 0, \"Invalid number of rows to read: %s\", numRowsToRead);", "author": "rdblue", "createdAt": "2020-04-18T00:27:22Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDI2Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524262", "bodyText": "I'm seeing a lot of parameterized types without parameters. Can you make sure you add <?> here and check for that in other places?", "author": "rdblue", "createdAt": "2020-04-18T00:28:24Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDM1Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524353", "bodyText": "This can be a for-each loop:\n    for (VectorizedArrowReader reader : readers) {\n      if (reader != null) {\n        reader.setRowGroupInfo(pageStore, metaData);\n      }\n    }", "author": "rdblue", "createdAt": "2020-04-18T00:28:52Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDY5MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524690", "bodyText": "This should also use a format string.", "author": "rdblue", "createdAt": "2020-04-18T00:31:18Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int prevNum = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numRowsToRead);\n+      int numRowsInVector = holder.numValues();\n+      Preconditions.checkState(\n+          numRowsInVector == numRowsToRead,\n+          \"Number of rows in the vector \" + numRowsInVector + \" didn't match expected \" +\n+              numRowsToRead);", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDc2Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524767", "bodyText": "If this is always expected to return the requested number of rows, then there is no need for the prevNum check because this would fail before that check would.", "author": "rdblue", "createdAt": "2020-04-18T00:31:54Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int prevNum = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numRowsToRead);\n+      int numRowsInVector = holder.numValues();\n+      Preconditions.checkState(\n+          numRowsInVector == numRowsToRead,", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2Mzc2OA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418763768", "bodyText": "Good point. Will remove the check.", "author": "samarthjain", "createdAt": "2020-05-01T22:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDc2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nsimilarity index 57%\nrename from spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\nrename to spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\nindex 81545a7b5..e825c82fc 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReader.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.lang.reflect.Array;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTI0Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410525242", "bodyText": "Are these types rejected when we create Iceberg tables?", "author": "rdblue", "createdAt": "2020-04-18T00:34:46Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2NDY2Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418764662", "bodyText": "I don't see BYTE and SHORT type in https://github.com/apache/incubator-iceberg/blob/master/api/src/main/java/org/apache/iceberg/types/Type.java\nSo I would assume so?", "author": "samarthjain", "createdAt": "2020-05-01T22:31:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTI0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MjE3MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420292171", "bodyText": "Looks good to me.", "author": "rdblue", "createdAt": "2020-05-05T17:43:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTI0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex 5f18e9d36..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -22,7 +22,6 @@ package org.apache.iceberg.spark.data.vectorized;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.parquet.Preconditions;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjAxNg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526016", "bodyText": "This is the only place that uses childColumns and it seems really strange to only set them in a sub-class, even though they are available in the constructor.\nCould you make the superclass childColumns a private final field instead, and pass the child columns through a second constructor? Like this:\n    StructAccessor(StructVector structVector) {\n      super(structVector,\n          IntStream.range(0, structVector.size())\n              .mapToObj(structVector::getVectorById)\n              .map(ArrowColumnVector::new)\n              .toArray(ArrowColumnVector[]::new));\n    }", "author": "rdblue", "createdAt": "2020-04-18T00:40:20Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DECIMAL:\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        return new StructAccessor(structVector);\n+      }\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return parquetDictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return parquetDictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private static class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private abstract static class DictionaryArrowVectorAccessor extends ArrowVectorAccessor {\n+    final Dictionary parquetDictionary;\n+    final IntVector dictionaryVector;\n+\n+    private DictionaryArrowVectorAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.dictionaryVector = vector;\n+      this.parquetDictionary = dictionary;\n+    }\n+  }\n+\n+  private static class DictionaryStringAccessor extends DictionaryArrowVectorAccessor {\n+\n+    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      Binary binary = parquetDictionary.decodeToBinary(dictionaryVector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private static class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private static class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryBinaryAccessor extends DictionaryArrowVectorAccessor {\n+\n+    DictionaryBinaryAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = parquetDictionary.decodeToBinary(dictionaryVector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private static class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector vector;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class TimestampAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector vector;\n+\n+    TimestampAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class ArrayAccessor extends ArrowVectorAccessor {\n+\n+    private final ListVector vector;\n+    private final ArrowColumnVector arrayData;\n+\n+    ArrayAccessor(ListVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+      this.arrayData = new ArrowColumnVector(vector.getDataVector());\n+    }\n+\n+    @Override\n+    final ColumnarArray getArray(int rowId) {\n+      ArrowBuf offsets = vector.getOffsetBuffer();\n+      int index = rowId * ListVector.OFFSET_WIDTH;\n+      int start = offsets.getInt(index);\n+      int end = offsets.getInt(index + ListVector.OFFSET_WIDTH);\n+      return new ColumnarArray(arrayData, start, end - start);\n+    }\n+  }\n+\n+  /**\n+   * Use {@link IcebergArrowColumnVector#getChild(int)} to get hold of the {@link ArrowColumnVector} vectors holding the\n+   * struct values.\n+   */\n+  private static class StructAccessor extends ArrowVectorAccessor {\n+    StructAccessor(StructVector structVector) {\n+      super(structVector);\n+      childColumns = new ArrowColumnVector[structVector.size()];", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\nindex ba1d3eb95..74732a3e4 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n\n@@ -21,11 +21,11 @@ package org.apache.iceberg.spark.data.vectorized;\n \n import io.netty.buffer.ArrowBuf;\n import java.math.BigInteger;\n+import java.util.stream.IntStream;\n import org.apache.arrow.vector.BigIntVector;\n import org.apache.arrow.vector.BitVector;\n import org.apache.arrow.vector.DateDayVector;\n import org.apache.arrow.vector.FieldVector;\n-import org.apache.arrow.vector.FixedSizeBinaryVector;\n import org.apache.arrow.vector.Float4Vector;\n import org.apache.arrow.vector.Float8Vector;\n import org.apache.arrow.vector.IntVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526180", "bodyText": "Did I already ask why this is necessary?", "author": "rdblue", "createdAt": "2020-04-18T00:41:40Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1MTMxNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418751314", "bodyText": "This was copied over from Spark's ArrowColumnVector\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java#L108\nThe difference from Spark's implementation is that our isNullAt(rowId) goes to the NullabilityHolder where as Spark's uses vector.isNull(rowId);\nFor spark, we don't set the validity bits in the validity vector since we track that piece of information in NullabilityHolder.", "author": "samarthjain", "createdAt": "2020-05-01T21:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MTg4MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420291880", "bodyText": "But doesn't Spark already check nullability before calling this method?", "author": "rdblue", "createdAt": "2020-05-05T17:43:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyMDA0Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420620042", "bodyText": "No, spark doesn't. See - https://github.com/apache/spark/blob/branch-2.4/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java#L135", "author": "samarthjain", "createdAt": "2020-05-06T08:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNTczNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420635734", "bodyText": "Also, depending on the accessor, it may or may not have the nullability information. For ex- for DictionaryStringAccessor,\n    final UTF8String getUTF8String(int rowId) {\n      int offset = offsetVector.get(rowId);\n      return decodedDictionary[offset];\n    }\n\nwithout the null check, the offsetVector could return a wrong value when the value at the rowId was actually null.", "author": "samarthjain", "createdAt": "2020-05-06T08:48:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex 5f18e9d36..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -22,7 +22,6 @@ package org.apache.iceberg.spark.data.vectorized;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.parquet.Preconditions;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjMxNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526314", "bodyText": "Instead of childColumns() and then accessing an ordinal, why not change the method to accessor.childColumn(int pos)?", "author": "rdblue", "createdAt": "2020-04-18T00:42:35Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    ArrowColumnVector[] childColumns = accessor.childColumns();", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex 5f18e9d36..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -22,7 +22,6 @@ package org.apache.iceberg.spark.data.vectorized;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.parquet.Preconditions;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjQ2MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526460", "bodyText": "Let's remove this Precondition. An invalid ordinal will cause an automatic exception and there is no need to check twice.", "author": "rdblue", "createdAt": "2020-04-18T00:43:33Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    ArrowColumnVector[] childColumns = accessor.childColumns();\n+    Preconditions.checkArgument(childColumns != null && ordinal < childColumns.length, \"Invalid call for getChild() \" +\n+        \"with ordinal \" + ordinal);", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\nindex 5f18e9d36..9d10cd935 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java\n\n@@ -22,7 +22,6 @@ package org.apache.iceberg.spark.data.vectorized;\n import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n import org.apache.iceberg.arrow.vectorized.VectorHolder;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.parquet.Preconditions;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.sql.vectorized.ArrowColumnVector;\n import org.apache.spark.sql.vectorized.ColumnVector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410527253", "bodyText": "We need to make sure the extra filter columns are added in the reader and returned to Spark (so Spark expects the projection). This can't return a different set of columns than expected because columns are accessed by ordinal.\nAnother option is to do the ColumnarBatch projection here (remove filter columns that have been used), but I would prefer to just return everything to Spark. That simplifies both row and batch read paths.", "author": "rdblue", "createdAt": "2020-04-18T00:48:49Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+    Iterator<ColumnarBatch> iter;\n+    if (hasExtraFilterColumns) {", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc4MjU2Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418782567", "bodyText": "Upon looking closer, I think BaseDataReader and RowDataReader classes can be merged. The only difference between BatchDataReader and RowDataReader is how in the former case we don't currently handle identity partition columns. I will spin up a PR for getting that refactoring into master. Once that is in, the only method that we would need to override in BatchDataReader would be Iterator<ColumnarBatch> open(FileScanTask task, Schema readSchema)", "author": "samarthjain", "createdAt": "2020-05-01T23:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTcxNzU3Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r419717573", "bodyText": "Turns out, I think it makes sense to not merge the two classes. However, a lot of the functionality can be moved to the BaseDataReader class with override for specialized handling of identity partition columns. PR for refactoring - #1000", "author": "samarthjain", "createdAt": "2020-05-04T20:46:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MDY3Ng==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420290676", "bodyText": "I opened #1004 to remove the special handling for extra filter columns. When that's merged, we won't need to handle it. That's what I was talking about with this comment. Can you see how that changes the need for #1000?", "author": "rdblue", "createdAt": "2020-05-05T17:41:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDMyODgzNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420328834", "bodyText": "So the idea behind the need for #1000 was to provide only those methods that need to be customized for the two readers. In turn, it would have taken care of the special handling for extra filter columns and other common functionality that would have been introduced in the future.\nI think #1000 is still relevant. It will need to change though if your PR goes in first :)", "author": "samarthjain", "createdAt": "2020-05-05T18:44:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java b/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\nindex 0814f819a..eeb3ad559 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\n\n@@ -19,68 +19,49 @@\n \n package org.apache.iceberg.spark.source;\n \n-import com.google.common.base.Preconditions;\n-import java.util.Iterator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.parquet.Parquet;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n-import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n \n class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n-  private final Schema tableSchema;\n   private final Schema expectedSchema;\n   private final boolean caseSensitive;\n   private final int batchSize;\n \n   BatchDataReader(\n-      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      CombinedScanTask task, Schema expectedSchema, FileIO fileIo,\n       EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n     super(task, fileIo, encryptionManager);\n-    this.tableSchema = tableSchema;\n     this.expectedSchema = expectedSchema;\n     this.caseSensitive = caseSensitive;\n     this.batchSize = size;\n   }\n \n   @Override\n-  Iterator<ColumnarBatch> open(FileScanTask task) {\n-    // schema or rows returned by readers\n-    Schema finalSchema = expectedSchema;\n-    // schema needed for the projection and filtering\n-    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n-    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n-    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n-    Iterator<ColumnarBatch> iter;\n-    if (hasExtraFilterColumns) {\n-      iter = open(task, requiredSchema);\n-    } else {\n-      iter = open(task, finalSchema);\n-    }\n-    return iter;\n-  }\n-\n-  private Iterator<ColumnarBatch> open(FileScanTask task, Schema readSchema) {\n+  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n     CloseableIterable<ColumnarBatch> iter;\n     InputFile location = getInputFile(task);\n     Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n     if (task.file().format() == FileFormat.PARQUET) {\n       iter = Parquet.read(location)\n-          .project(readSchema)\n+          .project(expectedSchema)\n           .split(task.start(), task.length())\n-          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(tableSchema, readSchema,\n-              fileSchema, batchSize))\n+          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n+              fileSchema, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED))\n+          .recordsPerBatch(batchSize)\n           .filter(task.residual())\n           .caseSensitive(caseSensitive)\n-          .recordsPerBatch(batchSize)\n           // Spark eagerly consumes the batches. So the underlying memory allocated could be reused\n           // without worrying about subsequent reads clobbering over each other. This improves\n           // read performance as every batch read doesn't have to pay the cost of allocating memory.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyODU3OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410528579", "bodyText": "I think this method would be simpler by not exiting early. Something like this:\n  if (enableBatchRead == null) {\n    boolean batchReadEnabled = ...;\n    boolean allParquetFileScanTasks = ...;\n    boolean atLeastOneColumn = lazySchema().columsn().size() > 0;\n    boolean hasNoIdentityProjections = ...;\n    enableBatchRead = batchReadEnabled && allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections;\n  }\n  return enableBatchRead;", "author": "rdblue", "createdAt": "2020-04-18T00:57:34Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -238,6 +279,46 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();\n+  }\n+\n+  private boolean lazyCheckEnableBatchRead() {\n+    if (enableBatchRead == null) {\n+      boolean allParquetFileScanTasks =\n+          tasks().stream()\n+              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n+                  .stream()\n+                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                      FileFormat.PARQUET)));\n+      if (!allParquetFileScanTasks) {\n+        this.enableBatchRead = false;\n+        return false;", "originalCommit": "714c943b3812c8c65309e8161e2801cecafb4dbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1ODA5Ng==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418758096", "bodyText": "While it makes it cleaner, it is less efficient than exiting early. For ex-\nboolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\nThe above is probably a somewhat expensive operation since it is iterating through all the files. We don't want to execute this is batch read has been disabled by config.", "author": "samarthjain", "createdAt": "2020-05-01T22:06:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyODU3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MTUxOQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420291519", "bodyText": "Why does this need to be efficient? I think that the primary concern should be readability here because this is executed at planning time and the result is cached.", "author": "rdblue", "createdAt": "2020-05-05T17:42:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyODU3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex dd56fff13..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -281,42 +290,34 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n \n   @Override\n   public boolean enableBatchRead() {\n-    return lazyCheckEnableBatchRead();\n-  }\n-\n-  private boolean lazyCheckEnableBatchRead() {\n-    if (enableBatchRead == null) {\n+    if (readUsingBatch == null) {\n       boolean allParquetFileScanTasks =\n           tasks().stream()\n               .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                   .stream()\n                   .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                       FileFormat.PARQUET)));\n-      if (!allParquetFileScanTasks) {\n-        this.enableBatchRead = false;\n-        return false;\n-      }\n \n-      int numColumns = lazySchema().columns().size();\n-      if (numColumns == 0) {\n-        this.enableBatchRead = false;\n-        return false;\n-      }\n+      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n \n-      boolean projectIdentityPartitionColumn =\n-          tasks().stream()\n-              .anyMatch(combinedScanTask -> combinedScanTask.files()\n-                  .stream()\n-                  .anyMatch(fileScanTask -> !fileScanTask.spec().identitySourceIds().isEmpty()));\n-      if (projectIdentityPartitionColumn) {\n-        this.enableBatchRead = false;\n-        return false;\n-      }\n+      boolean hasNoIdentityProjections = tasks().stream()\n+          .allMatch(combinedScanTask -> combinedScanTask.files()\n+              .stream()\n+              .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n+\n+      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n \n-      // Enable batched reads only if all requested columns are primitive otherwise revert to row-based reads\n-      this.enableBatchRead = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n+      this.readUsingBatch = batchReadsEnabled && allParquetFileScanTasks && atLeastOneColumn &&\n+          hasNoIdentityProjections && onlyPrimitives;\n     }\n-    return enableBatchRead;\n+    return readUsingBatch;\n+  }\n+\n+  private static void mergeIcebergHadoopConfs(\n+      Configuration baseConf, Map<String, String> options) {\n+    options.keySet().stream()\n+        .filter(key -> key.startsWith(\"hadoop.\"))\n+        .forEach(key -> baseConf.set(key.replaceFirst(\"hadoop.\", \"\"), options.get(key)));\n   }\n \n   private List<CombinedScanTask> tasks() {\n"}}, {"oid": "98da629a6c7b010817c2939c19ac3f715e55d3fe", "url": "https://github.com/apache/iceberg/commit/98da629a6c7b010817c2939c19ac3f715e55d3fe", "message": "Code review comments", "committedDate": "2020-05-07T05:51:13Z", "type": "forcePushed"}, {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "url": "https://github.com/apache/iceberg/commit/53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "message": "Code review comments", "committedDate": "2020-05-07T06:41:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NTAwNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429495004", "bodyText": "This has a warning that it may produce NullPointerException. It would be better to initialize childColumns to new ArrowColumnVector[0] so that this throws an IndexOutOfBoundsException instead. That would also avoid needing to check if it is null in close.", "author": "rdblue", "createdAt": "2020-05-23T00:15:13Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class ArrowVectorAccessor {\n+\n+  private final ValueVector vector;\n+  private final ArrowColumnVector[] childColumns;\n+\n+  ArrowVectorAccessor(ValueVector vector) {\n+    this.vector = vector;\n+    this.childColumns = null;\n+  }\n+\n+  ArrowVectorAccessor(ValueVector vector, ArrowColumnVector[] children) {\n+    this.vector = vector;\n+    this.childColumns = children;\n+  }\n+\n+  final void close() {\n+    if (childColumns != null) {\n+      for (ArrowColumnVector column : childColumns) {\n+        // Closing an ArrowColumnVector is expected to not throw any exception\n+        column.close();\n+      }\n+    }\n+    vector.close();\n+  }\n+\n+  boolean getBoolean(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: boolean\");\n+  }\n+\n+  int getInt(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: int\");\n+  }\n+\n+  long getLong(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: long\");\n+  }\n+\n+  float getFloat(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: float\");\n+  }\n+\n+  double getDouble(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: double\");\n+  }\n+\n+  Decimal getDecimal(int rowId, int precision, int scale) {\n+    throw new UnsupportedOperationException(\"Unsupported type: decimal\");\n+  }\n+\n+  UTF8String getUTF8String(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: UTF8String\");\n+  }\n+\n+  byte[] getBinary(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: binary\");\n+  }\n+\n+  ColumnarArray getArray(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: array\");\n+  }\n+\n+  ArrowColumnVector childColumn(int pos) {\n+    return childColumns[pos];", "originalCommit": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java\nindex 244856a34..c9c9959c9 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java\n\n@@ -26,14 +26,14 @@ import org.apache.spark.sql.vectorized.ColumnarArray;\n import org.apache.spark.unsafe.types.UTF8String;\n \n @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-abstract class ArrowVectorAccessor {\n+public abstract class ArrowVectorAccessor {\n \n   private final ValueVector vector;\n   private final ArrowColumnVector[] childColumns;\n \n   ArrowVectorAccessor(ValueVector vector) {\n     this.vector = vector;\n-    this.childColumns = null;\n+    this.childColumns = new ArrowColumnVector[0];\n   }\n \n   ArrowVectorAccessor(ValueVector vector, ArrowColumnVector[] children) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NTQ4MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429495480", "bodyText": "This updated DictionaryDoubleAccessor to decode into an array, but the other types weren't similarly updated. I think they should be for consistency and so it is clear what's happening.", "author": "rdblue", "createdAt": "2020-05-23T00:19:52Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,505 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.jetbrains.annotations.NotNull;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      return getDictionaryVectorAccessor(dictionary, desc, vector, primitive);\n+    } else {\n+      return getPlainVectorAccessor(vector);\n+    }\n+  }\n+\n+  @NotNull\n+  private static ArrowVectorAccessor getDictionaryVectorAccessor(\n+      Dictionary dictionary,\n+      ColumnDescriptor desc,\n+      FieldVector vector, PrimitiveType primitive) {\n+    Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+    if (primitive.getOriginalType() != null) {\n+      switch (desc.getPrimitiveType().getOriginalType()) {\n+        case ENUM:\n+        case JSON:\n+        case UTF8:\n+        case BSON:\n+          return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+        case INT_64:\n+        case TIMESTAMP_MILLIS:\n+        case TIMESTAMP_MICROS:\n+          return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+        case DECIMAL:\n+          switch (primitive.getPrimitiveTypeName()) {\n+            case BINARY:\n+            case FIXED_LEN_BYTE_ARRAY:\n+              return new DictionaryDecimalBinaryAccessor(\n+                  (IntVector) vector,\n+                  dictionary);\n+            case INT64:\n+              return new DictionaryDecimalLongAccessor(\n+                  (IntVector) vector,\n+                  dictionary);\n+            case INT32:\n+              return new DictionaryDecimalIntAccessor(\n+                  (IntVector) vector,\n+                  dictionary);\n+            default:\n+              throw new UnsupportedOperationException(\n+                  \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+          }\n+        default:\n+          throw new UnsupportedOperationException(\n+              \"Unsupported logical type: \" + primitive.getOriginalType());\n+      }\n+    } else {\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+        case FLOAT:\n+          return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+        case INT64:\n+          return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+        case DOUBLE:\n+          return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  @NotNull\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static ArrowVectorAccessor getPlainVectorAccessor(FieldVector vector) {\n+    if (vector instanceof BitVector) {\n+      return new BooleanAccessor((BitVector) vector);\n+    } else if (vector instanceof IntVector) {\n+      return new IntAccessor((IntVector) vector);\n+    } else if (vector instanceof BigIntVector) {\n+      return new LongAccessor((BigIntVector) vector);\n+    } else if (vector instanceof Float4Vector) {\n+      return new FloatAccessor((Float4Vector) vector);\n+    } else if (vector instanceof Float8Vector) {\n+      return new DoubleAccessor((Float8Vector) vector);\n+    } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+      return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+    } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+      return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+    } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+      return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+    } else if (vector instanceof DateDayVector) {\n+      return new DateAccessor((DateDayVector) vector);\n+    } else if (vector instanceof TimeStampMicroTZVector) {\n+      return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+    } else if (vector instanceof ListVector) {\n+      ListVector listVector = (ListVector) vector;\n+      return new ArrayAccessor(listVector);\n+    } else if (vector instanceof StructVector) {\n+      StructVector structVector = (StructVector) vector;\n+      return new StructAccessor(structVector);\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported vector: \" + vector.getClass());\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final Dictionary parquetDictionary;\n+    private final IntVector offsetVector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.parquetDictionary = dictionary;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(offsetVector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector offsetVector;\n+    private final Dictionary parquetDictionary;", "originalCommit": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\nindex 5a215b2cb..74732a3e4 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java\n\n@@ -137,8 +137,8 @@ public class ArrowVectorAccessors {\n       return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n     } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n       return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n-    } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n-      return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+    } else if (vector instanceof VarBinaryVector) {\n+      return new BinaryAccessor((VarBinaryVector) vector);\n     } else if (vector instanceof DateDayVector) {\n       return new DateAccessor((DateDayVector) vector);\n     } else if (vector instanceof TimeStampMicroTZVector) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5Njc1Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429496753", "bodyText": "Is this needed since you've updated build.gradle?", "author": "rdblue", "createdAt": "2020-05-23T00:31:13Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java", "diffHunk": "@@ -54,6 +56,11 @@ protected abstract Record writeAndRead(String desc,\n   @Rule\n   public TemporaryFolder temp = new TemporaryFolder();\n \n+  @BeforeClass\n+  public static void beforeClass() {", "originalCommit": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\nindex 1fb327413..8d65b64ca 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n\n@@ -56,11 +54,6 @@ public abstract class TestReadProjection {\n   @Rule\n   public TemporaryFolder temp = new TemporaryFolder();\n \n-  @BeforeClass\n-  public static void beforeClass() {\n-    TestHelpers.setArrowFlagsForVectorizedReads();\n-  }\n-\n   @Test\n   public void testFullProjection() throws Exception {\n     Schema schema = new Schema(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NzgxOQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429497819", "bodyText": "This is missing the type parameter for VectorizedReader.", "author": "rdblue", "createdAt": "2020-05-23T00:41:53Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader message(", "originalCommit": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\nindex b506557c2..01cbe6f28 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n\n@@ -19,19 +19,18 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.IntStream;\n import org.apache.arrow.memory.BufferAllocator;\n-import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.arrow.ArrowAllocation;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.schema.GroupType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5Nzg4Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429497882", "bodyText": "IntelliJ is telling me that this isn't needed?", "author": "rdblue", "createdAt": "2020-05-23T00:42:31Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")", "originalCommit": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\nindex b506557c2..01cbe6f28 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n\n@@ -19,19 +19,18 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.IntStream;\n import org.apache.arrow.memory.BufferAllocator;\n-import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.arrow.ArrowAllocation;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.schema.GroupType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5ODA2Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429498062", "bodyText": "Now that #1004 is in, this can be removed. Now, we guarantee that the expected schema has all of the necessary columns. No need to create requiredSchema and handle it differently.", "author": "rdblue", "createdAt": "2020-05-23T00:44:32Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();", "originalCommit": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java b/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\nindex f14f1c542..eeb3ad559 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java\n\n@@ -19,68 +19,49 @@\n \n package org.apache.iceberg.spark.source;\n \n-import com.google.common.base.Preconditions;\n-import java.util.Iterator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.parquet.Parquet;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n-import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n \n class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n-  private final Schema tableSchema;\n   private final Schema expectedSchema;\n   private final boolean caseSensitive;\n   private final int batchSize;\n \n   BatchDataReader(\n-      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      CombinedScanTask task, Schema expectedSchema, FileIO fileIo,\n       EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n     super(task, fileIo, encryptionManager);\n-    this.tableSchema = tableSchema;\n     this.expectedSchema = expectedSchema;\n     this.caseSensitive = caseSensitive;\n     this.batchSize = size;\n   }\n \n   @Override\n-  Iterator<ColumnarBatch> open(FileScanTask task) {\n-    // schema or rows returned by readers\n-    Schema finalSchema = expectedSchema;\n-    // schema needed for the projection and filtering\n-    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n-    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n-    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n-    Iterator<ColumnarBatch> iter;\n-    if (hasExtraFilterColumns) {\n-      iter = open(task, requiredSchema);\n-    } else {\n-      iter = open(task, finalSchema);\n-    }\n-    return iter;\n-  }\n-\n-  private Iterator<ColumnarBatch> open(FileScanTask task, Schema readSchema) {\n+  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n     CloseableIterable<ColumnarBatch> iter;\n     InputFile location = getInputFile(task);\n     Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n     if (task.file().format() == FileFormat.PARQUET) {\n       iter = Parquet.read(location)\n-          .project(readSchema)\n+          .project(expectedSchema)\n           .split(task.start(), task.length())\n-          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(readSchema,\n-              fileSchema, batchSize))\n+          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n+              fileSchema, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED))\n+          .recordsPerBatch(batchSize)\n           .filter(task.residual())\n           .caseSensitive(caseSensitive)\n-          .recordsPerBatch(batchSize)\n           // Spark eagerly consumes the batches. So the underlying memory allocated could be reused\n           // without worrying about subsequent reads clobbering over each other. This improves\n           // read performance as every batch read doesn't have to pay the cost of allocating memory.\n"}}, {"oid": "65700df38f4763edfb8e7bdd30013aec413a65c5", "url": "https://github.com/apache/iceberg/commit/65700df38f4763edfb8e7bdd30013aec413a65c5", "message": "Code review comments", "committedDate": "2020-05-26T23:44:12Z", "type": "forcePushed"}, {"oid": "032c8c12c972614ee5514b7c586021331c8bf002", "url": "https://github.com/apache/iceberg/commit/032c8c12c972614ee5514b7c586021331c8bf002", "message": "Code review comments", "committedDate": "2020-05-27T00:12:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MDkwMQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434840901", "bodyText": "This method isn't used, and the main difference between it and assertArrowVectors is that the other one asserts that each vector is an IcebergArrowColumnVector, which I don't think is actually necessary. Is there a reason to require a specific type instead of just validating the data in each row?\nAlso, since the ColumnarBatch provides access to an InternalRow that is already supported, I was able to use the existing assertEqualsUnsafe(StructType, Record, InternalRow) in a loop, like this:\n  public static void assertEqualsBatch(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {\n    for (int r = 0; r < batch.numRows(); r++) {\n      assertEqualsUnsafe(struct, expected.get(r), batch.getRow(r));\n    }\n  }\nThat required fixing null handling in assertEqualsUnsafe, which wasn't calling isNullAt, but once that was updated everything works without these two fairly large methods. I'd prefer to move to using assertEqualsBatch instead unless you think that introduces a problem.", "author": "rdblue", "createdAt": "2020-06-03T20:41:40Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java", "diffHunk": "@@ -78,6 +81,48 @@ public static void assertEqualsSafe(Types.StructType struct, Record rec, Row row\n     }\n   }\n \n+  public static void assertEqualsUnsafe(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODE1MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918151", "bodyText": "Fixed in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:52:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MDkwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\nindex 7b0450561..f603757c2 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n\n@@ -81,47 +83,27 @@ public class TestHelpers {\n     }\n   }\n \n-  public static void assertEqualsUnsafe(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {\n-    List<Types.NestedField> fields = struct.fields();\n-    for (int r = 0; r < batch.numRows(); r++) {\n-\n-      Record expRec = expected.get(r);\n-      InternalRow actualRow = batch.getRow(r);\n-\n+  public static void assertEqualsBatch(Types.StructType struct, Iterator<Record> expected, ColumnarBatch batch,\n+                                       boolean checkArrowValidityVector) {\n+    for (int rowId = 0; rowId < batch.numRows(); rowId++) {\n+      List<Types.NestedField> fields = struct.fields();\n+      InternalRow row = batch.getRow(rowId);\n+      Record rec = expected.next();\n       for (int i = 0; i < fields.size(); i += 1) {\n-\n         Type fieldType = fields.get(i).type();\n-        Object expectedValue = expRec.get(i);\n-        if (actualRow.isNullAt(i)) {\n-          Assert.assertTrue(\"Expect null at \" + r, expectedValue == null);\n-        } else {\n-          Object actualValue = actualRow.get(i, convert(fieldType));\n-          assertEqualsUnsafe(fieldType, expectedValue, actualValue);\n+        Object expectedValue = rec.get(i);\n+        Object actualValue = row.isNullAt(i) ? null : row.get(i, convert(fieldType));\n+        assertEqualsUnsafe(fieldType, expectedValue, actualValue);\n+\n+        if (checkArrowValidityVector) {\n+          ColumnVector columnVector = batch.column(i);\n+          ValueVector arrowVector = ((IcebergArrowColumnVector) columnVector).vectorAccessor().getVector();\n+          Assert.assertEquals(\"Nullability doesn't match\", expectedValue == null, arrowVector.isNull(rowId));\n         }\n       }\n     }\n   }\n \n-  public static void assertArrowVectors(Types.StructType struct, List<Record> expected,\n-                                        ColumnarBatch batch) {\n-    List<Types.NestedField> fields = struct.fields();\n-    for (int r = 0; r < batch.numRows(); r++) {\n-      Record expRec = expected.get(r);\n-      InternalRow actualRow = batch.getRow(r);\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        ColumnVector vector = batch.column(i);\n-        Assert.assertTrue(vector instanceof IcebergArrowColumnVector);\n-        Type fieldType = fields.get(i).type();\n-        Object expectedValue = expRec.get(i);\n-        if (actualRow.isNullAt(i)) {\n-          Assert.assertNull(expectedValue);\n-        } else {\n-          Object actualValue = actualRow.get(i, convert(fieldType));\n-          assertEqualsUnsafe(fieldType, expectedValue, actualValue);\n-        }\n-      }\n-    }\n-  }\n \n   private static void assertEqualsSafe(Types.ListType list, Collection<?> expected, List actual) {\n     Type elementType = list.elementType();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MjIwOQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434842209", "bodyText": "Can we use Arrays.fill instead of leaving some bytes uninitialized?", "author": "rdblue", "createdAt": "2020-06-03T20:44:22Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+      // 3 choices\n+      int choice = random.nextInt(3);\n+      switch (primitive.typeId()) {\n+        case BOOLEAN:\n+          return true; // doesn't really matter for booleans since they are not dictionary encoded\n+\n+        case INTEGER:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case LONG:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case FLOAT:\n+          switch (choice) {\n+            case 0:\n+              return 0.0f;\n+            case 1:\n+              return 1.0f;\n+            case 2:\n+              return 2.0f;\n+          }\n+\n+        case DOUBLE:\n+          switch (choice) {\n+            case 0:\n+              return 0.0d;\n+            case 1:\n+              return 1.0d;\n+            case 2:\n+              return 2.0d;\n+          }\n+\n+        case DATE:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case TIME:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case TIMESTAMP:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case STRING:\n+          switch (choice) {\n+            case 0:\n+              return UTF8String.fromString(\"0\");\n+            case 1:\n+              return UTF8String.fromString(\"1\");\n+            case 2:\n+              return UTF8String.fromString(\"2\");\n+          }\n+\n+        case FIXED:\n+          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n+          switch (choice) {\n+            case 0:\n+              fixed[0] = 0;", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\nindex 4b9c4ef61..f99c0fccb 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n\n@@ -320,380 +319,82 @@ public class RandomData {\n \n     @Override\n     public Object primitive(Type.PrimitiveType primitive) {\n-      return generatePrimitive(primitive, random);\n+      Object obj = RandomUtil.generatePrimitive(primitive, random);\n+      switch (primitive.typeId()) {\n+        case STRING:\n+          return UTF8String.fromString((String) obj);\n+        case DECIMAL:\n+          return Decimal.apply((BigDecimal) obj);\n+        default:\n+          return obj;\n+      }\n     }\n   }\n \n-  @SuppressWarnings(\"RandomModInteger\")\n-  private static Object generatePrimitive(Type.PrimitiveType primitive,\n-                                         Random random) {\n-    int choice = random.nextInt(20);\n-\n+  private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+    int value = random.nextInt(3);\n     switch (primitive.typeId()) {\n       case BOOLEAN:\n-        return choice < 10;\n-\n+        return true; // doesn't really matter for booleans since they are not dictionary encoded\n       case INTEGER:\n-        switch (choice) {\n-          case 1:\n-            return Integer.MIN_VALUE;\n-          case 2:\n-            return Integer.MAX_VALUE;\n-          case 3:\n-            return 0;\n-          default:\n-            return random.nextInt();\n-        }\n-\n-      case LONG:\n-        switch (choice) {\n-          case 1:\n-            return Long.MIN_VALUE;\n-          case 2:\n-            return Long.MAX_VALUE;\n-          case 3:\n-            return 0L;\n-          default:\n-            return random.nextLong();\n-        }\n-\n+      case DATE:\n+        return value;\n       case FLOAT:\n-        switch (choice) {\n-          case 1:\n-            return Float.MIN_VALUE;\n-          case 2:\n-            return -Float.MIN_VALUE;\n-          case 3:\n-            return Float.MAX_VALUE;\n-          case 4:\n-            return -Float.MAX_VALUE;\n-          case 5:\n-            return Float.NEGATIVE_INFINITY;\n-          case 6:\n-            return Float.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0F;\n-          case 8:\n-            return Float.NaN;\n-          default:\n-            return random.nextFloat();\n-        }\n-\n+        return (float) value;\n       case DOUBLE:\n-        switch (choice) {\n-          case 1:\n-            return Double.MIN_VALUE;\n-          case 2:\n-            return -Double.MIN_VALUE;\n-          case 3:\n-            return Double.MAX_VALUE;\n-          case 4:\n-            return -Double.MAX_VALUE;\n-          case 5:\n-            return Double.NEGATIVE_INFINITY;\n-          case 6:\n-            return Double.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0D;\n-          case 8:\n-            return Double.NaN;\n-          default:\n-            return random.nextDouble();\n-        }\n-\n-      case DATE:\n-        // this will include negative values (dates before 1970-01-01)\n-        return random.nextInt() % ABOUT_380_YEARS_IN_DAYS;\n-\n+        return (double) value;\n+      case LONG:\n       case TIME:\n-        return (random.nextLong() & Integer.MAX_VALUE) % ONE_DAY_IN_MICROS;\n-\n       case TIMESTAMP:\n-        return random.nextLong() % FIFTY_YEARS_IN_MICROS;\n-\n+        return (long) value;\n       case STRING:\n-        return randomString(random);\n-\n-      case UUID:\n-        byte[] uuidBytes = new byte[16];\n-        random.nextBytes(uuidBytes);\n-        // this will hash the uuidBytes\n-        return uuidBytes;\n-\n+        return String.valueOf(value);\n       case FIXED:\n         byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-        random.nextBytes(fixed);\n+        Arrays.fill(fixed, (byte) value);\n         return fixed;\n-\n       case BINARY:\n-        byte[] binary = new byte[random.nextInt(50)];\n-        random.nextBytes(binary);\n+        byte[] binary = new byte[value + 1];\n+        Arrays.fill(binary, (byte) value);\n         return binary;\n-\n       case DECIMAL:\n         Types.DecimalType type = (Types.DecimalType) primitive;\n-        BigInteger unscaled = randomUnscaled(type.precision(), random);\n-        return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-\n+        BigInteger unscaled = new BigInteger(String.valueOf(value + 1));\n+        return new BigDecimal(unscaled, type.scale());\n       default:\n         throw new IllegalArgumentException(\n             \"Cannot generate random value for unknown type: \" + primitive);\n     }\n   }\n \n-  private static final long FIFTY_YEARS_IN_MICROS =\n-      (50L * (365 * 3 + 366) * 24 * 60 * 60 * 1_000_000) / 4;\n-  private static final int ABOUT_380_YEARS_IN_DAYS = 380 * 365;\n-  private static final long ONE_DAY_IN_MICROS = 24 * 60 * 60 * 1_000_000L;\n-  private static final String CHARS =\n-      \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!?\";\n-\n-  private static UTF8String randomString(Random random) {\n-    int length = random.nextInt(50);\n-    byte[] buffer = new byte[length];\n-\n-    for (int i = 0; i < length; i += 1) {\n-      buffer[i] = (byte) CHARS.charAt(random.nextInt(CHARS.length()));\n-    }\n-\n-    return UTF8String.fromBytes(buffer);\n-  }\n-\n-  private static final String DIGITS = \"0123456789\";\n-\n-  private static BigInteger randomUnscaled(int precision, Random random) {\n-    int length = random.nextInt(precision);\n-    if (length == 0) {\n-      return BigInteger.ZERO;\n-    }\n-\n-    StringBuilder sb = new StringBuilder();\n-    for (int i = 0; i < length; i += 1) {\n-      sb.append(DIGITS.charAt(random.nextInt(DIGITS.length())));\n-    }\n-\n-    return new BigInteger(sb.toString());\n-  }\n-\n   private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n-\n-    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n-      super(schema, seed);\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed, float nullPercentage) {\n+      super(schema, seed, nullPercentage);\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n-      return super.getPrimitive(primitive, result);\n-    }\n-\n-    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n-      // 3 choices\n-      int choice = random.nextInt(3);\n-      switch (primitive.typeId()) {\n-        case BOOLEAN:\n-          return true; // doesn't really matter for booleans since they are not dictionary encoded\n-\n-        case INTEGER:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case LONG:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case FLOAT:\n-          switch (choice) {\n-            case 0:\n-              return 0.0f;\n-            case 1:\n-              return 1.0f;\n-            case 2:\n-              return 2.0f;\n-          }\n-\n-        case DOUBLE:\n-          switch (choice) {\n-            case 0:\n-              return 0.0d;\n-            case 1:\n-              return 1.0d;\n-            case 2:\n-              return 2.0d;\n-          }\n-\n-        case DATE:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case TIME:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case TIMESTAMP:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case STRING:\n-          switch (choice) {\n-            case 0:\n-              return UTF8String.fromString(\"0\");\n-            case 1:\n-              return UTF8String.fromString(\"1\");\n-            case 2:\n-              return UTF8String.fromString(\"2\");\n-          }\n-\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          switch (choice) {\n-            case 0:\n-              fixed[0] = 0;\n-              return fixed;\n-            case 1:\n-              fixed[0] = 1;\n-              return fixed;\n-            case 2:\n-              fixed[0] = 2;\n-              return fixed;\n-          }\n-\n-        case BINARY:\n-          byte[] binary = new byte[4];\n-          switch (choice) {\n-            case 0:\n-              binary[0] = 0;\n-              return binary;\n-            case 1:\n-              binary[0] = 1;\n-              return binary;\n-            case 2:\n-              binary[0] = 2;\n-              return binary;\n-          }\n-\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          switch (choice) {\n-            case 0:\n-              BigInteger unscaled = new BigInteger(\"1\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 1:\n-              unscaled = new BigInteger(\"2\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 2:\n-              unscaled = new BigInteger(\"3\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          }\n-\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate random value for unknown type: \" + primitive);\n-      }\n+    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n+      return generateDictionaryEncodablePrimitive(primitive, random);\n     }\n   }\n \n-  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n-    private final long numValues;\n-    private final float fraction;\n-    private int current;\n-\n-    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {\n-      super(schema, seed);\n-      // for now, vectorized reads are only supported for primitive types\n-      this.numValues =\n-          numRecords * schema.columns().stream().filter(nestedField -> nestedField.type().isPrimitiveType()).count();\n-      this.fraction = fraction;\n+  private static class FallbackDataGenerator extends RandomDataGenerator {\n+    private final long dictionaryEncodedRows;\n+    private long rowCount = 0;\n+\n+    private FallbackDataGenerator(Schema schema, long seed, long numDictionaryEncoded) {\n+      super(schema, seed, DEFAULT_NULL_PERCENTAGE);\n+      this.dictionaryEncodedRows = numDictionaryEncoded;\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      current++;\n-      boolean dictionaryEncodable = current < fraction * numValues;\n-      Object result;\n-      switch (primitive.typeId()) {\n-        case STRING:\n-          result = dictionaryEncodable ? UTF8String.fromString(\"ABC\") : randomString(random);\n-          break;\n-        case BOOLEAN:\n-          result = true; // doesn't really matter for booleans since they are not dictionary encoded\n-          break;\n-        case INTEGER:\n-        case DATE:\n-          result = dictionaryEncodable ? 1 : random.nextInt();\n-          break;\n-        case LONG:\n-        case TIME:\n-        case TIMESTAMP:\n-          result = dictionaryEncodable ? 1L : random.nextLong();\n-          break;\n-        case FLOAT:\n-          result = dictionaryEncodable ? 1.0f : random.nextFloat();\n-          break;\n-        case DOUBLE:\n-          result = dictionaryEncodable ? 1.0d : random.nextDouble();\n-          break;\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          if (dictionaryEncodable) {\n-            fixed[0] = 1;\n-          } else {\n-            random.nextBytes(fixed);\n-          }\n-          result = fixed;\n-          break;\n-        case BINARY:\n-          byte[] binary;\n-          if (dictionaryEncodable) {\n-            binary = new byte[1];\n-            binary[0] = 1;\n-          } else {\n-            binary = new byte[random.nextInt(50)];\n-            random.nextBytes(binary);\n-          }\n-          result = binary;\n-          break;\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          BigInteger unscaled = dictionaryEncodable ? new BigInteger(\"1\") : randomUnscaled(type.precision(), random);\n-          result = Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          break;\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate value for unknown type: \" + primitive);\n+    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n+      this.rowCount += 1;\n+      if (rowCount > dictionaryEncodedRows) {\n+        return RandomUtil.generatePrimitive(primitive, rand);\n+      } else {\n+        return generateDictionaryEncodablePrimitive(primitive, rand);\n       }\n-      return super.getPrimitive(primitive, result);\n     }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1MTY1MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434851651", "bodyText": "I think the way that this extends the base class is awkward. It overrides primitive, but then needs to call getPrimitive. What getPrimitive is doing is not obvious in the child class, and the Random variable needs to be directly accessible causing the need to override checkstyle.\nInstead, the base class should add a method to generate a value for a primitive, like randomValue and pass a Random into it. Then random can stay private and we don't need to override checkstyle. Also, the conversion only needs to happen in one place, the implementation of primitive.\n    @Override\n    public Object primitive(Type.PrimitiveType primitive) {\n      Object result = randomValue(primitive, random);\n      // For the primitives that Avro needs a different type than Spark, fix\n      // them here.\n      switch (primitive.typeId()) {\n        ...\n      }\n    }\n\n    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n      return generatePrimitive(primitive, rand);\n    }", "author": "rdblue", "createdAt": "2020-06-03T21:01:05Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODA4OQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918089", "bodyText": "Included in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:52:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1MTY1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\nindex 4b9c4ef61..f99c0fccb 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n\n@@ -320,380 +319,82 @@ public class RandomData {\n \n     @Override\n     public Object primitive(Type.PrimitiveType primitive) {\n-      return generatePrimitive(primitive, random);\n+      Object obj = RandomUtil.generatePrimitive(primitive, random);\n+      switch (primitive.typeId()) {\n+        case STRING:\n+          return UTF8String.fromString((String) obj);\n+        case DECIMAL:\n+          return Decimal.apply((BigDecimal) obj);\n+        default:\n+          return obj;\n+      }\n     }\n   }\n \n-  @SuppressWarnings(\"RandomModInteger\")\n-  private static Object generatePrimitive(Type.PrimitiveType primitive,\n-                                         Random random) {\n-    int choice = random.nextInt(20);\n-\n+  private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+    int value = random.nextInt(3);\n     switch (primitive.typeId()) {\n       case BOOLEAN:\n-        return choice < 10;\n-\n+        return true; // doesn't really matter for booleans since they are not dictionary encoded\n       case INTEGER:\n-        switch (choice) {\n-          case 1:\n-            return Integer.MIN_VALUE;\n-          case 2:\n-            return Integer.MAX_VALUE;\n-          case 3:\n-            return 0;\n-          default:\n-            return random.nextInt();\n-        }\n-\n-      case LONG:\n-        switch (choice) {\n-          case 1:\n-            return Long.MIN_VALUE;\n-          case 2:\n-            return Long.MAX_VALUE;\n-          case 3:\n-            return 0L;\n-          default:\n-            return random.nextLong();\n-        }\n-\n+      case DATE:\n+        return value;\n       case FLOAT:\n-        switch (choice) {\n-          case 1:\n-            return Float.MIN_VALUE;\n-          case 2:\n-            return -Float.MIN_VALUE;\n-          case 3:\n-            return Float.MAX_VALUE;\n-          case 4:\n-            return -Float.MAX_VALUE;\n-          case 5:\n-            return Float.NEGATIVE_INFINITY;\n-          case 6:\n-            return Float.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0F;\n-          case 8:\n-            return Float.NaN;\n-          default:\n-            return random.nextFloat();\n-        }\n-\n+        return (float) value;\n       case DOUBLE:\n-        switch (choice) {\n-          case 1:\n-            return Double.MIN_VALUE;\n-          case 2:\n-            return -Double.MIN_VALUE;\n-          case 3:\n-            return Double.MAX_VALUE;\n-          case 4:\n-            return -Double.MAX_VALUE;\n-          case 5:\n-            return Double.NEGATIVE_INFINITY;\n-          case 6:\n-            return Double.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0D;\n-          case 8:\n-            return Double.NaN;\n-          default:\n-            return random.nextDouble();\n-        }\n-\n-      case DATE:\n-        // this will include negative values (dates before 1970-01-01)\n-        return random.nextInt() % ABOUT_380_YEARS_IN_DAYS;\n-\n+        return (double) value;\n+      case LONG:\n       case TIME:\n-        return (random.nextLong() & Integer.MAX_VALUE) % ONE_DAY_IN_MICROS;\n-\n       case TIMESTAMP:\n-        return random.nextLong() % FIFTY_YEARS_IN_MICROS;\n-\n+        return (long) value;\n       case STRING:\n-        return randomString(random);\n-\n-      case UUID:\n-        byte[] uuidBytes = new byte[16];\n-        random.nextBytes(uuidBytes);\n-        // this will hash the uuidBytes\n-        return uuidBytes;\n-\n+        return String.valueOf(value);\n       case FIXED:\n         byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-        random.nextBytes(fixed);\n+        Arrays.fill(fixed, (byte) value);\n         return fixed;\n-\n       case BINARY:\n-        byte[] binary = new byte[random.nextInt(50)];\n-        random.nextBytes(binary);\n+        byte[] binary = new byte[value + 1];\n+        Arrays.fill(binary, (byte) value);\n         return binary;\n-\n       case DECIMAL:\n         Types.DecimalType type = (Types.DecimalType) primitive;\n-        BigInteger unscaled = randomUnscaled(type.precision(), random);\n-        return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-\n+        BigInteger unscaled = new BigInteger(String.valueOf(value + 1));\n+        return new BigDecimal(unscaled, type.scale());\n       default:\n         throw new IllegalArgumentException(\n             \"Cannot generate random value for unknown type: \" + primitive);\n     }\n   }\n \n-  private static final long FIFTY_YEARS_IN_MICROS =\n-      (50L * (365 * 3 + 366) * 24 * 60 * 60 * 1_000_000) / 4;\n-  private static final int ABOUT_380_YEARS_IN_DAYS = 380 * 365;\n-  private static final long ONE_DAY_IN_MICROS = 24 * 60 * 60 * 1_000_000L;\n-  private static final String CHARS =\n-      \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!?\";\n-\n-  private static UTF8String randomString(Random random) {\n-    int length = random.nextInt(50);\n-    byte[] buffer = new byte[length];\n-\n-    for (int i = 0; i < length; i += 1) {\n-      buffer[i] = (byte) CHARS.charAt(random.nextInt(CHARS.length()));\n-    }\n-\n-    return UTF8String.fromBytes(buffer);\n-  }\n-\n-  private static final String DIGITS = \"0123456789\";\n-\n-  private static BigInteger randomUnscaled(int precision, Random random) {\n-    int length = random.nextInt(precision);\n-    if (length == 0) {\n-      return BigInteger.ZERO;\n-    }\n-\n-    StringBuilder sb = new StringBuilder();\n-    for (int i = 0; i < length; i += 1) {\n-      sb.append(DIGITS.charAt(random.nextInt(DIGITS.length())));\n-    }\n-\n-    return new BigInteger(sb.toString());\n-  }\n-\n   private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n-\n-    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n-      super(schema, seed);\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed, float nullPercentage) {\n+      super(schema, seed, nullPercentage);\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n-      return super.getPrimitive(primitive, result);\n-    }\n-\n-    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n-      // 3 choices\n-      int choice = random.nextInt(3);\n-      switch (primitive.typeId()) {\n-        case BOOLEAN:\n-          return true; // doesn't really matter for booleans since they are not dictionary encoded\n-\n-        case INTEGER:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case LONG:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case FLOAT:\n-          switch (choice) {\n-            case 0:\n-              return 0.0f;\n-            case 1:\n-              return 1.0f;\n-            case 2:\n-              return 2.0f;\n-          }\n-\n-        case DOUBLE:\n-          switch (choice) {\n-            case 0:\n-              return 0.0d;\n-            case 1:\n-              return 1.0d;\n-            case 2:\n-              return 2.0d;\n-          }\n-\n-        case DATE:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case TIME:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case TIMESTAMP:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case STRING:\n-          switch (choice) {\n-            case 0:\n-              return UTF8String.fromString(\"0\");\n-            case 1:\n-              return UTF8String.fromString(\"1\");\n-            case 2:\n-              return UTF8String.fromString(\"2\");\n-          }\n-\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          switch (choice) {\n-            case 0:\n-              fixed[0] = 0;\n-              return fixed;\n-            case 1:\n-              fixed[0] = 1;\n-              return fixed;\n-            case 2:\n-              fixed[0] = 2;\n-              return fixed;\n-          }\n-\n-        case BINARY:\n-          byte[] binary = new byte[4];\n-          switch (choice) {\n-            case 0:\n-              binary[0] = 0;\n-              return binary;\n-            case 1:\n-              binary[0] = 1;\n-              return binary;\n-            case 2:\n-              binary[0] = 2;\n-              return binary;\n-          }\n-\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          switch (choice) {\n-            case 0:\n-              BigInteger unscaled = new BigInteger(\"1\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 1:\n-              unscaled = new BigInteger(\"2\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 2:\n-              unscaled = new BigInteger(\"3\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          }\n-\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate random value for unknown type: \" + primitive);\n-      }\n+    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n+      return generateDictionaryEncodablePrimitive(primitive, random);\n     }\n   }\n \n-  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n-    private final long numValues;\n-    private final float fraction;\n-    private int current;\n-\n-    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {\n-      super(schema, seed);\n-      // for now, vectorized reads are only supported for primitive types\n-      this.numValues =\n-          numRecords * schema.columns().stream().filter(nestedField -> nestedField.type().isPrimitiveType()).count();\n-      this.fraction = fraction;\n+  private static class FallbackDataGenerator extends RandomDataGenerator {\n+    private final long dictionaryEncodedRows;\n+    private long rowCount = 0;\n+\n+    private FallbackDataGenerator(Schema schema, long seed, long numDictionaryEncoded) {\n+      super(schema, seed, DEFAULT_NULL_PERCENTAGE);\n+      this.dictionaryEncodedRows = numDictionaryEncoded;\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      current++;\n-      boolean dictionaryEncodable = current < fraction * numValues;\n-      Object result;\n-      switch (primitive.typeId()) {\n-        case STRING:\n-          result = dictionaryEncodable ? UTF8String.fromString(\"ABC\") : randomString(random);\n-          break;\n-        case BOOLEAN:\n-          result = true; // doesn't really matter for booleans since they are not dictionary encoded\n-          break;\n-        case INTEGER:\n-        case DATE:\n-          result = dictionaryEncodable ? 1 : random.nextInt();\n-          break;\n-        case LONG:\n-        case TIME:\n-        case TIMESTAMP:\n-          result = dictionaryEncodable ? 1L : random.nextLong();\n-          break;\n-        case FLOAT:\n-          result = dictionaryEncodable ? 1.0f : random.nextFloat();\n-          break;\n-        case DOUBLE:\n-          result = dictionaryEncodable ? 1.0d : random.nextDouble();\n-          break;\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          if (dictionaryEncodable) {\n-            fixed[0] = 1;\n-          } else {\n-            random.nextBytes(fixed);\n-          }\n-          result = fixed;\n-          break;\n-        case BINARY:\n-          byte[] binary;\n-          if (dictionaryEncodable) {\n-            binary = new byte[1];\n-            binary[0] = 1;\n-          } else {\n-            binary = new byte[random.nextInt(50)];\n-            random.nextBytes(binary);\n-          }\n-          result = binary;\n-          break;\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          BigInteger unscaled = dictionaryEncodable ? new BigInteger(\"1\") : randomUnscaled(type.precision(), random);\n-          result = Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          break;\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate value for unknown type: \" + primitive);\n+    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n+      this.rowCount += 1;\n+      if (rowCount > dictionaryEncodedRows) {\n+        return RandomUtil.generatePrimitive(primitive, rand);\n+      } else {\n+        return generateDictionaryEncodablePrimitive(primitive, rand);\n       }\n-      return super.getPrimitive(primitive, result);\n     }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1NDAwOA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434854008", "bodyText": "This implementation seems over-complicated with a lot of unnecessary switch statements. I think checkstyle was right.\nI think you can simplify most of the implementations for types by just converting the choice variable (maybe name it better?)\n    @Override\n    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n      // 3 choices\n      int choice = random.nextInt(3);\n      switch (primitive.typeId()) {\n        case BOOLEAN:\n          return true; // doesn't really matter for booleans since they are not dictionary encoded\n        case INTEGER:\n        case DATE:\n          return choice;\n        case FLOAT:\n          return (float) choice;\n        case DOUBLE:\n          return (double) choice;\n        case LONG:\n        case TIME:\n        case TIMESTAMP:\n          return (long) choice;\n        case STRING:\n          return UTF8String.fromString(String.valueOf(choice));\n        case FIXED:\n          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n          Arrays.fill(fixed, (byte) choice);\n          return fixed;\n        case BINARY:\n          byte[] binary = new byte[choice + 1];\n          Arrays.fill(binary, (byte) choice);\n          return binary;\n        case DECIMAL:\n          Types.DecimalType type = (Types.DecimalType) primitive;\n          BigInteger unscaled = new BigInteger(String.valueOf(choice + 1));\n          return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n        default:\n          throw new IllegalArgumentException(\n              \"Cannot generate random value for unknown type: \" + primitive);\n      }\n    }\nThis also uses Arrays.fill like I suggested below and updates binary to test different lengths.", "author": "rdblue", "createdAt": "2020-06-03T21:04:02Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODA0NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918044", "bodyText": "Included in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1NDAwOA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\nindex 4b9c4ef61..f99c0fccb 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n\n@@ -320,380 +319,82 @@ public class RandomData {\n \n     @Override\n     public Object primitive(Type.PrimitiveType primitive) {\n-      return generatePrimitive(primitive, random);\n+      Object obj = RandomUtil.generatePrimitive(primitive, random);\n+      switch (primitive.typeId()) {\n+        case STRING:\n+          return UTF8String.fromString((String) obj);\n+        case DECIMAL:\n+          return Decimal.apply((BigDecimal) obj);\n+        default:\n+          return obj;\n+      }\n     }\n   }\n \n-  @SuppressWarnings(\"RandomModInteger\")\n-  private static Object generatePrimitive(Type.PrimitiveType primitive,\n-                                         Random random) {\n-    int choice = random.nextInt(20);\n-\n+  private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+    int value = random.nextInt(3);\n     switch (primitive.typeId()) {\n       case BOOLEAN:\n-        return choice < 10;\n-\n+        return true; // doesn't really matter for booleans since they are not dictionary encoded\n       case INTEGER:\n-        switch (choice) {\n-          case 1:\n-            return Integer.MIN_VALUE;\n-          case 2:\n-            return Integer.MAX_VALUE;\n-          case 3:\n-            return 0;\n-          default:\n-            return random.nextInt();\n-        }\n-\n-      case LONG:\n-        switch (choice) {\n-          case 1:\n-            return Long.MIN_VALUE;\n-          case 2:\n-            return Long.MAX_VALUE;\n-          case 3:\n-            return 0L;\n-          default:\n-            return random.nextLong();\n-        }\n-\n+      case DATE:\n+        return value;\n       case FLOAT:\n-        switch (choice) {\n-          case 1:\n-            return Float.MIN_VALUE;\n-          case 2:\n-            return -Float.MIN_VALUE;\n-          case 3:\n-            return Float.MAX_VALUE;\n-          case 4:\n-            return -Float.MAX_VALUE;\n-          case 5:\n-            return Float.NEGATIVE_INFINITY;\n-          case 6:\n-            return Float.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0F;\n-          case 8:\n-            return Float.NaN;\n-          default:\n-            return random.nextFloat();\n-        }\n-\n+        return (float) value;\n       case DOUBLE:\n-        switch (choice) {\n-          case 1:\n-            return Double.MIN_VALUE;\n-          case 2:\n-            return -Double.MIN_VALUE;\n-          case 3:\n-            return Double.MAX_VALUE;\n-          case 4:\n-            return -Double.MAX_VALUE;\n-          case 5:\n-            return Double.NEGATIVE_INFINITY;\n-          case 6:\n-            return Double.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0D;\n-          case 8:\n-            return Double.NaN;\n-          default:\n-            return random.nextDouble();\n-        }\n-\n-      case DATE:\n-        // this will include negative values (dates before 1970-01-01)\n-        return random.nextInt() % ABOUT_380_YEARS_IN_DAYS;\n-\n+        return (double) value;\n+      case LONG:\n       case TIME:\n-        return (random.nextLong() & Integer.MAX_VALUE) % ONE_DAY_IN_MICROS;\n-\n       case TIMESTAMP:\n-        return random.nextLong() % FIFTY_YEARS_IN_MICROS;\n-\n+        return (long) value;\n       case STRING:\n-        return randomString(random);\n-\n-      case UUID:\n-        byte[] uuidBytes = new byte[16];\n-        random.nextBytes(uuidBytes);\n-        // this will hash the uuidBytes\n-        return uuidBytes;\n-\n+        return String.valueOf(value);\n       case FIXED:\n         byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-        random.nextBytes(fixed);\n+        Arrays.fill(fixed, (byte) value);\n         return fixed;\n-\n       case BINARY:\n-        byte[] binary = new byte[random.nextInt(50)];\n-        random.nextBytes(binary);\n+        byte[] binary = new byte[value + 1];\n+        Arrays.fill(binary, (byte) value);\n         return binary;\n-\n       case DECIMAL:\n         Types.DecimalType type = (Types.DecimalType) primitive;\n-        BigInteger unscaled = randomUnscaled(type.precision(), random);\n-        return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-\n+        BigInteger unscaled = new BigInteger(String.valueOf(value + 1));\n+        return new BigDecimal(unscaled, type.scale());\n       default:\n         throw new IllegalArgumentException(\n             \"Cannot generate random value for unknown type: \" + primitive);\n     }\n   }\n \n-  private static final long FIFTY_YEARS_IN_MICROS =\n-      (50L * (365 * 3 + 366) * 24 * 60 * 60 * 1_000_000) / 4;\n-  private static final int ABOUT_380_YEARS_IN_DAYS = 380 * 365;\n-  private static final long ONE_DAY_IN_MICROS = 24 * 60 * 60 * 1_000_000L;\n-  private static final String CHARS =\n-      \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!?\";\n-\n-  private static UTF8String randomString(Random random) {\n-    int length = random.nextInt(50);\n-    byte[] buffer = new byte[length];\n-\n-    for (int i = 0; i < length; i += 1) {\n-      buffer[i] = (byte) CHARS.charAt(random.nextInt(CHARS.length()));\n-    }\n-\n-    return UTF8String.fromBytes(buffer);\n-  }\n-\n-  private static final String DIGITS = \"0123456789\";\n-\n-  private static BigInteger randomUnscaled(int precision, Random random) {\n-    int length = random.nextInt(precision);\n-    if (length == 0) {\n-      return BigInteger.ZERO;\n-    }\n-\n-    StringBuilder sb = new StringBuilder();\n-    for (int i = 0; i < length; i += 1) {\n-      sb.append(DIGITS.charAt(random.nextInt(DIGITS.length())));\n-    }\n-\n-    return new BigInteger(sb.toString());\n-  }\n-\n   private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n-\n-    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n-      super(schema, seed);\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed, float nullPercentage) {\n+      super(schema, seed, nullPercentage);\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n-      return super.getPrimitive(primitive, result);\n-    }\n-\n-    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n-      // 3 choices\n-      int choice = random.nextInt(3);\n-      switch (primitive.typeId()) {\n-        case BOOLEAN:\n-          return true; // doesn't really matter for booleans since they are not dictionary encoded\n-\n-        case INTEGER:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case LONG:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case FLOAT:\n-          switch (choice) {\n-            case 0:\n-              return 0.0f;\n-            case 1:\n-              return 1.0f;\n-            case 2:\n-              return 2.0f;\n-          }\n-\n-        case DOUBLE:\n-          switch (choice) {\n-            case 0:\n-              return 0.0d;\n-            case 1:\n-              return 1.0d;\n-            case 2:\n-              return 2.0d;\n-          }\n-\n-        case DATE:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case TIME:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case TIMESTAMP:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case STRING:\n-          switch (choice) {\n-            case 0:\n-              return UTF8String.fromString(\"0\");\n-            case 1:\n-              return UTF8String.fromString(\"1\");\n-            case 2:\n-              return UTF8String.fromString(\"2\");\n-          }\n-\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          switch (choice) {\n-            case 0:\n-              fixed[0] = 0;\n-              return fixed;\n-            case 1:\n-              fixed[0] = 1;\n-              return fixed;\n-            case 2:\n-              fixed[0] = 2;\n-              return fixed;\n-          }\n-\n-        case BINARY:\n-          byte[] binary = new byte[4];\n-          switch (choice) {\n-            case 0:\n-              binary[0] = 0;\n-              return binary;\n-            case 1:\n-              binary[0] = 1;\n-              return binary;\n-            case 2:\n-              binary[0] = 2;\n-              return binary;\n-          }\n-\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          switch (choice) {\n-            case 0:\n-              BigInteger unscaled = new BigInteger(\"1\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 1:\n-              unscaled = new BigInteger(\"2\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 2:\n-              unscaled = new BigInteger(\"3\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          }\n-\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate random value for unknown type: \" + primitive);\n-      }\n+    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n+      return generateDictionaryEncodablePrimitive(primitive, random);\n     }\n   }\n \n-  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n-    private final long numValues;\n-    private final float fraction;\n-    private int current;\n-\n-    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {\n-      super(schema, seed);\n-      // for now, vectorized reads are only supported for primitive types\n-      this.numValues =\n-          numRecords * schema.columns().stream().filter(nestedField -> nestedField.type().isPrimitiveType()).count();\n-      this.fraction = fraction;\n+  private static class FallbackDataGenerator extends RandomDataGenerator {\n+    private final long dictionaryEncodedRows;\n+    private long rowCount = 0;\n+\n+    private FallbackDataGenerator(Schema schema, long seed, long numDictionaryEncoded) {\n+      super(schema, seed, DEFAULT_NULL_PERCENTAGE);\n+      this.dictionaryEncodedRows = numDictionaryEncoded;\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      current++;\n-      boolean dictionaryEncodable = current < fraction * numValues;\n-      Object result;\n-      switch (primitive.typeId()) {\n-        case STRING:\n-          result = dictionaryEncodable ? UTF8String.fromString(\"ABC\") : randomString(random);\n-          break;\n-        case BOOLEAN:\n-          result = true; // doesn't really matter for booleans since they are not dictionary encoded\n-          break;\n-        case INTEGER:\n-        case DATE:\n-          result = dictionaryEncodable ? 1 : random.nextInt();\n-          break;\n-        case LONG:\n-        case TIME:\n-        case TIMESTAMP:\n-          result = dictionaryEncodable ? 1L : random.nextLong();\n-          break;\n-        case FLOAT:\n-          result = dictionaryEncodable ? 1.0f : random.nextFloat();\n-          break;\n-        case DOUBLE:\n-          result = dictionaryEncodable ? 1.0d : random.nextDouble();\n-          break;\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          if (dictionaryEncodable) {\n-            fixed[0] = 1;\n-          } else {\n-            random.nextBytes(fixed);\n-          }\n-          result = fixed;\n-          break;\n-        case BINARY:\n-          byte[] binary;\n-          if (dictionaryEncodable) {\n-            binary = new byte[1];\n-            binary[0] = 1;\n-          } else {\n-            binary = new byte[random.nextInt(50)];\n-            random.nextBytes(binary);\n-          }\n-          result = binary;\n-          break;\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          BigInteger unscaled = dictionaryEncodable ? new BigInteger(\"1\") : randomUnscaled(type.precision(), random);\n-          result = Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          break;\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate value for unknown type: \" + primitive);\n+    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n+      this.rowCount += 1;\n+      if (rowCount > dictionaryEncodedRows) {\n+        return RandomUtil.generatePrimitive(primitive, rand);\n+      } else {\n+        return generateDictionaryEncodablePrimitive(primitive, rand);\n       }\n-      return super.getPrimitive(primitive, result);\n     }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MDY5Mg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434860692", "bodyText": "I think it would be easier to understand if this were a combination of the normal generator and the dictionary-encoded generator and used just some number of records before falling back. Then you would only need the two existing ways to generate primitives. Something like this, where generateDictionaryEncodablePrimitive() is what I pasted for DictionaryEncodedDataGenerator above:\n  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n      super(schema, seed);\n    }\n\n    @Override\n    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n      return generateDictionaryEncodablePrimitive(primitive, random);\n    }\n  }\n\n  private static class FallbackDataGenerator extends RandomDataGenerator {\n    private final long dictionaryEncodedRows;\n    private long rowCount = 0;\n\n    private FallbackDataGenerator(Schema schema, long seed, long numDictionaryEncoded) {\n      super(schema, seed);\n      this.dictionaryEncodedRows = numDictionaryEncoded;\n    }\n\n    @Override\n    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n      this.rowCount += 1;\n      if (rowCount > dictionaryEncodedRows) {\n        return generatePrimitive(primitive, rand);\n      } else {\n        return generateDictionaryEncodablePrimitive(primitive, rand);\n      }\n    }\n  }", "author": "rdblue", "createdAt": "2020-06-03T21:12:53Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+      // 3 choices\n+      int choice = random.nextInt(3);\n+      switch (primitive.typeId()) {\n+        case BOOLEAN:\n+          return true; // doesn't really matter for booleans since they are not dictionary encoded\n+\n+        case INTEGER:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case LONG:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case FLOAT:\n+          switch (choice) {\n+            case 0:\n+              return 0.0f;\n+            case 1:\n+              return 1.0f;\n+            case 2:\n+              return 2.0f;\n+          }\n+\n+        case DOUBLE:\n+          switch (choice) {\n+            case 0:\n+              return 0.0d;\n+            case 1:\n+              return 1.0d;\n+            case 2:\n+              return 2.0d;\n+          }\n+\n+        case DATE:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case TIME:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case TIMESTAMP:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case STRING:\n+          switch (choice) {\n+            case 0:\n+              return UTF8String.fromString(\"0\");\n+            case 1:\n+              return UTF8String.fromString(\"1\");\n+            case 2:\n+              return UTF8String.fromString(\"2\");\n+          }\n+\n+        case FIXED:\n+          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n+          switch (choice) {\n+            case 0:\n+              fixed[0] = 0;\n+              return fixed;\n+            case 1:\n+              fixed[0] = 1;\n+              return fixed;\n+            case 2:\n+              fixed[0] = 2;\n+              return fixed;\n+          }\n+\n+        case BINARY:\n+          byte[] binary = new byte[4];\n+          switch (choice) {\n+            case 0:\n+              binary[0] = 0;\n+              return binary;\n+            case 1:\n+              binary[0] = 1;\n+              return binary;\n+            case 2:\n+              binary[0] = 2;\n+              return binary;\n+          }\n+\n+        case DECIMAL:\n+          Types.DecimalType type = (Types.DecimalType) primitive;\n+          switch (choice) {\n+            case 0:\n+              BigInteger unscaled = new BigInteger(\"1\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+            case 1:\n+              unscaled = new BigInteger(\"2\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+            case 2:\n+              unscaled = new BigInteger(\"3\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+          }\n+\n+        default:\n+          throw new IllegalArgumentException(\n+              \"Cannot generate random value for unknown type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n+    private final long numValues;\n+    private final float fraction;\n+    private int current;\n+\n+    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODAwNw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918007", "bodyText": "Included in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MDY5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\nindex 4b9c4ef61..f99c0fccb 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java\n\n@@ -320,380 +319,82 @@ public class RandomData {\n \n     @Override\n     public Object primitive(Type.PrimitiveType primitive) {\n-      return generatePrimitive(primitive, random);\n+      Object obj = RandomUtil.generatePrimitive(primitive, random);\n+      switch (primitive.typeId()) {\n+        case STRING:\n+          return UTF8String.fromString((String) obj);\n+        case DECIMAL:\n+          return Decimal.apply((BigDecimal) obj);\n+        default:\n+          return obj;\n+      }\n     }\n   }\n \n-  @SuppressWarnings(\"RandomModInteger\")\n-  private static Object generatePrimitive(Type.PrimitiveType primitive,\n-                                         Random random) {\n-    int choice = random.nextInt(20);\n-\n+  private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+    int value = random.nextInt(3);\n     switch (primitive.typeId()) {\n       case BOOLEAN:\n-        return choice < 10;\n-\n+        return true; // doesn't really matter for booleans since they are not dictionary encoded\n       case INTEGER:\n-        switch (choice) {\n-          case 1:\n-            return Integer.MIN_VALUE;\n-          case 2:\n-            return Integer.MAX_VALUE;\n-          case 3:\n-            return 0;\n-          default:\n-            return random.nextInt();\n-        }\n-\n-      case LONG:\n-        switch (choice) {\n-          case 1:\n-            return Long.MIN_VALUE;\n-          case 2:\n-            return Long.MAX_VALUE;\n-          case 3:\n-            return 0L;\n-          default:\n-            return random.nextLong();\n-        }\n-\n+      case DATE:\n+        return value;\n       case FLOAT:\n-        switch (choice) {\n-          case 1:\n-            return Float.MIN_VALUE;\n-          case 2:\n-            return -Float.MIN_VALUE;\n-          case 3:\n-            return Float.MAX_VALUE;\n-          case 4:\n-            return -Float.MAX_VALUE;\n-          case 5:\n-            return Float.NEGATIVE_INFINITY;\n-          case 6:\n-            return Float.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0F;\n-          case 8:\n-            return Float.NaN;\n-          default:\n-            return random.nextFloat();\n-        }\n-\n+        return (float) value;\n       case DOUBLE:\n-        switch (choice) {\n-          case 1:\n-            return Double.MIN_VALUE;\n-          case 2:\n-            return -Double.MIN_VALUE;\n-          case 3:\n-            return Double.MAX_VALUE;\n-          case 4:\n-            return -Double.MAX_VALUE;\n-          case 5:\n-            return Double.NEGATIVE_INFINITY;\n-          case 6:\n-            return Double.POSITIVE_INFINITY;\n-          case 7:\n-            return 0.0D;\n-          case 8:\n-            return Double.NaN;\n-          default:\n-            return random.nextDouble();\n-        }\n-\n-      case DATE:\n-        // this will include negative values (dates before 1970-01-01)\n-        return random.nextInt() % ABOUT_380_YEARS_IN_DAYS;\n-\n+        return (double) value;\n+      case LONG:\n       case TIME:\n-        return (random.nextLong() & Integer.MAX_VALUE) % ONE_DAY_IN_MICROS;\n-\n       case TIMESTAMP:\n-        return random.nextLong() % FIFTY_YEARS_IN_MICROS;\n-\n+        return (long) value;\n       case STRING:\n-        return randomString(random);\n-\n-      case UUID:\n-        byte[] uuidBytes = new byte[16];\n-        random.nextBytes(uuidBytes);\n-        // this will hash the uuidBytes\n-        return uuidBytes;\n-\n+        return String.valueOf(value);\n       case FIXED:\n         byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-        random.nextBytes(fixed);\n+        Arrays.fill(fixed, (byte) value);\n         return fixed;\n-\n       case BINARY:\n-        byte[] binary = new byte[random.nextInt(50)];\n-        random.nextBytes(binary);\n+        byte[] binary = new byte[value + 1];\n+        Arrays.fill(binary, (byte) value);\n         return binary;\n-\n       case DECIMAL:\n         Types.DecimalType type = (Types.DecimalType) primitive;\n-        BigInteger unscaled = randomUnscaled(type.precision(), random);\n-        return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-\n+        BigInteger unscaled = new BigInteger(String.valueOf(value + 1));\n+        return new BigDecimal(unscaled, type.scale());\n       default:\n         throw new IllegalArgumentException(\n             \"Cannot generate random value for unknown type: \" + primitive);\n     }\n   }\n \n-  private static final long FIFTY_YEARS_IN_MICROS =\n-      (50L * (365 * 3 + 366) * 24 * 60 * 60 * 1_000_000) / 4;\n-  private static final int ABOUT_380_YEARS_IN_DAYS = 380 * 365;\n-  private static final long ONE_DAY_IN_MICROS = 24 * 60 * 60 * 1_000_000L;\n-  private static final String CHARS =\n-      \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-.!?\";\n-\n-  private static UTF8String randomString(Random random) {\n-    int length = random.nextInt(50);\n-    byte[] buffer = new byte[length];\n-\n-    for (int i = 0; i < length; i += 1) {\n-      buffer[i] = (byte) CHARS.charAt(random.nextInt(CHARS.length()));\n-    }\n-\n-    return UTF8String.fromBytes(buffer);\n-  }\n-\n-  private static final String DIGITS = \"0123456789\";\n-\n-  private static BigInteger randomUnscaled(int precision, Random random) {\n-    int length = random.nextInt(precision);\n-    if (length == 0) {\n-      return BigInteger.ZERO;\n-    }\n-\n-    StringBuilder sb = new StringBuilder();\n-    for (int i = 0; i < length; i += 1) {\n-      sb.append(DIGITS.charAt(random.nextInt(DIGITS.length())));\n-    }\n-\n-    return new BigInteger(sb.toString());\n-  }\n-\n   private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n-\n-    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n-      super(schema, seed);\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed, float nullPercentage) {\n+      super(schema, seed, nullPercentage);\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n-      return super.getPrimitive(primitive, result);\n-    }\n-\n-    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n-    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n-      // 3 choices\n-      int choice = random.nextInt(3);\n-      switch (primitive.typeId()) {\n-        case BOOLEAN:\n-          return true; // doesn't really matter for booleans since they are not dictionary encoded\n-\n-        case INTEGER:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case LONG:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case FLOAT:\n-          switch (choice) {\n-            case 0:\n-              return 0.0f;\n-            case 1:\n-              return 1.0f;\n-            case 2:\n-              return 2.0f;\n-          }\n-\n-        case DOUBLE:\n-          switch (choice) {\n-            case 0:\n-              return 0.0d;\n-            case 1:\n-              return 1.0d;\n-            case 2:\n-              return 2.0d;\n-          }\n-\n-        case DATE:\n-          switch (choice) {\n-            case 0:\n-              return 0;\n-            case 1:\n-              return 1;\n-            case 2:\n-              return 2;\n-          }\n-\n-        case TIME:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case TIMESTAMP:\n-          switch (choice) {\n-            case 0:\n-              return 0L;\n-            case 1:\n-              return 1L;\n-            case 2:\n-              return 2L;\n-          }\n-\n-        case STRING:\n-          switch (choice) {\n-            case 0:\n-              return UTF8String.fromString(\"0\");\n-            case 1:\n-              return UTF8String.fromString(\"1\");\n-            case 2:\n-              return UTF8String.fromString(\"2\");\n-          }\n-\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          switch (choice) {\n-            case 0:\n-              fixed[0] = 0;\n-              return fixed;\n-            case 1:\n-              fixed[0] = 1;\n-              return fixed;\n-            case 2:\n-              fixed[0] = 2;\n-              return fixed;\n-          }\n-\n-        case BINARY:\n-          byte[] binary = new byte[4];\n-          switch (choice) {\n-            case 0:\n-              binary[0] = 0;\n-              return binary;\n-            case 1:\n-              binary[0] = 1;\n-              return binary;\n-            case 2:\n-              binary[0] = 2;\n-              return binary;\n-          }\n-\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          switch (choice) {\n-            case 0:\n-              BigInteger unscaled = new BigInteger(\"1\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 1:\n-              unscaled = new BigInteger(\"2\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-            case 2:\n-              unscaled = new BigInteger(\"3\");\n-              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          }\n-\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate random value for unknown type: \" + primitive);\n-      }\n+    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n+      return generateDictionaryEncodablePrimitive(primitive, random);\n     }\n   }\n \n-  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n-    private final long numValues;\n-    private final float fraction;\n-    private int current;\n-\n-    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {\n-      super(schema, seed);\n-      // for now, vectorized reads are only supported for primitive types\n-      this.numValues =\n-          numRecords * schema.columns().stream().filter(nestedField -> nestedField.type().isPrimitiveType()).count();\n-      this.fraction = fraction;\n+  private static class FallbackDataGenerator extends RandomDataGenerator {\n+    private final long dictionaryEncodedRows;\n+    private long rowCount = 0;\n+\n+    private FallbackDataGenerator(Schema schema, long seed, long numDictionaryEncoded) {\n+      super(schema, seed, DEFAULT_NULL_PERCENTAGE);\n+      this.dictionaryEncodedRows = numDictionaryEncoded;\n     }\n \n     @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      current++;\n-      boolean dictionaryEncodable = current < fraction * numValues;\n-      Object result;\n-      switch (primitive.typeId()) {\n-        case STRING:\n-          result = dictionaryEncodable ? UTF8String.fromString(\"ABC\") : randomString(random);\n-          break;\n-        case BOOLEAN:\n-          result = true; // doesn't really matter for booleans since they are not dictionary encoded\n-          break;\n-        case INTEGER:\n-        case DATE:\n-          result = dictionaryEncodable ? 1 : random.nextInt();\n-          break;\n-        case LONG:\n-        case TIME:\n-        case TIMESTAMP:\n-          result = dictionaryEncodable ? 1L : random.nextLong();\n-          break;\n-        case FLOAT:\n-          result = dictionaryEncodable ? 1.0f : random.nextFloat();\n-          break;\n-        case DOUBLE:\n-          result = dictionaryEncodable ? 1.0d : random.nextDouble();\n-          break;\n-        case FIXED:\n-          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n-          if (dictionaryEncodable) {\n-            fixed[0] = 1;\n-          } else {\n-            random.nextBytes(fixed);\n-          }\n-          result = fixed;\n-          break;\n-        case BINARY:\n-          byte[] binary;\n-          if (dictionaryEncodable) {\n-            binary = new byte[1];\n-            binary[0] = 1;\n-          } else {\n-            binary = new byte[random.nextInt(50)];\n-            random.nextBytes(binary);\n-          }\n-          result = binary;\n-          break;\n-        case DECIMAL:\n-          Types.DecimalType type = (Types.DecimalType) primitive;\n-          BigInteger unscaled = dictionaryEncodable ? new BigInteger(\"1\") : randomUnscaled(type.precision(), random);\n-          result = Decimal.apply(new BigDecimal(unscaled, type.scale()));\n-          break;\n-        default:\n-          throw new IllegalArgumentException(\n-              \"Cannot generate value for unknown type: \" + primitive);\n+    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n+      this.rowCount += 1;\n+      if (rowCount > dictionaryEncodedRows) {\n+        return RandomUtil.generatePrimitive(primitive, rand);\n+      } else {\n+        return generateDictionaryEncodablePrimitive(primitive, rand);\n       }\n-      return super.getPrimitive(primitive, result);\n     }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MzM5Mw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434863393", "bodyText": "Why were these converted to optional?", "author": "rdblue", "createdAt": "2020-06-03T21:18:15Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java", "diffHunk": "@@ -44,18 +44,18 @@\n       optional(101, \"data\", Types.StringType.get()),\n       required(102, \"b\", Types.BooleanType.get()),\n       optional(103, \"i\", Types.IntegerType.get()),\n-      required(104, \"l\", LongType.get()),\n+      optional(104, \"l\", LongType.get()),\n       optional(105, \"f\", Types.FloatType.get()),\n-      required(106, \"d\", Types.DoubleType.get()),\n+      optional(106, \"d\", Types.DoubleType.get()),\n       optional(107, \"date\", Types.DateType.get()),\n-      required(108, \"ts\", Types.TimestampType.withZone()),\n-      required(110, \"s\", Types.StringType.get()),\n+      optional(108, \"ts\", Types.TimestampType.withZone()),\n+      optional(110, \"s\", Types.StringType.get()),\n       //required(111, \"uuid\", Types.UUIDType.get()),\n-      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(112, \"fixed\", Types.FixedType.ofLength(7)),\n       optional(113, \"bytes\", Types.BinaryType.get()),\n-      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n-      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n-      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+      optional(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      optional(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      optional(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODY0Ng==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918646", "bodyText": "Reverted this change in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MzM5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java b/spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java\nindex bf72e3da5..966a0e656 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java\n\n@@ -44,18 +45,18 @@ public abstract class AvroDataTest {\n       optional(101, \"data\", Types.StringType.get()),\n       required(102, \"b\", Types.BooleanType.get()),\n       optional(103, \"i\", Types.IntegerType.get()),\n-      optional(104, \"l\", LongType.get()),\n+      required(104, \"l\", LongType.get()),\n       optional(105, \"f\", Types.FloatType.get()),\n-      optional(106, \"d\", Types.DoubleType.get()),\n+      required(106, \"d\", Types.DoubleType.get()),\n       optional(107, \"date\", Types.DateType.get()),\n-      optional(108, \"ts\", Types.TimestampType.withZone()),\n-      optional(110, \"s\", Types.StringType.get()),\n+      required(108, \"ts\", Types.TimestampType.withZone()),\n+      required(110, \"s\", Types.StringType.get()),\n       //required(111, \"uuid\", Types.UUIDType.get()),\n-      optional(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n       optional(113, \"bytes\", Types.BinaryType.get()),\n-      optional(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n-      optional(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n-      optional(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n   );\n \n   @Rule\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzAwMQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434867001", "bodyText": "I think this should create a different base class instead of using AvroDataTest. This removes quite a few methods from AvroDataTest and also makes changes to it, like making all of the fields optional. Rather than doing that, I think it would make sense to just copy those test cases into a new VectorizedDataTest, since they are just schemas. And you could also add other cases, like these:\n  @Test\n  public void testStructWithRequiredFields() throws IOException {\n    writeAndValidate(TypeUtil.assignIncreasingFreshIds(new Schema(\n        Lists.transform(SUPPORTED_PRIMITIVES.fields(), Types.NestedField::asRequired))));\n  }\n\n  @Test\n  public void testStructWithOptionalFields() throws IOException {\n    writeAndValidate(TypeUtil.assignIncreasingFreshIds(new Schema(\n        Lists.transform(SUPPORTED_PRIMITIVES.fields(), Types.NestedField::asOptional))));\n  }\n\n  @Test\n  public void testNestedStruct() throws IOException {\n    writeAndValidate(TypeUtil.assignIncreasingFreshIds(new Schema(required(1, \"struct\", SUPPORTED_PRIMITIVES))));\n  }", "author": "rdblue", "createdAt": "2020-06-03T21:25:53Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.parquet.vectorized;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.data.AvroDataTest;\n+import org.apache.iceberg.spark.data.RandomData;\n+import org.apache.iceberg.spark.data.TestHelpers;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+public class TestParquetVectorizedReads extends AvroDataTest {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzkzNQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917935", "bodyText": "Included in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:52:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzAwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java b/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\nindex 812137101..30e935ba0 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n\n@@ -21,37 +21,50 @@ package org.apache.iceberg.spark.data.parquet.vectorized;\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Iterator;\n-import java.util.List;\n import org.apache.avro.generic.GenericData;\n+import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.data.AvroDataTest;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n import org.apache.iceberg.types.TypeUtil;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.junit.Assert;\n import org.junit.Assume;\n import org.junit.Ignore;\n import org.junit.Test;\n \n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n public class TestParquetVectorizedReads extends AvroDataTest {\n+  private static final int NUM_ROWS = 1_000_000;\n \n   @Override\n   protected void writeAndValidate(Schema schema) throws IOException {\n+    writeAndValidate(schema, NUM_ROWS, 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+  }\n+\n+  private void writeAndValidate(\n+      Schema schema, int numRecords, long seed, float nullPercentage,\n+      boolean setAndCheckArrowValidityVector, boolean reuseContainers)\n+      throws IOException {\n     // Write test data\n     Assume.assumeTrue(\"Parquet Avro cannot write non-string map keys\", null == TypeUtil.find(\n         schema,\n         type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n \n-    List<GenericData.Record> expected = generateData(schema);\n+    Iterable<GenericData.Record> expected = generateData(schema, numRecords, seed, nullPercentage);\n \n     // write a test parquet file using iceberg writer\n     File testFile = temp.newFile();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzQ5NQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434867495", "bodyText": "All this method does is call another method. Do we need both or can we move the body of lazyCheckEnableBatchRead here?", "author": "rdblue", "createdAt": "2020-06-03T21:27:03Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -249,6 +290,34 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzMzMw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917333", "bodyText": "Done in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:50:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzQ5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 43f21f463..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -292,11 +290,7 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n \n   @Override\n   public boolean enableBatchRead() {\n-    return lazyCheckEnableBatchRead();\n-  }\n-\n-  private boolean lazyCheckEnableBatchRead() {\n-    if (enableBatchRead == null) {\n+    if (readUsingBatch == null) {\n       boolean allParquetFileScanTasks =\n           tasks().stream()\n               .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2ODU5Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434868597", "bodyText": "This method isn't used and can be removed.", "author": "rdblue", "createdAt": "2020-06-03T21:29:31Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader<?>> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      IntStream.range(0, fields.size())\n+          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+      List<Types.NestedField> icebergFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+\n+      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          icebergFields.size());\n+\n+      for (Types.NestedField field : icebergFields) {\n+        int id = field.fieldId();\n+        VectorizedReader<?> reader = readersById.get(id);\n+        if (reader != null) {\n+          reorderedFields.add(reader);\n+        } else {\n+          reorderedFields.add(VectorizedArrowReader.nulls());\n+        }\n+      }\n+      return new ColumnarBatchReader(reorderedFields);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> primitive(\n+        org.apache.iceberg.types.Type.PrimitiveType expected,\n+        PrimitiveType primitive) {\n+\n+      // Create arrow vector for this field\n+      int parquetFieldId = primitive.getId().intValue();\n+      ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n+      // Nested types not yet supported for vectorized reads\n+      if (desc.getMaxRepetitionLevel() > 0) {\n+        return null;\n+      }\n+      Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n+      if (icebergField == null) {\n+        return null;\n+      }\n+      // Set the validity buffer if null checking is enabled in arrow\n+      return new VectorizedArrowReader(desc, icebergField, rootAllocator,\n+          batchSize, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED);\n+    }\n+\n+    protected MessageType type() {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzQ3OA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917478", "bodyText": "Done in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:50:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2ODU5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\nindex 812d1fa35..01cbe6f28 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n\n@@ -19,19 +19,18 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.IntStream;\n import org.apache.arrow.memory.BufferAllocator;\n-import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.arrow.ArrowAllocation;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.schema.GroupType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2OTY5NQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434869695", "bodyText": "Coverage shows that this branch is never taken because there are no tests for read projection that use the vectorized path. Can you add tests based on TestReadProjection like the tests you added using AvroDataTest?", "author": "rdblue", "createdAt": "2020-06-03T21:31:45Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader<?>> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      IntStream.range(0, fields.size())\n+          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+      List<Types.NestedField> icebergFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+\n+      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          icebergFields.size());\n+\n+      for (Types.NestedField field : icebergFields) {\n+        int id = field.fieldId();\n+        VectorizedReader<?> reader = readersById.get(id);\n+        if (reader != null) {\n+          reorderedFields.add(reader);\n+        } else {\n+          reorderedFields.add(VectorizedArrowReader.nulls());", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzgwMw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917803", "bodyText": "Coverage shows that this is now tested with the update in my PR.", "author": "rdblue", "createdAt": "2020-06-03T23:51:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2OTY5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\nindex 812d1fa35..01cbe6f28 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java\n\n@@ -19,19 +19,18 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.IntStream;\n import org.apache.arrow.memory.BufferAllocator;\n-import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.arrow.ArrowAllocation;\n import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.schema.GroupType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzE0MQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434873141", "bodyText": "Coverage shows that this branch isn't taken -- line 213 is never used by the existing tests. The equivalent line for doubles is taken, though. Can you find out what's happening?", "author": "rdblue", "createdAt": "2020-06-03T21:39:27Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -202,7 +209,7 @@ public int nextBatchFloats(\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (eagerDecodeDictionary) {\n+    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3NzA1MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434877050", "bodyText": "Actually, this was in my local copy. When I reverted the changes I see that the tests are hitting this line. But the decimal line below is not showing up in coverage.", "author": "rdblue", "createdAt": "2020-06-03T21:48:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzE0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "e3f72438b503419a1a02c76a98f1d179babaf712", "chunk": "diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java\nindex 2aa6f2c07..7cc32e06a 100644\n--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java\n\n@@ -209,7 +202,7 @@ public class VectorizedPageIterator extends BasePageIterator {\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {\n+    if (eagerDecodeDictionary) {\n       vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFloats(\n           vector,\n           numValsInVector,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzQ3Nw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434873477", "bodyText": "Coverage shows that this path isn't taken either.", "author": "rdblue", "createdAt": "2020-06-03T21:40:12Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -312,7 +319,7 @@ public int nextBatchFixedLengthDecimal(\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (eagerDecodeDictionary) {\n+    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3Mzg1NA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434873854", "bodyText": "Looks like DictionaryDecimalBinaryAccessor is also not used by tests. That's probably related. Maybe decimals are not getting dictionary encoded?", "author": "rdblue", "createdAt": "2020-06-03T21:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzQ3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e3f72438b503419a1a02c76a98f1d179babaf712", "chunk": "diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java\nindex 2aa6f2c07..7cc32e06a 100644\n--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java\n\n@@ -319,7 +312,7 @@ public class VectorizedPageIterator extends BasePageIterator {\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {\n+    if (eagerDecodeDictionary) {\n       vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFixedLengthDecimals(\n           vector,\n           numValsInVector,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzUyMw==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434903523", "bodyText": "I think this is correct. We don't need to have multiple task instances, especially since this will go away in 3.0.\nInstead, it's cleaner if we pass a reader factory into a single read task and call that factory in createPartitionReader:\n  private interface ReaderFactory<T> {\n    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n                                   EncryptionManager encryptionManager, boolean caseSensitive);\n  }\n\n  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n\n    private InternalRowReaderFactory() {\n    }\n\n    @Override\n    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n                                                    FileIO io, EncryptionManager encryptionManager,\n                                                    boolean caseSensitive) {\n      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n    }\n  }\n\n  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n    private final int batchSize;\n\n    BatchReaderFactory(int batchSize) {\n      this.batchSize = batchSize;\n    }\n\n    @Override\n    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n                                                    FileIO io, EncryptionManager encryptionManager,\n                                                    boolean caseSensitive) {\n      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n    }\n  }", "author": "rdblue", "createdAt": "2020-06-03T23:00:59Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -310,26 +379,27 @@ private static void mergeIcebergHadoopConfs(\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzczNA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434903734", "bodyText": "Then the planBatchInputPartitions method is updated like this:\n      readTasks.add(new ReadTask<>(\n          task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive, localityPreferred,\n          new BatchReaderFactory(batchSize)));", "author": "rdblue", "createdAt": "2020-06-03T23:01:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzUyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNjUzNg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434916536", "bodyText": "This is in the PR against your branch.", "author": "rdblue", "createdAt": "2020-06-03T23:47:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzUyMw=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 43f21f463..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -383,23 +378,23 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n         table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n-  private abstract static class BaseReadTask<T> implements Serializable, InputPartition<T> {\n-    final CombinedScanTask task;\n+  private static class ReadTask<T> implements Serializable, InputPartition<T> {\n+    private final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    final Broadcast<FileIO> io;\n-    final Broadcast<EncryptionManager> encryptionManager;\n-    final boolean caseSensitive;\n+    private final Broadcast<FileIO> io;\n+    private final Broadcast<EncryptionManager> encryptionManager;\n+    private final boolean caseSensitive;\n     private final boolean localityPreferred;\n+    private final ReaderFactory<T> readerFactory;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n     private transient String[] preferredLocations;\n \n-    private BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-        boolean caseSensitive, boolean localityPreferred) {\n+    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+                     boolean caseSensitive, boolean localityPreferred, ReaderFactory<T> readerFactory) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNDE0Ng==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434904146", "bodyText": "I don't think this needed to be reformatted. Can you revert this change?", "author": "rdblue", "createdAt": "2020-06-03T23:03:16Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -87,14 +93,17 @@\n   private List<Expression> filterExpressions = null;\n   private Filter[] pushedFilters = NO_FILTERS;\n   private final boolean localityPreferred;\n+  private final int batchSize;\n \n   // lazy variables\n   private Schema schema = null;\n   private StructType type = null; // cached because Spark accesses it multiple times\n   private List<CombinedScanTask> tasks = null; // lazy cache of tasks\n+  private Boolean enableBatchRead = null; // cache variable for enabling batched reads\n \n-  Reader(Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-         boolean caseSensitive, DataSourceOptions options) {\n+  Reader(\n+      Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+      boolean caseSensitive, DataSourceOptions options) {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 43f21f463..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -93,16 +92,16 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n   private List<Expression> filterExpressions = null;\n   private Filter[] pushedFilters = NO_FILTERS;\n   private final boolean localityPreferred;\n+  private final boolean batchReadsEnabled;\n   private final int batchSize;\n \n   // lazy variables\n   private Schema schema = null;\n   private StructType type = null; // cached because Spark accesses it multiple times\n   private List<CombinedScanTask> tasks = null; // lazy cache of tasks\n-  private Boolean enableBatchRead = null; // cache variable for enabling batched reads\n+  private Boolean readUsingBatch = null;\n \n-  Reader(\n-      Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+  Reader(Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n       boolean caseSensitive, DataSourceOptions options) {\n     this.table = table;\n     this.snapshotId = options.get(\"snapshot-id\").map(Long::parseLong).orElse(null);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNDcyOA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434904728", "bodyText": "I don't see a test for this. Can you update some of the Spark tests to run both vectorized and non-vectorized? We can also do this in a follow-up, but we need to make sure that this code path is being tested as thoroughly as the non-vectorized code path.", "author": "rdblue", "createdAt": "2020-06-03T23:05:12Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -178,6 +195,30 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into {@link ColumnarBatch}\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNjQzMg==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434916432", "bodyText": "I added cases to TestSparkReadProjection that use this path in the PR against your branch.", "author": "rdblue", "createdAt": "2020-06-03T23:46:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNDcyOA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 43f21f463..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -207,9 +205,9 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n \n     List<InputPartition<ColumnarBatch>> readTasks = Lists.newArrayList();\n     for (CombinedScanTask task : tasks()) {\n-      readTasks.add(\n-          new ColumnarBatchReadTask(task, tableSchemaString, expectedSchemaString,\n-              io, encryptionManager, caseSensitive, localityPreferred, batchSize));\n+      readTasks.add(new ReadTask<>(\n+          task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive, localityPreferred,\n+          new BatchReaderFactory(batchSize)));\n     }\n     LOG.info(\"Batching input partitions with {} tasks.\", readTasks.size());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNjc2MA==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434906760", "bodyText": "Options passed into the read here should be short options because they come from the DataFrameReader interface:\nspark.read.format(\"iceberg\").config(\"snapshot-id\", snapId).load(\"db.table\");\nThe long option names like the ones you have here are what we use for table properties, which are tracked in TableProperties and documented. These options should default to the table property value, but a DataFrameReader option should override.\nFor short names, how about vectorization-enabled and batch-size? We should also add constants in TableProperties for the properties you have here. Let's turn off vectorization by default.\nAlso, we can drop \"iceberg\" from the start of the properties.", "author": "rdblue", "createdAt": "2020-06-03T23:12:23Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -145,6 +154,14 @@\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+\n+    boolean enableBatchReadsConfig =\n+        options.get(\"iceberg.read.parquet-vectorization.enabled\").map(Boolean::parseBoolean).orElse(true);\n+    if (!enableBatchReadsConfig) {\n+      enableBatchRead = Boolean.FALSE;\n+    }\n+    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.parquet-vectorization.batch-size\");", "originalCommit": "032c8c12c972614ee5514b7c586021331c8bf002", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNjMwMQ==", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434916301", "bodyText": "I included this change in the PR against your branch.", "author": "rdblue", "createdAt": "2020-06-03T23:46:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNjc2MA=="}], "type": "inlineReview", "revised_code": {"commit": "be26a697c78dd47963c03dd3d363a40bb34ff14e", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\nindex 43f21f463..d205c22a7 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/source/Reader.java\n\n@@ -155,13 +154,12 @@ class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPus\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n \n-    boolean enableBatchReadsConfig =\n-        options.get(\"iceberg.read.parquet-vectorization.enabled\").map(Boolean::parseBoolean).orElse(true);\n-    if (!enableBatchReadsConfig) {\n-      enableBatchRead = Boolean.FALSE;\n-    }\n-    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.parquet-vectorization.batch-size\");\n-    this.batchSize = numRecordsPerBatchOpt.map(Integer::parseInt).orElse(VectorizedArrowReader.DEFAULT_BATCH_SIZE);\n+    this.batchReadsEnabled = options.get(\"vectorization-enabled\").map(Boolean::parseBoolean).orElse(\n+        PropertyUtil.propertyAsBoolean(table.properties(),\n+            TableProperties.PARQUET_VECTORIZATION_ENABLED, TableProperties.PARQUET_VECTORIZATION_ENABLED_DEFAULT));\n+    this.batchSize = options.get(\"batch-size\").map(Integer::parseInt).orElse(\n+        PropertyUtil.propertyAsInt(table.properties(),\n+          TableProperties.PARQUET_BATCH_SIZE, TableProperties.PARQUET_BATCH_SIZE_DEFAULT));\n   }\n \n   private Schema lazySchema() {\n"}}, {"oid": "e3f72438b503419a1a02c76a98f1d179babaf712", "url": "https://github.com/apache/iceberg/commit/e3f72438b503419a1a02c76a98f1d179babaf712", "message": "iceberg-spark changes for vectorized reads", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "b33f6d876a0b4ebdc93e0a74af1b437eea1336bc", "url": "https://github.com/apache/iceberg/commit/b33f6d876a0b4ebdc93e0a74af1b437eea1336bc", "message": "Minor cleanup", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "492c0b0845722c3e9aaef9f108880cff1de672ec", "url": "https://github.com/apache/iceberg/commit/492c0b0845722c3e9aaef9f108880cff1de672ec", "message": "Address code review comments", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "dc93427018ae3a7256ed20c203466885d61713fb", "url": "https://github.com/apache/iceberg/commit/dc93427018ae3a7256ed20c203466885d61713fb", "message": "Remove benchmarks from the code review", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "4760f790b60b456b214dd9db1d8c94a594bb7995", "url": "https://github.com/apache/iceberg/commit/4760f790b60b456b214dd9db1d8c94a594bb7995", "message": "Merge reader changes from master. Rebase branch to master", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "41064aa505d80537e358f19029178d85bb22014e", "url": "https://github.com/apache/iceberg/commit/41064aa505d80537e358f19029178d85bb22014e", "message": "Code review comments", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "c5347bf7af610c7af3ac418261f4a182cd1441cc", "url": "https://github.com/apache/iceberg/commit/c5347bf7af610c7af3ac418261f4a182cd1441cc", "message": "Code review comments", "committedDate": "2020-06-12T08:35:35Z", "type": "commit"}, {"oid": "b7b68f5153dcdd55b57fadecf010cbcfce084652", "url": "https://github.com/apache/iceberg/commit/b7b68f5153dcdd55b57fadecf010cbcfce084652", "message": "Some changes from the review.", "committedDate": "2020-06-12T08:36:59Z", "type": "commit"}, {"oid": "58f2cf79ea49e89cf217c773a0591c4de6795428", "url": "https://github.com/apache/iceberg/commit/58f2cf79ea49e89cf217c773a0591c4de6795428", "message": "Enable projection tests for vectorized Parquet.", "committedDate": "2020-06-12T08:36:59Z", "type": "commit"}, {"oid": "edecc2a4c3887fc04b5ce54ad4818b971de5850c", "url": "https://github.com/apache/iceberg/commit/edecc2a4c3887fc04b5ce54ad4818b971de5850c", "message": "Revert changes to AvroDataTest.", "committedDate": "2020-06-12T08:36:59Z", "type": "commit"}, {"oid": "f094507acfc77715803606729c38a690407ecf10", "url": "https://github.com/apache/iceberg/commit/f094507acfc77715803606729c38a690407ecf10", "message": "Run TestParquetScan tests with vectorization enabled.", "committedDate": "2020-06-12T08:36:59Z", "type": "commit"}, {"oid": "8dc98ce60d8df6f7de4f229a81516eea7d58583e", "url": "https://github.com/apache/iceberg/commit/8dc98ce60d8df6f7de4f229a81516eea7d58583e", "message": "Summary of changes:\n1) Below new test cases added:\n   - Test for code path when optional values are mostly null\n   - Test for case when containers are not reused for every batch\n   - Test for case to verify arrow's validity vector is set correctly when setArrowValidityVector = true\n2) Reuse container logic is now similar to row based read path\n3) We now always set the nullability holder. Arrow validity vector is set only for purpose of supplying complete arrow vectors when requested to do so.", "committedDate": "2020-06-12T09:39:22Z", "type": "forcePushed"}, {"oid": "6eee8a798219a09ed194a5f0abbbd09959936ef2", "url": "https://github.com/apache/iceberg/commit/6eee8a798219a09ed194a5f0abbbd09959936ef2", "message": "Cleanup and address code review comments.\n\nSummary of changes:\n1) Below new test cases added:\n   - Test for code path when optional values are mostly null\n   - Test for case when containers are not reused for every batch\n   - Test for case to verify arrow's validity vector is set correctly when setArrowValidityVector = true\n2) Reuse container logic is now similar to row based read path\n3) We now always set the nullability holder. Arrow validity vector is set only for purpose of supplying complete arrow vectors when requested to do so.", "committedDate": "2020-06-12T19:02:42Z", "type": "forcePushed"}]}