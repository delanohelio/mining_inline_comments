{"pr_number": 1563, "pr_title": "Close source data iterator for Spark data reader: fixing timeout due to S3A connection pool", "pr_createdAt": "2020-10-07T23:08:23Z", "pr_url": "https://github.com/apache/iceberg/pull/1563", "timeline": [{"oid": "8ac80a3e6fcbb6c3a6a30995192de94eee74a5d1", "url": "https://github.com/apache/iceberg/commit/8ac80a3e6fcbb6c3a6a30995192de94eee74a5d1", "message": "close base data reader", "committedDate": "2020-10-07T22:47:05Z", "type": "commit"}, {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6", "url": "https://github.com/apache/iceberg/commit/83f5cc54ca026bdc6740780fa61713b4063353d6", "message": "fix test style", "committedDate": "2020-10-07T22:52:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MTM1Ng==", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501361356", "bodyText": "Looks correct to me.", "author": "rdblue", "createdAt": "2020-10-07T23:12:10Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -84,6 +84,7 @@ public boolean next() throws IOException {\n         this.currentIterator.close();\n         this.currentIterator = open(tasks.next());\n       } else {\n+        this.currentIterator.close();", "originalCommit": "83f5cc54ca026bdc6740780fa61713b4063353d6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MTg0OA==", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501361848", "bodyText": "Can you remove this? There is no need to modify this file.", "author": "rdblue", "createdAt": "2020-10-07T23:13:54Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java", "diffHunk": "@@ -101,6 +101,7 @@ protected Record writeAndRead(String desc, Schema writeSchema, Schema readSchema\n     File testFile = new File(dataFolder, format.addExtension(UUID.randomUUID().toString()));\n \n     Table table = TestTables.create(location, desc, writeSchema, PartitionSpec.unpartitioned());\n+", "originalCommit": "83f5cc54ca026bdc6740780fa61713b4063353d6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a79bc20c68fe9765e4498c62b6034a550d9d6ecd", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java\nindex dfb70e0d6..b3c8e3134 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java\n\n@@ -101,7 +101,6 @@ public abstract class TestSparkReadProjection extends TestReadProjection {\n     File testFile = new File(dataFolder, format.addExtension(UUID.randomUUID().toString()));\n \n     Table table = TestTables.create(location, desc, writeSchema, PartitionSpec.unpartitioned());\n-\n     try {\n       // Important: use the table's schema for the rest of the test\n       // When tables are created, the column ids are reassigned.\n"}}, {"oid": "a79bc20c68fe9765e4498c62b6034a550d9d6ecd", "url": "https://github.com/apache/iceberg/commit/a79bc20c68fe9765e4498c62b6034a550d9d6ecd", "message": "remove line", "committedDate": "2020-10-07T23:17:08Z", "type": "commit"}, {"oid": "9cca21df5be3dc6f14454ea063397173ee6f40a7", "url": "https://github.com/apache/iceberg/commit/9cca21df5be3dc6f14454ea063397173ee6f40a7", "message": "fix test compile", "committedDate": "2020-10-07T23:22:13Z", "type": "commit"}, {"oid": "9cca21df5be3dc6f14454ea063397173ee6f40a7", "url": "https://github.com/apache/iceberg/commit/9cca21df5be3dc6f14454ea063397173ee6f40a7", "message": "fix test compile", "committedDate": "2020-10-07T23:22:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2NTQ4MQ==", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501365481", "bodyText": "I think this is a good test to have, but I'm a little concerned about the number of classes it takes.\nHave you considered an alternative structure? What about using a spy object that is injected the same way, in the ClosureTrackingReader.open method? Then you'd be able to use normal file splits and planning with TableTestBase, and you'd be able to keep track of all the spies in the reader you instantiate.\nI think that would get rid of a few of these static classes, which would make this easier to understand and maintain.", "author": "rdblue", "createdAt": "2020-10-07T23:24:55Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseCombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.PlaintextEncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public abstract class TestSparkBaseDataReader {\n+\n+  private static final Configuration CONF = new Configuration();\n+\n+  // Simulates the closeable iterator of data to be read\n+  private static class CloseableIntegerRange implements CloseableIterator<Integer> {", "originalCommit": "83f5cc54ca026bdc6740780fa61713b4063353d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2NjA0Mw==", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501366043", "bodyText": "ah yup will use mockito here!", "author": "mickjermsurawong-stripe", "createdAt": "2020-10-07T23:26:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2NTQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM4ODE0Nw==", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501388147", "bodyText": "I retain 2 helper classes here. One is the reader to test, and the other is this CloseableIterator.\nI was hoping i could somehow open the FileScanTask to a spy of CloseableIterator, and I could check invocation on close. However, I don't seem to find any impl of CloseableIterator readily available here. Any existing opening of task to iterator would also invoke other unneeded logic. As a result, this class CloseableIntegerRange still remains.\nTableTestBase in core isn't actually visible from this test. So instead of mocking the file scan task, I create dummy table and extract the planned files instead.", "author": "mickjermsurawong-stripe", "createdAt": "2020-10-08T00:45:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2NTQ4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "8c036461bb28f5def37a2d0aec430d0f79df1377", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java\nindex 8ac8a6763..51b47cbd9 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java\n\n@@ -19,6 +19,7 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.File;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.Iterator;\n"}}, {"oid": "8c036461bb28f5def37a2d0aec430d0f79df1377", "url": "https://github.com/apache/iceberg/commit/8c036461bb28f5def37a2d0aec430d0f79df1377", "message": "reduce helper class", "committedDate": "2020-10-08T00:39:34Z", "type": "commit"}, {"oid": "8c036461bb28f5def37a2d0aec430d0f79df1377", "url": "https://github.com/apache/iceberg/commit/8c036461bb28f5def37a2d0aec430d0f79df1377", "message": "reduce helper class", "committedDate": "2020-10-08T00:39:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM4OTE1OA==", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501389158", "bodyText": "This method relies on planFiles contract that each file corresponds to one scan task, to get desired number of total task argument", "author": "mickjermsurawong-stripe", "createdAt": "2020-10-08T00:49:50Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.BaseCombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.PlaintextEncryptionManager;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.data.RandomData;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.FileFormat.PARQUET;\n+import static org.apache.iceberg.Files.localOutput;\n+\n+public abstract class TestSparkBaseDataReader {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static final Configuration CONFD = new Configuration();\n+\n+  // Simulates the closeable iterator of data to be read\n+  private static class CloseableIntegerRange implements CloseableIterator<Integer> {\n+    boolean closed;\n+    Iterator<Integer> iter;\n+\n+    CloseableIntegerRange(long range) {\n+      this.closed = false;\n+      this.iter = IntStream.range(0, (int) range).iterator();\n+    }\n+\n+    @Override\n+    public void close() {\n+      this.closed = true;\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+      return iter.hasNext();\n+    }\n+\n+    @Override\n+    public Integer next() {\n+      return iter.next();\n+    }\n+  }\n+\n+  // Main reader class to test base class iteration logic.\n+  // Keeps track of iterator closure.\n+  private static class ClosureTrackingReader extends BaseDataReader<Integer> {\n+    private Map<String, CloseableIntegerRange> tracker = new HashMap<>();\n+\n+    ClosureTrackingReader(List<FileScanTask> tasks) {\n+      super(new BaseCombinedScanTask(tasks),\n+          new HadoopFileIO(CONFD),\n+          new PlaintextEncryptionManager());\n+    }\n+\n+    @Override\n+    CloseableIterator<Integer> open(FileScanTask task) {\n+      CloseableIntegerRange intRange = new CloseableIntegerRange(task.file().recordCount());\n+      tracker.put(getKey(task), intRange);\n+      return intRange;\n+    }\n+\n+    public Boolean isIteratorClosed(FileScanTask task) {\n+      return tracker.get(getKey(task)).closed;\n+    }\n+\n+    public Boolean hasIterator(FileScanTask task) {\n+      return tracker.containsKey(getKey(task));\n+    }\n+\n+    private String getKey(FileScanTask task) {\n+      return task.file().path().toString();\n+    }\n+  }\n+\n+  @Test\n+  public void testClosureOnDataExhaustion() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    int countRecords = 0;\n+    while (reader.next()) {\n+      countRecords += 1;\n+      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+    }\n+\n+    Assert.assertEquals(\"Reader returned incorrect number of records\",\n+        totalTasks * recordPerTask,\n+        countRecords\n+    );\n+    tasks.forEach(t ->\n+        Assert.assertTrue(\"All iterators should be closed after read exhausion\",\n+            reader.isIteratorClosed(t))\n+    );\n+  }\n+\n+  @Test\n+  public void testClosureDuringIteration() throws IOException {\n+    Integer totalTasks = 2;\n+    Integer recordPerTask = 1;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+    Assert.assertEquals(2, tasks.size());\n+    FileScanTask firstTask = tasks.get(0);\n+    FileScanTask secondTask = tasks.get(1);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    // Total of 2 elements\n+    Assert.assertTrue(reader.next());\n+    Assert.assertFalse(\"First iter should not be closed on its last element\",\n+        reader.isIteratorClosed(firstTask));\n+\n+    Assert.assertTrue(reader.next());\n+    Assert.assertTrue(\"First iter should be closed after moving to second iter\",\n+        reader.isIteratorClosed(firstTask));\n+    Assert.assertFalse(\"Second iter should not be closed on its last element\",\n+        reader.isIteratorClosed(secondTask));\n+\n+    Assert.assertFalse(reader.next());\n+    Assert.assertTrue(reader.isIteratorClosed(firstTask));\n+    Assert.assertTrue(reader.isIteratorClosed(secondTask));\n+  }\n+\n+  @Test\n+  public void testClosureWithoutAnyRead() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    reader.close();\n+\n+    tasks.forEach(t ->\n+        Assert.assertFalse(\"Iterator should not be created eagerly for tasks\",\n+            reader.hasIterator(t))\n+    );\n+  }\n+\n+  @Test\n+  public void testExplicitClosure() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    Integer halfDataSize = (totalTasks * recordPerTask) / 2;\n+    for (int i = 0; i < halfDataSize; i++) {\n+      Assert.assertTrue(\"Reader should have some element\", reader.next());\n+      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+    }\n+\n+    reader.close();\n+\n+    // Some tasks might have not been opened yet, so we don't have corresponding tracker for it.\n+    // But all that have been created must be closed.\n+    tasks.forEach(t -> {\n+      if (reader.hasIterator(t)) {\n+        Assert.assertTrue(\"Iterator should be closed after read exhausion\",\n+            reader.isIteratorClosed(t));\n+      }\n+    });\n+  }\n+\n+  @Test\n+  public void testIdempotentExplicitClosure() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    // Total 100 elements, only 5 iterators have been created\n+    for (int i = 0; i < 45; i++) {\n+      Assert.assertTrue(\"eader should have some element\", reader.next());\n+      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+    }\n+\n+    for (int closeAttempt = 0; closeAttempt < 5; closeAttempt++) {\n+      reader.close();\n+      for (int i = 0; i < 5; i++) {\n+        Assert.assertTrue(\"Iterator should be closed after read exhausion\",\n+            reader.isIteratorClosed(tasks.get(i)));\n+      }\n+      for (int i = 5; i < 10; i++) {\n+        Assert.assertFalse(\"Iterator should not be created eagerly for tasks\",\n+            reader.hasIterator(tasks.get(i)));\n+      }\n+    }\n+  }\n+\n+  private List<FileScanTask> createFileScanTasks(Integer totalTasks, Integer recordPerTask) throws IOException {", "originalCommit": "8c036461bb28f5def37a2d0aec430d0f79df1377", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}