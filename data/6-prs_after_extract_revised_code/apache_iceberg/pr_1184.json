{"pr_number": 1184, "pr_title": "Read support for parquet int96 timestamps", "pr_createdAt": "2020-07-08T19:11:24Z", "pr_url": "https://github.com/apache/iceberg/pull/1184", "timeline": [{"oid": "ca3e955ed9a5f67314e43c6d1c73332a6633da76", "url": "https://github.com/apache/iceberg/commit/ca3e955ed9a5f67314e43c6d1c73332a6633da76", "message": "Read support for int96 as timestamp", "committedDate": "2020-07-08T00:05:32Z", "type": "commit"}, {"oid": "cea839f58b8b0e3d73f415b2770913e1d0955f31", "url": "https://github.com/apache/iceberg/commit/cea839f58b8b0e3d73f415b2770913e1d0955f31", "message": "Parquet int96 timestamp spark read tests", "committedDate": "2020-07-08T19:07:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzNTgwNQ==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r454035805", "bodyText": "Is it possible to avoid creating a Spark session just to write a timestamp? What about calling Spark's FileFormat to write directly instead?\nWe wrap Spark's FileFormat in our DSv2 table implementation: https://github.com/Netflix/iceberg/blob/netflix-spark-2.4/metacat/src/main/java/com/netflix/iceberg/batch/BatchPatternWrite.java#L90\nThis test would run much faster by using that to create a file instead of creating a Spark context.", "author": "rdblue", "createdAt": "2020-07-14T01:01:54Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java", "diffHunk": "@@ -67,4 +78,49 @@ protected void writeAndValidate(Schema schema) throws IOException {\n       Assert.assertFalse(\"Should not have extra rows\", rows.hasNext());\n     }\n   }\n+\n+  protected List<InternalRow> rowsFromFile(InputFile inputFile, Schema schema) throws IOException {\n+    try (CloseableIterable<InternalRow> reader =\n+        Parquet.read(inputFile)\n+            .project(schema)\n+            .createReaderFunc(type -> SparkParquetReaders.buildReader(schema, type))\n+            .build()) {\n+      return Lists.newArrayList(reader);\n+    }\n+  }\n+\n+  @Test\n+  public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n+    final SparkSession spark =\n+        SparkSession.builder()\n+            .master(\"local[2]\")\n+            .config(\"spark.sql.parquet.int96AsTimestamp\", \"false\")\n+            .getOrCreate();", "originalCommit": "cea839f58b8b0e3d73f415b2770913e1d0955f31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU5NzI1Ng==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r454597256", "bodyText": "Yes, I would much rather avoid creating a SparkSession here if possible. However, looking into ParquetFileFormat it seems like we would still need to pass a SparkSession to create the writer.\nI can look at ParquetOutputWriter but I might need to match the configuration there with what Spark uses to write int96.", "author": "gustavoatt", "createdAt": "2020-07-14T19:38:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzNTgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTExMDcwMg==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459110702", "bodyText": "Another approach would be to check-in a parquet file written by a spark and have the test just read it?\nA drawback with that approach is that updating this file would be brittle, but I can check in the code that writes the file in an ignored test, but that should avoid us from creating a spark session during unit tests. What do you think @rdblue?", "author": "gustavoatt", "createdAt": "2020-07-22T22:05:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzNTgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTEzNjEyOQ==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459136129", "bodyText": "At one point, we supported writing to Parquet using Spark's built-in ReadSupport. I think we can probably get that working again to create the files.", "author": "rdblue", "createdAt": "2020-07-22T23:17:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzNTgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyODc5Mw==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459728793", "bodyText": "Yes, looking at one of the tests we do support writing parquet files using Spark's WriteSupport.\nTo be able to use a FileAppender I had to add a TimestampAsInt96 type (that can only be written using Spark's builtin WriteSupport) so that schema conversion within Iceberg's ParquetWriteSupport knows that this timestamps should be encoded as int96 in the  parquet  schema.", "author": "gustavoatt", "createdAt": "2020-07-23T21:07:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzNTgwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "0c94f883d73fa552eb33ebe620945e00710185f5", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\nindex ed4750e54..ebe73dd26 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n\n@@ -91,36 +89,28 @@ public class TestSparkParquetReader extends AvroDataTest {\n \n   @Test\n   public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n-    final SparkSession spark =\n-        SparkSession.builder()\n-            .master(\"local[2]\")\n-            .config(\"spark.sql.parquet.int96AsTimestamp\", \"false\")\n-            .getOrCreate();\n+    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n+    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n+    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n \n-    final String parquetPath = temp.getRoot().getAbsolutePath() + \"/parquet_int96\";\n-    final java.sql.Timestamp ts = java.sql.Timestamp.valueOf(\"2014-01-01 23:00:01\");\n-    spark.createDataset(ImmutableList.of(ts), Encoders.TIMESTAMP()).write().parquet(parquetPath);\n-    spark.stop();\n-\n-    // Get the single parquet file produced by spark.\n-    List<Path> parquetOutputs =\n-        java.nio.file.Files.find(\n-            java.nio.file.Paths.get(parquetPath),\n-            1,\n-            (path, basicFileAttributes) -> path.toString().endsWith(\".parquet\"))\n-        .collect(Collectors.toList());\n-    Assert.assertEquals(1, parquetOutputs.size());\n-\n-    List<InternalRow> rows =\n-        rowsFromFile(\n-            Files.localInput(parquetOutputs.get(0).toFile()),\n-            new Schema(optional(1, \"timestamp\", Types.TimestampType.withoutZone())));\n-    Assert.assertEquals(1, rows.size());\n-    Assert.assertEquals(1, rows.get(0).numFields());\n+    try (FileAppender<InternalRow> writer =\n+        Parquet.write(Files.localOutput(parquetFile.toString()))\n+            .writeSupport(\n+                new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport())\n+            .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n+            .set(\"org.apache.spark.legacyDateTime\", \"false\")\n+            .set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n+            .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n+            .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n+            .schema(schema)\n+            .build()) {\n+      writer.addAll(rows);\n+    }\n \n-    // Spark represents Timestamps as epoch micros and are stored as longs.\n-    Assert.assertEquals(\n-        ts.toInstant(),\n-        Instant.ofEpochMilli(TimeUnit.MICROSECONDS.toMillis(rows.get(0).getLong(0))));\n+    final List<InternalRow> readRows =\n+        rowsFromFile(Files.localInput(parquetFile.toString()), schema);\n+    Assert.assertEquals(rows.size(), readRows.size());\n+    Assert.assertThat(readRows, CoreMatchers.is(rows));\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzNjE1Mg==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r454036152", "bodyText": "Using Spark's FileFormat would also make this test easier. You'd be able to pass in a value in micros and validate that you get the same value back, unmodified. You'd also not need to locate the Parquet file using find.", "author": "rdblue", "createdAt": "2020-07-14T01:03:22Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java", "diffHunk": "@@ -67,4 +78,49 @@ protected void writeAndValidate(Schema schema) throws IOException {\n       Assert.assertFalse(\"Should not have extra rows\", rows.hasNext());\n     }\n   }\n+\n+  protected List<InternalRow> rowsFromFile(InputFile inputFile, Schema schema) throws IOException {\n+    try (CloseableIterable<InternalRow> reader =\n+        Parquet.read(inputFile)\n+            .project(schema)\n+            .createReaderFunc(type -> SparkParquetReaders.buildReader(schema, type))\n+            .build()) {\n+      return Lists.newArrayList(reader);\n+    }\n+  }\n+\n+  @Test\n+  public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n+    final SparkSession spark =\n+        SparkSession.builder()\n+            .master(\"local[2]\")\n+            .config(\"spark.sql.parquet.int96AsTimestamp\", \"false\")\n+            .getOrCreate();\n+\n+    final String parquetPath = temp.getRoot().getAbsolutePath() + \"/parquet_int96\";\n+    final java.sql.Timestamp ts = java.sql.Timestamp.valueOf(\"2014-01-01 23:00:01\");\n+    spark.createDataset(ImmutableList.of(ts), Encoders.TIMESTAMP()).write().parquet(parquetPath);", "originalCommit": "cea839f58b8b0e3d73f415b2770913e1d0955f31", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0c94f883d73fa552eb33ebe620945e00710185f5", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\nindex ed4750e54..ebe73dd26 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n\n@@ -91,36 +89,28 @@ public class TestSparkParquetReader extends AvroDataTest {\n \n   @Test\n   public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n-    final SparkSession spark =\n-        SparkSession.builder()\n-            .master(\"local[2]\")\n-            .config(\"spark.sql.parquet.int96AsTimestamp\", \"false\")\n-            .getOrCreate();\n+    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n+    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n+    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n \n-    final String parquetPath = temp.getRoot().getAbsolutePath() + \"/parquet_int96\";\n-    final java.sql.Timestamp ts = java.sql.Timestamp.valueOf(\"2014-01-01 23:00:01\");\n-    spark.createDataset(ImmutableList.of(ts), Encoders.TIMESTAMP()).write().parquet(parquetPath);\n-    spark.stop();\n-\n-    // Get the single parquet file produced by spark.\n-    List<Path> parquetOutputs =\n-        java.nio.file.Files.find(\n-            java.nio.file.Paths.get(parquetPath),\n-            1,\n-            (path, basicFileAttributes) -> path.toString().endsWith(\".parquet\"))\n-        .collect(Collectors.toList());\n-    Assert.assertEquals(1, parquetOutputs.size());\n-\n-    List<InternalRow> rows =\n-        rowsFromFile(\n-            Files.localInput(parquetOutputs.get(0).toFile()),\n-            new Schema(optional(1, \"timestamp\", Types.TimestampType.withoutZone())));\n-    Assert.assertEquals(1, rows.size());\n-    Assert.assertEquals(1, rows.get(0).numFields());\n+    try (FileAppender<InternalRow> writer =\n+        Parquet.write(Files.localOutput(parquetFile.toString()))\n+            .writeSupport(\n+                new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport())\n+            .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n+            .set(\"org.apache.spark.legacyDateTime\", \"false\")\n+            .set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n+            .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n+            .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n+            .schema(schema)\n+            .build()) {\n+      writer.addAll(rows);\n+    }\n \n-    // Spark represents Timestamps as epoch micros and are stored as longs.\n-    Assert.assertEquals(\n-        ts.toInstant(),\n-        Instant.ofEpochMilli(TimeUnit.MICROSECONDS.toMillis(rows.get(0).getLong(0))));\n+    final List<InternalRow> readRows =\n+        rowsFromFile(Files.localInput(parquetFile.toString()), schema);\n+    Assert.assertEquals(rows.size(), readRows.size());\n+    Assert.assertThat(readRows, CoreMatchers.is(rows));\n   }\n }\n"}}, {"oid": "944c325ec3c92530ddebae31e8f12da36ccbf9ec", "url": "https://github.com/apache/iceberg/commit/944c325ec3c92530ddebae31e8f12da36ccbf9ec", "message": "Add int96 timestamp type for parquet-read support", "committedDate": "2020-07-23T21:09:21Z", "type": "commit"}, {"oid": "0c94f883d73fa552eb33ebe620945e00710185f5", "url": "https://github.com/apache/iceberg/commit/0c94f883d73fa552eb33ebe620945e00710185f5", "message": "Use spark's ParquetWriteSupport to test int96 timestamps read support", "committedDate": "2020-07-23T21:10:12Z", "type": "commit"}, {"oid": "b35027be3f3056b23a546f02ea9e4a1fcc91392c", "url": "https://github.com/apache/iceberg/commit/b35027be3f3056b23a546f02ea9e4a1fcc91392c", "message": "Fix style checks", "committedDate": "2020-07-23T23:20:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjE3NQ==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459826175", "bodyText": "I don't think we should change the type system to support this. INT96 may be something that we can read, but Iceberg cannot write it, per the spec.\nWas this needed to build the tests?", "author": "rdblue", "createdAt": "2020-07-24T02:36:33Z", "path": "api/src/main/java/org/apache/iceberg/types/Types.java", "diffHunk": "@@ -219,16 +220,30 @@ public static TimestampType withoutZone() {\n       return INSTANCE_WITHOUT_ZONE;\n     }\n \n+    /**\n+     * @return Timestamp type (with timezone) represented as INT96. This is only added for compatibility reasons\n+     * and can only be written using a Spark's ParquetWriteSupport. Writing this type should be avoided.\n+     */", "originalCommit": "b35027be3f3056b23a546f02ea9e4a1fcc91392c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDI1NDcyOQ==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r460254729", "bodyText": "Agreed. I found a way to have tests running that doesn't add a new type, I had to create an implementation of ParquetWriter.Builder that uses Spark's ParquetWriteSupport and Iceberg's ParquetWriteAdapter to avoid creating a SparkSession.", "author": "gustavoatt", "createdAt": "2020-07-24T19:45:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjE3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "7e48187c68417a6c947480022dc8a6e6945ed5ca", "chunk": "diff --git a/api/src/main/java/org/apache/iceberg/types/Types.java b/api/src/main/java/org/apache/iceberg/types/Types.java\nindex 4d3bec18d..a6f4d36b0 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Types.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Types.java\n\n@@ -220,30 +219,16 @@ public class Types {\n       return INSTANCE_WITHOUT_ZONE;\n     }\n \n-    /**\n-     * @return Timestamp type (with timezone) represented as INT96. This is only added for compatibility reasons\n-     * and can only be written using a Spark's ParquetWriteSupport. Writing this type should be avoided.\n-     */\n-    public static TimestampType asSparkInt96() {\n-      return INSTANCE_SPARK_INT96;\n-    }\n-\n     private final boolean adjustToUTC;\n-    private final boolean asSparkInt96;\n \n-    private TimestampType(boolean adjustToUTC, boolean asSparkInt96) {\n+    private TimestampType(boolean adjustToUTC) {\n       this.adjustToUTC = adjustToUTC;\n-      this.asSparkInt96 = asSparkInt96;\n     }\n \n     public boolean shouldAdjustToUTC() {\n       return adjustToUTC;\n     }\n \n-    public boolean shouldRepresentAsInt96() {\n-      return asSparkInt96;\n-    }\n-\n     @Override\n     public TypeID typeId() {\n       return TypeID.TIMESTAMP;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjQzNg==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459826436", "bodyText": "Nit: we don't use final for local variables.", "author": "rdblue", "createdAt": "2020-07-24T02:37:56Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java", "diffHunk": "@@ -67,4 +76,41 @@ protected void writeAndValidate(Schema schema) throws IOException {\n       Assert.assertFalse(\"Should not have extra rows\", rows.hasNext());\n     }\n   }\n+\n+  protected List<InternalRow> rowsFromFile(InputFile inputFile, Schema schema) throws IOException {\n+    try (CloseableIterable<InternalRow> reader =\n+        Parquet.read(inputFile)\n+            .project(schema)\n+            .createReaderFunc(type -> SparkParquetReaders.buildReader(schema, type))\n+            .build()) {\n+      return Lists.newArrayList(reader);\n+    }\n+  }\n+\n+  @Test\n+  public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n+    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n+    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n+    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));", "originalCommit": "b35027be3f3056b23a546f02ea9e4a1fcc91392c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzMjAxMw==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r460132013", "bodyText": "Done. Removed these final modifiers.", "author": "gustavoatt", "createdAt": "2020-07-24T15:39:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjQzNg=="}], "type": "inlineReview", "revised_code": {"commit": "7e48187c68417a6c947480022dc8a6e6945ed5ca", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\nindex ebe73dd26..d16591ed0 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n\n@@ -89,28 +98,63 @@ public class TestSparkParquetReader extends AvroDataTest {\n \n   @Test\n   public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n-    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n-    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n-    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n-    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n+    String outputFilePath = String.format(\"%s/%s\", temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    HadoopOutputFile outputFile =\n+        HadoopOutputFile.fromPath(\n+            new org.apache.hadoop.fs.Path(outputFilePath), new Configuration());\n+    Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.withZone()));\n+    StructType sparkSchema =\n+        new StructType(\n+            new StructField[] {\n+              new StructField(\"ts\", DataTypes.TimestampType, true, Metadata.empty())\n+            });\n+    List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n \n     try (FileAppender<InternalRow> writer =\n-        Parquet.write(Files.localOutput(parquetFile.toString()))\n-            .writeSupport(\n-                new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport())\n-            .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n-            .set(\"org.apache.spark.legacyDateTime\", \"false\")\n-            .set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n-            .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n-            .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n-            .schema(schema)\n-            .build()) {\n+        new ParquetWriteAdapter<>(\n+            new NativeSparkWriterBuilder(outputFile)\n+                .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n+                .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n+                .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n+                .build(),\n+            MetricsConfig.getDefault())) {\n       writer.addAll(rows);\n     }\n \n-    final List<InternalRow> readRows =\n-        rowsFromFile(Files.localInput(parquetFile.toString()), schema);\n+    List<InternalRow> readRows = rowsFromFile(Files.localInput(outputFilePath), schema);\n     Assert.assertEquals(rows.size(), readRows.size());\n     Assert.assertThat(readRows, CoreMatchers.is(rows));\n   }\n+\n+  /**\n+   * Native Spark ParquetWriter.Builder implementation so that we can write timestamps using Spark's native\n+   * ParquetWriteSupport.\n+   */\n+  private static class NativeSparkWriterBuilder\n+      extends ParquetWriter.Builder<InternalRow, NativeSparkWriterBuilder> {\n+    private final Map<String, String> config = Maps.newHashMap();\n+\n+    public NativeSparkWriterBuilder(org.apache.parquet.io.OutputFile path) {\n+      super(path);\n+    }\n+\n+    public NativeSparkWriterBuilder set(String property, String value) {\n+      this.config.put(property, value);\n+      return self();\n+    }\n+\n+    @Override\n+    protected NativeSparkWriterBuilder self() {\n+      return this;\n+    }\n+\n+    @Override\n+    protected WriteSupport<InternalRow> getWriteSupport(Configuration configuration) {\n+      for (Map.Entry<String, String> entry : config.entrySet()) {\n+        configuration.set(entry.getKey(), entry.getValue());\n+      }\n+\n+      return new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport();\n+    }\n+  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjY1MA==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459826650", "bodyText": "Why not use temp.newFile?", "author": "rdblue", "createdAt": "2020-07-24T02:39:09Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java", "diffHunk": "@@ -67,4 +76,41 @@ protected void writeAndValidate(Schema schema) throws IOException {\n       Assert.assertFalse(\"Should not have extra rows\", rows.hasNext());\n     }\n   }\n+\n+  protected List<InternalRow> rowsFromFile(InputFile inputFile, Schema schema) throws IOException {\n+    try (CloseableIterable<InternalRow> reader =\n+        Parquet.read(inputFile)\n+            .project(schema)\n+            .createReaderFunc(type -> SparkParquetReaders.buildReader(schema, type))\n+            .build()) {\n+      return Lists.newArrayList(reader);\n+    }\n+  }\n+\n+  @Test\n+  public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n+    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n+    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n+    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");", "originalCommit": "b35027be3f3056b23a546f02ea9e4a1fcc91392c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzMTY2OA==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r460131668", "bodyText": "I initially tried that way but the writer fails because the file already exists.", "author": "gustavoatt", "createdAt": "2020-07-24T15:38:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjY1MA=="}], "type": "inlineReview", "revised_code": {"commit": "7e48187c68417a6c947480022dc8a6e6945ed5ca", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\nindex ebe73dd26..d16591ed0 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n\n@@ -89,28 +98,63 @@ public class TestSparkParquetReader extends AvroDataTest {\n \n   @Test\n   public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n-    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n-    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n-    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n-    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n+    String outputFilePath = String.format(\"%s/%s\", temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    HadoopOutputFile outputFile =\n+        HadoopOutputFile.fromPath(\n+            new org.apache.hadoop.fs.Path(outputFilePath), new Configuration());\n+    Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.withZone()));\n+    StructType sparkSchema =\n+        new StructType(\n+            new StructField[] {\n+              new StructField(\"ts\", DataTypes.TimestampType, true, Metadata.empty())\n+            });\n+    List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n \n     try (FileAppender<InternalRow> writer =\n-        Parquet.write(Files.localOutput(parquetFile.toString()))\n-            .writeSupport(\n-                new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport())\n-            .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n-            .set(\"org.apache.spark.legacyDateTime\", \"false\")\n-            .set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n-            .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n-            .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n-            .schema(schema)\n-            .build()) {\n+        new ParquetWriteAdapter<>(\n+            new NativeSparkWriterBuilder(outputFile)\n+                .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n+                .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n+                .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n+                .build(),\n+            MetricsConfig.getDefault())) {\n       writer.addAll(rows);\n     }\n \n-    final List<InternalRow> readRows =\n-        rowsFromFile(Files.localInput(parquetFile.toString()), schema);\n+    List<InternalRow> readRows = rowsFromFile(Files.localInput(outputFilePath), schema);\n     Assert.assertEquals(rows.size(), readRows.size());\n     Assert.assertThat(readRows, CoreMatchers.is(rows));\n   }\n+\n+  /**\n+   * Native Spark ParquetWriter.Builder implementation so that we can write timestamps using Spark's native\n+   * ParquetWriteSupport.\n+   */\n+  private static class NativeSparkWriterBuilder\n+      extends ParquetWriter.Builder<InternalRow, NativeSparkWriterBuilder> {\n+    private final Map<String, String> config = Maps.newHashMap();\n+\n+    public NativeSparkWriterBuilder(org.apache.parquet.io.OutputFile path) {\n+      super(path);\n+    }\n+\n+    public NativeSparkWriterBuilder set(String property, String value) {\n+      this.config.put(property, value);\n+      return self();\n+    }\n+\n+    @Override\n+    protected NativeSparkWriterBuilder self() {\n+      return this;\n+    }\n+\n+    @Override\n+    protected WriteSupport<InternalRow> getWriteSupport(Configuration configuration) {\n+      for (Map.Entry<String, String> entry : config.entrySet()) {\n+        configuration.set(entry.getKey(), entry.getValue());\n+      }\n+\n+      return new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport();\n+    }\n+  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjc1OQ==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r459826759", "bodyText": "I'd prefer to pass in a normal timestamp type and set a property, if needed, to enable INT96 support.", "author": "rdblue", "createdAt": "2020-07-24T02:39:45Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java", "diffHunk": "@@ -67,4 +76,41 @@ protected void writeAndValidate(Schema schema) throws IOException {\n       Assert.assertFalse(\"Should not have extra rows\", rows.hasNext());\n     }\n   }\n+\n+  protected List<InternalRow> rowsFromFile(InputFile inputFile, Schema schema) throws IOException {\n+    try (CloseableIterable<InternalRow> reader =\n+        Parquet.read(inputFile)\n+            .project(schema)\n+            .createReaderFunc(type -> SparkParquetReaders.buildReader(schema, type))\n+            .build()) {\n+      return Lists.newArrayList(reader);\n+    }\n+  }\n+\n+  @Test\n+  public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n+    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n+    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n+    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n+\n+    try (FileAppender<InternalRow> writer =\n+        Parquet.write(Files.localOutput(parquetFile.toString()))\n+            .writeSupport(\n+                new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport())\n+            .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n+            .set(\"org.apache.spark.legacyDateTime\", \"false\")\n+            .set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n+            .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n+            .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n+            .schema(schema)", "originalCommit": "b35027be3f3056b23a546f02ea9e4a1fcc91392c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDI1NTU2Mw==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r460255563", "bodyText": "I'm not sure I fully understand this comment.\nBut I did change my approach here, and while still writing InternalRow I removed most of these properties and left only the relevant ones to make sure that Spark writes these as int96.", "author": "gustavoatt", "createdAt": "2020-07-24T19:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjc1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "7e48187c68417a6c947480022dc8a6e6945ed5ca", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\nindex ebe73dd26..d16591ed0 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReader.java\n\n@@ -89,28 +98,63 @@ public class TestSparkParquetReader extends AvroDataTest {\n \n   @Test\n   public void testInt96TimestampProducedBySparkIsReadCorrectly() throws IOException {\n-    final Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.asSparkInt96()));\n-    final StructType sparkSchema = SparkSchemaUtil.convert(schema);\n-    final Path parquetFile = Paths.get(temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n-    final List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n+    String outputFilePath = String.format(\"%s/%s\", temp.getRoot().getAbsolutePath(), \"parquet_int96.parquet\");\n+    HadoopOutputFile outputFile =\n+        HadoopOutputFile.fromPath(\n+            new org.apache.hadoop.fs.Path(outputFilePath), new Configuration());\n+    Schema schema = new Schema(required(1, \"ts\", Types.TimestampType.withZone()));\n+    StructType sparkSchema =\n+        new StructType(\n+            new StructField[] {\n+              new StructField(\"ts\", DataTypes.TimestampType, true, Metadata.empty())\n+            });\n+    List<InternalRow> rows = Lists.newArrayList(RandomData.generateSpark(schema, 10, 0L));\n \n     try (FileAppender<InternalRow> writer =\n-        Parquet.write(Files.localOutput(parquetFile.toString()))\n-            .writeSupport(\n-                new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport())\n-            .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n-            .set(\"org.apache.spark.legacyDateTime\", \"false\")\n-            .set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n-            .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n-            .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n-            .schema(schema)\n-            .build()) {\n+        new ParquetWriteAdapter<>(\n+            new NativeSparkWriterBuilder(outputFile)\n+                .set(\"org.apache.spark.sql.parquet.row.attributes\", sparkSchema.json())\n+                .set(\"spark.sql.parquet.writeLegacyFormat\", \"false\")\n+                .set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\n+                .build(),\n+            MetricsConfig.getDefault())) {\n       writer.addAll(rows);\n     }\n \n-    final List<InternalRow> readRows =\n-        rowsFromFile(Files.localInput(parquetFile.toString()), schema);\n+    List<InternalRow> readRows = rowsFromFile(Files.localInput(outputFilePath), schema);\n     Assert.assertEquals(rows.size(), readRows.size());\n     Assert.assertThat(readRows, CoreMatchers.is(rows));\n   }\n+\n+  /**\n+   * Native Spark ParquetWriter.Builder implementation so that we can write timestamps using Spark's native\n+   * ParquetWriteSupport.\n+   */\n+  private static class NativeSparkWriterBuilder\n+      extends ParquetWriter.Builder<InternalRow, NativeSparkWriterBuilder> {\n+    private final Map<String, String> config = Maps.newHashMap();\n+\n+    public NativeSparkWriterBuilder(org.apache.parquet.io.OutputFile path) {\n+      super(path);\n+    }\n+\n+    public NativeSparkWriterBuilder set(String property, String value) {\n+      this.config.put(property, value);\n+      return self();\n+    }\n+\n+    @Override\n+    protected NativeSparkWriterBuilder self() {\n+      return this;\n+    }\n+\n+    @Override\n+    protected WriteSupport<InternalRow> getWriteSupport(Configuration configuration) {\n+      for (Map.Entry<String, String> entry : config.entrySet()) {\n+        configuration.set(entry.getKey(), entry.getValue());\n+      }\n+\n+      return new org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport();\n+    }\n+  }\n }\n"}}, {"oid": "7e48187c68417a6c947480022dc8a6e6945ed5ca", "url": "https://github.com/apache/iceberg/commit/7e48187c68417a6c947480022dc8a6e6945ed5ca", "message": "Rewrite spark int96 test without creating Int96 timestamp type", "committedDate": "2020-07-24T19:39:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDI2MTYyOA==", "url": "https://github.com/apache/iceberg/pull/1184#discussion_r460261628", "bodyText": "Note for reviewers (and future me): toByteBuffer returns a duplicate of the internal buffer so that it is safe for uses of it to modify the buffer's position with methods like getLong.", "author": "rdblue", "createdAt": "2020-07-24T20:01:13Z", "path": "data/src/main/java/org/apache/iceberg/data/parquet/BaseParquetReaders.java", "diffHunk": "@@ -345,6 +352,25 @@ public LocalDateTime read(LocalDateTime reuse) {\n     }\n   }\n \n+  private static class TimestampInt96Reader extends ParquetValueReaders.PrimitiveReader<LocalDateTime> {\n+    private static final long UNIX_EPOCH_JULIAN = 2_440_588L;\n+\n+    private TimestampInt96Reader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public LocalDateTime read(LocalDateTime reuse) {\n+      final ByteBuffer byteBuffer = column.nextBinary().toByteBuffer().order(ByteOrder.LITTLE_ENDIAN);", "originalCommit": "7e48187c68417a6c947480022dc8a6e6945ed5ca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "68dc4c5e091ea610011f3264f611615f73c1b05d", "url": "https://github.com/apache/iceberg/commit/68dc4c5e091ea610011f3264f611615f73c1b05d", "message": "Test checkstyle fixes", "committedDate": "2020-07-24T20:13:06Z", "type": "commit"}]}