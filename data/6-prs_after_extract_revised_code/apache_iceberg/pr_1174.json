{"pr_number": 1174, "pr_title": "Add TypeToFlinkType: convert iceberg types to Flink types", "pr_createdAt": "2020-07-07T05:50:27Z", "pr_url": "https://github.com/apache/iceberg/pull/1174", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjExOTE5Mw==", "url": "https://github.com/apache/iceberg/pull/1174#discussion_r452119193", "bodyText": "I'd prefer to provide some separate unit tests to address the conversion differences ( so that we won't regress).", "author": "openinx", "createdAt": "2020-07-09T10:25:54Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java", "diffHunk": "@@ -218,6 +218,24 @@ public void testListField() {\n             ))\n     );\n \n-    Assert.assertEquals(expectedSchema.asStruct(), actualSchema.asStruct());\n+    checkSchema(flinkSchema, icebergSchema);\n+  }\n+\n+  private void checkSchema(TableSchema flinkSchema, Schema icebergSchema) {\n+    Assert.assertEquals(icebergSchema.asStruct(), FlinkSchemaUtil.convert(flinkSchema).asStruct());\n+    // The conversion is not a 1:1 mapping, so we just check iceberg types.\n+    Assert.assertEquals(\n+        icebergSchema.asStruct(),\n+        FlinkSchemaUtil.convert(FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema))).asStruct());", "originalCommit": "884e7b4b568a44f637e7d83d90125e0933c1a823", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQwNTYxMQ==", "url": "https://github.com/apache/iceberg/pull/1174#discussion_r453405611", "bodyText": "I'll add tests for the conversion differences.", "author": "JingsongLi", "createdAt": "2020-07-13T02:38:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjExOTE5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "ab99a29617544dd3f21c961d69e6319361f39e4f", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java\nindex 3fa4739b3..731cf2c07 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java\n\n@@ -230,12 +237,33 @@ public class TestFlinkSchemaUtil {\n   }\n \n   @Test\n-  public void testUUID() {\n-    LogicalType flinkUUID = FlinkSchemaUtil.convert(Types.UUIDType.get());\n-    Assert.assertEquals(new CharType(16), flinkUUID);\n+  public void testInconsistentTypes() {\n+    checkInconsistentType(\n+        Types.UUIDType.get(), new BinaryType(16),\n+        new BinaryType(16), Types.FixedType.ofLength(16));\n+    checkInconsistentType(\n+        Types.StringType.get(), new VarCharType(VarCharType.MAX_LENGTH),\n+        new CharType(100), Types.StringType.get());\n+    checkInconsistentType(\n+        Types.BinaryType.get(), new VarBinaryType(VarBinaryType.MAX_LENGTH),\n+        new VarBinaryType(100), Types.BinaryType.get());\n+    checkInconsistentType(\n+        Types.TimeType.get(), new TimeType(6),\n+        new TimeType(3), Types.TimeType.get());\n+    checkInconsistentType(\n+        Types.TimestampType.withoutZone(), new TimestampType(6),\n+        new TimestampType(3), Types.TimestampType.withoutZone());\n+    checkInconsistentType(\n+        Types.TimestampType.withZone(), new LocalZonedTimestampType(6),\n+        new LocalZonedTimestampType(3), Types.TimestampType.withZone());\n+  }\n \n+  private void checkInconsistentType(\n+      Type icebergType, LogicalType flinkExpectedType,\n+      LogicalType flinkType, Type icebergExpectedType) {\n+    Assert.assertEquals(flinkExpectedType, FlinkSchemaUtil.convert(icebergType));\n     Assert.assertEquals(\n-        Types.StructType.of(Types.NestedField.optional(0, \"f0\", Types.StringType.get())),\n-        FlinkSchemaUtil.convert(FlinkSchemaUtil.toSchema(RowType.of(flinkUUID))).asStruct());\n+        Types.StructType.of(Types.NestedField.optional(0, \"f0\", icebergExpectedType)),\n+        FlinkSchemaUtil.convert(FlinkSchemaUtil.toSchema(RowType.of(flinkType))).asStruct());\n   }\n }\n"}}, {"oid": "ab99a29617544dd3f21c961d69e6319361f39e4f", "url": "https://github.com/apache/iceberg/commit/ab99a29617544dd3f21c961d69e6319361f39e4f", "message": "Fix comments", "committedDate": "2020-07-14T02:48:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUxMzgwMA==", "url": "https://github.com/apache/iceberg/pull/1174#discussion_r454513800", "bodyText": "Precision isn't lost for date/time types. It is always microseconds.", "author": "rdblue", "createdAt": "2020-07-14T17:14:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -21,10 +21,30 @@\n \n import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.types.FieldsDataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n \n+/**\n+ * Converter between Flink types and Iceberg type.\n+ * The conversion is not a 1:1 mapping that not allows back-and-forth conversion. So some information might get lost\n+ * during the back-and-forth conversion.\n+ * <p>\n+ * This inconsistent types:\n+ * <ul>\n+ *   <li>map Iceberg UUID type to Flink BinaryType(16)</li>\n+ *   <li>map Flink VarCharType and CharType to Iceberg String type (lost precision)</li>\n+ *   <li>map Flink VarBinaryType to Iceberg Binary type (lost precision)</li>\n+ *   <li>map Flink TimeType to Iceberg Time type (lost precision)</li>\n+ *   <li>map Flink TimestampType to Iceberg Timestamp without zone type (lost precision)</li>\n+ *   <li>map Flink LocalZonedTimestampType to Iceberg Timestamp with zone type (lost precision)</li>", "originalCommit": "ab99a29617544dd3f21c961d69e6319361f39e4f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDc0NTU4Mg==", "url": "https://github.com/apache/iceberg/pull/1174#discussion_r454745582", "bodyText": "Yes, I'll modify comments.", "author": "JingsongLi", "createdAt": "2020-07-15T02:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUxMzgwMA=="}], "type": "inlineReview", "revised_code": {"commit": "a80363691ef2ede2e7c7967bafdd06e1e39ad7b4", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\nindex e1b6c5f1d..f565bd40b 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\n\n@@ -20,7 +20,6 @@\n package org.apache.iceberg.flink;\n \n import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.types.FieldsDataType;\n import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.table.types.utils.TypeConversions;\n"}}, {"oid": "a80363691ef2ede2e7c7967bafdd06e1e39ad7b4", "url": "https://github.com/apache/iceberg/commit/a80363691ef2ede2e7c7967bafdd06e1e39ad7b4", "message": "Add TypeToFlinkType: convert iceberg types to Flink types", "committedDate": "2020-07-15T02:02:47Z", "type": "commit"}, {"oid": "a7fb7d403489d11550323051f49ee7227a94630b", "url": "https://github.com/apache/iceberg/commit/a7fb7d403489d11550323051f49ee7227a94630b", "message": "Fix comments", "committedDate": "2020-07-15T02:02:47Z", "type": "commit"}, {"oid": "814672b6f0da44309785d740e8ecb32be9dd1877", "url": "https://github.com/apache/iceberg/commit/814672b6f0da44309785d740e8ecb32be9dd1877", "message": "Rebase&Fix comments", "committedDate": "2020-07-15T02:06:18Z", "type": "commit"}, {"oid": "814672b6f0da44309785d740e8ecb32be9dd1877", "url": "https://github.com/apache/iceberg/commit/814672b6f0da44309785d740e8ecb32be9dd1877", "message": "Rebase&Fix comments", "committedDate": "2020-07-15T02:06:18Z", "type": "forcePushed"}]}