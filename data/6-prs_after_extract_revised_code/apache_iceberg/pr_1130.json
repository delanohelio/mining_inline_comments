{"pr_number": 1130, "pr_title": "Identity Partitioning: Use PartitionUtil.constantsMap in InputFormat", "pr_createdAt": "2020-06-19T23:33:20Z", "pr_url": "https://github.com/apache/iceberg/pull/1130", "timeline": [{"oid": "dc548deb2be122339ab5e2e1ea3382e902a83a15", "url": "https://github.com/apache/iceberg/commit/dc548deb2be122339ab5e2e1ea3382e902a83a15", "message": "Identity Partitioning: Use partitionsMap in InputFormat", "committedDate": "2020-06-19T23:31:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ==", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443077759", "bodyText": "I was hesitant to use the converter from TableScanIterable as it seems it might only supports Avro. Also we might require different maps for Hive and Pig", "author": "rdsr", "createdAt": "2020-06-19T23:35:27Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -348,49 +342,38 @@ public void close() throws IOException {\n       currentIterator.close();\n     }\n \n-    private static Map<String, Integer> buildNameToPos(Schema expectedSchema) {\n-      Map<String, Integer> nameToPos = Maps.newHashMap();\n-      for (int pos = 0; pos < expectedSchema.asStruct().fields().size(); pos++) {\n-        Types.NestedField field = expectedSchema.asStruct().fields().get(pos);\n-        nameToPos.put(field.name(), pos);\n-      }\n-      return nameToPos;\n-    }\n-\n-    private CloseableIterator<T> open(FileScanTask currentTask) {\n-      DataFile file = currentTask.file();\n-      // schema of rows returned by readers\n-      PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(expectedSchema));\n-      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    private CloseableIterator<T> open(FileScanTask task) {\n+      PartitionSpec spec = task.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n+      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n \n       CloseableIterable<T> iterable;\n-      if (hasJoinedPartitionColumns) {\n-        Schema readDataSchema = TypeUtil.selectNot(expectedSchema, idColumns);\n-        Schema identityPartitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-        iterable = CloseableIterable.transform(open(currentTask, readDataSchema),\n-            row -> withIdentityPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      if (projectsIdentityPartitionColumns) {\n+        //TODO: seems like we have to specify a converter to convert the partition values", "originalCommit": "dc548deb2be122339ab5e2e1ea3382e902a83a15", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwMTQ3Nw==", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443701477", "bodyText": "Yes, we should use the one from TableScanIterable. That conversion is from internal representations (e.g., long for timestamps) to Iceberg generics (LocalDateTime or OffsetDateTime for timestamps) so it is the right one to use here. When we add support for Pig or Hive objects, we would need to convert differently for those.", "author": "rdblue", "createdAt": "2020-06-22T16:59:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwMzYyNQ==", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443703625", "bodyText": "Any thoughts on which is the right place to put the converter? Or maybe I can just make the one in TableScanIterable public and use that?", "author": "rdsr", "createdAt": "2020-06-22T17:03:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcyNTQ5Mg==", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443725492", "bodyText": "Now that the generics for Avro are in core, we can put them in the org.apache.iceberg.data package in iceberg-core. How about a new Converters class?", "author": "rdblue", "createdAt": "2020-06-22T17:45:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzc0MDgxNQ==", "url": "https://github.com/apache/iceberg/pull/1130#discussion_r443740815", "bodyText": "Fixed", "author": "rdsr", "createdAt": "2020-06-22T18:13:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3Nzc1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "0ec3fccc76ba2122a268506ae2463bc6b4183c98", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java b/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java\nindex acb80c321..b209a80e4 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java\n\n@@ -342,38 +345,20 @@ public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n       currentIterator.close();\n     }\n \n-    private CloseableIterator<T> open(FileScanTask task) {\n-      PartitionSpec spec = task.spec();\n-      Set<Integer> idColumns = spec.identitySourceIds();\n-      Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-      boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n-\n-      CloseableIterable<T> iterable;\n-      if (projectsIdentityPartitionColumns) {\n-        //TODO: seems like we have to specify a converter to convert the partition values\n-        // Maybe we can use the one in TableScanIterable?\n-        iterable = open(task, expectedSchema, PartitionUtil.constantsMap(task));\n-      } else {\n-        iterable = open(task, expectedSchema, Collections.emptyMap());\n-      }\n-\n-      return iterable.iterator();\n-    }\n-\n-    private CloseableIterable<T> open(FileScanTask currentTask, Schema readSchema, Map<Integer, ?> idToConstant) {\n+    private CloseableIterable<T> open(FileScanTask currentTask, Schema readSchema) {\n       DataFile file = currentTask.file();\n       // TODO we should make use of FileIO to create inputFile\n       InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n       CloseableIterable<T> iterable;\n       switch (file.format()) {\n         case AVRO:\n-          iterable = newAvroIterable(inputFile, currentTask, readSchema, idToConstant);\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n           break;\n         case ORC:\n-          iterable = newOrcIterable(inputFile, currentTask, readSchema, idToConstant);\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n           break;\n         case PARQUET:\n-          iterable = newParquetIterable(inputFile, currentTask, readSchema, idToConstant);\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n           break;\n         default:\n           throw new UnsupportedOperationException(\n"}}, {"oid": "0ec3fccc76ba2122a268506ae2463bc6b4183c98", "url": "https://github.com/apache/iceberg/commit/0ec3fccc76ba2122a268506ae2463bc6b4183c98", "message": "Address review comments", "committedDate": "2020-06-22T18:47:02Z", "type": "commit"}, {"oid": "0ec3fccc76ba2122a268506ae2463bc6b4183c98", "url": "https://github.com/apache/iceberg/commit/0ec3fccc76ba2122a268506ae2463bc6b4183c98", "message": "Address review comments", "committedDate": "2020-06-22T18:47:02Z", "type": "forcePushed"}]}