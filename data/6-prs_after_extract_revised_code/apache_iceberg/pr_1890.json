{"pr_number": 1890, "pr_title": "Change Procedures to use Identifiers instead of Namespace/Table Params", "pr_createdAt": "2020-12-08T17:14:13Z", "pr_url": "https://github.com/apache/iceberg/pull/1890", "timeline": [{"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "url": "https://github.com/apache/iceberg/commit/2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "message": "Change Procedures to use Identifiers instead of Namespace/Table Params\n\nPrevious procedures would take namespace and table seperately. In order to\nfacilitate future developments in table identifiers like path based identifiers,\nwe have switched to using single string parameters for identifying the table.", "committedDate": "2020-12-08T17:13:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538621508", "bodyText": "Last time we talked, we thought about defaulting the catalog to the procedure catalog, not the default catalog.\nI thought we would simply take the first parsed name part and check if spark.sql.catalog._name_part_ is set. If yes, it means our identifier contains a catalog and we don't have to prepend the procedure catalog to name parts and should resolve the catalog and validate the resolved catalog matches tableCatalog. If there is no such catalog, we can just call tableCatalog.load with the constructed identifier.", "author": "aokolnychyi", "createdAt": "2020-12-08T17:16:22Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTc5NA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538621794", "bodyText": "@RussellSpitzer @rdblue, thoughts?", "author": "aokolnychyi", "createdAt": "2020-12-08T17:16:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNDQxMw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538624413", "bodyText": "I did this based on some of the discussion me and Ryan were having\nWe know we have a catalog if there is a registered catalog with the name\n\n\n\n\n\n11:45\nIn Spark, the rules are:\n1. If the identifier is a single part, it is a catalog name. Fill in current catalog and current namespace\n2. If the first part of the identifier is a known catalog, it is a fully qualified name\n3. If the first part is not a known catalog, fill in the current catalog (not namespace because it is already there)\n11:46\nThe only difference here would be that we consider the current catalog to be the procedure catalog because this is in the procedure's context\n\nI thought this was reasonable but we could force the catalog as well", "author": "RussellSpitzer", "createdAt": "2020-12-08T17:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNjIxNQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538626215", "bodyText": "Isn't that what Spark3Util.catalogAndIdentifier does? I think the only difference is that it uses Spark's catalogManager instead of simply checking table properties.", "author": "rdblue", "createdAt": "2020-12-08T17:20:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNjU2Mg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538626562", "bodyText": "I think I missed an update here.", "author": "aokolnychyi", "createdAt": "2020-12-08T17:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMzE4OQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538633189", "bodyText": "I added an argument to catalogAndIdentifier, the last parameter is the \"current catalog\" that's used", "author": "RussellSpitzer", "createdAt": "2020-12-08T17:27:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex a7db1588a..3b385a145 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -70,14 +71,18 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String identifier) {\n-    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n       throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n+              identifierAsString, catalogAndIdentifier.catalog()));\n+    }\n \n     return catalogAndIdentifier.identifier();\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538622083", "bodyText": "We should validate the resolved catalog is tableCatalog to make sure we don't modify tables in other catalogs.", "author": "aokolnychyi", "createdAt": "2020-12-08T17:16:54Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNTgwNA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538625804", "bodyText": "I didn't want that to be a general requirement, But I could add it here. Ie if we want other procedures which operate on tables in other catalogs", "author": "RussellSpitzer", "createdAt": "2020-12-08T17:20:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNzIzMw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538637233", "bodyText": "Well, maybe the validation should not happen here. We could call it toCatalogAndIdentifier and validate above.", "author": "aokolnychyi", "createdAt": "2020-12-08T17:30:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY4Nzk4OA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538687988", "bodyText": "That's a good idea", "author": "RussellSpitzer", "createdAt": "2020-12-08T18:19:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2OTA0Mg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538769042", "bodyText": "Looks like this validation is still here?", "author": "rdblue", "createdAt": "2020-12-08T20:04:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjU3OQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538802579", "bodyText": "Ah yes I got very confused. I think we should just let this be for now and remove the check. I think instead \"loadSparkTable\" can have the check since it assumes that we are loading out of this catalog.", "author": "RussellSpitzer", "createdAt": "2020-12-08T20:58:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNDUxMw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538804513", "bodyText": "Sorry I flipped back again, i'm just going to not put the condition here. We'll get an error in \"loadSparkTable\" which won't have the type argument but should be clear enough?", "author": "RussellSpitzer", "createdAt": "2020-12-08T21:01:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg0MjUzNg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538842536", "bodyText": "The check definitely needs to happen somewhere, and the error should be specific: \"Cannot run stored procedure %s.%s: %s is not in catalog %s\", catalogName, procName, tableName, catalogName.\nI don't see a way for loadSparkTable to catch that the original identifier was actually for a different catalog because the catalog for this identifier is discarded.", "author": "rdblue", "createdAt": "2020-12-08T22:05:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex a7db1588a..3b385a145 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -70,14 +71,18 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String identifier) {\n-    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n       throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n+              identifierAsString, catalogAndIdentifier.catalog()));\n+    }\n \n     return catalogAndIdentifier.identifier();\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNTc5OA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538625798", "bodyText": "I think we better call our variables tableIdent rather than tableName now. I think we can keep the parameter name as table.", "author": "aokolnychyi", "createdAt": "2020-12-08T17:20:23Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java", "diffHunk": "@@ -77,12 +76,11 @@ public StructType outputType() {\n \n   @Override\n   public InternalRow[] call(InternalRow args) {\n-    String namespace = args.getString(0);\n-    String tableName = args.getString(1);\n-    Long olderThanMillis = args.isNullAt(2) ? null : DateTimeUtils.toMillis(args.getLong(2));\n-    Integer retainLastNum = args.isNullAt(3) ? null : args.getInt(3);\n+    String tableName = args.getString(0);", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY5ODI3OA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538698278", "bodyText": "sounds good, renames all around", "author": "RussellSpitzer", "createdAt": "2020-12-08T18:29:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNTc5OA=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java\nindex 1b3bd104f..c4eaeec9a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java\n\n@@ -76,11 +76,11 @@ public class ExpireSnapshotsProcedure extends BaseProcedure {\n \n   @Override\n   public InternalRow[] call(InternalRow args) {\n-    String tableName = args.getString(0);\n+    String tableIdent = args.getString(0);\n     Long olderThanMillis = args.isNullAt(1) ? null : DateTimeUtils.toMillis(args.getLong(1));\n     Integer retainLastNum = args.isNullAt(2) ? null : args.getInt(2);\n \n-    return modifyIcebergTable(tableName, table -> {\n+    return modifyIcebergTable(tableIdent, table -> {\n       Actions actions = Actions.forTable(table);\n \n       ExpireSnapshotsAction action = actions.expireSnapshots();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyODMxNA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538628314", "bodyText": "nit: shall we call it identifierAsString to be consistent?", "author": "aokolnychyi", "createdAt": "2020-12-08T17:22:44Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +46,18 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(String identifier, Function<org.apache.iceberg.Table, T> func) {", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY5ODQ2NA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538698464", "bodyText": "changed", "author": "RussellSpitzer", "createdAt": "2020-12-08T18:29:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyODMxNA=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex a7db1588a..3b385a145 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -46,12 +47,12 @@ abstract class BaseProcedure implements Procedure {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String identifier, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(identifier, true, func);\n+  protected <T> T modifyIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String identifier, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(identifier, false, func);\n+  protected <T> T withIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, false, func);\n   }\n \n   private <T> T execute(String identifierAsString, boolean refreshSparkCache,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyOTE4MQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538629181", "bodyText": "Shall we do an import so that we can refer to CatalogAndIdentifier directly?", "author": "aokolnychyi", "createdAt": "2020-12-08T17:23:34Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNDg5MQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538634891", "bodyText": "Can do", "author": "RussellSpitzer", "createdAt": "2020-12-08T17:29:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyOTE4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex a7db1588a..3b385a145 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -70,14 +71,18 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String identifier) {\n-    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n       throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n+              identifierAsString, catalogAndIdentifier.catalog()));\n+    }\n \n     return catalogAndIdentifier.identifier();\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyOTU1NQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538629555", "bodyText": "nit: identifier -> identifierAsString?", "author": "aokolnychyi", "createdAt": "2020-12-08T17:23:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex a7db1588a..3b385a145 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -70,14 +71,18 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String identifier) {\n-    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n       throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n+              identifierAsString, catalogAndIdentifier.catalog()));\n+    }\n \n     return catalogAndIdentifier.identifier();\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMDMyNQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538630325", "bodyText": "nit: formatting is off", "author": "aokolnychyi", "createdAt": "2020-12-08T17:24:42Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                            CatalogPlugin fallBackCatalog) throws ParseException {", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNTk5MQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538635991", "bodyText": "I wish I could teach IDEA this, every time I run reformat it swaps it back", "author": "RussellSpitzer", "createdAt": "2020-12-08T17:29:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMDMyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex 3922c300b..b2acc3f1a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -609,11 +609,11 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n-                                                            CatalogPlugin fallBackCatalog) throws ParseException {\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier, fallBackCatalog);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMTIzNA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538631234", "bodyText": "Shall we call it defaultCatalog?", "author": "aokolnychyi", "createdAt": "2020-12-08T17:25:30Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                            CatalogPlugin fallBackCatalog) throws ParseException {", "originalCommit": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNTQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538635475", "bodyText": "I think that's a fine name as well", "author": "RussellSpitzer", "createdAt": "2020-12-08T17:29:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMTIzNA=="}], "type": "inlineReview", "revised_code": {"commit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex 3922c300b..b2acc3f1a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -609,11 +609,11 @@ public class Spark3Util {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n-                                                            CatalogPlugin fallBackCatalog) throws ParseException {\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier, fallBackCatalog);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n"}}, {"oid": "54302cd3be45ae68906a21213c7d64b6145c4940", "url": "https://github.com/apache/iceberg/commit/54302cd3be45ae68906a21213c7d64b6145c4940", "message": "Checkstyle", "committedDate": "2020-12-08T17:26:23Z", "type": "commit"}, {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "url": "https://github.com/apache/iceberg/commit/8d650b1fb2599ee913a88e1a5d99008b13589a71", "message": "Reviewer Comments", "committedDate": "2020-12-08T19:11:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Mjg2OQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538762869", "bodyText": "Should this have a test for empty table name?", "author": "rdblue", "createdAt": "2020-12-08T19:55:00Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "diffHunk": "@@ -171,18 +167,10 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't', 2.2)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('', 't', 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4MzY1Ng==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538783656", "bodyText": "+1", "author": "aokolnychyi", "createdAt": "2020-12-08T20:28:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Mjg2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMDM3Ng==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538820376", "bodyText": "added", "author": "RussellSpitzer", "createdAt": "2020-12-08T21:28:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Mjg2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\nindex cfe9d41c4..7c77d8f3e 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n\n@@ -169,6 +169,10 @@ public class TestCherrypickSnapshotProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Missing required parameters\",\n         () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n \n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('', 1L)\", catalogName));\n+\n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n         () -> sql(\"CALL %s.system.cherrypick_snapshot('t', 2.2)\", catalogName));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2NDA1OQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538764059", "bodyText": "Nit: whitespace change?", "author": "rdblue", "createdAt": "2020-12-08T19:56:50Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "diffHunk": "@@ -184,8 +184,8 @@ public void testRollbackToTimestampWithQuotedIdentifiers() {\n     String quotedNamespace = quotedNamespaceBuilder.toString();\n \n     List<Object[]> output = sql(\n-        \"CALL %s.system.rollback_to_timestamp('%s', '`%s`', TIMESTAMP '%s')\",\n-        catalogName, quotedNamespace, tableIdent.name(), firstSnapshotTimestamp);\n+        \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n+            catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNjcwOQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538806709", "bodyText": "got it, bad copy from the other rollback method", "author": "RussellSpitzer", "createdAt": "2020-12-08T21:05:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2NDA1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\nindex 09c1fe039..65ee27a0f 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n\n@@ -185,7 +185,7 @@ public class TestRollbackToTimestampProcedure extends SparkExtensionsTestBase {\n \n     List<Object[]> output = sql(\n         \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n-            catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);\n+         catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);\n \n     assertEquals(\"Procedure output must match\",\n         ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538766764", "bodyText": "This namespace is associated with a catalog. I don't think this should use the catalog passed in with this namespace unless that catalog is the current catalog. I would do the following:\n\nIf the fallback catalog is the current catalog, use the current namespace\nIf the fallback catalog is not the current catalog, use its default namespace (catalog.defaultNamespace())\n\nThe catalog's default namespace is used when you switch to that catalog. The default namespace becomes the current namespace. Using the default fits with the idea that the fallback catalog is the current catalog for the context of the stored procedure.\nIt this is difficult to implement, then we can always go back to using the current catalog rather than the procedure catalog.", "author": "rdblue", "createdAt": "2020-12-08T20:01:09Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2NzM1Mg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538767352", "bodyText": "Yeah I was wondering a bit about this, I think that's a reasonable solve", "author": "RussellSpitzer", "createdAt": "2020-12-08T20:02:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc3ODc0MQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538778741", "bodyText": "Does this mean we can always call \"defaultCatalog.defaultNamespace\"? What is the difference between catalogManager.currentNamespace and the default catalog's implementation?", "author": "RussellSpitzer", "createdAt": "2020-12-08T20:20:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4MjE5MQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538782191", "bodyText": "def currentNamespace: Array[String] = synchronized {\n    _currentNamespace.getOrElse {\n      if (currentCatalog.name() == SESSION_CATALOG_NAME) {\n        Array(v1SessionCatalog.getCurrentDatabase)\n      } else {\n        currentCatalog.defaultNamespace()\n      }\n    }\n  }\nV1 always making things more difficult\nOh also _currentNamespace. ok np", "author": "RussellSpitzer", "createdAt": "2020-12-08T20:26:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5MjU2MQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538792561", "bodyText": "The current namespace is whatever you've set with USE. The default namespace is the default when you change catalogs without specifying a namespace (USE CATALOG cat).", "author": "rdblue", "createdAt": "2020-12-08T20:44:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxNjc4Mg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538816782", "bodyText": "Ugh :(", "author": "aokolnychyi", "createdAt": "2020-12-08T21:22:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE0NzQ1Mg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539147452", "bodyText": "The current logic seems to be as discussed here so I am resolving this thread.", "author": "aokolnychyi", "createdAt": "2020-12-09T09:31:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}], "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex b2acc3f1a..1bfc1f8d9 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -625,21 +625,26 @@ public class Spark3Util {\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n-   * @param fallBackCatalog Catalog to use if none is specified\n+   * @param defaultCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n-                                                          CatalogPlugin fallBackCatalog) {\n+                                                          CatalogPlugin defaultCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    String[] currentNamespace = catalogManager.currentNamespace();\n     int lastElementIndex = nameParts.size() - 1;\n     String name = nameParts.get(lastElementIndex);\n+    String[] currentNamespace;\n+    if (defaultCatalog.equals(catalogManager.currentCatalog())) {\n+      currentNamespace = catalogManager.currentNamespace();\n+    } else {\n+      currentNamespace = defaultCatalog.defaultNamespace();\n+    }\n \n     if (nameParts.size() == 1) {\n       // Only a single element, use current catalog and namespace\n-      return new CatalogAndIdentifier(fallBackCatalog, Identifier.of(currentNamespace, name));\n+      return new CatalogAndIdentifier(defaultCatalog, Identifier.of(currentNamespace, name));\n     } else {\n       try {\n         // Assume the first element is a valid catalog\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Nzk3OA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538767978", "bodyText": "It would be nice to follow this recommendation from the other review: https://github.com/apache/iceberg/pull/1525/files#r538758721\nThe description could be the argument name.", "author": "rdblue", "createdAt": "2020-12-08T20:03:05Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5MDIyOA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538790228", "bodyText": "This will require a bit more refactoring since our callers are usually via modifyIcebergTable/withIcebergTable", "author": "RussellSpitzer", "createdAt": "2020-12-08T20:40:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Nzk3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5NzI1OQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538797259", "bodyText": "Moved the \"toIdentifier\" calls into the Procedures and now the modify/with functions take identifiers directly", "author": "RussellSpitzer", "createdAt": "2020-12-08T20:52:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Nzk3OA=="}], "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 3b385a145..f201ab481 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -71,17 +71,16 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  private Identifier toIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifierAsString, String argName) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n+\n     CatalogAndIdentifier catalogAndIdentifier;\n     try {\n       catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n-    }\n-    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n-      throw new IllegalArgumentException(\n-          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n-              identifierAsString, catalogAndIdentifier.catalog()));\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier [%s] for argument %s\",\n+          identifierAsString, argName), e);\n     }\n \n     return catalogAndIdentifier.identifier();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2ODEzMg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538768132", "bodyText": "Nit: missing newline between control flow.", "author": "rdblue", "createdAt": "2020-12-08T20:03:19Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n+    }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 3b385a145..f201ab481 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -71,17 +71,16 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  private Identifier toIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifierAsString, String argName) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n+\n     CatalogAndIdentifier catalogAndIdentifier;\n     try {\n       catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n-    }\n-    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n-      throw new IllegalArgumentException(\n-          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n-              identifierAsString, catalogAndIdentifier.catalog()));\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier [%s] for argument %s\",\n+          identifierAsString, argName), e);\n     }\n \n     return catalogAndIdentifier.identifier();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2ODI0NA==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538768244", "bodyText": "Use a precondition?", "author": "rdblue", "createdAt": "2020-12-08T20:03:34Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n+    }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n+      throw new IllegalArgumentException(", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 3b385a145..f201ab481 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -71,17 +71,16 @@ abstract class BaseProcedure implements Procedure {\n     return result;\n   }\n \n-  private Identifier toIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifierAsString, String argName) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n+\n     CatalogAndIdentifier catalogAndIdentifier;\n     try {\n       catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n-    }\n-    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n-      throw new IllegalArgumentException(\n-          String.format(\"Cannot call procedure on table from another catalog. %s resolved to a table in catalog %s\",\n-              identifierAsString, catalogAndIdentifier.catalog()));\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier [%s] for argument %s\",\n+          identifierAsString, argName), e);\n     }\n \n     return catalogAndIdentifier.identifier();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4NzQyMQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538787421", "bodyText": "nit: I think we call it fallBackCatalog here and defaultCatalog above", "author": "aokolnychyi", "createdAt": "2020-12-08T20:35:29Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex b2acc3f1a..1bfc1f8d9 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n\n@@ -625,21 +625,26 @@ public class Spark3Util {\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n-   * @param fallBackCatalog Catalog to use if none is specified\n+   * @param defaultCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n-                                                          CatalogPlugin fallBackCatalog) {\n+                                                          CatalogPlugin defaultCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    String[] currentNamespace = catalogManager.currentNamespace();\n     int lastElementIndex = nameParts.size() - 1;\n     String name = nameParts.get(lastElementIndex);\n+    String[] currentNamespace;\n+    if (defaultCatalog.equals(catalogManager.currentCatalog())) {\n+      currentNamespace = catalogManager.currentNamespace();\n+    } else {\n+      currentNamespace = defaultCatalog.defaultNamespace();\n+    }\n \n     if (nameParts.size() == 1) {\n       // Only a single element, use current catalog and namespace\n-      return new CatalogAndIdentifier(fallBackCatalog, Identifier.of(currentNamespace, name));\n+      return new CatalogAndIdentifier(defaultCatalog, Identifier.of(currentNamespace, name));\n     } else {\n       try {\n         // Assume the first element is a valid catalog\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4OTMwNg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538789306", "bodyText": "Should we keep one precondition for identifierAsString?", "author": "aokolnychyi", "createdAt": "2020-12-08T20:38:54Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +47,18 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, false, func);\n+  protected <T> T withIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, false, func);\n   }\n \n-  private <T> T execute(String namespace, String tableName, boolean refreshSparkCache,\n+  private <T> T execute(String identifierAsString, boolean refreshSparkCache,\n                         Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");", "originalCommit": "8d650b1fb2599ee913a88e1a5d99008b13589a71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyOTg3OQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538829879", "bodyText": "There is a precondition on the null/empty issue in \"toIdentifier\"", "author": "RussellSpitzer", "createdAt": "2020-12-08T21:44:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4OTMwNg=="}], "type": "inlineReview", "revised_code": {"commit": "308aa914b87ae563205cb5c085cc129290e6621a", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 3b385a145..f201ab481 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -47,18 +48,17 @@ abstract class BaseProcedure implements Procedure {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(identifierAsString, true, func);\n+  protected <T> T modifyIcebergTable(Identifier ident, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(ident, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(identifierAsString, false, func);\n+  protected <T> T withIcebergTable(Identifier ident, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(ident, false, func);\n   }\n \n-  private <T> T execute(String identifierAsString, boolean refreshSparkCache,\n+  private <T> T execute(Identifier ident, boolean refreshSparkCache,\n                         Function<org.apache.iceberg.Table, T> func) {\n \n-    Identifier ident = toIdentifier(identifierAsString);\n     SparkTable sparkTable = loadSparkTable(ident);\n     org.apache.iceberg.Table icebergTable = sparkTable.table();\n \n"}}, {"oid": "308aa914b87ae563205cb5c085cc129290e6621a", "url": "https://github.com/apache/iceberg/commit/308aa914b87ae563205cb5c085cc129290e6621a", "message": "Reviewer Comments", "committedDate": "2020-12-08T21:29:15Z", "type": "commit"}, {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "url": "https://github.com/apache/iceberg/commit/094987f1c263aa1bdeaca422260a2f9fc41cea7b", "message": "Add test and Check for Same Catalog", "committedDate": "2020-12-08T23:13:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MTAxNQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538891015", "bodyText": "This is really wordy. I'd probably simplify it to \"Cannot run procedure %s in catalog %s: %s is a table in catalog %s\".", "author": "rdblue", "createdAt": "2020-12-08T23:35:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -79,10 +79,14 @@ protected Identifier toIdentifier(String identifierAsString, String argName) {\n     try {\n       catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new IllegalArgumentException(String.format(\"Cannot parse identifier [%s] for argument %s\",\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",\n           identifierAsString, argName), e);\n     }\n \n+    Preconditions.checkArgument(catalogAndIdentifier.catalog().equals(tableCatalog), \"Cannot run procedure\" +\n+        \" in catalog '%s': Argument %s was set to '%s' which resolves to a table in a different catalog '%s'\",\n+        tableCatalog.name(), argName, identifierAsString, catalogAndIdentifier.catalog().name());", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 08639c7d8..68239504c 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -75,19 +73,18 @@ abstract class BaseProcedure implements Procedure {\n     Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n         \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    CatalogAndIdentifier catalogAndIdentifier;\n-    try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n-    } catch (ParseException e) {\n-      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",\n-          identifierAsString, argName), e);\n-    }\n+    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n+        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n+\n+    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n+    Identifier identifier = catalogAndIdentifier.identifier();\n \n-    Preconditions.checkArgument(catalogAndIdentifier.catalog().equals(tableCatalog), \"Cannot run procedure\" +\n-        \" in catalog '%s': Argument %s was set to '%s' which resolves to a table in a different catalog '%s'\",\n-        tableCatalog.name(), argName, identifierAsString, catalogAndIdentifier.catalog().name());\n+    Preconditions.checkArgument(\n+        catalog.equals(tableCatalog),\n+        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n+        tableCatalog.name(), identifierAsString, catalog);\n \n-    return catalogAndIdentifier.identifier();\n+    return identifier;\n   }\n \n   protected SparkTable loadSparkTable(Identifier ident) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3NzE4Mw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539077183", "bodyText": "nit: with?", "author": "aokolnychyi", "createdAt": "2020-12-09T07:45:09Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "diffHunk": "@@ -171,18 +167,14 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\nindex 7c77d8f3e..ba1c5db16 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n\n@@ -169,7 +169,7 @@ public class TestCherrypickSnapshotProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Missing required parameters\",\n         () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.cherrypick_snapshot('', 1L)\", catalogName));\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3NzUxMQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539077511", "bodyText": "nit: with?", "author": "aokolnychyi", "createdAt": "2020-12-09T07:45:47Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java", "diffHunk": "@@ -163,18 +163,28 @@ public void testInvalidExpireSnapshotsCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.expire_snapshots()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.expire_snapshots('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('', 't')\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\nindex 4d98e1f38..ce1bf9be6 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n\n@@ -169,7 +169,7 @@ public class TestExpireSnapshotsProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.expire_snapshots('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.expire_snapshots('')\", catalogName));\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODE1Mg==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078152", "bodyText": "nit: we can probably use our sql() helper method to get parameterization for free.", "author": "aokolnychyi", "createdAt": "2020-12-09T07:47:02Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java", "diffHunk": "@@ -163,18 +163,28 @@ public void testInvalidExpireSnapshotsCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.expire_snapshots()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.expire_snapshots('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('', 't')\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.expire_snapshots('')\", catalogName));\n+  }\n \n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('n', '')\", catalogName));\n+  @Test\n+  public void testResolvingTableInAnotherCatalog() throws IOException {\n+    String anotherCatalog = \"another_\" + catalogName;\n+    spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog, SparkCatalog.class.getName());\n+    spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog + \".type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog + \".warehouse\", \"file:\" + temp.newFolder().toString());\n+    spark.sql(String.format(\"CREATE TABLE %s.%s (id bigint NOT NULL, data string) USING iceberg\", anotherCatalog,", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\nindex 4d98e1f38..ce1bf9be6 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n\n@@ -169,7 +169,7 @@ public class TestExpireSnapshotsProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.expire_snapshots('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.expire_snapshots('')\", catalogName));\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODUyOQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078529", "bodyText": "nit: with?", "author": "aokolnychyi", "createdAt": "2020-12-09T07:47:40Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java", "diffHunk": "@@ -230,18 +222,14 @@ public void testInvalidRemoveOrphanFilesCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.remove_orphan_files('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.remove_orphan_files()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.remove_orphan_files('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.remove_orphan_files('', 't')\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.remove_orphan_files('n', '')\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\nindex cf75b631a..c1a4ec5d7 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n\n@@ -228,7 +228,7 @@ public class TestRemoveOrphanFilesProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.remove_orphan_files('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.remove_orphan_files('')\", catalogName));\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODY0NQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078645", "bodyText": "nit: with?", "author": "aokolnychyi", "createdAt": "2020-12-09T07:47:55Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java", "diffHunk": "@@ -162,22 +157,18 @@ public void testInvalidRewriteManifestsCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rewrite_manifests('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.rewrite_manifests()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.rewrite_manifests('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.rewrite_manifests('', 't')\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.rewrite_manifests('n', '')\", catalogName));\n-\n     AssertHelpers.assertThrows(\"Should reject duplicate arg names name\",\n         AnalysisException.class, \"Duplicate procedure argument: table\",\n         () -> sql(\"CALL %s.system.rewrite_manifests(table => 't', tAbLe => 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\nindex ac32d56a0..b04f17693 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n\n@@ -167,7 +167,7 @@ public class TestRewriteManifestsProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Duplicate procedure argument: table\",\n         () -> sql(\"CALL %s.system.rewrite_manifests(table => 't', tAbLe => 't')\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.rewrite_manifests('')\", catalogName));\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODc4NQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078785", "bodyText": "nit: with?", "author": "aokolnychyi", "createdAt": "2020-12-09T07:48:12Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java", "diffHunk": "@@ -243,30 +239,22 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot(1L)\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot(namespace => 'n', snapshot_id => 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n-        AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot(table => 't', snapshot_id => 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot(table => 't')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', 't', 2.2)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('', 't', 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot('t', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', '', 1L)\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 41f70862e..d3e6bdcbc 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n\n@@ -253,7 +253,7 @@ public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n         () -> sql(\"CALL %s.system.rollback_to_snapshot('t', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.rollback_to_snapshot('', 1L)\", catalogName));\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEyMjcxNw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539122717", "bodyText": "nit: can fit on one line now", "author": "aokolnychyi", "createdAt": "2020-12-09T08:57:33Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +48,17 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(Identifier ident, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(ident, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, false, func);\n+  protected <T> T withIcebergTable(Identifier ident, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(ident, false, func);\n   }\n \n-  private <T> T execute(String namespace, String tableName, boolean refreshSparkCache,\n+  private <T> T execute(Identifier ident, boolean refreshSparkCache,", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 08639c7d8..68239504c 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -56,9 +56,7 @@ abstract class BaseProcedure implements Procedure {\n     return execute(ident, false, func);\n   }\n \n-  private <T> T execute(Identifier ident, boolean refreshSparkCache,\n-                        Function<org.apache.iceberg.Table, T> func) {\n-\n+  private <T> T execute(Identifier ident, boolean refreshSparkCache, Function<org.apache.iceberg.Table, T> func) {\n     SparkTable sparkTable = loadSparkTable(ident);\n     org.apache.iceberg.Table icebergTable = sparkTable.table();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE0NTg4Nw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539145887", "bodyText": "I like Ryan's suggestion in another PR to add a variant of catalogAndIdentifier that does not throw a checked parse exception. Explicit handling of parse exceptions makes other places more complicated.\nIf we follow that idea and add this method to Spark3Util:\n\n  public static CatalogAndIdentifier catalogAndIdentifier(String description, SparkSession spark,\n                                                          String name, CatalogPlugin defaultCatalog) {\n    try {\n      return catalogAndIdentifier(spark, name, defaultCatalog);\n    } catch (ParseException e) {\n      throw new IllegalArgumentException(\"Cannot parse \" + description + \": \" + name, e);\n    }\n  }\n\nThis place can look like this:\n  protected Identifier toIdentifier(String identifierAsString, String argName) {\n    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n        \"Cannot handle an empty identifier for argument %s\", argName);\n\n    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n\n    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n    Identifier identifier = catalogAndIdentifier.identifier();\n\n    Preconditions.checkArgument(\n        catalog.equals(tableCatalog),\n        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n        tableCatalog.name(), identifierAsString, catalog);\n\n    return identifier;\n  }", "author": "aokolnychyi", "createdAt": "2020-12-09T09:29:21Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,23 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+  protected Identifier toIdentifier(String identifierAsString, String argName) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 08639c7d8..68239504c 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -75,19 +73,18 @@ abstract class BaseProcedure implements Procedure {\n     Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n         \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    CatalogAndIdentifier catalogAndIdentifier;\n-    try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n-    } catch (ParseException e) {\n-      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",\n-          identifierAsString, argName), e);\n-    }\n+    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n+        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n+\n+    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n+    Identifier identifier = catalogAndIdentifier.identifier();\n \n-    Preconditions.checkArgument(catalogAndIdentifier.catalog().equals(tableCatalog), \"Cannot run procedure\" +\n-        \" in catalog '%s': Argument %s was set to '%s' which resolves to a table in a different catalog '%s'\",\n-        tableCatalog.name(), argName, identifierAsString, catalogAndIdentifier.catalog().name());\n+    Preconditions.checkArgument(\n+        catalog.equals(tableCatalog),\n+        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n+        tableCatalog.name(), identifierAsString, catalog);\n \n-    return catalogAndIdentifier.identifier();\n+    return identifier;\n   }\n \n   protected SparkTable loadSparkTable(Identifier ident) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE1Njc3Nw==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539156777", "bodyText": "extra space", "author": "aokolnychyi", "createdAt": "2020-12-09T09:43:57Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "diffHunk": "@@ -184,8 +184,8 @@ public void testRollbackToTimestampWithQuotedIdentifiers() {\n     String quotedNamespace = quotedNamespaceBuilder.toString();\n \n     List<Object[]> output = sql(\n-        \"CALL %s.system.rollback_to_timestamp('%s', '`%s`', TIMESTAMP '%s')\",\n-        catalogName, quotedNamespace, tableIdent.name(), firstSnapshotTimestamp);\n+        \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n+         catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\nindex 65ee27a0f..52fc12c7d 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n\n@@ -185,7 +185,7 @@ public class TestRollbackToTimestampProcedure extends SparkExtensionsTestBase {\n \n     List<Object[]> output = sql(\n         \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n-         catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);\n+        catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);\n \n     assertEquals(\"Procedure output must match\",\n         ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE1NzMyOQ==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539157329", "bodyText": "nit: with?", "author": "aokolnychyi", "createdAt": "2020-12-09T09:44:41Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java", "diffHunk": "@@ -196,30 +196,26 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot(1L)\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot(namespace => 'n', snapshot_id => 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot(snapshot_id => 1L)\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot(table => 't', snapshot_id => 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot(table => 't')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', 't', 2.2)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot('t', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('', 't', 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', '', 1L)\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "originalCommit": "094987f1c263aa1bdeaca422260a2f9fc41cea7b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e44eaa4880fb045a1f185528aa930da12679127e", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\nindex aecf286db..0ea8c4861 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n\n@@ -214,7 +214,7 @@ public class TestSetCurrentSnapshotProcedure extends SparkExtensionsTestBase {\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n         () -> sql(\"CALL %s.system.set_current_snapshot('t', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n         IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n         () -> sql(\"CALL %s.system.set_current_snapshot('', 1L)\", catalogName));\n   }\n"}}, {"oid": "e44eaa4880fb045a1f185528aa930da12679127e", "url": "https://github.com/apache/iceberg/commit/e44eaa4880fb045a1f185528aa930da12679127e", "message": "Minor review comments", "committedDate": "2020-12-09T09:51:43Z", "type": "commit"}, {"oid": "2343e9569a98cb7ea019ce5a7d23249f78920f8b", "url": "https://github.com/apache/iceberg/commit/2343e9569a98cb7ea019ce5a7d23249f78920f8b", "message": "Merge pull request #1 from aokolnychyi/minor-changes\n\nMinor review comments", "committedDate": "2020-12-09T13:40:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTMxODE1Ng==", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539318156", "bodyText": "I missed to call catalog.name() on the third arg.", "author": "aokolnychyi", "createdAt": "2020-12-09T13:46:15Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,19 +73,18 @@ protected Identifier toIdentifier(String identifierAsString, String argName) {\n     Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n         \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    CatalogAndIdentifier catalogAndIdentifier;\n-    try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n-    } catch (ParseException e) {\n-      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",\n-          identifierAsString, argName), e);\n-    }\n+    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n+        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n+\n+    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n+    Identifier identifier = catalogAndIdentifier.identifier();\n \n-    Preconditions.checkArgument(catalogAndIdentifier.catalog().equals(tableCatalog), \"Cannot run procedure\" +\n-        \" in catalog '%s': Argument %s was set to '%s' which resolves to a table in a different catalog '%s'\",\n-        tableCatalog.name(), argName, identifierAsString, catalogAndIdentifier.catalog().name());\n+    Preconditions.checkArgument(\n+        catalog.equals(tableCatalog),\n+        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n+        tableCatalog.name(), identifierAsString, catalog);", "originalCommit": "2343e9569a98cb7ea019ce5a7d23249f78920f8b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3e9ecf828cc1ab329d73cc387b5eb79c6239e58e", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex 68239504c..ce5c16aac 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -81,8 +81,8 @@ abstract class BaseProcedure implements Procedure {\n \n     Preconditions.checkArgument(\n         catalog.equals(tableCatalog),\n-        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n-        tableCatalog.name(), identifierAsString, catalog);\n+        \"Cannot run procedure in catalog '%s': %s is a table in catalog '%s'\",\n+        tableCatalog.name(), identifierAsString, catalog.name());\n \n     return identifier;\n   }\n"}}, {"oid": "3e9ecf828cc1ab329d73cc387b5eb79c6239e58e", "url": "https://github.com/apache/iceberg/commit/3e9ecf828cc1ab329d73cc387b5eb79c6239e58e", "message": "Missed catalog.name and add back quotes", "committedDate": "2020-12-09T14:07:36Z", "type": "forcePushed"}, {"oid": "31fd07aa3915f7dfa345cb0f761dc59785e51881", "url": "https://github.com/apache/iceberg/commit/31fd07aa3915f7dfa345cb0f761dc59785e51881", "message": "Missed catalog.name and add back quotes", "committedDate": "2020-12-09T14:08:32Z", "type": "commit"}, {"oid": "31fd07aa3915f7dfa345cb0f761dc59785e51881", "url": "https://github.com/apache/iceberg/commit/31fd07aa3915f7dfa345cb0f761dc59785e51881", "message": "Missed catalog.name and add back quotes", "committedDate": "2020-12-09T14:08:32Z", "type": "forcePushed"}]}