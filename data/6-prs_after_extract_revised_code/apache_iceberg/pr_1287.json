{"pr_number": 1287, "pr_title": "Vectorized Reads of Parquet with Identity Partitions", "pr_createdAt": "2020-08-03T16:32:22Z", "pr_url": "https://github.com/apache/iceberg/pull/1287", "timeline": [{"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c", "url": "https://github.com/apache/iceberg/commit/71fe66885ce7099230441d4312e2c1e672c61d1c", "message": "Vectorized Reads of Parquet with Identity Partitions\n\nPreviously vectorization would be disabled whenever an underlying iceberg table\nwas using Parquet files and also used Identity transforms in it's partitioning.\nTo fix this we extend the DummyVectorReader to be a ConstantVectorReader which is\nused when a column's value can be determined from the PartitionSpec. Then when\nconstructing the reader we use a ConstantColumnVector to fill in the missing\ncolumn.", "committedDate": "2020-07-31T15:13:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2Nzc5NA==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464567794", "bodyText": "Could we rename this to ConstantVectorHolder instead of \"dummy\"? Now that it returns a constant value, I think it's more accurate to use \"constant\".", "author": "rdblue", "createdAt": "2020-08-03T17:46:24Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java", "diffHunk": "@@ -91,17 +91,44 @@ public int numValues() {\n     return vector.getValueCount();\n   }\n \n+  public static VectorHolder constantHolder(int numRows, Object constantValue) {\n+    return new DummyVectorHolder(numRows, constantValue);\n+  }\n+\n   public static VectorHolder dummyHolder(int numRows) {\n-    return new VectorHolder() {\n-      @Override\n-      public int numValues() {\n-        return numRows;\n-      }\n-    };\n+    return new DummyVectorHolder(numRows);\n   }\n \n   public boolean isDummy() {\n     return vector == null;\n   }\n \n+  /**\n+   * A Vector Holder which does not actually produce values, consumers of this class should\n+   * use the constantValue to populate their ColumnVector implementation.\n+   */\n+  public static class DummyVectorHolder extends VectorHolder {", "originalCommit": "71fe66885ce7099230441d4312e2c1e672c61d1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDU1OQ==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464570559", "bodyText": "Makes sense to me", "author": "RussellSpitzer", "createdAt": "2020-08-03T17:51:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2Nzc5NA=="}], "type": "inlineReview", "revised_code": {"commit": "8bd9c5ee77a712ba5eb9125d31c0c1de6220b54f", "chunk": "diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java\nindex 99ac81066..14062ff65 100644\n--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java\n+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java\n\n@@ -91,12 +91,12 @@ public class VectorHolder {\n     return vector.getValueCount();\n   }\n \n-  public static VectorHolder constantHolder(int numRows, Object constantValue) {\n-    return new DummyVectorHolder(numRows, constantValue);\n+  public static <T> VectorHolder constantHolder(int numRows, T constantValue) {\n+    return new ConstantVectorHolder(numRows, constantValue);\n   }\n \n   public static VectorHolder dummyHolder(int numRows) {\n-    return new DummyVectorHolder(numRows);\n+    return new ConstantVectorHolder(numRows);\n   }\n \n   public boolean isDummy() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2ODg4Mw==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464568883", "bodyText": "Nit: Looks like this Javadoc is missing the normal *  at the start of these lines?", "author": "rdblue", "createdAt": "2020-08-03T17:48:35Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.", "originalCommit": "71fe66885ce7099230441d4312e2c1e672c61d1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzkwNQ==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464683905", "bodyText": "I was just leaving a multiline comment, but I'll make it a javadoc", "author": "RussellSpitzer", "createdAt": "2020-08-03T21:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2ODg4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "8bd9c5ee77a712ba5eb9125d31c0c1de6220b54f", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\nindex aa1824a5a..755e4795d 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n\n@@ -113,9 +113,9 @@ public abstract class TestIdentityPartitionData  {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  /*\n-  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n-  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+  /**\n+   * Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+   * parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n    */\n   private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464569489", "bodyText": "Isn't this the default? Why was it necessary to add select?", "author": "rdblue", "createdAt": "2020-08-03T17:49:41Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "originalCommit": "71fe66885ce7099230441d4312e2c1e672c61d1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDE4Ng==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464570186", "bodyText": "When I added in the Hive Import it gets the schema in a different order, I think this may be an issue with the import code? I'm not sure, but I know the default column order does not come out the same way :/", "author": "RussellSpitzer", "createdAt": "2020-08-03T17:51:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3OTc1OQ==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464579759", "bodyText": "That's suspicious. We'll have to look into why the schema has the wrong order. I see select before all the writes, so it shouldn't need the reorder here.", "author": "rdblue", "createdAt": "2020-08-03T18:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5MDU5NA==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464590594", "bodyText": "I'll try to figure out the actual issue today, but I agree it shouldn't work this way. My assumption is that the Hive table schema is just being listed in a different order or when we use SparkSchemaUtil the order is getting scrambled.", "author": "RussellSpitzer", "createdAt": "2020-08-03T18:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzE5NA==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464683194", "bodyText": "I spent some time digging into this,\nWhen you call saveAsTable it ends up in this bit of code in DataFrameWriter\n    val tableDesc = CatalogTable(\n      identifier = tableIdent,\n      tableType = tableType,\n      storage = storage,\n      schema = new StructType,\n      provider = Some(source),\n      partitionColumnNames = partitioningColumns.getOrElse(Nil),\n      bucketSpec = getBucketSpec)\nWhich strips out whatever incoming schema you have. So the new table is created without any information about the actual ordering of columns you used in the create.\nThen when the Relation is resolved, that's when the attributes are looked up again and the schema is created from the Attribute output. So long story short, saveAsTable doesn't care about your field ordering as far as I can tell. This is all in Spark and I'm not sure we can do anything about it here.", "author": "RussellSpitzer", "createdAt": "2020-08-03T21:56:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMzEzMg==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r465413132", "bodyText": "I'm fine with this, then. Thanks for looking into it!", "author": "rdblue", "createdAt": "2020-08-05T01:06:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "8bd9c5ee77a712ba5eb9125d31c0c1de6220b54f", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\nindex aa1824a5a..755e4795d 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n\n@@ -113,9 +113,9 @@ public abstract class TestIdentityPartitionData  {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  /*\n-  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n-  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+  /**\n+   * Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+   * parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n    */\n   private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5OTI4OQ==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464599289", "bodyText": "VectorHolder.constantHolder and DummyVectorHolder aren't parameterized. It would make sense to have them accept value of type T as well.", "author": "samarthjain", "createdAt": "2020-08-03T18:50:10Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -356,5 +356,36 @@ public String toString() {\n     public void setBatchSize(int batchSize) {}\n   }\n \n+  /**\n+   * A Dummy Vector Reader which doesn't actually read files, instead it returns a dummy\n+   * VectorHolder which indicates the constant value which should be used for this column.\n+   * @param <T> The constant value to use\n+   */\n+  public static class ConstantVectorReader<T> extends VectorizedArrowReader {\n+    private final T value;\n+\n+    public ConstantVectorReader(T value) {\n+      this.value = value;\n+    }\n+\n+    @Override\n+    public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n+      return VectorHolder.constantHolder(numValsToRead, value);", "originalCommit": "71fe66885ce7099230441d4312e2c1e672c61d1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYwNzU0OQ==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464607549", "bodyText": "Of course, I was thinking about that but I got a little worried about the \"null\" version since I felt a little weird about creating a parameterized type with null. I forgot how Java handles that but I'll double check and change the class if possible. IE what happens with\nfoo<t> ( T bar) when you invoke foo(null)", "author": "RussellSpitzer", "createdAt": "2020-08-03T19:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5OTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY0Mzk5Mw==", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464643993", "bodyText": "If it is always null, you can use the Void type.", "author": "rdblue", "createdAt": "2020-08-03T20:27:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5OTI4OQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "8bd9c5ee77a712ba5eb9125d31c0c1de6220b54f", "url": "https://github.com/apache/iceberg/commit/8bd9c5ee77a712ba5eb9125d31c0c1de6220b54f", "message": "Reviewer Comments\n\nRenamed Classes\nParameterized Others", "committedDate": "2020-08-03T22:06:19Z", "type": "commit"}]}