{"pr_number": 1129, "pr_title": "Turn on name mapping for avro", "pr_createdAt": "2020-06-19T13:37:48Z", "pr_url": "https://github.com/apache/iceberg/pull/1129", "timeline": [{"oid": "ddf536dbfb75098000b9866b96edfed0202816f6", "url": "https://github.com/apache/iceberg/commit/ddf536dbfb75098000b9866b96edfed0202816f6", "message": "Add a unit test", "committedDate": "2020-06-22T07:40:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NDQzMw==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443694433", "bodyText": "FYI, this isn't necessary because the files added to a table don't have to match the format.", "author": "rdblue", "createdAt": "2020-06-22T16:47:40Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .set(DEFAULT_FILE_FORMAT, \"avro\")", "originalCommit": "ddf536dbfb75098000b9866b96edfed0202816f6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2456a86fd382fce43891a88887cfa1b777c69a99", "chunk": "diff --git a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\nindex 95636750f..37f57d483 100644\n--- a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n+++ b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n\n@@ -309,69 +298,4 @@ public class TestSparkTableUtil extends HiveTableBaseTest {\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n-\n-  @Test\n-  public void testAvroReaderWithNameMapping() throws IOException {\n-    File avroFile = temp.newFile();\n-    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n-        .namespace(\"org.apache.iceberg.spark.data\")\n-        .fields()\n-        .requiredInt(\"id\")\n-        .requiredString(\"name\")\n-        .endRecord();\n-\n-    GenericRecord record1 = new GenericData.Record(avroSchema);\n-    record1.put(\"id\", 1);\n-    record1.put(\"name\", \"Bob\");\n-\n-    GenericRecord record2 = new GenericData.Record(avroSchema);\n-    record2.put(\"id\", 2);\n-    record2.put(\"name\", \"Alice\");\n-\n-    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n-    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n-\n-    dataFileWriter.create(avroSchema, avroFile);\n-    dataFileWriter.append(record1);\n-    dataFileWriter.append(record2);\n-    dataFileWriter.close();\n-\n-    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n-        .withFormat(\"avro\")\n-        .withFileSizeInBytes(avroFile.length())\n-        .withPath(avroFile.getAbsolutePath())\n-        .withRecordCount(2)\n-        .build();\n-\n-    Schema filteredSchema = new Schema(\n-        required(1, \"name\", Types.StringType.get())\n-    );\n-\n-    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n-\n-    Table table = catalog.createTable(\n-        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n-        filteredSchema,\n-        PartitionSpec.unpartitioned());\n-\n-    table.updateProperties()\n-        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n-        .set(DEFAULT_FILE_FORMAT, \"avro\")\n-        .commit();\n-\n-    table.newFastAppend().appendFile(avroDataFile).commit();\n-\n-    List<String> actual = spark.read().format(\"iceberg\")\n-        .load(DB_NAME + \".avro_table\")\n-        .select(\"name\")\n-        .filter(\"name='Alice'\")\n-        .collectAsList()\n-        .stream()\n-        .map(r -> r.getString(0))\n-        .collect(Collectors.toList());\n-\n-    List<GenericRecord> expected = Lists.newArrayList(record2);\n-\n-    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NTcxMA==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443695710", "bodyText": "This should not project because ID is being ignored.", "author": "rdblue", "createdAt": "2020-06-22T16:49:57Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .set(DEFAULT_FILE_FORMAT, \"avro\")\n+        .commit();\n+\n+    table.newFastAppend().appendFile(avroDataFile).commit();\n+\n+    List<String> actual = spark.read().format(\"iceberg\")\n+        .load(DB_NAME + \".avro_table\")\n+        .select(\"name\")", "originalCommit": "ddf536dbfb75098000b9866b96edfed0202816f6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2456a86fd382fce43891a88887cfa1b777c69a99", "chunk": "diff --git a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\nindex 95636750f..37f57d483 100644\n--- a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n+++ b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n\n@@ -309,69 +298,4 @@ public class TestSparkTableUtil extends HiveTableBaseTest {\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n-\n-  @Test\n-  public void testAvroReaderWithNameMapping() throws IOException {\n-    File avroFile = temp.newFile();\n-    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n-        .namespace(\"org.apache.iceberg.spark.data\")\n-        .fields()\n-        .requiredInt(\"id\")\n-        .requiredString(\"name\")\n-        .endRecord();\n-\n-    GenericRecord record1 = new GenericData.Record(avroSchema);\n-    record1.put(\"id\", 1);\n-    record1.put(\"name\", \"Bob\");\n-\n-    GenericRecord record2 = new GenericData.Record(avroSchema);\n-    record2.put(\"id\", 2);\n-    record2.put(\"name\", \"Alice\");\n-\n-    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n-    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n-\n-    dataFileWriter.create(avroSchema, avroFile);\n-    dataFileWriter.append(record1);\n-    dataFileWriter.append(record2);\n-    dataFileWriter.close();\n-\n-    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n-        .withFormat(\"avro\")\n-        .withFileSizeInBytes(avroFile.length())\n-        .withPath(avroFile.getAbsolutePath())\n-        .withRecordCount(2)\n-        .build();\n-\n-    Schema filteredSchema = new Schema(\n-        required(1, \"name\", Types.StringType.get())\n-    );\n-\n-    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n-\n-    Table table = catalog.createTable(\n-        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n-        filteredSchema,\n-        PartitionSpec.unpartitioned());\n-\n-    table.updateProperties()\n-        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n-        .set(DEFAULT_FILE_FORMAT, \"avro\")\n-        .commit();\n-\n-    table.newFastAppend().appendFile(avroDataFile).commit();\n-\n-    List<String> actual = spark.read().format(\"iceberg\")\n-        .load(DB_NAME + \".avro_table\")\n-        .select(\"name\")\n-        .filter(\"name='Alice'\")\n-        .collectAsList()\n-        .stream()\n-        .map(r -> r.getString(0))\n-        .collect(Collectors.toList());\n-\n-    List<GenericRecord> expected = Lists.newArrayList(record2);\n-\n-    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NjUxNg==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443696516", "bodyText": "I like that filteredSchema is used to create the mapping. Could you use a full schema for the table (1: name, 2: id) so that we can verify that the ID is projected as null?", "author": "rdblue", "createdAt": "2020-06-22T16:51:16Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);", "originalCommit": "ddf536dbfb75098000b9866b96edfed0202816f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTI1MQ==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443989251", "bodyText": "We can not use a full schema for the table because it cannot build Avro read schema (with name and id column) by using pruned schema (with name column) in ProjectionDatumReader#setSchema.\nAlso, setSchema will ignore the name mapping if the Avro file schema has Ids. So we might need to utilize the RemoveIds from the Avro test via moving it to source dir and changing it to the public class. What do you think about this?", "author": "chenjunjiedada", "createdAt": "2020-06-23T06:27:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NjUxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3Mzk1MA==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r444373950", "bodyText": "Can you clarify what you mean by \"it cannot build Avro read schema by using pruned schema\"? We should be able to use any table schema here, and without a mapping for id, it should be ignored.\nWe added special handling for this case where the table schema and the file schema use the same name, but the column should not be projected because this ID is missing.\n\nsetSchema will ignore the name mapping if the Avro file schema has Ids\n\nIsn't this why the Avro file schema is created without IDs and no Iceberg classes are used for the write? I don't see why we would have IDs in the Avro file.", "author": "rdblue", "createdAt": "2020-06-23T17:01:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NjUxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUzMzQ3Nw==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r444533477", "bodyText": "This is the stack trace that when using a full schema for the table:\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.IllegalArgumentException: Missing required field: id\nat org.apache.iceberg.relocated.com.google.common.base.Preconditions.checkArgument(Preconditions.java:217)\nat org.apache.iceberg.avro.BuildAvroProjection.record(BuildAvroProjection.java:96)\nat org.apache.iceberg.avro.BuildAvroProjection.record(BuildAvroProjection.java:41)\nat org.apache.iceberg.avro.AvroCustomOrderSchemaVisitor.visit(AvroCustomOrderSchemaVisitor.java:51)\nat org.apache.iceberg.avro.AvroSchemaUtil.buildAvroProjection(AvroSchemaUtil.java:104)\nat org.apache.iceberg.avro.ProjectionDatumReader.setSchema(ProjectionDatumReader.java:60)\nat org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:132)\nat org.apache.avro.file.DataFileReader.(DataFileReader.java:106)\nat org.apache.avro.file.DataFileReader.(DataFileReader.java:98)\nat org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:66)                                                                                                                                          at org.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:94)\nat org.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:77)\nat org.apache.iceberg.spark.source.RowDataReader.open(RowDataReader.java:106)                                                                      at org.apache.iceberg.spark.source.BaseDataReader.next(BaseDataReader.java:73)\nat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:49)\nat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\nat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\nat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n...\nSo that I didn't use the full schema and assert the id column.\n\nIsn't this why the Avro file schema is created without IDs and no Iceberg classes are used for the write? I don't see why we would have IDs in the Avro file.\n\nYes, agree to create Avro file without IDs. Here I just want to use RemoveIds class from Iceberg.", "author": "chenjunjiedada", "createdAt": "2020-06-23T22:01:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NjUxNg=="}], "type": "inlineReview", "revised_code": {"commit": "2456a86fd382fce43891a88887cfa1b777c69a99", "chunk": "diff --git a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\nindex 95636750f..37f57d483 100644\n--- a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n+++ b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n\n@@ -309,69 +298,4 @@ public class TestSparkTableUtil extends HiveTableBaseTest {\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n-\n-  @Test\n-  public void testAvroReaderWithNameMapping() throws IOException {\n-    File avroFile = temp.newFile();\n-    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n-        .namespace(\"org.apache.iceberg.spark.data\")\n-        .fields()\n-        .requiredInt(\"id\")\n-        .requiredString(\"name\")\n-        .endRecord();\n-\n-    GenericRecord record1 = new GenericData.Record(avroSchema);\n-    record1.put(\"id\", 1);\n-    record1.put(\"name\", \"Bob\");\n-\n-    GenericRecord record2 = new GenericData.Record(avroSchema);\n-    record2.put(\"id\", 2);\n-    record2.put(\"name\", \"Alice\");\n-\n-    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n-    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n-\n-    dataFileWriter.create(avroSchema, avroFile);\n-    dataFileWriter.append(record1);\n-    dataFileWriter.append(record2);\n-    dataFileWriter.close();\n-\n-    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n-        .withFormat(\"avro\")\n-        .withFileSizeInBytes(avroFile.length())\n-        .withPath(avroFile.getAbsolutePath())\n-        .withRecordCount(2)\n-        .build();\n-\n-    Schema filteredSchema = new Schema(\n-        required(1, \"name\", Types.StringType.get())\n-    );\n-\n-    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n-\n-    Table table = catalog.createTable(\n-        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n-        filteredSchema,\n-        PartitionSpec.unpartitioned());\n-\n-    table.updateProperties()\n-        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n-        .set(DEFAULT_FILE_FORMAT, \"avro\")\n-        .commit();\n-\n-    table.newFastAppend().appendFile(avroDataFile).commit();\n-\n-    List<String> actual = spark.read().format(\"iceberg\")\n-        .load(DB_NAME + \".avro_table\")\n-        .select(\"name\")\n-        .filter(\"name='Alice'\")\n-        .collectAsList()\n-        .stream()\n-        .map(r -> r.getString(0))\n-        .collect(Collectors.toList());\n-\n-    List<GenericRecord> expected = Lists.newArrayList(record2);\n-\n-    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5NzcwNA==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443697704", "bodyText": "I think this validation looks strange. There's no need to create a list with just record2, and no need to transform that list. It is easier to read and maintain tests with simple assertions, like assertEquals(\"Should project 1 record\", 1, actual.size());. And each field should have its own assertion since you're hard-coding the filter for Alice and we know that id will be null.", "author": "rdblue", "createdAt": "2020-06-22T16:53:14Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchema);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchema);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchema, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .set(DEFAULT_FILE_FORMAT, \"avro\")\n+        .commit();\n+\n+    table.newFastAppend().appendFile(avroDataFile).commit();\n+\n+    List<String> actual = spark.read().format(\"iceberg\")\n+        .load(DB_NAME + \".avro_table\")\n+        .select(\"name\")\n+        .filter(\"name='Alice'\")\n+        .collectAsList()\n+        .stream()\n+        .map(r -> r.getString(0))\n+        .collect(Collectors.toList());\n+\n+    List<GenericRecord> expected = Lists.newArrayList(record2);\n+\n+    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);", "originalCommit": "ddf536dbfb75098000b9866b96edfed0202816f6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2456a86fd382fce43891a88887cfa1b777c69a99", "chunk": "diff --git a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\nindex 95636750f..37f57d483 100644\n--- a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n+++ b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n\n@@ -309,69 +298,4 @@ public class TestSparkTableUtil extends HiveTableBaseTest {\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n-\n-  @Test\n-  public void testAvroReaderWithNameMapping() throws IOException {\n-    File avroFile = temp.newFile();\n-    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n-        .namespace(\"org.apache.iceberg.spark.data\")\n-        .fields()\n-        .requiredInt(\"id\")\n-        .requiredString(\"name\")\n-        .endRecord();\n-\n-    GenericRecord record1 = new GenericData.Record(avroSchema);\n-    record1.put(\"id\", 1);\n-    record1.put(\"name\", \"Bob\");\n-\n-    GenericRecord record2 = new GenericData.Record(avroSchema);\n-    record2.put(\"id\", 2);\n-    record2.put(\"name\", \"Alice\");\n-\n-    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n-    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n-\n-    dataFileWriter.create(avroSchema, avroFile);\n-    dataFileWriter.append(record1);\n-    dataFileWriter.append(record2);\n-    dataFileWriter.close();\n-\n-    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n-        .withFormat(\"avro\")\n-        .withFileSizeInBytes(avroFile.length())\n-        .withPath(avroFile.getAbsolutePath())\n-        .withRecordCount(2)\n-        .build();\n-\n-    Schema filteredSchema = new Schema(\n-        required(1, \"name\", Types.StringType.get())\n-    );\n-\n-    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n-\n-    Table table = catalog.createTable(\n-        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n-        filteredSchema,\n-        PartitionSpec.unpartitioned());\n-\n-    table.updateProperties()\n-        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n-        .set(DEFAULT_FILE_FORMAT, \"avro\")\n-        .commit();\n-\n-    table.newFastAppend().appendFile(avroDataFile).commit();\n-\n-    List<String> actual = spark.read().format(\"iceberg\")\n-        .load(DB_NAME + \".avro_table\")\n-        .select(\"name\")\n-        .filter(\"name='Alice'\")\n-        .collectAsList()\n-        .stream()\n-        .map(r -> r.getString(0))\n-        .collect(Collectors.toList());\n-\n-    List<GenericRecord> expected = Lists.newArrayList(record2);\n-\n-    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5ODIwOA==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443698208", "bodyText": "Why was this added to SparkTableUtil? It doesn't use SparkTableUtil, so it is not related.\nCould you start a test suite, TestNameMappingProjection?", "author": "rdblue", "createdAt": "2020-06-22T16:54:08Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -298,4 +309,69 @@ public void testImportWithNameMappingForVectorizedParquetReader() throws Excepti\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {", "originalCommit": "ddf536dbfb75098000b9866b96edfed0202816f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTIzOA==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r443989238", "bodyText": "will do.", "author": "chenjunjiedada", "createdAt": "2020-06-23T06:27:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY5ODIwOA=="}], "type": "inlineReview", "revised_code": {"commit": "2456a86fd382fce43891a88887cfa1b777c69a99", "chunk": "diff --git a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\nindex 95636750f..37f57d483 100644\n--- a/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n+++ b/spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java\n\n@@ -309,69 +298,4 @@ public class TestSparkTableUtil extends HiveTableBaseTest {\n \n     Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n   }\n-\n-  @Test\n-  public void testAvroReaderWithNameMapping() throws IOException {\n-    File avroFile = temp.newFile();\n-    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n-        .namespace(\"org.apache.iceberg.spark.data\")\n-        .fields()\n-        .requiredInt(\"id\")\n-        .requiredString(\"name\")\n-        .endRecord();\n-\n-    GenericRecord record1 = new GenericData.Record(avroSchema);\n-    record1.put(\"id\", 1);\n-    record1.put(\"name\", \"Bob\");\n-\n-    GenericRecord record2 = new GenericData.Record(avroSchema);\n-    record2.put(\"id\", 2);\n-    record2.put(\"name\", \"Alice\");\n-\n-    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);\n-    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n-\n-    dataFileWriter.create(avroSchema, avroFile);\n-    dataFileWriter.append(record1);\n-    dataFileWriter.append(record2);\n-    dataFileWriter.close();\n-\n-    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n-        .withFormat(\"avro\")\n-        .withFileSizeInBytes(avroFile.length())\n-        .withPath(avroFile.getAbsolutePath())\n-        .withRecordCount(2)\n-        .build();\n-\n-    Schema filteredSchema = new Schema(\n-        required(1, \"name\", Types.StringType.get())\n-    );\n-\n-    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n-\n-    Table table = catalog.createTable(\n-        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n-        filteredSchema,\n-        PartitionSpec.unpartitioned());\n-\n-    table.updateProperties()\n-        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n-        .set(DEFAULT_FILE_FORMAT, \"avro\")\n-        .commit();\n-\n-    table.newFastAppend().appendFile(avroDataFile).commit();\n-\n-    List<String> actual = spark.read().format(\"iceberg\")\n-        .load(DB_NAME + \".avro_table\")\n-        .select(\"name\")\n-        .filter(\"name='Alice'\")\n-        .collectAsList()\n-        .stream()\n-        .map(r -> r.getString(0))\n-        .collect(Collectors.toList());\n-\n-    List<GenericRecord> expected = Lists.newArrayList(record2);\n-\n-    Assert.assertEquals(expected.stream().map(r -> r.get(\"name\")).collect(Collectors.toList()), actual);\n-  }\n }\n"}}, {"oid": "2456a86fd382fce43891a88887cfa1b777c69a99", "url": "https://github.com/apache/iceberg/commit/2456a86fd382fce43891a88887cfa1b777c69a99", "message": "Turn on name mapping for avro", "committedDate": "2020-06-23T07:32:03Z", "type": "commit"}, {"oid": "5fe0e6f1971eb2f3d0e6d030847a8865d8461201", "url": "https://github.com/apache/iceberg/commit/5fe0e6f1971eb2f3d0e6d030847a8865d8461201", "message": "Add a unit test", "committedDate": "2020-06-23T07:32:03Z", "type": "commit"}, {"oid": "8f104a34bd90b42f224cf2f4bb1d8a1127633336", "url": "https://github.com/apache/iceberg/commit/8f104a34bd90b42f224cf2f4bb1d8a1127633336", "message": "address comments", "committedDate": "2020-06-23T08:06:35Z", "type": "forcePushed"}, {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51", "url": "https://github.com/apache/iceberg/commit/e89ec2d544510dd252bbcdda9fa08b78dd517f51", "message": "address comments", "committedDate": "2020-06-23T08:37:25Z", "type": "commit"}, {"oid": "e89ec2d544510dd252bbcdda9fa08b78dd517f51", "url": "https://github.com/apache/iceberg/commit/e89ec2d544510dd252bbcdda9fa08b78dd517f51", "message": "address comments", "committedDate": "2020-06-23T08:37:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODQ2Mg==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r444388462", "bodyText": "This is the line that I would change, so that the table also has an ID column.", "author": "rdblue", "createdAt": "2020-06-23T17:26:27Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestNameMappingProjection.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.RemoveIds;\n+import org.apache.iceberg.hive.HiveTableBaseTest;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestNameMappingProjection extends HiveTableBaseTest {\n+  private static final Configuration CONF = HiveTableBaseTest.hiveConf;\n+  private static SparkSession spark = null;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    String metastoreURI = CONF.get(HiveConf.ConfVars.METASTOREURIS.varname);\n+\n+    // Create a spark session.\n+    TestNameMappingProjection.spark = SparkSession.builder().master(\"local[2]\")\n+        .enableHiveSupport()\n+        .config(\"spark.hadoop.hive.metastore.uris\", metastoreURI)\n+        .config(\"hive.exec.dynamic.partition\", \"true\")\n+        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+        .config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestNameMappingProjection.spark;\n+    // Stop the spark session.\n+    TestNameMappingProjection.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    org.apache.avro.Schema avroSchemaWithoutIds = RemoveIds.removeIds(avroSchema);\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchemaWithoutIds);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchemaWithoutIds);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchemaWithoutIds);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchemaWithoutIds, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,", "originalCommit": "e89ec2d544510dd252bbcdda9fa08b78dd517f51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODcwMQ==", "url": "https://github.com/apache/iceberg/pull/1129#discussion_r444388701", "bodyText": "And we would need a second assertion here that the id value is null.", "author": "rdblue", "createdAt": "2020-06-23T17:26:52Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestNameMappingProjection.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.RemoveIds;\n+import org.apache.iceberg.hive.HiveTableBaseTest;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestNameMappingProjection extends HiveTableBaseTest {\n+  private static final Configuration CONF = HiveTableBaseTest.hiveConf;\n+  private static SparkSession spark = null;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    String metastoreURI = CONF.get(HiveConf.ConfVars.METASTOREURIS.varname);\n+\n+    // Create a spark session.\n+    TestNameMappingProjection.spark = SparkSession.builder().master(\"local[2]\")\n+        .enableHiveSupport()\n+        .config(\"spark.hadoop.hive.metastore.uris\", metastoreURI)\n+        .config(\"hive.exec.dynamic.partition\", \"true\")\n+        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+        .config(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestNameMappingProjection.spark;\n+    // Stop the spark session.\n+    TestNameMappingProjection.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testAvroReaderWithNameMapping() throws IOException {\n+    File avroFile = temp.newFile();\n+    org.apache.avro.Schema avroSchema = SchemaBuilder.record(\"TestRecord\")\n+        .namespace(\"org.apache.iceberg.spark.data\")\n+        .fields()\n+        .requiredInt(\"id\")\n+        .requiredString(\"name\")\n+        .endRecord();\n+\n+    org.apache.avro.Schema avroSchemaWithoutIds = RemoveIds.removeIds(avroSchema);\n+\n+    GenericRecord record1 = new GenericData.Record(avroSchemaWithoutIds);\n+    record1.put(\"id\", 1);\n+    record1.put(\"name\", \"Bob\");\n+\n+    GenericRecord record2 = new GenericData.Record(avroSchemaWithoutIds);\n+    record2.put(\"id\", 2);\n+    record2.put(\"name\", \"Alice\");\n+\n+    DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchemaWithoutIds);\n+    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(datumWriter);\n+\n+    dataFileWriter.create(avroSchemaWithoutIds, avroFile);\n+    dataFileWriter.append(record1);\n+    dataFileWriter.append(record2);\n+    dataFileWriter.close();\n+\n+    DataFile avroDataFile = DataFiles.builder(PartitionSpec.unpartitioned())\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(avroFile.length())\n+        .withPath(avroFile.getAbsolutePath())\n+        .withRecordCount(2)\n+        .build();\n+\n+    Schema filteredSchema = new Schema(\n+        required(1, \"name\", Types.StringType.get())\n+    );\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"avro_table\"),\n+        filteredSchema,\n+        PartitionSpec.unpartitioned());\n+\n+    table.updateProperties()\n+        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n+        .commit();\n+\n+    table.newFastAppend().appendFile(avroDataFile).commit();\n+\n+    List<Row> actual = spark.read().format(\"iceberg\")\n+        .load(DB_NAME + \".avro_table\")\n+        .filter(\"name='Alice'\")\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Should project 1 record\", 1, actual.size());\n+    Assert.assertEquals(\"Should equal to 'Alice'\", \"Alice\", actual.get(0).getString(0));", "originalCommit": "e89ec2d544510dd252bbcdda9fa08b78dd517f51", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}