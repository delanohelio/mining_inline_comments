{"pr_number": 1481, "pr_title": "Core: Implement Catalogs.createTable and Catalogs.dropTable", "pr_createdAt": "2020-09-21T13:59:33Z", "pr_url": "https://github.com/apache/iceberg/pull/1481", "timeline": [{"oid": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "url": "https://github.com/apache/iceberg/commit/c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "message": "Javadoc warning fixed", "committedDate": "2020-09-23T07:12:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NTY3Mg==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493965672", "bodyText": "Because this was protected, I think we should keep it and just delegate to where the method moved. We can also deprecated it and set a version when it will be removed:\n  /**\n   * ...\n   * @deprecated will be removed in 0.11.0; use CatalogUtil.dropTableData instead.\n   */", "author": "rdblue", "createdAt": "2020-09-24T00:07:30Z", "path": "core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java", "diffHunk": "@@ -278,84 +268,6 @@ private Transaction newReplaceTableTransaction(boolean orCreate) {\n     }\n   }\n \n-  /**\n-   * Drops all data and metadata files referenced by TableMetadata.\n-   * <p>\n-   * This should be called by dropTable implementations to clean up table files once the table has been dropped in the\n-   * metastore.\n-   *\n-   * @param io a FileIO to use for deletes\n-   * @param metadata the last valid TableMetadata instance for a dropped table.\n-   */\n-  protected static void dropTableData(FileIO io, TableMetadata metadata) {", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE5NjM4Nw==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494196387", "bodyText": "Good point!\nDone", "author": "pvary", "createdAt": "2020-09-24T10:10:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NTY3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java b/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java\nindex f9a6e70d1..2efcf80d2 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java\n\n@@ -268,6 +278,85 @@ public abstract class BaseMetastoreCatalog implements Catalog {\n     }\n   }\n \n+  /**\n+   * Drops all data and metadata files referenced by TableMetadata.\n+   * <p>\n+   * This should be called by dropTable implementations to clean up table files once the table has been dropped in the\n+   * metastore.\n+   *\n+   * @param io a FileIO to use for deletes\n+   * @param metadata the last valid TableMetadata instance for a dropped table.\n+   */\n+  protected static void dropTableData(FileIO io, TableMetadata metadata) {\n+    // Reads and deletes are done using Tasks.foreach(...).suppressFailureWhenFinished to complete\n+    // as much of the delete work as possible and avoid orphaned data or manifest files.\n+\n+    Set<String> manifestListsToDelete = Sets.newHashSet();\n+    Set<ManifestFile> manifestsToDelete = Sets.newHashSet();\n+    for (Snapshot snapshot : metadata.snapshots()) {\n+      // add all manifests to the delete set because both data and delete files should be removed\n+      Iterables.addAll(manifestsToDelete, snapshot.allManifests());\n+      // add the manifest list to the delete set, if present\n+      if (snapshot.manifestListLocation() != null) {\n+        manifestListsToDelete.add(snapshot.manifestListLocation());\n+      }\n+    }\n+\n+    LOG.info(\"Manifests to delete: {}\", Joiner.on(\", \").join(manifestsToDelete));\n+\n+    // run all of the deletes\n+\n+    deleteFiles(io, manifestsToDelete);\n+\n+    Tasks.foreach(Iterables.transform(manifestsToDelete, ManifestFile::path))\n+        .noRetry().suppressFailureWhenFinished()\n+        .onFailure((manifest, exc) -> LOG.warn(\"Delete failed for manifest: {}\", manifest, exc))\n+        .run(io::deleteFile);\n+\n+    Tasks.foreach(manifestListsToDelete)\n+        .noRetry().suppressFailureWhenFinished()\n+        .onFailure((list, exc) -> LOG.warn(\"Delete failed for manifest list: {}\", list, exc))\n+        .run(io::deleteFile);\n+\n+    Tasks.foreach(metadata.metadataFileLocation())\n+        .noRetry().suppressFailureWhenFinished()\n+        .onFailure((list, exc) -> LOG.warn(\"Delete failed for metadata file: {}\", list, exc))\n+        .run(io::deleteFile);\n+  }\n+\n+  @SuppressWarnings(\"DangerousStringInternUsage\")\n+  private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n+    // keep track of deleted files in a map that can be cleaned up when memory runs low\n+    Map<String, Boolean> deletedFiles = new MapMaker()\n+        .concurrencyLevel(ThreadPools.WORKER_THREAD_POOL_SIZE)\n+        .weakKeys()\n+        .makeMap();\n+\n+    Tasks.foreach(allManifests)\n+        .noRetry().suppressFailureWhenFinished()\n+        .executeWith(ThreadPools.getWorkerPool())\n+        .onFailure((item, exc) -> LOG.warn(\"Failed to get deleted files: this may cause orphaned data files\", exc))\n+        .run(manifest -> {\n+          try (ManifestReader<?> reader = ManifestFiles.open(manifest, io)) {\n+            for (ManifestEntry<?> entry : reader.entries()) {\n+              // intern the file path because the weak key map uses identity (==) instead of equals\n+              String path = entry.file().path().toString().intern();\n+              Boolean alreadyDeleted = deletedFiles.putIfAbsent(path, true);\n+              if (alreadyDeleted == null || !alreadyDeleted) {\n+                try {\n+                  io.deleteFile(path);\n+                } catch (RuntimeException e) {\n+                  // this may happen if the map of deleted files gets cleaned up by gc\n+                  LOG.warn(\"Delete failed for data file: {}\", path, e);\n+                }\n+              }\n+            }\n+          } catch (IOException e) {\n+            throw new RuntimeIOException(e, \"Failed to read manifest file: %s\", manifest.path());\n+          }\n+        });\n+  }\n+\n   protected static String fullTableName(String catalogName, TableIdentifier identifier) {\n     StringBuilder sb = new StringBuilder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NTk5OA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493965998", "bodyText": "Nit: accidentally duplicated line?", "author": "rdblue", "createdAt": "2020-09-24T00:08:46Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -196,9 +197,10 @@ public boolean dropTable(TableIdentifier identifier, boolean purge) {\n \n     try {\n       if (purge && lastMetadata != null) {\n+        // Since the data files and the metadata files may store in different locations,", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE5Njc1MQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494196751", "bodyText": "Removed. Thanks", "author": "pvary", "createdAt": "2020-09-24T10:10:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NTk5OA=="}], "type": "inlineReview", "revised_code": {"commit": "1960cde82595ac31d5c2beee5e806ae6bb9c5eb6", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java\nindex 4941b1c14..e9408d7f2 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java\n\n@@ -197,7 +197,6 @@ public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, Su\n \n     try {\n       if (purge && lastMetadata != null) {\n-        // Since the data files and the metadata files may store in different locations,\n         // Since the data files and the metadata files may store in different locations,\n         // so it has to call dropTableData to force delete the data file.\n         CatalogUtil.dropTableData(ops.io(), lastMetadata);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NjcxOA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493966718", "bodyText": "I think this wasn't implemented before because it is not part of the Tables API, but now that this is the only implementation, maybe we should consider just deprecating the Tables API and making HadoopTables a stand-alone class.", "author": "rdblue", "createdAt": "2020-09-24T00:11:09Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java", "diffHunk": "@@ -144,6 +147,52 @@ public Table create(Schema schema, PartitionSpec spec, SortOrder order,\n     return new BaseTable(ops, location);\n   }\n \n+  /**\n+   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location) {", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE5NzMzMA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494197330", "bodyText": "Maybe this would merit another discussion, and another PR", "author": "pvary", "createdAt": "2020-09-24T10:11:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NjcxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ0MzA4OQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494443089", "bodyText": "Agreed.", "author": "rdblue", "createdAt": "2020-09-24T16:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NjcxOA=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\nindex e2b32f596..8bfafb40f 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n\n@@ -147,26 +147,10 @@ public class HadoopTables implements Tables, Configurable {\n     return new BaseTable(ops, location);\n   }\n \n-  /**\n-   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location) {\n     return dropTable(location, true);\n   }\n \n-  /**\n-   * Drop a table; optionally delete data and metadata files.\n-   * <p>\n-   * If purge is set to true the implementation should delete all data and metadata files.\n-   * Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @param purge if true, delete all data and metadata files in the table\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location, boolean purge) {\n     // Just for checking if the table exists or not\n     load(location);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2Njg2MA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493966860", "bodyText": "I think this should be a @throws instead of text.", "author": "rdblue", "createdAt": "2020-09-24T00:11:33Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java", "diffHunk": "@@ -144,6 +147,52 @@ public Table create(Schema schema, PartitionSpec spec, SortOrder order,\n     return new BaseTable(ops, location);\n   }\n \n+  /**\n+   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location) {\n+    return dropTable(location, true);\n+  }\n+\n+  /**\n+   * Drop a table; optionally delete data and metadata files.\n+   * <p>\n+   * If purge is set to true the implementation should delete all data and metadata files.\n+   * Throws NoSuchTableException if the table does not exists.", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE5Nzk1MQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494197951", "bodyText": "Done", "author": "pvary", "createdAt": "2020-09-24T10:12:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2Njg2MA=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\nindex e2b32f596..8bfafb40f 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n\n@@ -147,26 +147,10 @@ public class HadoopTables implements Tables, Configurable {\n     return new BaseTable(ops, location);\n   }\n \n-  /**\n-   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location) {\n     return dropTable(location, true);\n   }\n \n-  /**\n-   * Drop a table; optionally delete data and metadata files.\n-   * <p>\n-   * If purge is set to true the implementation should delete all data and metadata files.\n-   * Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @param purge if true, delete all data and metadata files in the table\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location, boolean purge) {\n     // Just for checking if the table exists or not\n     load(location);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NzUyNg==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493967526", "bodyText": "The TableOperations created just below can tell you the same information. I think this should use that instead to avoid creating two.", "author": "rdblue", "createdAt": "2020-09-24T00:13:58Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java", "diffHunk": "@@ -144,6 +147,52 @@ public Table create(Schema schema, PartitionSpec spec, SortOrder order,\n     return new BaseTable(ops, location);\n   }\n \n+  /**\n+   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location) {\n+    return dropTable(location, true);\n+  }\n+\n+  /**\n+   * Drop a table; optionally delete data and metadata files.\n+   * <p>\n+   * If purge is set to true the implementation should delete all data and metadata files.\n+   * Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @param purge if true, delete all data and metadata files in the table\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location, boolean purge) {\n+    // Just for checking if the table exists or not\n+    load(location);", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDIwNjY3OA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494206678", "bodyText": "Thanks for the idea!\nDone", "author": "pvary", "createdAt": "2020-09-24T10:28:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NzUyNg=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\nindex e2b32f596..8bfafb40f 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n\n@@ -147,26 +147,10 @@ public class HadoopTables implements Tables, Configurable {\n     return new BaseTable(ops, location);\n   }\n \n-  /**\n-   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location) {\n     return dropTable(location, true);\n   }\n \n-  /**\n-   * Drop a table; optionally delete data and metadata files.\n-   * <p>\n-   * If purge is set to true the implementation should delete all data and metadata files.\n-   * Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @param purge if true, delete all data and metadata files in the table\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location, boolean purge) {\n     // Just for checking if the table exists or not\n     load(location);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NzY0NQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493967645", "bodyText": "Since this is a new method, you can use UncheckedIOException directly.", "author": "rdblue", "createdAt": "2020-09-24T00:14:23Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java", "diffHunk": "@@ -144,6 +147,52 @@ public Table create(Schema schema, PartitionSpec spec, SortOrder order,\n     return new BaseTable(ops, location);\n   }\n \n+  /**\n+   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location) {\n+    return dropTable(location, true);\n+  }\n+\n+  /**\n+   * Drop a table; optionally delete data and metadata files.\n+   * <p>\n+   * If purge is set to true the implementation should delete all data and metadata files.\n+   * Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @param purge if true, delete all data and metadata files in the table\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location, boolean purge) {\n+    // Just for checking if the table exists or not\n+    load(location);\n+\n+    TableOperations ops = newTableOps(location);\n+    TableMetadata lastMetadata;\n+    if (purge && ops.current() != null) {\n+      lastMetadata = ops.current();\n+    } else {\n+      lastMetadata = null;\n+    }\n+\n+    try {\n+      if (purge && lastMetadata != null) {\n+        // Since the data files and the metadata files may store in different locations,\n+        // so it has to call dropTableData to force delete the data file.\n+        CatalogUtil.dropTableData(ops.io(), lastMetadata);\n+      }\n+      Path tablePath = new Path(location);\n+      Util.getFs(tablePath, conf).delete(tablePath, true /* recursive */);\n+      return true;\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to delete file: %s\", location);", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDIwNjc2MQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494206761", "bodyText": "Done", "author": "pvary", "createdAt": "2020-09-24T10:28:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2NzY0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\nindex e2b32f596..8bfafb40f 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n\n@@ -147,26 +147,10 @@ public class HadoopTables implements Tables, Configurable {\n     return new BaseTable(ops, location);\n   }\n \n-  /**\n-   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location) {\n     return dropTable(location, true);\n   }\n \n-  /**\n-   * Drop a table; optionally delete data and metadata files.\n-   * <p>\n-   * If purge is set to true the implementation should delete all data and metadata files.\n-   * Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @param purge if true, delete all data and metadata files in the table\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location, boolean purge) {\n     // Just for checking if the table exists or not\n     load(location);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2Nzk0MQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493967941", "bodyText": "Would it be faster to call this instead of calling dropTableData? There seems to be no need to read files to find what to delete, only to delete the remaining files afterward.", "author": "rdblue", "createdAt": "2020-09-24T00:15:14Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java", "diffHunk": "@@ -144,6 +147,52 @@ public Table create(Schema schema, PartitionSpec spec, SortOrder order,\n     return new BaseTable(ops, location);\n   }\n \n+  /**\n+   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location) {\n+    return dropTable(location, true);\n+  }\n+\n+  /**\n+   * Drop a table; optionally delete data and metadata files.\n+   * <p>\n+   * If purge is set to true the implementation should delete all data and metadata files.\n+   * Throws NoSuchTableException if the table does not exists.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @param purge if true, delete all data and metadata files in the table\n+   * @return true if the table was dropped\n+   */\n+  public boolean dropTable(String location, boolean purge) {\n+    // Just for checking if the table exists or not\n+    load(location);\n+\n+    TableOperations ops = newTableOps(location);\n+    TableMetadata lastMetadata;\n+    if (purge && ops.current() != null) {\n+      lastMetadata = ops.current();\n+    } else {\n+      lastMetadata = null;\n+    }\n+\n+    try {\n+      if (purge && lastMetadata != null) {\n+        // Since the data files and the metadata files may store in different locations,\n+        // so it has to call dropTableData to force delete the data file.\n+        CatalogUtil.dropTableData(ops.io(), lastMetadata);\n+      }\n+      Path tablePath = new Path(location);\n+      Util.getFs(tablePath, conf).delete(tablePath, true /* recursive */);", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2ODM4NQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493968385", "bodyText": "Nevermind, I just remembered. And there's already a comment for it.", "author": "rdblue", "createdAt": "2020-09-24T00:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2Nzk0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\nindex e2b32f596..8bfafb40f 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n\n@@ -147,26 +147,10 @@ public class HadoopTables implements Tables, Configurable {\n     return new BaseTable(ops, location);\n   }\n \n-  /**\n-   * Drop a table and delete all data and metadata files. Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location) {\n     return dropTable(location, true);\n   }\n \n-  /**\n-   * Drop a table; optionally delete data and metadata files.\n-   * <p>\n-   * If purge is set to true the implementation should delete all data and metadata files.\n-   * Throws NoSuchTableException if the table does not exists.\n-   *\n-   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @param purge if true, delete all data and metadata files in the table\n-   * @return true if the table was dropped\n-   */\n   public boolean dropTable(String location, boolean purge) {\n     // Just for checking if the table exists or not\n     load(location);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2ODY1Mg==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493968652", "bodyText": "Normally, we would use ImmutableSet.of(...). That's simpler and creates a set that can't be modified.", "author": "rdblue", "createdAt": "2020-09-24T00:17:41Z", "path": "mr/src/main/java/org/apache/iceberg/mr/Catalogs.java", "diffHunk": "@@ -44,23 +61,31 @@\n   private static final String NAME = \"name\";\n   private static final String LOCATION = \"location\";\n \n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, LOCATION, NAME)\n+      .collect(Collectors.toCollection(HashSet::new));", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDIwNzM5OQ==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494207399", "bodyText": "Done", "author": "pvary", "createdAt": "2020-09-24T10:29:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2ODY1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\nindex 44481ece2..75790479f 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n\n@@ -61,15 +44,15 @@ public final class Catalogs {\n   private static final String NAME = \"name\";\n   private static final String LOCATION = \"location\";\n \n-  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n-      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, LOCATION, NAME)\n-      .collect(Collectors.toCollection(HashSet::new));\n-\n   private Catalogs() {\n   }\n \n   /**\n    * Load an Iceberg table using the catalog and table identifier (or table path) specified by the configuration.\n+   * Catalog resolution happens in this order:\n+   * 1. Custom catalog if specified by {@link InputFormatConfig#CATALOG_LOADER_CLASS}\n+   * 2. Hadoop or Hive catalog if specified by {@link InputFormatConfig#CATALOG}\n+   * 3. Hadoop Tables\n    * @param conf a Hadoop conf\n    * @return an Iceberg table\n    */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2OTM5OA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493969398", "bodyText": "Minor: Javadoc won't automatically break paragraphs, so you have to use <p> between them.", "author": "rdblue", "createdAt": "2020-09-24T00:20:16Z", "path": "mr/src/main/java/org/apache/iceberg/mr/Catalogs.java", "diffHunk": "@@ -44,23 +61,31 @@\n   private static final String NAME = \"name\";\n   private static final String LOCATION = \"location\";\n \n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, LOCATION, NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n   private Catalogs() {\n   }\n \n   /**\n    * Load an Iceberg table using the catalog and table identifier (or table path) specified by the configuration.\n-   * Catalog resolution happens in this order:\n-   * 1. Custom catalog if specified by {@link InputFormatConfig#CATALOG_LOADER_CLASS}\n-   * 2. Hadoop or Hive catalog if specified by {@link InputFormatConfig#CATALOG}\n-   * 3. Hadoop Tables\n    * @param conf a Hadoop conf\n    * @return an Iceberg table\n    */\n   public static Table loadTable(Configuration conf) {\n     return loadTable(conf, conf.get(InputFormatConfig.TABLE_IDENTIFIER), conf.get(InputFormatConfig.TABLE_LOCATION));\n   }\n \n-  // For use in HiveIcebergSerDe and HiveIcebergStorageHandler\n+  /**\n+   * Load an Iceberg table using the catalog specified by the configuration.\n+   * The table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) should be specified by\n+   * the controlling properties.\n+   * Used by HiveIcebergSerDe and HiveIcebergStorageHandler", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDIxMzEzMA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494213130", "bodyText": "Happily relearning javadoc formatting \ud83d\ude04\nDone", "author": "pvary", "createdAt": "2020-09-24T10:40:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk2OTM5OA=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\nindex 44481ece2..75790479f 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n\n@@ -61,15 +44,15 @@ public final class Catalogs {\n   private static final String NAME = \"name\";\n   private static final String LOCATION = \"location\";\n \n-  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n-      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, LOCATION, NAME)\n-      .collect(Collectors.toCollection(HashSet::new));\n-\n   private Catalogs() {\n   }\n \n   /**\n    * Load an Iceberg table using the catalog and table identifier (or table path) specified by the configuration.\n+   * Catalog resolution happens in this order:\n+   * 1. Custom catalog if specified by {@link InputFormatConfig#CATALOG_LOADER_CLASS}\n+   * 2. Hadoop or Hive catalog if specified by {@link InputFormatConfig#CATALOG}\n+   * 3. Hadoop Tables\n    * @param conf a Hadoop conf\n    * @return an Iceberg table\n    */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3MDAxNg==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493970016", "bodyText": "It looks like this is the reason why the examples specify a table property. Can we instead use Hive schema DDL and convert it to Iceberg? Similarly, can we get the identity partition fields that way to create a spec?", "author": "rdblue", "createdAt": "2020-09-24T00:22:15Z", "path": "mr/src/main/java/org/apache/iceberg/mr/Catalogs.java", "diffHunk": "@@ -77,6 +102,77 @@ private static Table loadTable(Configuration conf, String tableIdentifier, Strin\n     return new HadoopTables(conf).load(tableLocation);\n   }\n \n+  /**\n+   * Creates an Iceberg table using the catalog specified by the configuration.\n+   * The properties should contain the following values:\n+   * <p><ul>\n+   * <li>Table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) is required\n+   * <li>Table schema ({@link InputFormatConfig#TABLE_SCHEMA}) is required\n+   * <li>Partition specification ({@link InputFormatConfig#PARTITION_SPEC}) is optional. Table will be unpartitioned if\n+   *  not provided\n+   * </ul><p>\n+   * Other properties will be handled over to the Table creation. The controlling properties above will not be\n+   * propagated.\n+   * @param conf a Hadoop conf\n+   * @param props the controlling properties\n+   * @return the created Iceberg table\n+   */\n+  public static Table createTable(Configuration conf, Properties props) {\n+    String schemaString = props.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+    Preconditions.checkNotNull(schemaString, \"Table schema not set\");\n+    Schema schema = SchemaParser.fromJson(props.getProperty(InputFormatConfig.TABLE_SCHEMA));", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDIxNjE2MA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494216160", "bodyText": "I think we should keep the serialized schema for the Catalogs interface. Other systems like Impala, Presto, etc. might want to use it as well.\nI would like to tackle the Hive schema DDL in another PR. The data is available in HiveIcebergSerDe.initialize in a somewhat convoluted way. I would like to get it there and convert it to the Iceberg Schema string. From there I would only push the Iceberg related stuff down further.\nWhat do you think?", "author": "pvary", "createdAt": "2020-09-24T10:46:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3MDAxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ0Mjk3Ng==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494442976", "bodyText": "I think it's fine to do this in a separate PR. I just really don't want to require setting properties with JSON schema or spec representations as the way to use Iceberg. It's okay for a way to customize if there isn't syntax, but normal cases should just use DDL.", "author": "rdblue", "createdAt": "2020-09-24T16:12:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3MDAxNg=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\nindex 44481ece2..75790479f 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n\n@@ -102,77 +77,6 @@ public final class Catalogs {\n     return new HadoopTables(conf).load(tableLocation);\n   }\n \n-  /**\n-   * Creates an Iceberg table using the catalog specified by the configuration.\n-   * The properties should contain the following values:\n-   * <p><ul>\n-   * <li>Table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) is required\n-   * <li>Table schema ({@link InputFormatConfig#TABLE_SCHEMA}) is required\n-   * <li>Partition specification ({@link InputFormatConfig#PARTITION_SPEC}) is optional. Table will be unpartitioned if\n-   *  not provided\n-   * </ul><p>\n-   * Other properties will be handled over to the Table creation. The controlling properties above will not be\n-   * propagated.\n-   * @param conf a Hadoop conf\n-   * @param props the controlling properties\n-   * @return the created Iceberg table\n-   */\n-  public static Table createTable(Configuration conf, Properties props) {\n-    String schemaString = props.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Table schema not set\");\n-    Schema schema = SchemaParser.fromJson(props.getProperty(InputFormatConfig.TABLE_SCHEMA));\n-\n-    String specString = props.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    PartitionSpec spec = PartitionSpec.unpartitioned();\n-    if (specString != null) {\n-      spec = PartitionSpecParser.fromJson(schema, specString);\n-    }\n-\n-    String location = props.getProperty(LOCATION);\n-\n-    // Create a table property map without the controlling properties\n-    Map<String, String> map = new HashMap<>(props.size());\n-    for (Object key : props.keySet()) {\n-      if (!PROPERTIES_TO_REMOVE.contains(key)) {\n-        map.put(key.toString(), props.get(key).toString());\n-      }\n-    }\n-\n-    Optional<Catalog> catalog = loadCatalog(conf);\n-\n-    if (catalog.isPresent()) {\n-      String name = props.getProperty(NAME);\n-      Preconditions.checkNotNull(name, \"Table identifier not set\");\n-      return catalog.get().createTable(TableIdentifier.parse(name), schema, spec, location, map);\n-    }\n-\n-    Preconditions.checkNotNull(location, \"Table location not set\");\n-    return new HadoopTables(conf).create(schema, spec, map, location);\n-  }\n-\n-  /**\n-   * Drops an Iceberg table using the catalog specified by the configuration.\n-   * The table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) should be specified by\n-   * the controlling properties.\n-   * @param conf a Hadoop conf\n-   * @param props the controlling properties\n-   * @return the created Iceberg table\n-   */\n-  public static boolean dropTable(Configuration conf, Properties props) {\n-    String location = props.getProperty(LOCATION);\n-\n-    Optional<Catalog> catalog = loadCatalog(conf);\n-\n-    if (catalog.isPresent()) {\n-      String name = props.getProperty(NAME);\n-      Preconditions.checkNotNull(name, \"Table identifier not set\");\n-      return catalog.get().dropTable(TableIdentifier.parse(name));\n-    }\n-\n-    Preconditions.checkNotNull(location, \"Table location not set\");\n-    return new HadoopTables(conf).dropTable(location);\n-  }\n-\n   @VisibleForTesting\n   static Optional<Catalog> loadCatalog(Configuration conf) {\n     String catalogLoaderClass = conf.get(InputFormatConfig.CATALOG_LOADER_CLASS);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3MDU2Ng==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r493970566", "bodyText": "Somewhat out of scope: We might want to build a Catalog for this logic so that this class can avoid loading and checking the catalog in every method. The catalog would get created with the configuration and handle this delegation internally.", "author": "rdblue", "createdAt": "2020-09-24T00:24:20Z", "path": "mr/src/main/java/org/apache/iceberg/mr/Catalogs.java", "diffHunk": "@@ -77,6 +102,77 @@ private static Table loadTable(Configuration conf, String tableIdentifier, Strin\n     return new HadoopTables(conf).load(tableLocation);\n   }\n \n+  /**\n+   * Creates an Iceberg table using the catalog specified by the configuration.\n+   * The properties should contain the following values:\n+   * <p><ul>\n+   * <li>Table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) is required\n+   * <li>Table schema ({@link InputFormatConfig#TABLE_SCHEMA}) is required\n+   * <li>Partition specification ({@link InputFormatConfig#PARTITION_SPEC}) is optional. Table will be unpartitioned if\n+   *  not provided\n+   * </ul><p>\n+   * Other properties will be handled over to the Table creation. The controlling properties above will not be\n+   * propagated.\n+   * @param conf a Hadoop conf\n+   * @param props the controlling properties\n+   * @return the created Iceberg table\n+   */\n+  public static Table createTable(Configuration conf, Properties props) {\n+    String schemaString = props.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+    Preconditions.checkNotNull(schemaString, \"Table schema not set\");\n+    Schema schema = SchemaParser.fromJson(props.getProperty(InputFormatConfig.TABLE_SCHEMA));\n+\n+    String specString = props.getProperty(InputFormatConfig.PARTITION_SPEC);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    if (specString != null) {\n+      spec = PartitionSpecParser.fromJson(schema, specString);\n+    }\n+\n+    String location = props.getProperty(LOCATION);\n+\n+    // Create a table property map without the controlling properties\n+    Map<String, String> map = new HashMap<>(props.size());\n+    for (Object key : props.keySet()) {\n+      if (!PROPERTIES_TO_REMOVE.contains(key)) {\n+        map.put(key.toString(), props.get(key).toString());\n+      }\n+    }\n+\n+    Optional<Catalog> catalog = loadCatalog(conf);", "originalCommit": "c584198d44cfa3b49fe93e078b1be11b1e66b8f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDIyNDE3MA==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494224170", "bodyText": "HiveCatalog has a cache, but this might be useful for other Catalogs as well.\nThis seems like a good idea to pursue, but I do not promise anything here, as I have too much on my plate currently", "author": "pvary", "createdAt": "2020-09-24T11:01:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3MDU2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "514575eeed1ded9c4232462e568fbaf3b845fe49", "chunk": "diff --git a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\nindex 44481ece2..75790479f 100644\n--- a/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n+++ b/mr/src/main/java/org/apache/iceberg/mr/Catalogs.java\n\n@@ -102,77 +77,6 @@ public final class Catalogs {\n     return new HadoopTables(conf).load(tableLocation);\n   }\n \n-  /**\n-   * Creates an Iceberg table using the catalog specified by the configuration.\n-   * The properties should contain the following values:\n-   * <p><ul>\n-   * <li>Table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) is required\n-   * <li>Table schema ({@link InputFormatConfig#TABLE_SCHEMA}) is required\n-   * <li>Partition specification ({@link InputFormatConfig#PARTITION_SPEC}) is optional. Table will be unpartitioned if\n-   *  not provided\n-   * </ul><p>\n-   * Other properties will be handled over to the Table creation. The controlling properties above will not be\n-   * propagated.\n-   * @param conf a Hadoop conf\n-   * @param props the controlling properties\n-   * @return the created Iceberg table\n-   */\n-  public static Table createTable(Configuration conf, Properties props) {\n-    String schemaString = props.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Table schema not set\");\n-    Schema schema = SchemaParser.fromJson(props.getProperty(InputFormatConfig.TABLE_SCHEMA));\n-\n-    String specString = props.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    PartitionSpec spec = PartitionSpec.unpartitioned();\n-    if (specString != null) {\n-      spec = PartitionSpecParser.fromJson(schema, specString);\n-    }\n-\n-    String location = props.getProperty(LOCATION);\n-\n-    // Create a table property map without the controlling properties\n-    Map<String, String> map = new HashMap<>(props.size());\n-    for (Object key : props.keySet()) {\n-      if (!PROPERTIES_TO_REMOVE.contains(key)) {\n-        map.put(key.toString(), props.get(key).toString());\n-      }\n-    }\n-\n-    Optional<Catalog> catalog = loadCatalog(conf);\n-\n-    if (catalog.isPresent()) {\n-      String name = props.getProperty(NAME);\n-      Preconditions.checkNotNull(name, \"Table identifier not set\");\n-      return catalog.get().createTable(TableIdentifier.parse(name), schema, spec, location, map);\n-    }\n-\n-    Preconditions.checkNotNull(location, \"Table location not set\");\n-    return new HadoopTables(conf).create(schema, spec, map, location);\n-  }\n-\n-  /**\n-   * Drops an Iceberg table using the catalog specified by the configuration.\n-   * The table identifier ({@link Catalogs#NAME}) or table path ({@link Catalogs#LOCATION}) should be specified by\n-   * the controlling properties.\n-   * @param conf a Hadoop conf\n-   * @param props the controlling properties\n-   * @return the created Iceberg table\n-   */\n-  public static boolean dropTable(Configuration conf, Properties props) {\n-    String location = props.getProperty(LOCATION);\n-\n-    Optional<Catalog> catalog = loadCatalog(conf);\n-\n-    if (catalog.isPresent()) {\n-      String name = props.getProperty(NAME);\n-      Preconditions.checkNotNull(name, \"Table identifier not set\");\n-      return catalog.get().dropTable(TableIdentifier.parse(name));\n-    }\n-\n-    Preconditions.checkNotNull(location, \"Table location not set\");\n-    return new HadoopTables(conf).dropTable(location);\n-  }\n-\n   @VisibleForTesting\n   static Optional<Catalog> loadCatalog(Configuration conf) {\n     String catalogLoaderClass = conf.get(InputFormatConfig.CATALOG_LOADER_CLASS);\n"}}, {"oid": "514575eeed1ded9c4232462e568fbaf3b845fe49", "url": "https://github.com/apache/iceberg/commit/514575eeed1ded9c4232462e568fbaf3b845fe49", "message": "Implement dropTable for HadoopTables", "committedDate": "2020-09-24T10:07:56Z", "type": "commit"}, {"oid": "a1207f1b2dfc78ede59bc41825aab4c6374d79f8", "url": "https://github.com/apache/iceberg/commit/a1207f1b2dfc78ede59bc41825aab4c6374d79f8", "message": "Implement Catalogs.createTable, Catalogs.dropTable", "committedDate": "2020-09-24T10:07:56Z", "type": "commit"}, {"oid": "e21ad740a497b209a4ae7fcb082ce77bb31857e2", "url": "https://github.com/apache/iceberg/commit/e21ad740a497b209a4ae7fcb082ce77bb31857e2", "message": "Controlling properties should not be pushed to the actual table creation.\nJavadoc is revisited", "committedDate": "2020-09-24T10:07:56Z", "type": "commit"}, {"oid": "3df7efd418452c837ef566c39ecf17d40c9d1bcb", "url": "https://github.com/apache/iceberg/commit/3df7efd418452c837ef566c39ecf17d40c9d1bcb", "message": "Fixing comment formatting", "committedDate": "2020-09-24T10:07:56Z", "type": "commit"}, {"oid": "fc28ebf41301e49939b571811645458152190dc8", "url": "https://github.com/apache/iceberg/commit/fc28ebf41301e49939b571811645458152190dc8", "message": "Javadoc warning fixed", "committedDate": "2020-09-24T10:07:56Z", "type": "commit"}, {"oid": "1960cde82595ac31d5c2beee5e806ae6bb9c5eb6", "url": "https://github.com/apache/iceberg/commit/1960cde82595ac31d5c2beee5e806ae6bb9c5eb6", "message": "Addressed review comments", "committedDate": "2020-09-24T11:11:57Z", "type": "commit"}, {"oid": "1960cde82595ac31d5c2beee5e806ae6bb9c5eb6", "url": "https://github.com/apache/iceberg/commit/1960cde82595ac31d5c2beee5e806ae6bb9c5eb6", "message": "Addressed review comments", "committedDate": "2020-09-24T11:11:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ0NDMwNg==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494444306", "bodyText": "I should have caught this yesterday, but shouldn't this return false instead of throwing the exception? That's what all the other drop methods do. If the table doesn't exist, it isn't an exceptional case. It just returns false to signal that it nothing needed to be done.", "author": "rdblue", "createdAt": "2020-09-24T16:14:55Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java", "diffHunk": "@@ -144,6 +147,52 @@ public Table create(Schema schema, PartitionSpec spec, SortOrder order,\n     return new BaseTable(ops, location);\n   }\n \n+  /**\n+   * Drop a table and delete all data and metadata files.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @return true if the table was dropped\n+   * @throws NoSuchTableException if the table does not exists.\n+   */\n+  public boolean dropTable(String location) {\n+    return dropTable(location, true);\n+  }\n+\n+  /**\n+   * Drop a table; optionally delete data and metadata files.\n+   * <p>\n+   * If purge is set to true the implementation should delete all data and metadata files.\n+   *\n+   * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n+   * @param purge if true, delete all data and metadata files in the table\n+   * @return true if the table was dropped\n+   * @throws NoSuchTableException if the table does not exists.\n+   */\n+  public boolean dropTable(String location, boolean purge) {\n+    TableOperations ops = newTableOps(location);\n+    TableMetadata lastMetadata = null;\n+    if (ops.current() != null) {\n+      if (purge) {\n+        lastMetadata = ops.current();\n+      }\n+    } else {\n+      throw new NoSuchTableException(\"Table does not exist at location: %s, so it can not be dropped\", location);", "originalCommit": "1960cde82595ac31d5c2beee5e806ae6bb9c5eb6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDkxMzY0Mg==", "url": "https://github.com/apache/iceberg/pull/1481#discussion_r494913642", "bodyText": "I should have caught this yesterday, but shouldn't this return false instead of throwing the exception? That's what all the other drop methods do. If the table doesn't exist, it isn't an exceptional case. It just returns false to signal that it nothing needed to be done.\n\nGood point! Finding it now is better than finding it after pushing the PR \ud83d\ude04\nFixed.", "author": "pvary", "createdAt": "2020-09-25T11:04:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ0NDMwNg=="}], "type": "inlineReview", "revised_code": {"commit": "66bb2e8999c174579d7c4e48f1f4cd5f7dd4d987", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\nindex 01e01c63c..f8c59f48a 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java\n\n@@ -151,8 +151,7 @@ public class HadoopTables implements Tables, Configurable {\n    * Drop a table and delete all data and metadata files.\n    *\n    * @param location a path URI (e.g. hdfs:///warehouse/my_table)\n-   * @return true if the table was dropped\n-   * @throws NoSuchTableException if the table does not exists.\n+   * @return true if the table was dropped, false if it did not exist\n    */\n   public boolean dropTable(String location) {\n     return dropTable(location, true);\n"}}, {"oid": "66bb2e8999c174579d7c4e48f1f4cd5f7dd4d987", "url": "https://github.com/apache/iceberg/commit/66bb2e8999c174579d7c4e48f1f4cd5f7dd4d987", "message": "Do not throw if the table did not exist", "committedDate": "2020-09-25T11:03:40Z", "type": "commit"}]}