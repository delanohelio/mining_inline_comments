{"pr_number": 1272, "pr_title": "Flink: use schema visitor for parquet writer", "pr_createdAt": "2020-07-30T11:04:03Z", "pr_url": "https://github.com/apache/iceberg/pull/1272", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzExNw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463043117", "bodyText": "This class seems don't have to be public, only the FlinkParquetReader will access those readers.  It also don't need to be accessed by other classes I think.", "author": "openinx", "createdAt": "2020-07-30T14:35:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM3ODU2MQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463378561", "bodyText": "Make sense to me.", "author": "chenjunjiedada", "createdAt": "2020-07-31T02:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzExNw=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\nindex a556832e4..ccea5d652 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n\n@@ -19,723 +19,64 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n-import java.math.BigInteger;\n-import java.nio.ByteBuffer;\n-import java.time.Instant;\n import java.util.List;\n-import java.util.Map;\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.apache.flink.table.data.ArrayData;\n-import org.apache.flink.table.data.DecimalData;\n-import org.apache.flink.table.data.GenericRowData;\n-import org.apache.flink.table.data.MapData;\n-import org.apache.flink.table.data.RawValueData;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.StringData;\n-import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.types.Row;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.parquet.ParquetSchemaUtil;\n+import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n-import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.GroupType;\n-import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders {\n-  private FlinkParquetReaders() {\n-  }\n-\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n-  }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n-                                                        MessageType fileSchema,\n-                                                        Map<Integer, ?> idToConstant) {\n-    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n-    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n-    } else {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n-              new FallbackReadBuilder(builder));\n-    }\n-  }\n-\n-  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private MessageType type;\n-    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n-\n-    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n-      this.builder = builder;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      // the top level matches by ID, but the remaining IDs are missing\n-      this.type = message;\n-      return builder.struct(expected, message, fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // the expected struct is ignored because nested fields are never found when the\n-      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n-          fieldReaders.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-        types.add(fieldType);\n-      }\n-\n-      return new RowDataReader(types, newFields);\n-    }\n-  }\n-\n-  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private final MessageType type;\n-    private final Map<Integer, ?> idToConstant;\n-\n-    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n-      this.type = type;\n-      this.idToConstant = idToConstant;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      return struct(expected, message.asGroupType(), fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // match the expected struct's order\n-      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n-      Map<Integer, Type> typesById = Maps.newHashMap();\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        if (fieldType.getId() != null) {\n-          int id = fieldType.getId().intValue();\n-          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-          typesById.put(id, fieldType);\n-        }\n-      }\n-\n-      List<Types.NestedField> expectedFields = expected != null ?\n-          expected.fields() : ImmutableList.of();\n-      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n-          expectedFields.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n-      for (Types.NestedField field : expectedFields) {\n-        int id = field.fieldId();\n-        if (idToConstant.containsKey(id)) {\n-          // containsKey is used because the constant may be null\n-          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n-          types.add(null);\n-        } else {\n-          ParquetValueReader<?> reader = readersById.get(id);\n-          if (reader != null) {\n-            reorderedFields.add(reader);\n-            types.add(typesById.get(id));\n-          } else {\n-            reorderedFields.add(ParquetValueReaders.nulls());\n-            types.add(null);\n-          }\n-        }\n-      }\n-\n-      return new RowDataReader(types, reorderedFields);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n-                                      ParquetValueReader<?> elementReader) {\n-      GroupType repeated = array.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type elementType = repeated.getType(0);\n-      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n-\n-      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n-                                     ParquetValueReader<?> keyReader,\n-                                     ParquetValueReader<?> valueReader) {\n-      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type keyType = repeatedKeyValue.getType(0);\n-      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n-      Type valueType = repeatedKeyValue.getType(1);\n-      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n-\n-      return new MapReader<>(repeatedD, repeatedR,\n-          ParquetValueReaders.option(keyType, keyD, keyReader),\n-          ParquetValueReaders.option(valueType, valueD, valueReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n-                                           PrimitiveType primitive) {\n-      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n-\n-      if (primitive.getOriginalType() != null) {\n-        switch (primitive.getOriginalType()) {\n-          case ENUM:\n-          case JSON:\n-          case UTF8:\n-            return new StringReader(desc);\n-          case INT_8:\n-          case INT_16:\n-          case INT_32:\n-          case DATE:\n-            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n-              return new ParquetValueReaders.IntAsLongReader(desc);\n-            } else {\n-              return new ParquetValueReaders.UnboxedReader<>(desc);\n-            }\n-          case TIME_MICROS:\n-            return new TimeMillisReader(desc);\n-          case INT_64:\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          case TIMESTAMP_MICROS:\n-            return new TimestampMicroReader(desc);\n-          case DECIMAL:\n-            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n-            switch (primitive.getPrimitiveTypeName()) {\n-              case BINARY:\n-              case FIXED_LEN_BYTE_ARRAY:\n-                return new BinaryDecimalReader(desc, decimal.getScale());\n-              case INT64:\n-                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              case INT32:\n-                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              default:\n-                throw new UnsupportedOperationException(\n-                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n-            }\n-          case BSON:\n-            return new ParquetValueReaders.ByteArrayReader(desc);\n-          default:\n-            throw new UnsupportedOperationException(\n-                \"Unsupported logical type: \" + primitive.getOriginalType());\n-        }\n-      }\n-\n-      switch (primitive.getPrimitiveTypeName()) {\n-        case FIXED_LEN_BYTE_ARRAY:\n-        case BINARY:\n-          return new ParquetValueReaders.ByteArrayReader(desc);\n-        case INT32:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n-            return new ParquetValueReaders.IntAsLongReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case FLOAT:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n-            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case BOOLEAN:\n-        case INT64:\n-        case DOUBLE:\n-          return new ParquetValueReaders.UnboxedReader<>(desc);\n-        default:\n-          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n-      }\n-    }\n-\n-    protected MessageType type() {\n-      return type;\n-    }\n-  }\n-\n-  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int scale;\n-\n-    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n-      super(desc);\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      Binary binary = column.nextBinary();\n-      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n-      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n-    }\n-  }\n-\n-  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n-\n-    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n-    }\n-  }\n+public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n \n-  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n+  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n \n-    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n-    }\n-  }\n-\n-  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n-    TimestampMicroReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public TimestampData read(TimestampData ignored) {\n-      long value = readLong();\n-      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n-    }\n-\n-    @Override\n-    public long readLong() {\n-      return column.nextLong();\n-    }\n+  private FlinkParquetReaders() {\n   }\n \n-  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n-    StringReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public StringData read(StringData ignored) {\n-      Binary binary = column.nextBinary();\n-      ByteBuffer buffer = binary.toByteBuffer();\n-      if (buffer.hasArray()) {\n-        return StringData.fromBytes(\n-            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-      } else {\n-        return StringData.fromBytes(binary.getBytes());\n-      }\n-    }\n+  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return INSTANCE.createReader(expectedSchema, fileSchema);\n   }\n \n-  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n-    TimeMillisReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public Integer read(Integer reuse) {\n-      // Flink only supports millisecond, so we discard microseconds in millisecond\n-      return (int) column.nextLong() / 1000;\n-    }\n+  @Override\n+  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n+                                                       List<ParquetValueReader<?>> fieldReaders,\n+                                                       Types.StructType structType) {\n+    return new RowReader(types, fieldReaders, structType);\n   }\n \n-  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n-      super(definitionLevel, repetitionLevel, reader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableArrayData newListData(ArrayData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableArrayData) {\n-        return (ReusableArrayData) reuse;\n-      } else {\n-        return new ReusableArrayData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected E getElement(ReusableArrayData list) {\n-      E value = null;\n-      if (readPos < list.capacity()) {\n-        value = (E) list.values[readPos];\n-      }\n-\n-      readPos += 1;\n-\n-      return value;\n-    }\n-\n-    @Override\n-    protected void addElement(ReusableArrayData reused, E element) {\n-      if (writePos >= reused.capacity()) {\n-        reused.grow();\n-      }\n-\n-      reused.values[writePos] = element;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected ArrayData buildList(ReusableArrayData list) {\n-      list.setNumElements(writePos);\n-      return list;\n-    }\n-  }\n-\n-  private static class MapReader<K, V> extends\n-      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n-    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n-\n-    MapReader(int definitionLevel, int repetitionLevel,\n-              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n-      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableMapData newMapData(MapData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableMapData) {\n-        return (ReusableMapData) reuse;\n-      } else {\n-        return new ReusableMapData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n-      Map.Entry<K, V> kv = nullEntry;\n-      if (readPos < map.capacity()) {\n-        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n-        kv = entry;\n-      }\n-\n-      readPos += 1;\n-\n-      return kv;\n-    }\n-\n-    @Override\n-    protected void addPair(ReusableMapData map, K key, V value) {\n-      if (writePos >= map.capacity()) {\n-        map.grow();\n-      }\n+  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n+    private final Types.StructType structType;\n \n-      map.keys.values[writePos] = key;\n-      map.values.values[writePos] = value;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected MapData buildMap(ReusableMapData map) {\n-      map.setNumElements(writePos);\n-      return map;\n-    }\n-  }\n-\n-  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n-    private final int numFields;\n-\n-    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n+    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n       super(types, readers);\n-      this.numFields = readers.size();\n+      this.structType = struct;\n     }\n \n     @Override\n-    protected GenericRowData newStructData(RowData reuse) {\n-      if (reuse instanceof GenericRowData) {\n-        return (GenericRowData) reuse;\n+    protected Row newStructData(Row reuse) {\n+      if (reuse != null) {\n+        return reuse;\n       } else {\n-        return new GenericRowData(numFields);\n+        return new Row(structType.fields().size());\n       }\n     }\n \n     @Override\n-    protected Object getField(GenericRowData intermediate, int pos) {\n-      return intermediate.getField(pos);\n-    }\n-\n-    @Override\n-    protected RowData buildStruct(GenericRowData struct) {\n-      return struct;\n-    }\n-\n-    @Override\n-    protected void set(GenericRowData row, int pos, Object value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setNull(GenericRowData row, int pos) {\n-      row.setField(pos, null);\n+    protected Object getField(Row row, int pos) {\n+      return row.getField(pos);\n     }\n \n     @Override\n-    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n-      row.setField(pos, value);\n+    protected Row buildStruct(Row row) {\n+      return row;\n     }\n \n     @Override\n-    protected void setInteger(GenericRowData row, int pos, int value) {\n+    protected void set(Row row, int pos, Object value) {\n       row.setField(pos, value);\n     }\n-\n-    @Override\n-    protected void setLong(GenericRowData row, int pos, long value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setFloat(GenericRowData row, int pos, float value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setDouble(GenericRowData row, int pos, double value) {\n-      row.setField(pos, value);\n-    }\n-  }\n-\n-  private static class ReusableMapData implements MapData {\n-    private final ReusableArrayData keys;\n-    private final ReusableArrayData values;\n-\n-    private int numElements;\n-\n-    private ReusableMapData() {\n-      this.keys = new ReusableArrayData();\n-      this.values = new ReusableArrayData();\n-    }\n-\n-    private void grow() {\n-      keys.grow();\n-      values.grow();\n-    }\n-\n-    private int capacity() {\n-      return keys.capacity();\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-      keys.setNumElements(numElements);\n-      values.setNumElements(numElements);\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public ReusableArrayData keyArray() {\n-      return keys;\n-    }\n-\n-    @Override\n-    public ReusableArrayData valueArray() {\n-      return values;\n-    }\n-  }\n-\n-  private static class ReusableArrayData implements ArrayData {\n-    private static final Object[] EMPTY = new Object[0];\n-\n-    private Object[] values = EMPTY;\n-    private int numElements = 0;\n-\n-    private void grow() {\n-      if (values.length == 0) {\n-        this.values = new Object[20];\n-      } else {\n-        Object[] old = values;\n-        this.values = new Object[old.length << 2];\n-        // copy the old array in case it has values that can be reused\n-        System.arraycopy(old, 0, values, 0, old.length);\n-      }\n-    }\n-\n-    private int capacity() {\n-      return values.length;\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public boolean isNullAt(int ordinal) {\n-      return null == values[ordinal];\n-    }\n-\n-    @Override\n-    public boolean getBoolean(int ordinal) {\n-      return (boolean) values[ordinal];\n-    }\n-\n-    @Override\n-    public byte getByte(int ordinal) {\n-      return (byte) values[ordinal];\n-    }\n-\n-    @Override\n-    public short getShort(int ordinal) {\n-      return (short) values[ordinal];\n-    }\n-\n-    @Override\n-    public int getInt(int ordinal) {\n-      return (int) values[ordinal];\n-    }\n-\n-    @Override\n-    public long getLong(int ordinal) {\n-      return (long) values[ordinal];\n-    }\n-\n-    @Override\n-    public float getFloat(int ordinal) {\n-      return (float) values[ordinal];\n-    }\n-\n-    @Override\n-    public double getDouble(int ordinal) {\n-      return (double) values[ordinal];\n-    }\n-\n-    @Override\n-    public StringData getString(int pos) {\n-      return (StringData) values[pos];\n-    }\n-\n-    @Override\n-    public DecimalData getDecimal(int pos, int precision, int scale) {\n-      return (DecimalData) values[pos];\n-    }\n-\n-    @Override\n-    public TimestampData getTimestamp(int pos, int precision) {\n-      return (TimestampData) values[pos];\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public <T> RawValueData<T> getRawValue(int pos) {\n-      return (RawValueData<T>) values[pos];\n-    }\n-\n-    @Override\n-    public byte[] getBinary(int ordinal) {\n-      return (byte[]) values[ordinal];\n-    }\n-\n-    @Override\n-    public ArrayData getArray(int ordinal) {\n-      return (ArrayData) values[ordinal];\n-    }\n-\n-    @Override\n-    public MapData getMap(int ordinal) {\n-      return (MapData) values[ordinal];\n-    }\n-\n-    @Override\n-    public RowData getRow(int pos, int numFields) {\n-      return (RowData) values[pos];\n-    }\n-\n-    @Override\n-    public boolean[] toBooleanArray() {\n-      return ArrayUtils.toPrimitive((Boolean[]) values);\n-    }\n-\n-    @Override\n-    public byte[] toByteArray() {\n-      return ArrayUtils.toPrimitive((Byte[]) values);\n-    }\n-\n-    @Override\n-    public short[] toShortArray() {\n-      return ArrayUtils.toPrimitive((Short[]) values);\n-    }\n-\n-    @Override\n-    public int[] toIntArray() {\n-      return ArrayUtils.toPrimitive((Integer[]) values);\n-    }\n-\n-    @Override\n-    public long[] toLongArray() {\n-      return ArrayUtils.toPrimitive((Long[]) values);\n-    }\n-\n-    @Override\n-    public float[] toFloatArray() {\n-      return ArrayUtils.toPrimitive((Float[]) values);\n-    }\n-\n-    @Override\n-    public double[] toDoubleArray() {\n-      return ArrayUtils.toPrimitive((Double[]) values);\n-    }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NTA4Ng==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463045086", "bodyText": "nit: the comment is not complete ?", "author": "openinx", "createdAt": "2020-07-30T14:38:18Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM4NDEwMA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463384100", "bodyText": "fixed.", "author": "chenjunjiedada", "createdAt": "2020-07-31T03:16:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NTA4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\nindex a556832e4..ccea5d652 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n\n@@ -19,723 +19,64 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n-import java.math.BigInteger;\n-import java.nio.ByteBuffer;\n-import java.time.Instant;\n import java.util.List;\n-import java.util.Map;\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.apache.flink.table.data.ArrayData;\n-import org.apache.flink.table.data.DecimalData;\n-import org.apache.flink.table.data.GenericRowData;\n-import org.apache.flink.table.data.MapData;\n-import org.apache.flink.table.data.RawValueData;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.StringData;\n-import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.types.Row;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.parquet.ParquetSchemaUtil;\n+import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n-import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.GroupType;\n-import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders {\n-  private FlinkParquetReaders() {\n-  }\n-\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n-  }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n-                                                        MessageType fileSchema,\n-                                                        Map<Integer, ?> idToConstant) {\n-    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n-    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n-    } else {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n-              new FallbackReadBuilder(builder));\n-    }\n-  }\n-\n-  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private MessageType type;\n-    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n-\n-    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n-      this.builder = builder;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      // the top level matches by ID, but the remaining IDs are missing\n-      this.type = message;\n-      return builder.struct(expected, message, fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // the expected struct is ignored because nested fields are never found when the\n-      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n-          fieldReaders.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-        types.add(fieldType);\n-      }\n-\n-      return new RowDataReader(types, newFields);\n-    }\n-  }\n-\n-  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private final MessageType type;\n-    private final Map<Integer, ?> idToConstant;\n-\n-    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n-      this.type = type;\n-      this.idToConstant = idToConstant;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      return struct(expected, message.asGroupType(), fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // match the expected struct's order\n-      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n-      Map<Integer, Type> typesById = Maps.newHashMap();\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        if (fieldType.getId() != null) {\n-          int id = fieldType.getId().intValue();\n-          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-          typesById.put(id, fieldType);\n-        }\n-      }\n-\n-      List<Types.NestedField> expectedFields = expected != null ?\n-          expected.fields() : ImmutableList.of();\n-      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n-          expectedFields.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n-      for (Types.NestedField field : expectedFields) {\n-        int id = field.fieldId();\n-        if (idToConstant.containsKey(id)) {\n-          // containsKey is used because the constant may be null\n-          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n-          types.add(null);\n-        } else {\n-          ParquetValueReader<?> reader = readersById.get(id);\n-          if (reader != null) {\n-            reorderedFields.add(reader);\n-            types.add(typesById.get(id));\n-          } else {\n-            reorderedFields.add(ParquetValueReaders.nulls());\n-            types.add(null);\n-          }\n-        }\n-      }\n-\n-      return new RowDataReader(types, reorderedFields);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n-                                      ParquetValueReader<?> elementReader) {\n-      GroupType repeated = array.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type elementType = repeated.getType(0);\n-      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n-\n-      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n-                                     ParquetValueReader<?> keyReader,\n-                                     ParquetValueReader<?> valueReader) {\n-      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type keyType = repeatedKeyValue.getType(0);\n-      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n-      Type valueType = repeatedKeyValue.getType(1);\n-      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n-\n-      return new MapReader<>(repeatedD, repeatedR,\n-          ParquetValueReaders.option(keyType, keyD, keyReader),\n-          ParquetValueReaders.option(valueType, valueD, valueReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n-                                           PrimitiveType primitive) {\n-      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n-\n-      if (primitive.getOriginalType() != null) {\n-        switch (primitive.getOriginalType()) {\n-          case ENUM:\n-          case JSON:\n-          case UTF8:\n-            return new StringReader(desc);\n-          case INT_8:\n-          case INT_16:\n-          case INT_32:\n-          case DATE:\n-            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n-              return new ParquetValueReaders.IntAsLongReader(desc);\n-            } else {\n-              return new ParquetValueReaders.UnboxedReader<>(desc);\n-            }\n-          case TIME_MICROS:\n-            return new TimeMillisReader(desc);\n-          case INT_64:\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          case TIMESTAMP_MICROS:\n-            return new TimestampMicroReader(desc);\n-          case DECIMAL:\n-            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n-            switch (primitive.getPrimitiveTypeName()) {\n-              case BINARY:\n-              case FIXED_LEN_BYTE_ARRAY:\n-                return new BinaryDecimalReader(desc, decimal.getScale());\n-              case INT64:\n-                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              case INT32:\n-                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              default:\n-                throw new UnsupportedOperationException(\n-                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n-            }\n-          case BSON:\n-            return new ParquetValueReaders.ByteArrayReader(desc);\n-          default:\n-            throw new UnsupportedOperationException(\n-                \"Unsupported logical type: \" + primitive.getOriginalType());\n-        }\n-      }\n-\n-      switch (primitive.getPrimitiveTypeName()) {\n-        case FIXED_LEN_BYTE_ARRAY:\n-        case BINARY:\n-          return new ParquetValueReaders.ByteArrayReader(desc);\n-        case INT32:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n-            return new ParquetValueReaders.IntAsLongReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case FLOAT:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n-            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case BOOLEAN:\n-        case INT64:\n-        case DOUBLE:\n-          return new ParquetValueReaders.UnboxedReader<>(desc);\n-        default:\n-          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n-      }\n-    }\n-\n-    protected MessageType type() {\n-      return type;\n-    }\n-  }\n-\n-  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int scale;\n-\n-    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n-      super(desc);\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      Binary binary = column.nextBinary();\n-      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n-      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n-    }\n-  }\n-\n-  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n-\n-    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n-    }\n-  }\n+public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n \n-  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n+  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n \n-    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n-    }\n-  }\n-\n-  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n-    TimestampMicroReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public TimestampData read(TimestampData ignored) {\n-      long value = readLong();\n-      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n-    }\n-\n-    @Override\n-    public long readLong() {\n-      return column.nextLong();\n-    }\n+  private FlinkParquetReaders() {\n   }\n \n-  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n-    StringReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public StringData read(StringData ignored) {\n-      Binary binary = column.nextBinary();\n-      ByteBuffer buffer = binary.toByteBuffer();\n-      if (buffer.hasArray()) {\n-        return StringData.fromBytes(\n-            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-      } else {\n-        return StringData.fromBytes(binary.getBytes());\n-      }\n-    }\n+  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return INSTANCE.createReader(expectedSchema, fileSchema);\n   }\n \n-  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n-    TimeMillisReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public Integer read(Integer reuse) {\n-      // Flink only supports millisecond, so we discard microseconds in millisecond\n-      return (int) column.nextLong() / 1000;\n-    }\n+  @Override\n+  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n+                                                       List<ParquetValueReader<?>> fieldReaders,\n+                                                       Types.StructType structType) {\n+    return new RowReader(types, fieldReaders, structType);\n   }\n \n-  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n-      super(definitionLevel, repetitionLevel, reader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableArrayData newListData(ArrayData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableArrayData) {\n-        return (ReusableArrayData) reuse;\n-      } else {\n-        return new ReusableArrayData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected E getElement(ReusableArrayData list) {\n-      E value = null;\n-      if (readPos < list.capacity()) {\n-        value = (E) list.values[readPos];\n-      }\n-\n-      readPos += 1;\n-\n-      return value;\n-    }\n-\n-    @Override\n-    protected void addElement(ReusableArrayData reused, E element) {\n-      if (writePos >= reused.capacity()) {\n-        reused.grow();\n-      }\n-\n-      reused.values[writePos] = element;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected ArrayData buildList(ReusableArrayData list) {\n-      list.setNumElements(writePos);\n-      return list;\n-    }\n-  }\n-\n-  private static class MapReader<K, V> extends\n-      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n-    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n-\n-    MapReader(int definitionLevel, int repetitionLevel,\n-              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n-      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableMapData newMapData(MapData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableMapData) {\n-        return (ReusableMapData) reuse;\n-      } else {\n-        return new ReusableMapData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n-      Map.Entry<K, V> kv = nullEntry;\n-      if (readPos < map.capacity()) {\n-        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n-        kv = entry;\n-      }\n-\n-      readPos += 1;\n-\n-      return kv;\n-    }\n-\n-    @Override\n-    protected void addPair(ReusableMapData map, K key, V value) {\n-      if (writePos >= map.capacity()) {\n-        map.grow();\n-      }\n+  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n+    private final Types.StructType structType;\n \n-      map.keys.values[writePos] = key;\n-      map.values.values[writePos] = value;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected MapData buildMap(ReusableMapData map) {\n-      map.setNumElements(writePos);\n-      return map;\n-    }\n-  }\n-\n-  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n-    private final int numFields;\n-\n-    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n+    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n       super(types, readers);\n-      this.numFields = readers.size();\n+      this.structType = struct;\n     }\n \n     @Override\n-    protected GenericRowData newStructData(RowData reuse) {\n-      if (reuse instanceof GenericRowData) {\n-        return (GenericRowData) reuse;\n+    protected Row newStructData(Row reuse) {\n+      if (reuse != null) {\n+        return reuse;\n       } else {\n-        return new GenericRowData(numFields);\n+        return new Row(structType.fields().size());\n       }\n     }\n \n     @Override\n-    protected Object getField(GenericRowData intermediate, int pos) {\n-      return intermediate.getField(pos);\n-    }\n-\n-    @Override\n-    protected RowData buildStruct(GenericRowData struct) {\n-      return struct;\n-    }\n-\n-    @Override\n-    protected void set(GenericRowData row, int pos, Object value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setNull(GenericRowData row, int pos) {\n-      row.setField(pos, null);\n+    protected Object getField(Row row, int pos) {\n+      return row.getField(pos);\n     }\n \n     @Override\n-    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n-      row.setField(pos, value);\n+    protected Row buildStruct(Row row) {\n+      return row;\n     }\n \n     @Override\n-    protected void setInteger(GenericRowData row, int pos, int value) {\n+    protected void set(Row row, int pos, Object value) {\n       row.setField(pos, value);\n     }\n-\n-    @Override\n-    protected void setLong(GenericRowData row, int pos, long value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setFloat(GenericRowData row, int pos, float value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setDouble(GenericRowData row, int pos, double value) {\n-      row.setField(pos, value);\n-    }\n-  }\n-\n-  private static class ReusableMapData implements MapData {\n-    private final ReusableArrayData keys;\n-    private final ReusableArrayData values;\n-\n-    private int numElements;\n-\n-    private ReusableMapData() {\n-      this.keys = new ReusableArrayData();\n-      this.values = new ReusableArrayData();\n-    }\n-\n-    private void grow() {\n-      keys.grow();\n-      values.grow();\n-    }\n-\n-    private int capacity() {\n-      return keys.capacity();\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-      keys.setNumElements(numElements);\n-      values.setNumElements(numElements);\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public ReusableArrayData keyArray() {\n-      return keys;\n-    }\n-\n-    @Override\n-    public ReusableArrayData valueArray() {\n-      return values;\n-    }\n-  }\n-\n-  private static class ReusableArrayData implements ArrayData {\n-    private static final Object[] EMPTY = new Object[0];\n-\n-    private Object[] values = EMPTY;\n-    private int numElements = 0;\n-\n-    private void grow() {\n-      if (values.length == 0) {\n-        this.values = new Object[20];\n-      } else {\n-        Object[] old = values;\n-        this.values = new Object[old.length << 2];\n-        // copy the old array in case it has values that can be reused\n-        System.arraycopy(old, 0, values, 0, old.length);\n-      }\n-    }\n-\n-    private int capacity() {\n-      return values.length;\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public boolean isNullAt(int ordinal) {\n-      return null == values[ordinal];\n-    }\n-\n-    @Override\n-    public boolean getBoolean(int ordinal) {\n-      return (boolean) values[ordinal];\n-    }\n-\n-    @Override\n-    public byte getByte(int ordinal) {\n-      return (byte) values[ordinal];\n-    }\n-\n-    @Override\n-    public short getShort(int ordinal) {\n-      return (short) values[ordinal];\n-    }\n-\n-    @Override\n-    public int getInt(int ordinal) {\n-      return (int) values[ordinal];\n-    }\n-\n-    @Override\n-    public long getLong(int ordinal) {\n-      return (long) values[ordinal];\n-    }\n-\n-    @Override\n-    public float getFloat(int ordinal) {\n-      return (float) values[ordinal];\n-    }\n-\n-    @Override\n-    public double getDouble(int ordinal) {\n-      return (double) values[ordinal];\n-    }\n-\n-    @Override\n-    public StringData getString(int pos) {\n-      return (StringData) values[pos];\n-    }\n-\n-    @Override\n-    public DecimalData getDecimal(int pos, int precision, int scale) {\n-      return (DecimalData) values[pos];\n-    }\n-\n-    @Override\n-    public TimestampData getTimestamp(int pos, int precision) {\n-      return (TimestampData) values[pos];\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public <T> RawValueData<T> getRawValue(int pos) {\n-      return (RawValueData<T>) values[pos];\n-    }\n-\n-    @Override\n-    public byte[] getBinary(int ordinal) {\n-      return (byte[]) values[ordinal];\n-    }\n-\n-    @Override\n-    public ArrayData getArray(int ordinal) {\n-      return (ArrayData) values[ordinal];\n-    }\n-\n-    @Override\n-    public MapData getMap(int ordinal) {\n-      return (MapData) values[ordinal];\n-    }\n-\n-    @Override\n-    public RowData getRow(int pos, int numFields) {\n-      return (RowData) values[pos];\n-    }\n-\n-    @Override\n-    public boolean[] toBooleanArray() {\n-      return ArrayUtils.toPrimitive((Boolean[]) values);\n-    }\n-\n-    @Override\n-    public byte[] toByteArray() {\n-      return ArrayUtils.toPrimitive((Byte[]) values);\n-    }\n-\n-    @Override\n-    public short[] toShortArray() {\n-      return ArrayUtils.toPrimitive((Short[]) values);\n-    }\n-\n-    @Override\n-    public int[] toIntArray() {\n-      return ArrayUtils.toPrimitive((Integer[]) values);\n-    }\n-\n-    @Override\n-    public long[] toLongArray() {\n-      return ArrayUtils.toPrimitive((Long[]) values);\n-    }\n-\n-    @Override\n-    public float[] toFloatArray() {\n-      return ArrayUtils.toPrimitive((Float[]) values);\n-    }\n-\n-    @Override\n-    public double[] toDoubleArray() {\n-      return ArrayUtils.toPrimitive((Double[]) values);\n-    }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0ODY4NQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463048685", "bodyText": "Q: is there any problem here ?  the original type is TIME_MICROS, while the reader name is TimeMillisReader ?", "author": "openinx", "createdAt": "2020-07-30T14:43:12Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM2NDI5NA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463364294", "bodyText": "This is because Flink only supports milliseconds and the parquet store microseconds, so the naming express that it reads out milliseconds.", "author": "chenjunjiedada", "createdAt": "2020-07-31T01:52:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0ODY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzgwNzQyMA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463807420", "bodyText": "I agree this is confusing. There are other places where we use a unit in the class name to indicate the unit being read. Instead, let's be more specific and use something like LossyMicrosToMillisTimeReader.", "author": "rdblue", "createdAt": "2020-07-31T20:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0ODY4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\nindex a556832e4..ccea5d652 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n\n@@ -19,723 +19,64 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n-import java.math.BigInteger;\n-import java.nio.ByteBuffer;\n-import java.time.Instant;\n import java.util.List;\n-import java.util.Map;\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.apache.flink.table.data.ArrayData;\n-import org.apache.flink.table.data.DecimalData;\n-import org.apache.flink.table.data.GenericRowData;\n-import org.apache.flink.table.data.MapData;\n-import org.apache.flink.table.data.RawValueData;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.StringData;\n-import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.types.Row;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.parquet.ParquetSchemaUtil;\n+import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n-import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.GroupType;\n-import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders {\n-  private FlinkParquetReaders() {\n-  }\n-\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n-  }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n-                                                        MessageType fileSchema,\n-                                                        Map<Integer, ?> idToConstant) {\n-    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n-    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n-    } else {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n-              new FallbackReadBuilder(builder));\n-    }\n-  }\n-\n-  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private MessageType type;\n-    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n-\n-    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n-      this.builder = builder;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      // the top level matches by ID, but the remaining IDs are missing\n-      this.type = message;\n-      return builder.struct(expected, message, fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // the expected struct is ignored because nested fields are never found when the\n-      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n-          fieldReaders.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-        types.add(fieldType);\n-      }\n-\n-      return new RowDataReader(types, newFields);\n-    }\n-  }\n-\n-  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private final MessageType type;\n-    private final Map<Integer, ?> idToConstant;\n-\n-    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n-      this.type = type;\n-      this.idToConstant = idToConstant;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      return struct(expected, message.asGroupType(), fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // match the expected struct's order\n-      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n-      Map<Integer, Type> typesById = Maps.newHashMap();\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        if (fieldType.getId() != null) {\n-          int id = fieldType.getId().intValue();\n-          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-          typesById.put(id, fieldType);\n-        }\n-      }\n-\n-      List<Types.NestedField> expectedFields = expected != null ?\n-          expected.fields() : ImmutableList.of();\n-      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n-          expectedFields.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n-      for (Types.NestedField field : expectedFields) {\n-        int id = field.fieldId();\n-        if (idToConstant.containsKey(id)) {\n-          // containsKey is used because the constant may be null\n-          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n-          types.add(null);\n-        } else {\n-          ParquetValueReader<?> reader = readersById.get(id);\n-          if (reader != null) {\n-            reorderedFields.add(reader);\n-            types.add(typesById.get(id));\n-          } else {\n-            reorderedFields.add(ParquetValueReaders.nulls());\n-            types.add(null);\n-          }\n-        }\n-      }\n-\n-      return new RowDataReader(types, reorderedFields);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n-                                      ParquetValueReader<?> elementReader) {\n-      GroupType repeated = array.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type elementType = repeated.getType(0);\n-      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n-\n-      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n-                                     ParquetValueReader<?> keyReader,\n-                                     ParquetValueReader<?> valueReader) {\n-      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type keyType = repeatedKeyValue.getType(0);\n-      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n-      Type valueType = repeatedKeyValue.getType(1);\n-      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n-\n-      return new MapReader<>(repeatedD, repeatedR,\n-          ParquetValueReaders.option(keyType, keyD, keyReader),\n-          ParquetValueReaders.option(valueType, valueD, valueReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n-                                           PrimitiveType primitive) {\n-      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n-\n-      if (primitive.getOriginalType() != null) {\n-        switch (primitive.getOriginalType()) {\n-          case ENUM:\n-          case JSON:\n-          case UTF8:\n-            return new StringReader(desc);\n-          case INT_8:\n-          case INT_16:\n-          case INT_32:\n-          case DATE:\n-            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n-              return new ParquetValueReaders.IntAsLongReader(desc);\n-            } else {\n-              return new ParquetValueReaders.UnboxedReader<>(desc);\n-            }\n-          case TIME_MICROS:\n-            return new TimeMillisReader(desc);\n-          case INT_64:\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          case TIMESTAMP_MICROS:\n-            return new TimestampMicroReader(desc);\n-          case DECIMAL:\n-            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n-            switch (primitive.getPrimitiveTypeName()) {\n-              case BINARY:\n-              case FIXED_LEN_BYTE_ARRAY:\n-                return new BinaryDecimalReader(desc, decimal.getScale());\n-              case INT64:\n-                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              case INT32:\n-                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              default:\n-                throw new UnsupportedOperationException(\n-                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n-            }\n-          case BSON:\n-            return new ParquetValueReaders.ByteArrayReader(desc);\n-          default:\n-            throw new UnsupportedOperationException(\n-                \"Unsupported logical type: \" + primitive.getOriginalType());\n-        }\n-      }\n-\n-      switch (primitive.getPrimitiveTypeName()) {\n-        case FIXED_LEN_BYTE_ARRAY:\n-        case BINARY:\n-          return new ParquetValueReaders.ByteArrayReader(desc);\n-        case INT32:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n-            return new ParquetValueReaders.IntAsLongReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case FLOAT:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n-            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case BOOLEAN:\n-        case INT64:\n-        case DOUBLE:\n-          return new ParquetValueReaders.UnboxedReader<>(desc);\n-        default:\n-          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n-      }\n-    }\n-\n-    protected MessageType type() {\n-      return type;\n-    }\n-  }\n-\n-  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int scale;\n-\n-    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n-      super(desc);\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      Binary binary = column.nextBinary();\n-      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n-      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n-    }\n-  }\n-\n-  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n-\n-    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n-    }\n-  }\n+public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n \n-  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n+  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n \n-    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n-    }\n-  }\n-\n-  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n-    TimestampMicroReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public TimestampData read(TimestampData ignored) {\n-      long value = readLong();\n-      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n-    }\n-\n-    @Override\n-    public long readLong() {\n-      return column.nextLong();\n-    }\n+  private FlinkParquetReaders() {\n   }\n \n-  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n-    StringReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public StringData read(StringData ignored) {\n-      Binary binary = column.nextBinary();\n-      ByteBuffer buffer = binary.toByteBuffer();\n-      if (buffer.hasArray()) {\n-        return StringData.fromBytes(\n-            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-      } else {\n-        return StringData.fromBytes(binary.getBytes());\n-      }\n-    }\n+  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return INSTANCE.createReader(expectedSchema, fileSchema);\n   }\n \n-  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n-    TimeMillisReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public Integer read(Integer reuse) {\n-      // Flink only supports millisecond, so we discard microseconds in millisecond\n-      return (int) column.nextLong() / 1000;\n-    }\n+  @Override\n+  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n+                                                       List<ParquetValueReader<?>> fieldReaders,\n+                                                       Types.StructType structType) {\n+    return new RowReader(types, fieldReaders, structType);\n   }\n \n-  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n-      super(definitionLevel, repetitionLevel, reader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableArrayData newListData(ArrayData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableArrayData) {\n-        return (ReusableArrayData) reuse;\n-      } else {\n-        return new ReusableArrayData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected E getElement(ReusableArrayData list) {\n-      E value = null;\n-      if (readPos < list.capacity()) {\n-        value = (E) list.values[readPos];\n-      }\n-\n-      readPos += 1;\n-\n-      return value;\n-    }\n-\n-    @Override\n-    protected void addElement(ReusableArrayData reused, E element) {\n-      if (writePos >= reused.capacity()) {\n-        reused.grow();\n-      }\n-\n-      reused.values[writePos] = element;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected ArrayData buildList(ReusableArrayData list) {\n-      list.setNumElements(writePos);\n-      return list;\n-    }\n-  }\n-\n-  private static class MapReader<K, V> extends\n-      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n-    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n-\n-    MapReader(int definitionLevel, int repetitionLevel,\n-              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n-      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableMapData newMapData(MapData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableMapData) {\n-        return (ReusableMapData) reuse;\n-      } else {\n-        return new ReusableMapData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n-      Map.Entry<K, V> kv = nullEntry;\n-      if (readPos < map.capacity()) {\n-        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n-        kv = entry;\n-      }\n-\n-      readPos += 1;\n-\n-      return kv;\n-    }\n-\n-    @Override\n-    protected void addPair(ReusableMapData map, K key, V value) {\n-      if (writePos >= map.capacity()) {\n-        map.grow();\n-      }\n+  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n+    private final Types.StructType structType;\n \n-      map.keys.values[writePos] = key;\n-      map.values.values[writePos] = value;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected MapData buildMap(ReusableMapData map) {\n-      map.setNumElements(writePos);\n-      return map;\n-    }\n-  }\n-\n-  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n-    private final int numFields;\n-\n-    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n+    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n       super(types, readers);\n-      this.numFields = readers.size();\n+      this.structType = struct;\n     }\n \n     @Override\n-    protected GenericRowData newStructData(RowData reuse) {\n-      if (reuse instanceof GenericRowData) {\n-        return (GenericRowData) reuse;\n+    protected Row newStructData(Row reuse) {\n+      if (reuse != null) {\n+        return reuse;\n       } else {\n-        return new GenericRowData(numFields);\n+        return new Row(structType.fields().size());\n       }\n     }\n \n     @Override\n-    protected Object getField(GenericRowData intermediate, int pos) {\n-      return intermediate.getField(pos);\n-    }\n-\n-    @Override\n-    protected RowData buildStruct(GenericRowData struct) {\n-      return struct;\n-    }\n-\n-    @Override\n-    protected void set(GenericRowData row, int pos, Object value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setNull(GenericRowData row, int pos) {\n-      row.setField(pos, null);\n+    protected Object getField(Row row, int pos) {\n+      return row.getField(pos);\n     }\n \n     @Override\n-    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n-      row.setField(pos, value);\n+    protected Row buildStruct(Row row) {\n+      return row;\n     }\n \n     @Override\n-    protected void setInteger(GenericRowData row, int pos, int value) {\n+    protected void set(Row row, int pos, Object value) {\n       row.setField(pos, value);\n     }\n-\n-    @Override\n-    protected void setLong(GenericRowData row, int pos, long value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setFloat(GenericRowData row, int pos, float value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setDouble(GenericRowData row, int pos, double value) {\n-      row.setField(pos, value);\n-    }\n-  }\n-\n-  private static class ReusableMapData implements MapData {\n-    private final ReusableArrayData keys;\n-    private final ReusableArrayData values;\n-\n-    private int numElements;\n-\n-    private ReusableMapData() {\n-      this.keys = new ReusableArrayData();\n-      this.values = new ReusableArrayData();\n-    }\n-\n-    private void grow() {\n-      keys.grow();\n-      values.grow();\n-    }\n-\n-    private int capacity() {\n-      return keys.capacity();\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-      keys.setNumElements(numElements);\n-      values.setNumElements(numElements);\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public ReusableArrayData keyArray() {\n-      return keys;\n-    }\n-\n-    @Override\n-    public ReusableArrayData valueArray() {\n-      return values;\n-    }\n-  }\n-\n-  private static class ReusableArrayData implements ArrayData {\n-    private static final Object[] EMPTY = new Object[0];\n-\n-    private Object[] values = EMPTY;\n-    private int numElements = 0;\n-\n-    private void grow() {\n-      if (values.length == 0) {\n-        this.values = new Object[20];\n-      } else {\n-        Object[] old = values;\n-        this.values = new Object[old.length << 2];\n-        // copy the old array in case it has values that can be reused\n-        System.arraycopy(old, 0, values, 0, old.length);\n-      }\n-    }\n-\n-    private int capacity() {\n-      return values.length;\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public boolean isNullAt(int ordinal) {\n-      return null == values[ordinal];\n-    }\n-\n-    @Override\n-    public boolean getBoolean(int ordinal) {\n-      return (boolean) values[ordinal];\n-    }\n-\n-    @Override\n-    public byte getByte(int ordinal) {\n-      return (byte) values[ordinal];\n-    }\n-\n-    @Override\n-    public short getShort(int ordinal) {\n-      return (short) values[ordinal];\n-    }\n-\n-    @Override\n-    public int getInt(int ordinal) {\n-      return (int) values[ordinal];\n-    }\n-\n-    @Override\n-    public long getLong(int ordinal) {\n-      return (long) values[ordinal];\n-    }\n-\n-    @Override\n-    public float getFloat(int ordinal) {\n-      return (float) values[ordinal];\n-    }\n-\n-    @Override\n-    public double getDouble(int ordinal) {\n-      return (double) values[ordinal];\n-    }\n-\n-    @Override\n-    public StringData getString(int pos) {\n-      return (StringData) values[pos];\n-    }\n-\n-    @Override\n-    public DecimalData getDecimal(int pos, int precision, int scale) {\n-      return (DecimalData) values[pos];\n-    }\n-\n-    @Override\n-    public TimestampData getTimestamp(int pos, int precision) {\n-      return (TimestampData) values[pos];\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public <T> RawValueData<T> getRawValue(int pos) {\n-      return (RawValueData<T>) values[pos];\n-    }\n-\n-    @Override\n-    public byte[] getBinary(int ordinal) {\n-      return (byte[]) values[ordinal];\n-    }\n-\n-    @Override\n-    public ArrayData getArray(int ordinal) {\n-      return (ArrayData) values[ordinal];\n-    }\n-\n-    @Override\n-    public MapData getMap(int ordinal) {\n-      return (MapData) values[ordinal];\n-    }\n-\n-    @Override\n-    public RowData getRow(int pos, int numFields) {\n-      return (RowData) values[pos];\n-    }\n-\n-    @Override\n-    public boolean[] toBooleanArray() {\n-      return ArrayUtils.toPrimitive((Boolean[]) values);\n-    }\n-\n-    @Override\n-    public byte[] toByteArray() {\n-      return ArrayUtils.toPrimitive((Byte[]) values);\n-    }\n-\n-    @Override\n-    public short[] toShortArray() {\n-      return ArrayUtils.toPrimitive((Short[]) values);\n-    }\n-\n-    @Override\n-    public int[] toIntArray() {\n-      return ArrayUtils.toPrimitive((Integer[]) values);\n-    }\n-\n-    @Override\n-    public long[] toLongArray() {\n-      return ArrayUtils.toPrimitive((Long[]) values);\n-    }\n-\n-    @Override\n-    public float[] toFloatArray() {\n-      return ArrayUtils.toPrimitive((Float[]) values);\n-    }\n-\n-    @Override\n-    public double[] toDoubleArray() {\n-      return ArrayUtils.toPrimitive((Double[]) values);\n-    }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1Mjg1NA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463052854", "bodyText": "Will any subclass of ReadBuilder access the message type ?", "author": "openinx", "createdAt": "2020-07-30T14:48:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,723 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+public class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+\n+    protected MessageType type() {", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM2NDkzOQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463364939", "bodyText": "Previously, the FallbackReader uses it. Now I think this could be removed since the fallback reader defines its own type . That is because we can't get the type from passing builder.", "author": "chenjunjiedada", "createdAt": "2020-07-31T01:54:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1Mjg1NA=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\nindex a556832e4..ccea5d652 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java\n\n@@ -19,723 +19,64 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n-import java.math.BigInteger;\n-import java.nio.ByteBuffer;\n-import java.time.Instant;\n import java.util.List;\n-import java.util.Map;\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.apache.flink.table.data.ArrayData;\n-import org.apache.flink.table.data.DecimalData;\n-import org.apache.flink.table.data.GenericRowData;\n-import org.apache.flink.table.data.MapData;\n-import org.apache.flink.table.data.RawValueData;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.StringData;\n-import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.types.Row;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.parquet.ParquetSchemaUtil;\n+import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n-import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.GroupType;\n-import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders {\n-  private FlinkParquetReaders() {\n-  }\n-\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n-  }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n-                                                        MessageType fileSchema,\n-                                                        Map<Integer, ?> idToConstant) {\n-    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n-    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n-    } else {\n-      return (ParquetValueReader<RowData>)\n-          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n-              new FallbackReadBuilder(builder));\n-    }\n-  }\n-\n-  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private MessageType type;\n-    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n-\n-    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n-      this.builder = builder;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      // the top level matches by ID, but the remaining IDs are missing\n-      this.type = message;\n-      return builder.struct(expected, message, fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // the expected struct is ignored because nested fields are never found when the\n-      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n-          fieldReaders.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-        types.add(fieldType);\n-      }\n-\n-      return new RowDataReader(types, newFields);\n-    }\n-  }\n-\n-  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n-    private final MessageType type;\n-    private final Map<Integer, ?> idToConstant;\n-\n-    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n-      this.type = type;\n-      this.idToConstant = idToConstant;\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n-                                         List<ParquetValueReader<?>> fieldReaders) {\n-      return struct(expected, message.asGroupType(), fieldReaders);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n-                                        List<ParquetValueReader<?>> fieldReaders) {\n-      // match the expected struct's order\n-      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n-      Map<Integer, Type> typesById = Maps.newHashMap();\n-      List<Type> fields = struct.getFields();\n-      for (int i = 0; i < fields.size(); i += 1) {\n-        Type fieldType = fields.get(i);\n-        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        if (fieldType.getId() != null) {\n-          int id = fieldType.getId().intValue();\n-          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-          typesById.put(id, fieldType);\n-        }\n-      }\n-\n-      List<Types.NestedField> expectedFields = expected != null ?\n-          expected.fields() : ImmutableList.of();\n-      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n-          expectedFields.size());\n-      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n-      for (Types.NestedField field : expectedFields) {\n-        int id = field.fieldId();\n-        if (idToConstant.containsKey(id)) {\n-          // containsKey is used because the constant may be null\n-          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n-          types.add(null);\n-        } else {\n-          ParquetValueReader<?> reader = readersById.get(id);\n-          if (reader != null) {\n-            reorderedFields.add(reader);\n-            types.add(typesById.get(id));\n-          } else {\n-            reorderedFields.add(ParquetValueReaders.nulls());\n-            types.add(null);\n-          }\n-        }\n-      }\n-\n-      return new RowDataReader(types, reorderedFields);\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n-                                      ParquetValueReader<?> elementReader) {\n-      GroupType repeated = array.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type elementType = repeated.getType(0);\n-      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n-\n-      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n-                                     ParquetValueReader<?> keyReader,\n-                                     ParquetValueReader<?> valueReader) {\n-      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n-      String[] repeatedPath = currentPath();\n-\n-      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n-      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n-\n-      Type keyType = repeatedKeyValue.getType(0);\n-      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n-      Type valueType = repeatedKeyValue.getType(1);\n-      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n-\n-      return new MapReader<>(repeatedD, repeatedR,\n-          ParquetValueReaders.option(keyType, keyD, keyReader),\n-          ParquetValueReaders.option(valueType, valueD, valueReader));\n-    }\n-\n-    @Override\n-    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n-                                           PrimitiveType primitive) {\n-      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n-\n-      if (primitive.getOriginalType() != null) {\n-        switch (primitive.getOriginalType()) {\n-          case ENUM:\n-          case JSON:\n-          case UTF8:\n-            return new StringReader(desc);\n-          case INT_8:\n-          case INT_16:\n-          case INT_32:\n-          case DATE:\n-            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n-              return new ParquetValueReaders.IntAsLongReader(desc);\n-            } else {\n-              return new ParquetValueReaders.UnboxedReader<>(desc);\n-            }\n-          case TIME_MICROS:\n-            return new TimeMillisReader(desc);\n-          case INT_64:\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          case TIMESTAMP_MICROS:\n-            return new TimestampMicroReader(desc);\n-          case DECIMAL:\n-            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n-            switch (primitive.getPrimitiveTypeName()) {\n-              case BINARY:\n-              case FIXED_LEN_BYTE_ARRAY:\n-                return new BinaryDecimalReader(desc, decimal.getScale());\n-              case INT64:\n-                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              case INT32:\n-                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n-              default:\n-                throw new UnsupportedOperationException(\n-                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n-            }\n-          case BSON:\n-            return new ParquetValueReaders.ByteArrayReader(desc);\n-          default:\n-            throw new UnsupportedOperationException(\n-                \"Unsupported logical type: \" + primitive.getOriginalType());\n-        }\n-      }\n-\n-      switch (primitive.getPrimitiveTypeName()) {\n-        case FIXED_LEN_BYTE_ARRAY:\n-        case BINARY:\n-          return new ParquetValueReaders.ByteArrayReader(desc);\n-        case INT32:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n-            return new ParquetValueReaders.IntAsLongReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case FLOAT:\n-          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n-            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n-          } else {\n-            return new ParquetValueReaders.UnboxedReader<>(desc);\n-          }\n-        case BOOLEAN:\n-        case INT64:\n-        case DOUBLE:\n-          return new ParquetValueReaders.UnboxedReader<>(desc);\n-        default:\n-          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n-      }\n-    }\n-\n-    protected MessageType type() {\n-      return type;\n-    }\n-  }\n-\n-  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int scale;\n-\n-    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n-      super(desc);\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      Binary binary = column.nextBinary();\n-      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n-      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n-    }\n-  }\n-\n-  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n-\n-    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n-    }\n-  }\n+public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n \n-  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n-    private final int precision;\n-    private final int scale;\n+  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n \n-    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n-      super(desc);\n-      this.precision = precision;\n-      this.scale = scale;\n-    }\n-\n-    @Override\n-    public DecimalData read(DecimalData ignored) {\n-      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n-    }\n-  }\n-\n-  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n-    TimestampMicroReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public TimestampData read(TimestampData ignored) {\n-      long value = readLong();\n-      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n-    }\n-\n-    @Override\n-    public long readLong() {\n-      return column.nextLong();\n-    }\n+  private FlinkParquetReaders() {\n   }\n \n-  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n-    StringReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public StringData read(StringData ignored) {\n-      Binary binary = column.nextBinary();\n-      ByteBuffer buffer = binary.toByteBuffer();\n-      if (buffer.hasArray()) {\n-        return StringData.fromBytes(\n-            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-      } else {\n-        return StringData.fromBytes(binary.getBytes());\n-      }\n-    }\n+  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return INSTANCE.createReader(expectedSchema, fileSchema);\n   }\n \n-  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n-    TimeMillisReader(ColumnDescriptor desc) {\n-      super(desc);\n-    }\n-\n-    @Override\n-    public Integer read(Integer reuse) {\n-      // Flink only supports millisecond, so we discard microseconds in millisecond\n-      return (int) column.nextLong() / 1000;\n-    }\n+  @Override\n+  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n+                                                       List<ParquetValueReader<?>> fieldReaders,\n+                                                       Types.StructType structType) {\n+    return new RowReader(types, fieldReaders, structType);\n   }\n \n-  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n-      super(definitionLevel, repetitionLevel, reader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableArrayData newListData(ArrayData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableArrayData) {\n-        return (ReusableArrayData) reuse;\n-      } else {\n-        return new ReusableArrayData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected E getElement(ReusableArrayData list) {\n-      E value = null;\n-      if (readPos < list.capacity()) {\n-        value = (E) list.values[readPos];\n-      }\n-\n-      readPos += 1;\n-\n-      return value;\n-    }\n-\n-    @Override\n-    protected void addElement(ReusableArrayData reused, E element) {\n-      if (writePos >= reused.capacity()) {\n-        reused.grow();\n-      }\n-\n-      reused.values[writePos] = element;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected ArrayData buildList(ReusableArrayData list) {\n-      list.setNumElements(writePos);\n-      return list;\n-    }\n-  }\n-\n-  private static class MapReader<K, V> extends\n-      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n-    private int readPos = 0;\n-    private int writePos = 0;\n-\n-    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n-    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n-\n-    MapReader(int definitionLevel, int repetitionLevel,\n-              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n-      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected ReusableMapData newMapData(MapData reuse) {\n-      this.readPos = 0;\n-      this.writePos = 0;\n-\n-      if (reuse instanceof ReusableMapData) {\n-        return (ReusableMapData) reuse;\n-      } else {\n-        return new ReusableMapData();\n-      }\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n-      Map.Entry<K, V> kv = nullEntry;\n-      if (readPos < map.capacity()) {\n-        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n-        kv = entry;\n-      }\n-\n-      readPos += 1;\n-\n-      return kv;\n-    }\n-\n-    @Override\n-    protected void addPair(ReusableMapData map, K key, V value) {\n-      if (writePos >= map.capacity()) {\n-        map.grow();\n-      }\n+  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n+    private final Types.StructType structType;\n \n-      map.keys.values[writePos] = key;\n-      map.values.values[writePos] = value;\n-\n-      writePos += 1;\n-    }\n-\n-    @Override\n-    protected MapData buildMap(ReusableMapData map) {\n-      map.setNumElements(writePos);\n-      return map;\n-    }\n-  }\n-\n-  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n-    private final int numFields;\n-\n-    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n+    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n       super(types, readers);\n-      this.numFields = readers.size();\n+      this.structType = struct;\n     }\n \n     @Override\n-    protected GenericRowData newStructData(RowData reuse) {\n-      if (reuse instanceof GenericRowData) {\n-        return (GenericRowData) reuse;\n+    protected Row newStructData(Row reuse) {\n+      if (reuse != null) {\n+        return reuse;\n       } else {\n-        return new GenericRowData(numFields);\n+        return new Row(structType.fields().size());\n       }\n     }\n \n     @Override\n-    protected Object getField(GenericRowData intermediate, int pos) {\n-      return intermediate.getField(pos);\n-    }\n-\n-    @Override\n-    protected RowData buildStruct(GenericRowData struct) {\n-      return struct;\n-    }\n-\n-    @Override\n-    protected void set(GenericRowData row, int pos, Object value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setNull(GenericRowData row, int pos) {\n-      row.setField(pos, null);\n+    protected Object getField(Row row, int pos) {\n+      return row.getField(pos);\n     }\n \n     @Override\n-    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n-      row.setField(pos, value);\n+    protected Row buildStruct(Row row) {\n+      return row;\n     }\n \n     @Override\n-    protected void setInteger(GenericRowData row, int pos, int value) {\n+    protected void set(Row row, int pos, Object value) {\n       row.setField(pos, value);\n     }\n-\n-    @Override\n-    protected void setLong(GenericRowData row, int pos, long value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setFloat(GenericRowData row, int pos, float value) {\n-      row.setField(pos, value);\n-    }\n-\n-    @Override\n-    protected void setDouble(GenericRowData row, int pos, double value) {\n-      row.setField(pos, value);\n-    }\n-  }\n-\n-  private static class ReusableMapData implements MapData {\n-    private final ReusableArrayData keys;\n-    private final ReusableArrayData values;\n-\n-    private int numElements;\n-\n-    private ReusableMapData() {\n-      this.keys = new ReusableArrayData();\n-      this.values = new ReusableArrayData();\n-    }\n-\n-    private void grow() {\n-      keys.grow();\n-      values.grow();\n-    }\n-\n-    private int capacity() {\n-      return keys.capacity();\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-      keys.setNumElements(numElements);\n-      values.setNumElements(numElements);\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public ReusableArrayData keyArray() {\n-      return keys;\n-    }\n-\n-    @Override\n-    public ReusableArrayData valueArray() {\n-      return values;\n-    }\n-  }\n-\n-  private static class ReusableArrayData implements ArrayData {\n-    private static final Object[] EMPTY = new Object[0];\n-\n-    private Object[] values = EMPTY;\n-    private int numElements = 0;\n-\n-    private void grow() {\n-      if (values.length == 0) {\n-        this.values = new Object[20];\n-      } else {\n-        Object[] old = values;\n-        this.values = new Object[old.length << 2];\n-        // copy the old array in case it has values that can be reused\n-        System.arraycopy(old, 0, values, 0, old.length);\n-      }\n-    }\n-\n-    private int capacity() {\n-      return values.length;\n-    }\n-\n-    public void setNumElements(int numElements) {\n-      this.numElements = numElements;\n-    }\n-\n-    @Override\n-    public int size() {\n-      return numElements;\n-    }\n-\n-    @Override\n-    public boolean isNullAt(int ordinal) {\n-      return null == values[ordinal];\n-    }\n-\n-    @Override\n-    public boolean getBoolean(int ordinal) {\n-      return (boolean) values[ordinal];\n-    }\n-\n-    @Override\n-    public byte getByte(int ordinal) {\n-      return (byte) values[ordinal];\n-    }\n-\n-    @Override\n-    public short getShort(int ordinal) {\n-      return (short) values[ordinal];\n-    }\n-\n-    @Override\n-    public int getInt(int ordinal) {\n-      return (int) values[ordinal];\n-    }\n-\n-    @Override\n-    public long getLong(int ordinal) {\n-      return (long) values[ordinal];\n-    }\n-\n-    @Override\n-    public float getFloat(int ordinal) {\n-      return (float) values[ordinal];\n-    }\n-\n-    @Override\n-    public double getDouble(int ordinal) {\n-      return (double) values[ordinal];\n-    }\n-\n-    @Override\n-    public StringData getString(int pos) {\n-      return (StringData) values[pos];\n-    }\n-\n-    @Override\n-    public DecimalData getDecimal(int pos, int precision, int scale) {\n-      return (DecimalData) values[pos];\n-    }\n-\n-    @Override\n-    public TimestampData getTimestamp(int pos, int precision) {\n-      return (TimestampData) values[pos];\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public <T> RawValueData<T> getRawValue(int pos) {\n-      return (RawValueData<T>) values[pos];\n-    }\n-\n-    @Override\n-    public byte[] getBinary(int ordinal) {\n-      return (byte[]) values[ordinal];\n-    }\n-\n-    @Override\n-    public ArrayData getArray(int ordinal) {\n-      return (ArrayData) values[ordinal];\n-    }\n-\n-    @Override\n-    public MapData getMap(int ordinal) {\n-      return (MapData) values[ordinal];\n-    }\n-\n-    @Override\n-    public RowData getRow(int pos, int numFields) {\n-      return (RowData) values[pos];\n-    }\n-\n-    @Override\n-    public boolean[] toBooleanArray() {\n-      return ArrayUtils.toPrimitive((Boolean[]) values);\n-    }\n-\n-    @Override\n-    public byte[] toByteArray() {\n-      return ArrayUtils.toPrimitive((Byte[]) values);\n-    }\n-\n-    @Override\n-    public short[] toShortArray() {\n-      return ArrayUtils.toPrimitive((Short[]) values);\n-    }\n-\n-    @Override\n-    public int[] toIntArray() {\n-      return ArrayUtils.toPrimitive((Integer[]) values);\n-    }\n-\n-    @Override\n-    public long[] toLongArray() {\n-      return ArrayUtils.toPrimitive((Long[]) values);\n-    }\n-\n-    @Override\n-    public float[] toFloatArray() {\n-      return ArrayUtils.toPrimitive((Float[]) values);\n-    }\n-\n-    @Override\n-    public double[] toDoubleArray() {\n-      return ArrayUtils.toPrimitive((Double[]) values);\n-    }\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2ODg5Mw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463068893", "bodyText": "Seems it could share the common code with RandomGenericData#generate ?  Make the RandomGenericData#generate to return a Iterable ?", "author": "openinx", "createdAt": "2020-07-30T15:10:54Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +105,187 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM3MzQyMg==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463373422", "bodyText": "You are right, let me refactor this.", "author": "chenjunjiedada", "createdAt": "2020-07-31T02:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2ODg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM4NTM1NA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463385354", "bodyText": "This method accepts a Record supplier and then generate records. We should keep it for generating fallback records and dictionary encoded records.  But for generateRecords method we can update it to call RandomGenericData#generate directly.", "author": "chenjunjiedada", "createdAt": "2020-07-31T03:22:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2ODg5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java b/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java\nindex b3ce8b642..7c1fc91bb 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java\n\n@@ -105,28 +104,6 @@ public class RandomData {\n     };\n   }\n \n-  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n-                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n-    return () -> new Iterator<Record>() {\n-      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n-      private int count = 0;\n-\n-      @Override\n-      public boolean hasNext() {\n-        return count < numRecords;\n-      }\n-\n-      @Override\n-      public Record next() {\n-        if (!hasNext()) {\n-          throw new NoSuchElementException();\n-        }\n-        ++count;\n-        return (Record) TypeUtil.visit(schema, generator);\n-      }\n-    };\n-  }\n-\n   private static Iterable<RowData> generateRowData(Schema schema, int numRecords,\n                                                    Supplier<RandomRowDataGenerator> supplier) {\n     return () -> new Iterator<RowData>() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MDEyNA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463080124", "bodyText": "The reader is named TimeMillisReader, and the writer is TimeMicrosWriter, could them be symmetrical ?", "author": "openinx", "createdAt": "2020-07-30T15:26:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM2NjI4Ng==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463366286", "bodyText": "The naming logic is what we actually perform. In the reader side, we read in the milliseconds for Flink. In the writer side, we write out microseconds for Parquet.", "author": "chenjunjiedada", "createdAt": "2020-07-31T02:00:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MDEyNA=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 47d657966..9ceac0670 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MjM0OA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463082348", "bodyText": "Seem the upper bound of precision of IntegerDecimalWriter is 9 ?  Could we add the precision <= 9  assertion ?", "author": "openinx", "createdAt": "2020-07-30T15:29:38Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM3MDMwNg==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463370306", "bodyText": "Will use the latest DecimalUtil.", "author": "chenjunjiedada", "createdAt": "2020-07-31T02:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MjM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODU4NjAzNQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r468586035", "bodyText": "Seems DecimalUtil doesn't handle this. I fixed in the new commit.", "author": "chenjunjiedada", "createdAt": "2020-08-11T13:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MjM0OA=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 47d657966..9ceac0670 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r463082888", "bodyText": "Also could we add the precision <= 18 assertion ?", "author": "openinx", "createdAt": "2020-07-30T15:30:21Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,457 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,", "originalCommit": "e78c2ecdee84c3cf0d54afa84e111faaa0e498df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE5NzIxOQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469197219", "bodyText": "How about adding this when allocating the writer? Seems like that would be a suitable place since here we are checking Flink type.", "author": "chenjunjiedada", "createdAt": "2020-08-12T11:43:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcwMzg5Nw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r473703897", "bodyText": "@openinx, any comment?", "author": "chenjunjiedada", "createdAt": "2020-08-20T07:38:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwMjUyNQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476002525", "bodyText": "I think it would be better to do this in the constructor, like @chenjunjiedada suggests. That way we have a check that precision is not larger than the maximum allowed by the type, and that the correct writer is used for the type.", "author": "rdblue", "createdAt": "2020-08-25T00:33:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4Mjg4OA=="}], "type": "inlineReview", "revised_code": {"commit": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 47d657966..9ceac0670 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -19,7 +19,6 @@\n \n package org.apache.iceberg.flink.data;\n \n-import java.math.BigDecimal;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n"}}, {"oid": "f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "url": "https://github.com/apache/iceberg/commit/f0641b4109f9215fb1c8db4748bd1a9483e4fc59", "message": "Flink: use schema visitor for flink parquet writer", "committedDate": "2020-08-11T13:30:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3ODAyMg==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469078022", "bodyText": "This should have simiar issue to the comment, which will break the unit test. If we rebase the master once  #1320 get merged, then it should have no problem.", "author": "openinx", "createdAt": "2020-08-12T08:02:09Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java", "diffHunk": "@@ -120,11 +121,12 @@ private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n     @Override\n     public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n       MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+      LogicalType logicalType = FlinkSchemaUtil.convert(schema);", "originalCommit": "940e4356566be9ca245b95c50a76f95727824b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA4NzI4MQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469087281", "bodyText": "BTW,  we may also need to add the parquet into the parameterized unit tests, such as TestIcebergStreamWriter & TestTaskWriters.", "author": "openinx", "createdAt": "2020-08-12T08:19:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3ODAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4MjA0NA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469182044", "bodyText": "Agreed,  will take a look when these PRs get in.", "author": "chenjunjiedada", "createdAt": "2020-08-12T11:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3ODAyMg=="}], "type": "inlineReview", "revised_code": {"commit": "c0ac93a391a6eef4556b72857a18cb95366b2595", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java b/flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java\nsimilarity index 62%\nrename from flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java\nrename to flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java\nindex dbc869744..f14aed220 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/RowTaskWriterFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java\n\n@@ -78,63 +81,63 @@ class RowTaskWriterFactory implements TaskWriterFactory<Row> {\n   }\n \n   @Override\n-  public TaskWriter<Row> create() {\n+  public TaskWriter<RowData> create() {\n     Preconditions.checkNotNull(outputFileFactory,\n         \"The outputFileFactory shouldn't be null if we have invoked the initialize().\");\n \n     if (spec.fields().isEmpty()) {\n       return new UnpartitionedWriter<>(spec, format, appenderFactory, outputFileFactory, io, targetFileSizeBytes);\n     } else {\n-      return new RowPartitionedFanoutWriter(spec, format, appenderFactory, outputFileFactory,\n-          io, targetFileSizeBytes, schema);\n+      return new RowDataPartitionedFanoutWriter(spec, format, appenderFactory, outputFileFactory,\n+          io, targetFileSizeBytes, schema, flinkSchema);\n     }\n   }\n \n-  private static class RowPartitionedFanoutWriter extends PartitionedFanoutWriter<Row> {\n+  private static class RowDataPartitionedFanoutWriter extends PartitionedFanoutWriter<RowData> {\n \n     private final PartitionKey partitionKey;\n-    private final RowWrapper rowWrapper;\n+    private final RowDataWrapper rowDataWrapper;\n \n-    RowPartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<Row> appenderFactory,\n-                               OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema schema) {\n+    RowDataPartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<RowData> appenderFactory,\n+                                   OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema schema,\n+                                   RowType flinkSchema) {\n       super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n       this.partitionKey = new PartitionKey(spec, schema);\n-      this.rowWrapper = new RowWrapper(schema.asStruct());\n+      this.rowDataWrapper = new RowDataWrapper(flinkSchema, schema.asStruct());\n     }\n \n     @Override\n-    protected PartitionKey partition(Row row) {\n-      partitionKey.partition(rowWrapper.wrap(row));\n+    protected PartitionKey partition(RowData row) {\n+      partitionKey.partition(rowDataWrapper.wrap(row));\n       return partitionKey;\n     }\n   }\n \n-  private static class FlinkFileAppenderFactory implements FileAppenderFactory<Row> {\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData> {\n     private final Schema schema;\n+    private final RowType flinkSchema;\n     private final Map<String, String> props;\n \n-    private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n+    private FlinkFileAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props) {\n       this.schema = schema;\n+      this.flinkSchema = flinkSchema;\n       this.props = props;\n     }\n \n     @Override\n-    public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n+    public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+      // TODO MetricsConfig will be used for building parquet RowData writer.\n       MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n-      LogicalType logicalType = FlinkSchemaUtil.convert(schema);\n       try {\n         switch (format) {\n-          case PARQUET:\n-            return Parquet.write(outputFile)\n-                .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(logicalType, msgType))\n+          case AVRO:\n+            return Avro.write(outputFile)\n+                .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n                 .setAll(props)\n-                .metricsConfig(metricsConfig)\n                 .schema(schema)\n                 .overwrite()\n                 .build();\n-\n-          case AVRO:\n-            // TODO add the Avro writer building once RowDataWrapper is ready.\n+          case PARQUET:\n           case ORC:\n           default:\n             throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA4NTA3Nw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469085077", "bodyText": "We could use RandomRowData#generate  when rebasing the patch https://github.com/apache/iceberg/pull/1320/files#diff-4b2a9fd76495497db9212d74bf03f671R33.", "author": "openinx", "createdAt": "2020-08-12T08:15:29Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +104,153 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<RowData> generateRowData(Schema schema, int numRecords,", "originalCommit": "940e4356566be9ca245b95c50a76f95727824b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4MTk1OA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469181958", "bodyText": "Will do.", "author": "chenjunjiedada", "createdAt": "2020-08-12T11:10:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA4NTA3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "a73a7d74007649bdc85197fa4d9027981745478d", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java b/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java\ndeleted file mode 100644\nindex 7c1fc91bb..000000000\n--- a/flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java\n+++ /dev/null\n\n@@ -1,311 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg.flink.data;\n-\n-import java.math.BigDecimal;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.NoSuchElementException;\n-import java.util.Random;\n-import java.util.Set;\n-import java.util.function.Supplier;\n-import org.apache.flink.table.data.DecimalData;\n-import org.apache.flink.table.data.GenericArrayData;\n-import org.apache.flink.table.data.GenericMapData;\n-import org.apache.flink.table.data.GenericRowData;\n-import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.StringData;\n-import org.apache.flink.table.data.TimestampData;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.RandomGenericData;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.types.Type;\n-import org.apache.iceberg.types.TypeUtil;\n-import org.apache.iceberg.types.Types;\n-import org.apache.iceberg.util.RandomUtil;\n-\n-import static org.apache.iceberg.types.Types.NestedField.optional;\n-import static org.apache.iceberg.types.Types.NestedField.required;\n-\n-public class RandomData {\n-  private RandomData() {\n-  }\n-\n-  static final Schema COMPLEX_SCHEMA = new Schema(\n-      required(1, \"roots\", Types.LongType.get()),\n-      optional(3, \"lime\", Types.ListType.ofRequired(4, Types.DoubleType.get())),\n-      required(5, \"strict\", Types.StructType.of(\n-          required(9, \"tangerine\", Types.StringType.get()),\n-          optional(6, \"hopeful\", Types.StructType.of(\n-              required(7, \"steel\", Types.FloatType.get()),\n-              required(8, \"lantern\", Types.DateType.get())\n-          )),\n-          optional(10, \"vehement\", Types.LongType.get())\n-      )),\n-      optional(11, \"metamorphosis\", Types.MapType.ofRequired(12, 13,\n-          Types.StringType.get(), Types.TimestampType.withZone())),\n-      required(14, \"winter\", Types.ListType.ofOptional(15, Types.StructType.of(\n-          optional(16, \"beet\", Types.DoubleType.get()),\n-          required(17, \"stamp\", Types.FloatType.get()),\n-          optional(18, \"wheeze\", Types.StringType.get())\n-      ))),\n-      optional(19, \"renovate\", Types.MapType.ofRequired(20, 21,\n-          Types.StringType.get(), Types.StructType.of(\n-              optional(22, \"jumpy\", Types.DoubleType.get()),\n-              required(23, \"koala\", Types.IntegerType.get()),\n-              required(24, \"couch rope\", Types.IntegerType.get())\n-          ))),\n-      optional(2, \"slide\", Types.StringType.get()),\n-      optional(25, \"binary\", Types.BinaryType.get()),\n-      optional(26, \"decimal\", Types.DecimalType.of(10, 2)),\n-      optional(27, \"time micro\", Types.TimeType.get()),\n-      optional(28, \"fix len binary\", Types.FixedType.ofLength(20))\n-  );\n-\n-  private static Iterable<Row> generateData(Schema schema, int numRecords, Supplier<RandomRowGenerator> supplier) {\n-    return () -> new Iterator<Row>() {\n-      private final RandomRowGenerator generator = supplier.get();\n-      private int count = 0;\n-\n-      @Override\n-      public boolean hasNext() {\n-        return count < numRecords;\n-      }\n-\n-      @Override\n-      public Row next() {\n-        if (!hasNext()) {\n-          throw new NoSuchElementException();\n-        }\n-        ++count;\n-        return (Row) TypeUtil.visit(schema, generator);\n-      }\n-    };\n-  }\n-\n-  private static Iterable<RowData> generateRowData(Schema schema, int numRecords,\n-                                                   Supplier<RandomRowDataGenerator> supplier) {\n-    return () -> new Iterator<RowData>() {\n-      private final RandomRowDataGenerator generator = supplier.get();\n-      private int count = 0;\n-\n-      @Override\n-      public boolean hasNext() {\n-        return count < numRecords;\n-      }\n-\n-      @Override\n-      public RowData next() {\n-        if (!hasNext()) {\n-          throw new NoSuchElementException();\n-        }\n-        ++count;\n-        return (RowData) TypeUtil.visit(schema, generator);\n-      }\n-    };\n-  }\n-\n-  public static Iterable<RowData> generateRowData(Schema schema, int numRecords, long seed) {\n-    return generateRowData(schema, numRecords, () -> new RandomRowDataGenerator(seed));\n-  }\n-\n-  public static Iterable<Row> generate(Schema schema, int numRecords, long seed) {\n-    return generateData(schema, numRecords, () -> new RandomRowGenerator(seed));\n-  }\n-\n-  public static Iterable<RowData> generateFallbackRowData(Schema schema, int numRecords, long seed, long numDictRows) {\n-    return generateRowData(schema, numRecords, () -> new FallbackRowDataGenerator(seed, numDictRows));\n-  }\n-\n-  public static Iterable<RowData> generateDictionaryEncodableRowData(Schema schema, int numRecords, long seed) {\n-    return generateRowData(schema, numRecords, () -> new DictionaryEncodedRowDataGenerator(seed));\n-  }\n-\n-  private static class RandomRowDataGenerator extends TypeUtil.CustomOrderSchemaVisitor<Object> {\n-    protected final Random random;\n-    private static final int MAX_ENTRIES = 20;\n-\n-    RandomRowDataGenerator(long seed) {\n-      this.random = new Random(seed);\n-    }\n-\n-    protected int getMaxEntries() {\n-      return MAX_ENTRIES;\n-    }\n-\n-    @Override\n-    public RowData schema(Schema schema, Supplier<Object> structResult) {\n-      return (RowData) structResult.get();\n-    }\n-\n-    @Override\n-    public RowData struct(Types.StructType struct, Iterable<Object> fieldResults) {\n-      GenericRowData row = new GenericRowData(struct.fields().size());\n-\n-      List<Object> values = Lists.newArrayList(fieldResults);\n-      for (int i = 0; i < values.size(); i += 1) {\n-        row.setField(i, values.get(i));\n-      }\n-\n-      return row;\n-    }\n-\n-    @Override\n-    public Object field(Types.NestedField field, Supplier<Object> fieldResult) {\n-      // return null 5% of the time when the value is optional\n-      if (field.isOptional() && random.nextInt(20) == 1) {\n-        return null;\n-      }\n-      return fieldResult.get();\n-    }\n-\n-    @Override\n-    public Object list(Types.ListType list, Supplier<Object> elementResult) {\n-      int numElements = random.nextInt(20);\n-      Object[] arr = new Object[numElements];\n-      GenericArrayData result = new GenericArrayData(arr);\n-\n-      for (int i = 0; i < numElements; i += 1) {\n-        // return null 5% of the time when the value is optional\n-        if (list.isElementOptional() && random.nextInt(20) == 1) {\n-          arr[i] = null;\n-        } else {\n-          arr[i] = elementResult.get();\n-        }\n-      }\n-\n-      return result;\n-    }\n-\n-    @Override\n-    public Object map(Types.MapType map, Supplier<Object> keyResult, Supplier<Object> valueResult) {\n-      int numEntries = random.nextInt(getMaxEntries());\n-\n-      Object[] keysArr = new Object[numEntries];\n-      Map<Object, Object> javaMap = new HashMap<>();\n-\n-      Set<Object> keySet = Sets.newHashSet();\n-      for (int i = 0; i < numEntries; i += 1) {\n-        Object key = keyResult.get();\n-        // ensure no collisions\n-        while (keySet.contains(key)) {\n-          key = keyResult.get();\n-        }\n-\n-        keySet.add(key);\n-        keysArr[i] = key;\n-\n-        if (map.isValueOptional() && random.nextInt(20) == 1) {\n-          javaMap.put(keysArr[i], null);\n-        } else {\n-          javaMap.put(keysArr[i], valueResult.get());\n-        }\n-      }\n-\n-      return new GenericMapData(javaMap);\n-    }\n-\n-    @Override\n-    public Object primitive(Type.PrimitiveType primitive) {\n-      Object obj = randomValue(primitive, random);\n-      switch (primitive.typeId()) {\n-        case STRING:\n-          return StringData.fromString((String) obj);\n-        case DECIMAL:\n-          return DecimalData.fromBigDecimal((BigDecimal) obj,\n-              ((BigDecimal) obj).precision(),\n-              ((BigDecimal) obj).scale());\n-        case TIMESTAMP:\n-          return TimestampData.fromEpochMillis((Long) obj);\n-        case TIME:\n-          return ((Long) ((Long) obj / 1000)).intValue();\n-        default:\n-          return obj;\n-      }\n-    }\n-\n-    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n-      return RandomUtil.generatePrimitive(primitive, random);\n-    }\n-  }\n-\n-  private static class RandomRowGenerator extends RandomGenericData.RandomDataGenerator<Row> {\n-    RandomRowGenerator(long seed) {\n-      super(seed);\n-    }\n-\n-    @Override\n-    public Row schema(Schema schema, Supplier<Object> structResult) {\n-      return (Row) structResult.get();\n-    }\n-\n-    @Override\n-    public Row struct(Types.StructType struct, Iterable<Object> fieldResults) {\n-      Row row = new Row(struct.fields().size());\n-\n-      List<Object> values = Lists.newArrayList(fieldResults);\n-      for (int i = 0; i < values.size(); i += 1) {\n-        row.setField(i, values.get(i));\n-      }\n-\n-      return row;\n-    }\n-  }\n-\n-  private static class DictionaryEncodedRowDataGenerator extends RandomRowDataGenerator  {\n-    DictionaryEncodedRowDataGenerator(long seed) {\n-      super(seed);\n-    }\n-\n-    @Override\n-    protected int getMaxEntries() {\n-      return 3;\n-    }\n-\n-    @Override\n-    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n-      return RandomUtil.generateDictionaryEncodablePrimitive(primitive, random);\n-    }\n-  }\n-\n-  private static class FallbackRowDataGenerator extends RandomRowDataGenerator {\n-    private final long dictionaryEncodedRows;\n-    private long rowCount = 0;\n-\n-    FallbackRowDataGenerator(long seed, long numDictionaryEncoded) {\n-      super(seed);\n-      this.dictionaryEncodedRows = numDictionaryEncoded;\n-    }\n-\n-    @Override\n-    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n-      this.rowCount += 1;\n-      if (rowCount > dictionaryEncodedRows) {\n-        return RandomUtil.generatePrimitive(primitive, rand);\n-      } else {\n-        return RandomUtil.generateDictionaryEncodablePrimitive(primitive, rand);\n-      }\n-    }\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5NTU2Ng==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469095566", "bodyText": "Seems it should be decimal.precision <= precision ?", "author": "openinx", "createdAt": "2020-08-12T08:32:09Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,", "originalCommit": "940e4356566be9ca245b95c50a76f95727824b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE5MTcyMA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469191720", "bodyText": "Seems like I misunderstood your comments, let me update this.", "author": "chenjunjiedada", "createdAt": "2020-08-12T11:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5NTU2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "09c1b41c555586c8cd2e0eafe92ffcf49ee7a72c", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 9ceac0670..0b77d0e87 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -260,7 +260,7 @@ public class FlinkParquetWriters {\n     public void write(int repetitionLevel, DecimalData decimal) {\n       Preconditions.checkArgument(decimal.scale() == scale,\n           \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-      Preconditions.checkArgument(decimal.precision() <= 9,\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n           \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n \n       column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5NTkzMA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469095930", "bodyText": "ditto", "author": "openinx", "createdAt": "2020-08-12T08:32:47Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 18,", "originalCommit": "940e4356566be9ca245b95c50a76f95727824b88", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "09c1b41c555586c8cd2e0eafe92ffcf49ee7a72c", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 9ceac0670..0b77d0e87 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -260,7 +260,7 @@ public class FlinkParquetWriters {\n     public void write(int repetitionLevel, DecimalData decimal) {\n       Preconditions.checkArgument(decimal.scale() == scale,\n           \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-      Preconditions.checkArgument(decimal.precision() <= 9,\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n           \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n \n       column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5Nzk5NA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469097994", "bodyText": "How about moving this ElementIterator to be a static class, then the map's EntryIterator could share it ?  Seems we could do it,  you could decide wether there is necessary.", "author": "openinx", "createdAt": "2020-08-12T08:36:21Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                            int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                             int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 9,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= 18,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }", "originalCommit": "940e4356566be9ca245b95c50a76f95727824b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4NjY2Mw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469186663", "bodyText": "I 'm not sure how can it be shared with EntryIterator.", "author": "chenjunjiedada", "createdAt": "2020-08-12T11:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA5Nzk5NA=="}], "type": "inlineReview", "revised_code": {"commit": "09c1b41c555586c8cd2e0eafe92ffcf49ee7a72c", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\nindex 9ceac0670..0b77d0e87 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java\n\n@@ -260,7 +260,7 @@ public class FlinkParquetWriters {\n     public void write(int repetitionLevel, DecimalData decimal) {\n       Preconditions.checkArgument(decimal.scale() == scale,\n           \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-      Preconditions.checkArgument(decimal.precision() <= 9,\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n           \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n \n       column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTMzMg==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469101332", "bodyText": "TODO: we could share both flink and spark ParquetSchemaVisitor in a common class , can be a separate issue.", "author": "openinx", "createdAt": "2020-08-12T08:42:11Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class ParquetWithFlinkSchemaVisitor<T> {", "originalCommit": "940e4356566be9ca245b95c50a76f95727824b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTE4NDM2Mw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r469184363", "bodyText": "Agreed, I would prefer to do the refactor in a separated PR.", "author": "chenjunjiedada", "createdAt": "2020-08-12T11:16:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTMzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwOTY5NQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476009695", "bodyText": "Yes, a WithPartner visitor like @JingsongLi added would be great.", "author": "rdblue", "createdAt": "2020-08-25T00:44:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTMzMg=="}], "type": "inlineReview", "revised_code": null}, {"oid": "09c1b41c555586c8cd2e0eafe92ffcf49ee7a72c", "url": "https://github.com/apache/iceberg/commit/09c1b41c555586c8cd2e0eafe92ffcf49ee7a72c", "message": "fix checkstyle", "committedDate": "2020-08-12T11:30:39Z", "type": "forcePushed"}, {"oid": "c0ac93a391a6eef4556b72857a18cb95366b2595", "url": "https://github.com/apache/iceberg/commit/c0ac93a391a6eef4556b72857a18cb95366b2595", "message": "Flink: use schema visitor for flink parquet writer", "committedDate": "2020-08-20T03:36:36Z", "type": "commit"}, {"oid": "a73a7d74007649bdc85197fa4d9027981745478d", "url": "https://github.com/apache/iceberg/commit/a73a7d74007649bdc85197fa4d9027981745478d", "message": "fix checkstyle", "committedDate": "2020-08-20T06:16:35Z", "type": "commit"}, {"oid": "a73a7d74007649bdc85197fa4d9027981745478d", "url": "https://github.com/apache/iceberg/commit/a73a7d74007649bdc85197fa4d9027981745478d", "message": "fix checkstyle", "committedDate": "2020-08-20T06:16:35Z", "type": "forcePushed"}, {"oid": "fabf81d94409b7dd9c51c6fcc8c93dd2b9e388b3", "url": "https://github.com/apache/iceberg/commit/fabf81d94409b7dd9c51c6fcc8c93dd2b9e388b3", "message": "minor fixes", "committedDate": "2020-08-20T07:33:10Z", "type": "commit"}, {"oid": "7aebde07958ac757cc4f4bf2ae3204787dd9c605", "url": "https://github.com/apache/iceberg/commit/7aebde07958ac757cc4f4bf2ae3204787dd9c605", "message": "fix RowDataConverter", "committedDate": "2020-08-20T08:46:15Z", "type": "commit"}, {"oid": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "url": "https://github.com/apache/iceberg/commit/caa4ca34dc48e334bc213c2117221d75fd597e8b", "message": "Merge branch 'master' into use-schema-visitor-for-flink-writer", "committedDate": "2020-08-21T01:14:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTk5ODA3MA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r475998070", "bodyText": "Nit: s in sType indicates Spark. The equivalent here would be fType or a better name.", "author": "rdblue", "createdAt": "2020-08-25T00:26:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {", "originalCommit": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwMTcxNw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476001717", "bodyText": "This conversion from Integer doesn't make much sense. Java exposes 2 valueOf with string arguments and one with a primitive long argument. The last is what is called here. In that case, this is implicitly casting Integer to long, boxing the result, and then multiplying to produce a primitive.\nIt would be better to use value.longValue() * 1000 instead.", "author": "rdblue", "createdAt": "2020-08-25T00:32:19Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;", "originalCommit": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTAxMw==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476005013", "bodyText": "This method is called in a tight loop, so for performance any preparation that can be done in advance should be.\nThat means this getter should be created in the constructor and stored as an instance field. Then it can be called here.\nAlso, if there is already a null check above, does this need to call getElementOrNull or should it just call a get variant that assumes the value is non-null?\nAlternatively, you could replace the if here:\nE element = (E) getter.getElementOrNull(list, index);", "author": "rdblue", "createdAt": "2020-08-25T00:37:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);", "originalCommit": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1MDU5OQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476050599", "bodyText": "That means this getter should be created in the constructor and stored as an instance field. Then it can be called here.\n\nYeah,  that sounds good to me, great point.\n\ndoes this need to call getElementOrNull or should it just call a get variant that assumes the value is non-null?\nThe getter in ArrayData don't have a get  interface,  it have only the interface:\n\n\t/**\n\t * Accessor for getting the elements of an array during runtime.\n\t *\n\t * @see #createElementGetter(LogicalType)\n\t */\n\tinterface ElementGetter extends Serializable {\n\t\t@Nullable Object getElementOrNull(ArrayData array, int pos);\n\t}\nReplacing the if-else to be E element = (E) getter.getElementOrNull(list, index);  sounds reasonable to me.", "author": "openinx", "createdAt": "2020-08-25T01:46:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTAxMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTQ5OA==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476005498", "bodyText": "Same here. The getters for keys and values should be instance fields.", "author": "rdblue", "createdAt": "2020-08-25T00:38:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueWriters.StructWriter<Row> createStructWriter(List<ParquetValueWriter<?>> writers) {\n-    return new RowWriter(writers);\n+  private static class MapDataWriter<K, V> extends ParquetValueWriters.RepeatedKeyValueWriter<MapData, K, V> {\n+    private final LogicalType keyType;\n+    private final LogicalType valueType;\n+\n+    private MapDataWriter(int definitionLevel, int repetitionLevel,\n+                          ParquetValueWriter<K> keyWriter, ParquetValueWriter<V> valueWriter,\n+                          LogicalType keyType, LogicalType valueType) {\n+      super(definitionLevel, repetitionLevel, keyWriter, valueWriter);\n+      this.keyType = keyType;\n+      this.valueType = valueType;\n+    }\n+\n+    @Override\n+    protected Iterator<Map.Entry<K, V>> pairs(MapData map) {\n+      return new EntryIterator<>(map);\n+    }\n+\n+    private class EntryIterator<K, V> implements Iterator<Map.Entry<K, V>> {\n+      private final int size;\n+      private final ArrayData keys;\n+      private final ArrayData values;\n+      private final ParquetValueReaders.ReusableEntry<K, V> entry;\n+      private int index;\n+\n+      private EntryIterator(MapData map) {\n+        size = map.size();\n+        keys = map.keyArray();\n+        values = map.valueArray();\n+        entry = new ParquetValueReaders.ReusableEntry<>();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public Map.Entry<K, V> next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        if (values.isNullAt(index)) {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index), null);", "originalCommit": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTg2NQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476005865", "bodyText": "Keys are not allowed to be null, so there should be no need to call getElementOrNull for the key.", "author": "rdblue", "createdAt": "2020-08-25T00:38:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNTQ5OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwNjE1OQ==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476006159", "bodyText": "Each getter should be stored as a field in an array.", "author": "rdblue", "createdAt": "2020-08-25T00:38:57Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java", "diffHunk": "@@ -19,38 +19,436 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.util.Iterator;\n import java.util.List;\n-import org.apache.flink.types.Row;\n-import org.apache.iceberg.data.parquet.BaseParquetWriter;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.iceberg.parquet.ParquetValueReaders;\n import org.apache.iceberg.parquet.ParquetValueWriter;\n import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.DecimalUtil;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetWriters extends BaseParquetWriter<Row> {\n+public class FlinkParquetWriters {\n+  private FlinkParquetWriters() {\n+  }\n \n-  private static final FlinkParquetWriters INSTANCE = new FlinkParquetWriters();\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> ParquetValueWriter<T> buildWriter(LogicalType schema, MessageType type) {\n+    return (ParquetValueWriter<T>) ParquetWithFlinkSchemaVisitor.visit(schema, type, new WriteBuilder(type));\n+  }\n \n-  private FlinkParquetWriters() {\n+  private static class WriteBuilder extends ParquetWithFlinkSchemaVisitor<ParquetValueWriter<?>> {\n+    private final MessageType type;\n+\n+    WriteBuilder(MessageType type) {\n+      this.type = type;\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> message(RowType sStruct, MessageType message, List<ParquetValueWriter<?>> fields) {\n+      return struct(sStruct, message.asGroupType(), fields);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> struct(RowType sStruct, GroupType struct,\n+                                        List<ParquetValueWriter<?>> fieldWriters) {\n+      List<Type> fields = struct.getFields();\n+      List<RowField> flinkFields = sStruct.getFields();\n+      List<ParquetValueWriter<?>> writers = Lists.newArrayListWithExpectedSize(fieldWriters.size());\n+      List<LogicalType> flinkTypes = Lists.newArrayList();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        writers.add(newOption(struct.getType(i), fieldWriters.get(i)));\n+        flinkTypes.add(flinkFields.get(i).getType());\n+      }\n+\n+      return new RowDataWriter(writers, flinkTypes);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> list(ArrayType sArray, GroupType array, ParquetValueWriter<?> elementWriter) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new ArrayDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeated.getType(0), elementWriter),\n+          sArray.getElementType());\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> map(MapType sMap, GroupType map,\n+                                     ParquetValueWriter<?> keyWriter, ParquetValueWriter<?> valueWriter) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath);\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath);\n+\n+      return new MapDataWriter<>(repeatedD, repeatedR,\n+          newOption(repeatedKeyValue.getType(0), keyWriter),\n+          newOption(repeatedKeyValue.getType(1), valueWriter),\n+          sMap.getKeyType(), sMap.getValueType());\n+    }\n+\n+\n+    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {\n+      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));\n+      return ParquetValueWriters.option(fieldType, maxD, writer);\n+    }\n+\n+    @Override\n+    public ParquetValueWriter<?> primitive(LogicalType sType, PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return strings(desc);\n+          case DATE:\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            return ints(sType, desc);\n+          case INT_64:\n+            return ParquetValueWriters.longs(desc);\n+          case TIME_MICROS:\n+            return timeMicros(desc);\n+          case TIMESTAMP_MICROS:\n+            return timestamps(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case INT32:\n+                return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return decimalAsLong(desc, decimal.getPrecision(), decimal.getScale());\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return decimalAsFixed(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return byteArrays(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return byteArrays(desc);\n+        case BOOLEAN:\n+          return ParquetValueWriters.booleans(desc);\n+        case INT32:\n+          return ints(sType, desc);\n+        case INT64:\n+          return ParquetValueWriters.longs(desc);\n+        case FLOAT:\n+          return ParquetValueWriters.floats(desc);\n+        case DOUBLE:\n+          return ParquetValueWriters.doubles(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<?> ints(LogicalType type, ColumnDescriptor desc) {\n+    if (type instanceof TinyIntType) {\n+      return ParquetValueWriters.tinyints(desc);\n+    } else if (type instanceof SmallIntType) {\n+      return ParquetValueWriters.shorts(desc);\n+    }\n+    return ParquetValueWriters.ints(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<StringData> strings(ColumnDescriptor desc) {\n+    return new StringDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<Integer> timeMicros(ColumnDescriptor desc) {\n+    return new TimeMicrosWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsInteger(ColumnDescriptor desc,\n+                                                                                   int precision, int scale) {\n+    return new IntegerDecimalWriter(desc, precision, scale);\n+  }\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsLong(ColumnDescriptor desc,\n+                                                                                int precision, int scale) {\n+    return new LongDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<DecimalData> decimalAsFixed(ColumnDescriptor desc,\n+                                                                                 int precision, int scale) {\n+    return new FixedDecimalWriter(desc, precision, scale);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<TimestampData> timestamps(ColumnDescriptor desc) {\n+    return new TimestampDataWriter(desc);\n+  }\n+\n+  private static ParquetValueWriters.PrimitiveWriter<byte[]> byteArrays(ColumnDescriptor desc) {\n+    return new ByteArrayWriter(desc);\n+  }\n+\n+  private static class StringDataWriter extends ParquetValueWriters.PrimitiveWriter<StringData> {\n+    private StringDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, StringData value) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(value.toBytes()));\n+    }\n+  }\n+\n+  private static class TimeMicrosWriter extends ParquetValueWriters.PrimitiveWriter<Integer> {\n+    private TimeMicrosWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, Integer value) {\n+      long micros = Long.valueOf(value) * 1000;\n+      column.writeLong(repetitionLevel, micros);\n+    }\n+  }\n+\n+  private static class IntegerDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private IntegerDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeInteger(repetitionLevel, (int) decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class LongDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    private LongDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      Preconditions.checkArgument(decimal.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+      Preconditions.checkArgument(decimal.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, decimal);\n+\n+      column.writeLong(repetitionLevel, decimal.toUnscaledLong());\n+    }\n+  }\n+\n+  private static class FixedDecimalWriter extends ParquetValueWriters.PrimitiveWriter<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+    private final ThreadLocal<byte[]> bytes;\n+\n+    private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+      this.bytes = ThreadLocal.withInitial(() -> new byte[TypeUtil.decimalRequiredBytes(precision)]);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, DecimalData decimal) {\n+      byte[] binary = DecimalUtil.toReusedFixLengthBytes(precision, scale, decimal.toBigDecimal(), bytes.get());\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(binary));\n+    }\n+  }\n+\n+  private static class TimestampDataWriter extends ParquetValueWriters.PrimitiveWriter<TimestampData> {\n+    private TimestampDataWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, TimestampData value) {\n+      column.writeLong(repetitionLevel, value.getMillisecond() * 1000 + value.getNanoOfMillisecond() / 1000);\n+    }\n+  }\n+\n+  private static class ByteArrayWriter extends ParquetValueWriters.PrimitiveWriter<byte[]> {\n+    private ByteArrayWriter(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public void write(int repetitionLevel, byte[] bytes) {\n+      column.writeBinary(repetitionLevel, Binary.fromReusedByteArray(bytes));\n+    }\n   }\n \n-  public static ParquetValueWriter<Row> buildWriter(MessageType type) {\n-    return INSTANCE.createWriter(type);\n+  private static class ArrayDataWriter<E> extends ParquetValueWriters.RepeatedWriter<ArrayData, E> {\n+    private final LogicalType elementType;\n+\n+    private ArrayDataWriter(int definitionLevel, int repetitionLevel,\n+                            ParquetValueWriter<E> writer, LogicalType elementType) {\n+      super(definitionLevel, repetitionLevel, writer);\n+      this.elementType = elementType;\n+    }\n+\n+    @Override\n+    protected Iterator<E> elements(ArrayData list) {\n+      return new ElementIterator<>(list);\n+    }\n+\n+    private class ElementIterator<E> implements Iterator<E> {\n+      private final int size;\n+      private final ArrayData list;\n+      private int index;\n+\n+      private ElementIterator(ArrayData list) {\n+        this.list = list;\n+        size = list.size();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public E next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        E element;\n+        if (list.isNullAt(index)) {\n+          element = null;\n+        } else {\n+          element = (E) ArrayData.createElementGetter(elementType).getElementOrNull(list, index);\n+        }\n+\n+        index += 1;\n+\n+        return element;\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueWriters.StructWriter<Row> createStructWriter(List<ParquetValueWriter<?>> writers) {\n-    return new RowWriter(writers);\n+  private static class MapDataWriter<K, V> extends ParquetValueWriters.RepeatedKeyValueWriter<MapData, K, V> {\n+    private final LogicalType keyType;\n+    private final LogicalType valueType;\n+\n+    private MapDataWriter(int definitionLevel, int repetitionLevel,\n+                          ParquetValueWriter<K> keyWriter, ParquetValueWriter<V> valueWriter,\n+                          LogicalType keyType, LogicalType valueType) {\n+      super(definitionLevel, repetitionLevel, keyWriter, valueWriter);\n+      this.keyType = keyType;\n+      this.valueType = valueType;\n+    }\n+\n+    @Override\n+    protected Iterator<Map.Entry<K, V>> pairs(MapData map) {\n+      return new EntryIterator<>(map);\n+    }\n+\n+    private class EntryIterator<K, V> implements Iterator<Map.Entry<K, V>> {\n+      private final int size;\n+      private final ArrayData keys;\n+      private final ArrayData values;\n+      private final ParquetValueReaders.ReusableEntry<K, V> entry;\n+      private int index;\n+\n+      private EntryIterator(MapData map) {\n+        size = map.size();\n+        keys = map.keyArray();\n+        values = map.valueArray();\n+        entry = new ParquetValueReaders.ReusableEntry<>();\n+        index = 0;\n+      }\n+\n+      @Override\n+      public boolean hasNext() {\n+        return index != size;\n+      }\n+\n+      @Override\n+      @SuppressWarnings(\"unchecked\")\n+      public Map.Entry<K, V> next() {\n+        if (index >= size) {\n+          throw new NoSuchElementException();\n+        }\n+\n+        if (values.isNullAt(index)) {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index), null);\n+        } else {\n+          entry.set((K) ArrayData.createElementGetter(keyType).getElementOrNull(keys, index),\n+              (V) ArrayData.createElementGetter(valueType).getElementOrNull(values, index));\n+        }\n+\n+        index += 1;\n+\n+        return entry;\n+      }\n+    }\n   }\n \n-  private static class RowWriter extends ParquetValueWriters.StructWriter<Row> {\n+  private static class RowDataWriter extends ParquetValueWriters.StructWriter<RowData> {\n+    private final List<LogicalType> types;\n \n-    private RowWriter(List<ParquetValueWriter<?>> writers) {\n+    RowDataWriter(List<ParquetValueWriter<?>> writers, List<LogicalType> types) {\n       super(writers);\n+      this.types = types;\n     }\n \n     @Override\n-    protected Object get(Row row, int index) {\n-      return row.getField(index);\n+    protected Object get(RowData struct, int index) {\n+      return RowData.createFieldGetter(types.get(index), index).getFieldOrNull(struct);", "originalCommit": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAwODY5Ng==", "url": "https://github.com/apache/iceberg/pull/1272#discussion_r476008696", "bodyText": "Thanks for fixing these.", "author": "rdblue", "createdAt": "2020-08-25T00:42:36Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -66,15 +65,12 @@ public static void assertRowData(Types.StructType structType, LogicalType rowTyp\n     for (int i = 0; i < types.size(); i += 1) {\n       Object expected = expectedRecord.get(i);\n       LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n-\n-      final int fieldPos = i;\n       assertEquals(types.get(i), logicalType, expected,\n-          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+          RowData.createFieldGetter(logicalType, i).getFieldOrNull(actualRowData));", "originalCommit": "caa4ca34dc48e334bc213c2117221d75fd597e8b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}