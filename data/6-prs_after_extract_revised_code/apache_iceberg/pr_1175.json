{"pr_number": 1175, "pr_title": "Flink: Add wrapper to adapt Row to StructLike", "pr_createdAt": "2020-07-07T07:56:28Z", "pr_url": "https://github.com/apache/iceberg/pull/1175", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDgyNzcxOQ==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r450827719", "bodyText": "It's quite confuse for me to see that the above else-if use accessor.getClass() == PositionAccessor.class while this line use accessor instanceof Position2Accessor.  I see that there's only one Position2Accessor implementation, so changing it from instanceof  to = should be OK here.", "author": "openinx", "createdAt": "2020-07-07T12:31:27Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/PartitionKey.java", "diffHunk": "@@ -176,61 +70,22 @@ public int hashCode() {\n     }\n   }\n \n-  private static Accessor<InternalRow> newAccessor(int position, boolean isOptional, Types.StructType type,\n-                                                   Accessor<InternalRow> accessor) {\n+  @Override\n+  protected Accessor<InternalRow> newAccessor(int position, boolean isOptional, Types.StructType type,\n+                                              Accessor<InternalRow> accessor) {\n     int size = type.fields().size();\n     if (isOptional) {\n       // the wrapped position handles null layers\n       return new WrappedPositionAccessor(position, size, accessor);\n     } else if (accessor.getClass() == PositionAccessor.class) {\n       return new Position2Accessor(position, size, (PositionAccessor) accessor);\n-    } else if (accessor instanceof Position2Accessor) {\n+    } else if (accessor.getClass() == Position2Accessor.class) {", "originalCommit": "37dc2f3cad2f3b9ab7e1feb5bd911192b9f612cf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA3MzY3MQ==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r453073671", "bodyText": "Yes, this should be fine and is also more correct. Originally, both used instanceof, but there was a bug when the behavior of PositionAccessor was overridden. Replacing a PositionAccessor subclass with a Position2Accessor removed the subclass's behavior. It is only safe to replace a PositionAccessor, not a subclass.", "author": "rdblue", "createdAt": "2020-07-10T20:57:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDgyNzcxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "72730aedf7cc645afbe18ed1a0ce1d1bbc101c26", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/source/PartitionKey.java b/spark/src/main/java/org/apache/iceberg/spark/source/PartitionKey.java\ndeleted file mode 100644\nindex 2fdbdd08..00000000\n--- a/spark/src/main/java/org/apache/iceberg/spark/source/PartitionKey.java\n+++ /dev/null\n\n@@ -1,219 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg.spark.source;\n-\n-import java.nio.ByteBuffer;\n-import java.util.Arrays;\n-import org.apache.iceberg.BasePartitionKey;\n-import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.iceberg.types.Type;\n-import org.apache.iceberg.types.Types;\n-import org.apache.spark.sql.catalyst.InternalRow;\n-import org.apache.spark.sql.types.DataType;\n-import org.apache.spark.sql.types.Decimal;\n-import org.apache.spark.unsafe.types.UTF8String;\n-\n-class PartitionKey extends BasePartitionKey<InternalRow> {\n-\n-  PartitionKey(PartitionSpec spec, Schema inputSchema) {\n-    super(spec, inputSchema);\n-  }\n-\n-  private PartitionKey(PartitionKey toCopy) {\n-    super(toCopy);\n-  }\n-\n-  @Override\n-  protected Object defensiveCopyIfNeeded(Object obj) {\n-    if (obj instanceof UTF8String) {\n-      // bytes backing the UTF8 string might be reused\n-      byte[] bytes = ((UTF8String) obj).getBytes();\n-      return UTF8String.fromBytes(Arrays.copyOf(bytes, bytes.length));\n-    }\n-    return obj;\n-  }\n-\n-  PartitionKey copy() {\n-    return new PartitionKey(this);\n-  }\n-\n-  @Override\n-  protected Accessor<InternalRow> newAccessor(int position, Type type) {\n-    switch (type.typeId()) {\n-      case STRING:\n-        return new StringAccessor(position, SparkSchemaUtil.convert(type));\n-      case DECIMAL:\n-        return new DecimalAccessor(position, SparkSchemaUtil.convert(type));\n-      case BINARY:\n-        return new BytesAccessor(position, SparkSchemaUtil.convert(type));\n-      default:\n-        return new PositionAccessor(position, SparkSchemaUtil.convert(type));\n-    }\n-  }\n-\n-  @Override\n-  protected Accessor<InternalRow> newAccessor(int position, boolean isOptional, Types.StructType type,\n-                                              Accessor<InternalRow> accessor) {\n-    int size = type.fields().size();\n-    if (isOptional) {\n-      // the wrapped position handles null layers\n-      return new WrappedPositionAccessor(position, size, accessor);\n-    } else if (accessor.getClass() == PositionAccessor.class) {\n-      return new Position2Accessor(position, size, (PositionAccessor) accessor);\n-    } else if (accessor.getClass() == Position2Accessor.class) {\n-      return new Position3Accessor(position, size, (Position2Accessor) accessor);\n-    } else {\n-      return new WrappedPositionAccessor(position, size, accessor);\n-    }\n-  }\n-\n-  private static class PositionAccessor implements Accessor<InternalRow> {\n-    private final DataType type;\n-    private int position;\n-\n-    private PositionAccessor(int position, DataType type) {\n-      this.position = position;\n-      this.type = type;\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      if (row.isNullAt(position)) {\n-        return null;\n-      }\n-      return row.get(position, type);\n-    }\n-\n-    DataType type() {\n-      return type;\n-    }\n-\n-    int position() {\n-      return position;\n-    }\n-  }\n-\n-  private static class StringAccessor extends PositionAccessor {\n-    private StringAccessor(int position, DataType type) {\n-      super(position, type);\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      if (row.isNullAt(position())) {\n-        return null;\n-      }\n-      return row.get(position(), type()).toString();\n-    }\n-  }\n-\n-  private static class DecimalAccessor extends PositionAccessor {\n-    private DecimalAccessor(int position, DataType type) {\n-      super(position, type);\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      if (row.isNullAt(position())) {\n-        return null;\n-      }\n-      return ((Decimal) row.get(position(), type())).toJavaBigDecimal();\n-    }\n-  }\n-\n-  private static class BytesAccessor extends PositionAccessor {\n-    private BytesAccessor(int position, DataType type) {\n-      super(position, type);\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      if (row.isNullAt(position())) {\n-        return null;\n-      }\n-      return ByteBuffer.wrap((byte[]) row.get(position(), type()));\n-    }\n-  }\n-\n-  private static class Position2Accessor implements Accessor<InternalRow> {\n-    private final int p0;\n-    private final int size0;\n-    private final int p1;\n-    private final DataType type;\n-\n-    private Position2Accessor(int position, int size, PositionAccessor wrapped) {\n-      this.p0 = position;\n-      this.size0 = size;\n-      this.p1 = wrapped.position;\n-      this.type = wrapped.type;\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      return row.getStruct(p0, size0).get(p1, type);\n-    }\n-  }\n-\n-  private static class Position3Accessor implements Accessor<InternalRow> {\n-    private final int p0;\n-    private final int size0;\n-    private final int p1;\n-    private final int size1;\n-    private final int p2;\n-    private final DataType type;\n-\n-    private Position3Accessor(int position, int size, Position2Accessor wrapped) {\n-      this.p0 = position;\n-      this.size0 = size;\n-      this.p1 = wrapped.p0;\n-      this.size1 = wrapped.size0;\n-      this.p2 = wrapped.p1;\n-      this.type = wrapped.type;\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      return row.getStruct(p0, size0).getStruct(p1, size1).get(p2, type);\n-    }\n-  }\n-\n-  private static class WrappedPositionAccessor implements Accessor<InternalRow> {\n-    private final int position;\n-    private final int size;\n-    private final Accessor<InternalRow> accessor;\n-\n-    private WrappedPositionAccessor(int position, int size, Accessor<InternalRow> accessor) {\n-      this.position = position;\n-      this.size = size;\n-      this.accessor = accessor;\n-    }\n-\n-    @Override\n-    public Object get(InternalRow row) {\n-      InternalRow inner = row.getStruct(position, size);\n-      if (inner != null) {\n-        return accessor.get(inner);\n-      }\n-      return null;\n-    }\n-  }\n-}\n"}}, {"oid": "72730aedf7cc645afbe18ed1a0ce1d1bbc101c26", "url": "https://github.com/apache/iceberg/commit/72730aedf7cc645afbe18ed1a0ce1d1bbc101c26", "message": "Flink: add flink row PartitionKey.", "committedDate": "2020-07-14T07:52:42Z", "type": "commit"}, {"oid": "72730aedf7cc645afbe18ed1a0ce1d1bbc101c26", "url": "https://github.com/apache/iceberg/commit/72730aedf7cc645afbe18ed1a0ce1d1bbc101c26", "message": "Flink: add flink row PartitionKey.", "committedDate": "2020-07-14T07:52:42Z", "type": "forcePushed"}, {"oid": "4bc4b84d01ab72ad8aca46cf5b566bbe6070b115", "url": "https://github.com/apache/iceberg/commit/4bc4b84d01ab72ad8aca46cf5b566bbe6070b115", "message": "Fix the checkstyle.", "committedDate": "2020-07-14T08:05:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUwOTA2NA==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r454509064", "bodyText": "The objects returned by this wrapper need to be Iceberg's internal representation:\n\nint for DateType: number of days from epoch\nlong for TimeType: number of microseconds from midnight\nlong for both TimestampType: number of microseconds from epoch\nByteBuffer for both fixed(L) and binary types\nBigDecimal for decimal(P, S)\n\nBecause we Flink uses the same in-memory representation as Iceberg generics, this should use the same conversions that we use for Record.", "author": "rdblue", "createdAt": "2020-07-14T17:06:47Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowWrapper.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.lang.reflect.Array;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class RowWrapper implements StructLike {\n+\n+  private final Type[] types;\n+  private final PositionalGetter[] getters;\n+  private Row row = null;\n+\n+  public RowWrapper(Types.StructType type) {\n+    int size = type.fields().size();\n+\n+    types = (Type[]) Array.newInstance(Type.class, size);\n+    for (int i = 0; i < size; i++) {\n+      types[i] = type.fields().get(i).type();\n+    }\n+\n+    getters = (PositionalGetter[]) Array.newInstance(PositionalGetter.class, size);\n+    for (int i = 0; i < size; i++) {\n+      getters[i] = buildGetter(types[i]);\n+    }\n+  }\n+\n+  RowWrapper wrap(Row data) {\n+    this.row = data;\n+    return this;\n+  }\n+\n+  @Override\n+  public int size() {\n+    return types.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    if (row.getField(pos) == null) {\n+      return null;\n+    } else if (getters[pos] != null) {\n+      return javaClass.cast(getters[pos].get(row, pos));\n+    }\n+\n+    return javaClass.cast(row.getField(pos));\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    row.setField(pos, value);\n+  }\n+\n+  private interface PositionalGetter<T> {\n+    T get(Row row, int pos);\n+  }\n+\n+  private static PositionalGetter buildGetter(Type type) {\n+    if (type instanceof Types.StructType) {", "originalCommit": "4bc4b84d01ab72ad8aca46cf5b566bbe6070b115", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDc3NDc1Ng==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r454774756", "bodyText": "Thanks for the details, we discussed about this thing in here, maybe you want to take a look :-) .", "author": "openinx", "createdAt": "2020-07-15T03:53:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUwOTA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI3OTA4NA==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r455279084", "bodyText": "Yes, this needs to convert to the representation that internal classes use.\nIceberg's generic data model is intended for passing data to and from Java applications, which is why they use friendlier classes. It is up to data models like Iceberg generics or Flink's data model to convert to that representation. Iceberg core should modify data as little as possible.", "author": "rdblue", "createdAt": "2020-07-15T19:07:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUwOTA2NA=="}], "type": "inlineReview", "revised_code": {"commit": "397acce20e488cbe70d49e007e34b4a1da7d600e", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/RowWrapper.java b/flink/src/main/java/org/apache/iceberg/flink/RowWrapper.java\nindex 3d75bc22..490c1b27 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/RowWrapper.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/RowWrapper.java\n\n@@ -20,10 +20,16 @@\n package org.apache.iceberg.flink;\n \n import java.lang.reflect.Array;\n+import java.nio.ByteBuffer;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.StructLike;\n import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n \n public class RowWrapper implements StructLike {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUxMTczMA==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r454511730", "bodyText": "Can you add a test based on Spark's TestPartitionValues? That tests every supported type, null values, and different column orders.", "author": "rdblue", "createdAt": "2020-07-14T17:11:15Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestPartitionKey {", "originalCommit": "4bc4b84d01ab72ad8aca46cf5b566bbe6070b115", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "397acce20e488cbe70d49e007e34b4a1da7d600e", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java b/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java\nindex fc1dd3a6..fde65af6 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java\n\n@@ -19,13 +19,19 @@\n \n package org.apache.iceberg.flink;\n \n+import java.nio.ByteBuffer;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.List;\n import org.apache.flink.types.Row;\n-import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.exceptions.ValidationException;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.flink.data.RandomData;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.DateTimeUtil;\n import org.junit.Assert;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUxMjUyMw==", "url": "https://github.com/apache/iceberg/pull/1175#discussion_r454512523", "bodyText": "Can you split each of these blocks into a separate test case? There are lots of different cases mixed together in this method. Mixing cases together makes it harder to see what is broken when tests fail because you don't get a picture of what is common across failed cases since many of them don't run.", "author": "rdblue", "createdAt": "2020-07-14T17:12:34Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestPartitionKey {\n+\n+  @Test\n+  public void testSimplePartition() {\n+    Schema schema = new Schema(\n+        Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+        Types.NestedField.optional(2, \"data\", Types.StringType.get()),\n+        Types.NestedField.optional(3, \"address\", Types.StringType.get())\n+    );\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(schema)\n+        .identity(\"address\")\n+        .build();\n+    RowWrapper rowWrapper = new RowWrapper(schema.asStruct());\n+\n+    Row row1 = Row.of(101, \"hello\", \"addr-1\");\n+    PartitionKey partitionKey = new PartitionKey(spec, schema);\n+    partitionKey.partition(rowWrapper.wrap(row1));\n+    Assert.assertEquals(partitionKey.size(), 1);\n+    Assert.assertEquals(partitionKey.get(0, String.class), \"addr-1\");\n+\n+    Row row2 = Row.of(102, \"world\", \"addr-2\");\n+    partitionKey.partition(rowWrapper.wrap(row2));\n+    Assert.assertEquals(partitionKey.size(), 1);\n+    Assert.assertEquals(partitionKey.get(0, String.class), \"addr-2\");\n+  }\n+\n+  @Test\n+  public void testPartitionWithNestedType() {\n+    Schema schema = new Schema(\n+        Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+        Types.NestedField.optional(2, \"structType\", Types.StructType.of(\n+            Types.NestedField.optional(3, \"innerStringType\", Types.StringType.get()),\n+            Types.NestedField.optional(4, \"innerIntegerType\", Types.IntegerType.get())\n+        )),\n+        Types.NestedField.optional(5, \"listType\", Types.ListType.ofOptional(6, Types.LongType.get())),\n+        Types.NestedField.optional(7, \"mapType\",\n+            Types.MapType.ofRequired(8, 9, Types.IntegerType.get(), Types.StringType.get())),\n+        Types.NestedField.required(10, \"ts\", Types.TimestampType.withZone())\n+    );\n+    RowWrapper rowWrapper = new RowWrapper(schema.asStruct());\n+\n+    Row row = Row.of(\n+        1001,\n+        Row.of(\"addr-1\", 200),\n+        new Long[] {101L, 102L},\n+        ImmutableMap.of(1001, \"1001-value\"),\n+        DateTimeUtil.microsFromTimestamp(DateTimeUtil.timestampFromMicros(0L))\n+    );\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(schema)\n+        .identity(\"structType.innerStringType\")\n+        .build();\n+    PartitionKey partitionKey = new PartitionKey(spec, schema);\n+    partitionKey.partition(rowWrapper.wrap(row));\n+    Assert.assertEquals(partitionKey.size(), 1);\n+    Assert.assertEquals(partitionKey.get(0, String.class), \"addr-1\");\n+    Assert.assertEquals(partitionKey.toPath(), \"structType.innerStringType=addr-1\");\n+\n+    PartitionSpec spec2 = PartitionSpec.builderFor(schema)\n+        .identity(\"structType.innerIntegerType\")\n+        .build();\n+    PartitionKey partitionKey2 = new PartitionKey(spec2, schema);\n+    partitionKey2.partition(rowWrapper.wrap(row));\n+    Assert.assertEquals(1, partitionKey2.size());\n+    Assert.assertEquals(200, (int) partitionKey2.get(0, Integer.class));\n+    Assert.assertEquals(partitionKey2.toPath(), \"structType.innerIntegerType=200\");", "originalCommit": "4bc4b84d01ab72ad8aca46cf5b566bbe6070b115", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "397acce20e488cbe70d49e007e34b4a1da7d600e", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java b/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java\nindex fc1dd3a6..fde65af6 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestPartitionKey.java\n\n@@ -19,13 +19,19 @@\n \n package org.apache.iceberg.flink;\n \n+import java.nio.ByteBuffer;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.List;\n import org.apache.flink.types.Row;\n-import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.exceptions.ValidationException;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.flink.data.RandomData;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.DateTimeUtil;\n import org.junit.Assert;\n"}}, {"oid": "397acce20e488cbe70d49e007e34b4a1da7d600e", "url": "https://github.com/apache/iceberg/commit/397acce20e488cbe70d49e007e34b4a1da7d600e", "message": "More unit tests & Add conversion for DATE, TIME, TIMESTAMP, FIXED, STRUCT", "committedDate": "2020-07-15T09:51:48Z", "type": "commit"}, {"oid": "6bc452c7195a86ee73709a6a211e03c608e09a15", "url": "https://github.com/apache/iceberg/commit/6bc452c7195a86ee73709a6a211e03c608e09a15", "message": "Remove the public modifier", "committedDate": "2020-07-15T10:02:45Z", "type": "commit"}]}