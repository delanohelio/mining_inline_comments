{"pr_number": 1182, "pr_title": "Flink: Integrate Iceberg catalog to Flink catalog", "pr_createdAt": "2020-07-08T06:12:41Z", "pr_url": "https://github.com/apache/iceberg/pull/1182", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTAxOQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453105019", "bodyText": "We would normally use Preconditions.checkArgument in this case.", "author": "rdblue", "createdAt": "2020-07-10T22:28:33Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTE3MA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453105170", "bodyText": "Nit: we like to add empty lines after control flow blocks (between the last } and this if).", "author": "rdblue", "createdAt": "2020-07-10T22:29:08Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjE3NQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453106175", "bodyText": "Why is this recursive? It seems unnecessary.", "author": "rdblue", "createdAt": "2020-07-10T22:32:49Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {\n+      for (int i = 0; i < levels.length; i++) {\n+        if (!baseNamespace[i].equals(levels[i])) {\n+          return Collections.emptyList();\n+        }\n+      }\n+      List<Namespace> ret = new ArrayList<>();\n+      asNamespaceCatalog.listNamespaces(namespace).forEach(n -> ret.addAll(listAllNamespaces(n)));", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM5NTExMw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453395113", "bodyText": "You are right, asNamespaceCatalog.listNamespaces(Namespace.of(baseNamespace)) is enough.", "author": "JingsongLi", "createdAt": "2020-07-13T01:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjE3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjQzMA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453106430", "bodyText": "Minor: We prefer using the factory methods in Maps instead of specific class names.", "author": "rdblue", "createdAt": "2020-07-10T22:33:54Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {\n+      for (int i = 0; i < levels.length; i++) {\n+        if (!baseNamespace[i].equals(levels[i])) {\n+          return Collections.emptyList();\n+        }\n+      }\n+      List<Namespace> ret = new ArrayList<>();\n+      asNamespaceCatalog.listNamespaces(namespace).forEach(n -> ret.addAll(listAllNamespaces(n)));\n+      return ret;\n+    }\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      if (!getDefaultDatabase().equals(databaseName)) {\n+        throw new DatabaseNotExistException(getName(), databaseName);\n+      } else {\n+        return new CatalogDatabaseImpl(new HashMap<>(), \"\");\n+      }\n+    } else {\n+      try {\n+        Map<String, String> metadata =\n+            new HashMap<>(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNzg3Mw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453107873", "bodyText": "This doesn't seem to handle the default database. What is the correct behavior when the catalog doesn't support namespaces, but this is called for the default database?", "author": "rdblue", "createdAt": "2020-07-10T22:39:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {\n+      for (int i = 0; i < levels.length; i++) {\n+        if (!baseNamespace[i].equals(levels[i])) {\n+          return Collections.emptyList();\n+        }\n+      }\n+      List<Namespace> ret = new ArrayList<>();\n+      asNamespaceCatalog.listNamespaces(namespace).forEach(n -> ret.addAll(listAllNamespaces(n)));\n+      return ret;\n+    }\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      if (!getDefaultDatabase().equals(databaseName)) {\n+        throw new DatabaseNotExistException(getName(), databaseName);\n+      } else {\n+        return new CatalogDatabaseImpl(new HashMap<>(), \"\");\n+      }\n+    } else {\n+      try {\n+        Map<String, String> metadata =\n+            new HashMap<>(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));\n+        String comment = metadata.remove(\"comment\");\n+        return new CatalogDatabaseImpl(metadata, comment);\n+      } catch (NoSuchNamespaceException e) {\n+        throw new DatabaseNotExistException(getName(), databaseName, e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean databaseExists(String databaseName) throws CatalogException {\n+    try {\n+      getDatabase(databaseName);\n+      return true;\n+    } catch (DatabaseNotExistException ignore) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)\n+      throws DatabaseAlreadyExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        asNamespaceCatalog.createNamespace(\n+            toNamespace(name),\n+            mergeComment(database.getProperties(), database.getComment()));\n+      } catch (AlreadyExistsException e) {\n+        if (!ignoreIfExists) {\n+          throw new DatabaseAlreadyExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Namespaces are not supported by catalog: \" + getName());\n+    }\n+  }\n+\n+  private Map<String, String> mergeComment(Map<String, String> metadata, String comment) {\n+    Map<String, String> ret = new HashMap<>(metadata);\n+    if (metadata.containsKey(\"comment\")) {\n+      throw new CatalogException(\"Database properties should not contain key: 'comment'.\");\n+    }\n+    if (!StringUtils.isNullOrWhitespaceOnly(comment)) {\n+      ret.put(\"comment\", comment);\n+    }\n+    return ret;\n+  }\n+\n+  @Override\n+  public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)\n+      throws DatabaseNotExistException, DatabaseNotEmptyException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        boolean success = asNamespaceCatalog.dropNamespace(toNamespace(name));\n+        if (!success && !ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name);\n+        }\n+      } catch (NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      } catch (NamespaceNotEmptyException e) {\n+        throw new DatabaseNotEmptyException(getName(), name, e);\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void alterDatabase(String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)\n+      throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      Namespace namespace = toNamespace(name);\n+      Map<String, String> updates = Maps.newHashMap();\n+      Set<String> removals = Sets.newHashSet();\n+\n+      try {\n+        Map<String, String> oldOptions = asNamespaceCatalog.loadNamespaceMetadata(namespace);\n+        Map<String, String> newOptions = mergeComment(newDatabase.getProperties(), newDatabase.getComment());\n+\n+        for (String key : oldOptions.keySet()) {\n+          if (!newOptions.containsKey(key)) {\n+            removals.add(key);\n+          }\n+        }\n+\n+        for (Map.Entry<String, String> entry : newOptions.entrySet()) {\n+          if (!entry.getValue().equals(oldOptions.get(entry.getKey()))) {\n+            updates.put(entry.getKey(), entry.getValue());\n+          }\n+        }\n+\n+        if (!updates.isEmpty()) {\n+          asNamespaceCatalog.setProperties(namespace, updates);\n+        }\n+\n+        if (!removals.isEmpty()) {\n+          asNamespaceCatalog.removeProperties(namespace, removals);\n+        }\n+\n+      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM5NzM2OA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453397368", "bodyText": "Yes, we need handle default database, the correct behavior when the catalog doesn't support namespaces should throw an exception to tell users that the default database can not be altered.", "author": "JingsongLi", "createdAt": "2020-07-13T01:57:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNzg3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODMyMw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453108323", "bodyText": "Passing an empty string is suspicious. Should that be null or omitted to let the impl default?", "author": "rdblue", "createdAt": "2020-07-10T22:41:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {\n+      for (int i = 0; i < levels.length; i++) {\n+        if (!baseNamespace[i].equals(levels[i])) {\n+          return Collections.emptyList();\n+        }\n+      }\n+      List<Namespace> ret = new ArrayList<>();\n+      asNamespaceCatalog.listNamespaces(namespace).forEach(n -> ret.addAll(listAllNamespaces(n)));\n+      return ret;\n+    }\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      if (!getDefaultDatabase().equals(databaseName)) {\n+        throw new DatabaseNotExistException(getName(), databaseName);\n+      } else {\n+        return new CatalogDatabaseImpl(new HashMap<>(), \"\");\n+      }\n+    } else {\n+      try {\n+        Map<String, String> metadata =\n+            new HashMap<>(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));\n+        String comment = metadata.remove(\"comment\");\n+        return new CatalogDatabaseImpl(metadata, comment);\n+      } catch (NoSuchNamespaceException e) {\n+        throw new DatabaseNotExistException(getName(), databaseName, e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean databaseExists(String databaseName) throws CatalogException {\n+    try {\n+      getDatabase(databaseName);\n+      return true;\n+    } catch (DatabaseNotExistException ignore) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)\n+      throws DatabaseAlreadyExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        asNamespaceCatalog.createNamespace(\n+            toNamespace(name),\n+            mergeComment(database.getProperties(), database.getComment()));\n+      } catch (AlreadyExistsException e) {\n+        if (!ignoreIfExists) {\n+          throw new DatabaseAlreadyExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Namespaces are not supported by catalog: \" + getName());\n+    }\n+  }\n+\n+  private Map<String, String> mergeComment(Map<String, String> metadata, String comment) {\n+    Map<String, String> ret = new HashMap<>(metadata);\n+    if (metadata.containsKey(\"comment\")) {\n+      throw new CatalogException(\"Database properties should not contain key: 'comment'.\");\n+    }\n+    if (!StringUtils.isNullOrWhitespaceOnly(comment)) {\n+      ret.put(\"comment\", comment);\n+    }\n+    return ret;\n+  }\n+\n+  @Override\n+  public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)\n+      throws DatabaseNotExistException, DatabaseNotEmptyException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        boolean success = asNamespaceCatalog.dropNamespace(toNamespace(name));\n+        if (!success && !ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name);\n+        }\n+      } catch (NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      } catch (NamespaceNotEmptyException e) {\n+        throw new DatabaseNotEmptyException(getName(), name, e);\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void alterDatabase(String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)\n+      throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      Namespace namespace = toNamespace(name);\n+      Map<String, String> updates = Maps.newHashMap();\n+      Set<String> removals = Sets.newHashSet();\n+\n+      try {\n+        Map<String, String> oldOptions = asNamespaceCatalog.loadNamespaceMetadata(namespace);\n+        Map<String, String> newOptions = mergeComment(newDatabase.getProperties(), newDatabase.getComment());\n+\n+        for (String key : oldOptions.keySet()) {\n+          if (!newOptions.containsKey(key)) {\n+            removals.add(key);\n+          }\n+        }\n+\n+        for (Map.Entry<String, String> entry : newOptions.entrySet()) {\n+          if (!entry.getValue().equals(oldOptions.get(entry.getKey()))) {\n+            updates.put(entry.getKey(), entry.getValue());\n+          }\n+        }\n+\n+        if (!updates.isEmpty()) {\n+          asNamespaceCatalog.setProperties(namespace, updates);\n+        }\n+\n+        if (!removals.isEmpty()) {\n+          asNamespaceCatalog.removeProperties(namespace, removals);\n+        }\n+\n+      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    try {\n+      return icebergCatalog.listTables(toNamespace(databaseName)).stream()\n+          .map(TableIdentifier::name)\n+          .collect(Collectors.toList());\n+    } catch (NoSuchNamespaceException e) {\n+      throw new DatabaseNotExistException(getName(), databaseName, e);\n+    }\n+  }\n+\n+  @Override\n+  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    try {\n+      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+\n+      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n+      // catalog table.\n+      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n+      return new CatalogTableImpl(tableSchema, table.properties(), \"\");", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM5OTY3MQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453399671", "bodyText": "I'll modify it to null.", "author": "JingsongLi", "createdAt": "2020-07-13T02:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODMyMw=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453108883", "bodyText": "Can we add partitioning to the Flink DDL parser instead? That seems like a more appropriate place for it.\nOtherwise, I'd recommend just using the PartitionSpecParser.fromJson method.", "author": "rdblue", "createdAt": "2020-07-10T22:43:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {\n+      for (int i = 0; i < levels.length; i++) {\n+        if (!baseNamespace[i].equals(levels[i])) {\n+          return Collections.emptyList();\n+        }\n+      }\n+      List<Namespace> ret = new ArrayList<>();\n+      asNamespaceCatalog.listNamespaces(namespace).forEach(n -> ret.addAll(listAllNamespaces(n)));\n+      return ret;\n+    }\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      if (!getDefaultDatabase().equals(databaseName)) {\n+        throw new DatabaseNotExistException(getName(), databaseName);\n+      } else {\n+        return new CatalogDatabaseImpl(new HashMap<>(), \"\");\n+      }\n+    } else {\n+      try {\n+        Map<String, String> metadata =\n+            new HashMap<>(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));\n+        String comment = metadata.remove(\"comment\");\n+        return new CatalogDatabaseImpl(metadata, comment);\n+      } catch (NoSuchNamespaceException e) {\n+        throw new DatabaseNotExistException(getName(), databaseName, e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean databaseExists(String databaseName) throws CatalogException {\n+    try {\n+      getDatabase(databaseName);\n+      return true;\n+    } catch (DatabaseNotExistException ignore) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)\n+      throws DatabaseAlreadyExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        asNamespaceCatalog.createNamespace(\n+            toNamespace(name),\n+            mergeComment(database.getProperties(), database.getComment()));\n+      } catch (AlreadyExistsException e) {\n+        if (!ignoreIfExists) {\n+          throw new DatabaseAlreadyExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Namespaces are not supported by catalog: \" + getName());\n+    }\n+  }\n+\n+  private Map<String, String> mergeComment(Map<String, String> metadata, String comment) {\n+    Map<String, String> ret = new HashMap<>(metadata);\n+    if (metadata.containsKey(\"comment\")) {\n+      throw new CatalogException(\"Database properties should not contain key: 'comment'.\");\n+    }\n+    if (!StringUtils.isNullOrWhitespaceOnly(comment)) {\n+      ret.put(\"comment\", comment);\n+    }\n+    return ret;\n+  }\n+\n+  @Override\n+  public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)\n+      throws DatabaseNotExistException, DatabaseNotEmptyException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        boolean success = asNamespaceCatalog.dropNamespace(toNamespace(name));\n+        if (!success && !ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name);\n+        }\n+      } catch (NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      } catch (NamespaceNotEmptyException e) {\n+        throw new DatabaseNotEmptyException(getName(), name, e);\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void alterDatabase(String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)\n+      throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      Namespace namespace = toNamespace(name);\n+      Map<String, String> updates = Maps.newHashMap();\n+      Set<String> removals = Sets.newHashSet();\n+\n+      try {\n+        Map<String, String> oldOptions = asNamespaceCatalog.loadNamespaceMetadata(namespace);\n+        Map<String, String> newOptions = mergeComment(newDatabase.getProperties(), newDatabase.getComment());\n+\n+        for (String key : oldOptions.keySet()) {\n+          if (!newOptions.containsKey(key)) {\n+            removals.add(key);\n+          }\n+        }\n+\n+        for (Map.Entry<String, String> entry : newOptions.entrySet()) {\n+          if (!entry.getValue().equals(oldOptions.get(entry.getKey()))) {\n+            updates.put(entry.getKey(), entry.getValue());\n+          }\n+        }\n+\n+        if (!updates.isEmpty()) {\n+          asNamespaceCatalog.setProperties(namespace, updates);\n+        }\n+\n+        if (!removals.isEmpty()) {\n+          asNamespaceCatalog.removeProperties(namespace, removals);\n+        }\n+\n+      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    try {\n+      return icebergCatalog.listTables(toNamespace(databaseName)).stream()\n+          .map(TableIdentifier::name)\n+          .collect(Collectors.toList());\n+    } catch (NoSuchNamespaceException e) {\n+      throw new DatabaseNotExistException(getName(), databaseName, e);\n+    }\n+  }\n+\n+  @Override\n+  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    try {\n+      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+\n+      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n+      // catalog table.\n+      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n+      return new CatalogTableImpl(tableSchema, table.properties(), \"\");\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new TableNotExistException(getName(), tablePath, e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean tableExists(ObjectPath tablePath) throws CatalogException {\n+    return icebergCatalog.tableExists(toIdentifier(tablePath));\n+  }\n+\n+  @Override\n+  public void dropTable(ObjectPath tablePath, boolean ignoreIfNotExists)\n+      throws TableNotExistException, CatalogException {\n+    try {\n+      icebergCatalog.dropTable(toIdentifier(tablePath));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new TableNotExistException(getName(), tablePath, e);\n+    }\n+  }\n+\n+  @Override\n+  public void renameTable(ObjectPath tablePath, String newTableName, boolean ignoreIfNotExists)\n+      throws TableNotExistException, TableAlreadyExistException, CatalogException {\n+    try {\n+      icebergCatalog.renameTable(\n+          toIdentifier(tablePath),\n+          toIdentifier(new ObjectPath(tablePath.getDatabaseName(), newTableName)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new TableNotExistException(getName(), tablePath, e);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n+  }\n+\n+  /**\n+   * TODO Implement DDL-string parser for PartitionSpec.", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQwMDI0Mg==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453400242", "bodyText": "I prefer that adding partitioning to the Flink DDL parser. I'll modify the comments.\nUsing PartitionSpecParser.fromJson looks very difficult to use.", "author": "JingsongLi", "createdAt": "2020-07-13T02:12:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyMzcxNw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453423717", "bodyText": "After some discussions with Flink developers, we can map Iceberg Partition Transform to Flink Computed Column and Partition. We can support it in future.", "author": "JingsongLi", "createdAt": "2020-07-13T04:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MTk2OA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453791968", "bodyText": "PartitionSpecParser.fromJson is how we serialize partition specs internally. It isn't great to expose it directly to users, but would at least make it possible to configure partitioning. If you have a different approach, that is much better!\nHow would the computed column and partition approach work?", "author": "rdblue", "createdAt": "2020-07-13T16:55:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1NDA4MQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r454054081", "bodyText": "A rough idea, Flink support computed column: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/create.html#create-table\nFlink DDL CREATE TABLE T (pk INT, ... dt STRING, year AS YEAR(dt), month AS MONTH(dt), d AS DAY(dt)) PARTITIONED BY(year, month, d) should be same to Spark DDL CREATE TABLE T (pk INT, ... dt STRING) PARTITIONED BY(YEAR(dt), MONTH(dt), DAY(dt)).\nThe computed columns are not stored in the real data, they are just virtual columns, which means we can map they to iceberg partition transforms of iceberg table in iceberg Flink Catalog.", "author": "JingsongLi", "createdAt": "2020-07-14T02:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxNjM5OQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r457716399", "bodyText": "Good idea, but there are a couple of things to watch out for:\n\nWhere possible, we avoid exposing the actual partition values, in order to maintain a separation between logical queries and physical layout. That way, the physical layout can change, but the logical queries will continue to work. In this case, we would need to make sure that the computed columns are tracked separately so that we don't drop the day column when the table gets converted to partitioning by hour.\nYear, month, and day are functions with concrete behavior for Flink SQL, and Iceberg's partitioning may not align with that behavior. So we probably would not want to supply the data for those columns using Iceberg partition values. Instead, I think we should derive them from the dt field.", "author": "rdblue", "createdAt": "2020-07-20T22:01:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMzI2MA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r457813260", "bodyText": "Good points.\nFor Flink SQL, computed columns are virtual columns, the source and sink can just ignore them, the source just produces columns without computed columns, the Flink core will generate computed columns for input records. For sink, Flink core just give the records without computed columns to connector sink.\n\nI see, you mean https://iceberg.apache.org/evolution/#partition-evolution , the computed columns should be calculated by Flink core, iceberg should just deal with its physical logical.\nThere are three types of function: 1.hour,day,month,year are the same as Flink's functions. 2. For truncate, Flink also supports this function, but not support truncate with input type string and bytes, iceberg can provides catalog function (Catalog.getFunction), users can use iceberg_catalog.truncate to create computed column. 3. For bucket, Flink not support this function, so iceberg can provides catalog functions, users can directly use it.", "author": "JingsongLi", "createdAt": "2020-07-21T03:23:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODg4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc1NQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453109755", "bodyText": "We prefer two options for formatting argument lists. Either aligned with the first argument:\npublic void alterPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSpec, CatalogPartition newPartition,\n                           boolean ignoreIfNotExists) throws CatalogException {\n  ...\n}\nOr, indented by 2 indents (4 spaces) and aligned with that position:\npublic void alterPartition(\n    ObjectPath tablePath, CatalogPartitionSpec partitionSpec, CatalogPartition newPartition, boolean ignoreIfNotExists)\n    throws CatalogException {\n  ...\n}\nthrows can be on the next line, indented to the same place.", "author": "rdblue", "createdAt": "2020-07-10T22:47:20Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -0,0 +1,508 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.AbstractCatalog;\n+import org.apache.flink.table.catalog.CatalogBaseTable;\n+import org.apache.flink.table.catalog.CatalogDatabase;\n+import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.CatalogFunction;\n+import org.apache.flink.table.catalog.CatalogPartition;\n+import org.apache.flink.table.catalog.CatalogPartitionSpec;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.CatalogException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotExistException;\n+import org.apache.flink.table.catalog.exceptions.FunctionNotExistException;\n+import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n+import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.util.StringUtils;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+\n+/**\n+ * A Flink Catalog implementation that wraps an Iceberg {@link Catalog}.\n+ * <p>\n+ * The mapping between Flink database and Iceberg namespace:\n+ * Supplying a base namespace for a given catalog, so if you have a catalog that supports a 2-level namespace, you\n+ * would supply the first level in the catalog configuration and the second level would be exposed as Flink databases.\n+ * <p>\n+ * The Iceberg table manages its partitions by itself. The partition of the Iceberg table is independent of the\n+ * partition of Flink.\n+ */\n+public class FlinkCatalog extends AbstractCatalog {\n+\n+  private final Catalog originalCatalog;\n+  private final Catalog icebergCatalog;\n+  private final String[] baseNamespace;\n+  private final SupportsNamespaces asNamespaceCatalog;\n+\n+  public FlinkCatalog(\n+      String catalogName,\n+      String defaultDatabase,\n+      String[] baseNamespace,\n+      Catalog icebergCatalog,\n+      boolean cacheEnabled) {\n+    super(catalogName, defaultDatabase);\n+    this.originalCatalog = icebergCatalog;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(icebergCatalog) : icebergCatalog;\n+    this.baseNamespace = baseNamespace;\n+    if (icebergCatalog instanceof SupportsNamespaces) {\n+      asNamespaceCatalog = (SupportsNamespaces) icebergCatalog;\n+    } else {\n+      asNamespaceCatalog = null;\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws CatalogException {\n+  }\n+\n+  @Override\n+  public void close() throws CatalogException {\n+    if (originalCatalog instanceof Closeable) {\n+      try {\n+        ((Closeable) originalCatalog).close();\n+      } catch (IOException e) {\n+        throw new CatalogException(e);\n+      }\n+    }\n+  }\n+\n+  private Namespace toNamespace(String database) {\n+    String[] namespace = new String[baseNamespace.length + 1];\n+    System.arraycopy(baseNamespace, 0, namespace, 0, baseNamespace.length);\n+    namespace[baseNamespace.length] = database;\n+    return Namespace.of(namespace);\n+  }\n+\n+  private TableIdentifier toIdentifier(ObjectPath path) {\n+    return TableIdentifier.of(toNamespace(path.getDatabaseName()), path.getObjectName());\n+  }\n+\n+  @Override\n+  public List<String> listDatabases() throws CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      return Collections.singletonList(getDefaultDatabase());\n+    }\n+\n+    return listAllNamespaces(Namespace.empty()).stream()\n+        .map(n -> n.level(n.levels().length - 1))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private List<Namespace> listAllNamespaces(Namespace namespace) {\n+    if (asNamespaceCatalog == null) {\n+      throw new RuntimeException(\"The asNamespaceCatalog should not be null.\");\n+    }\n+\n+    String[] levels = namespace.levels();\n+    if (levels.length == baseNamespace.length + 1) {\n+      return Collections.singletonList(namespace);\n+    }\n+    if (levels.length < baseNamespace.length + 1) {\n+      for (int i = 0; i < levels.length; i++) {\n+        if (!baseNamespace[i].equals(levels[i])) {\n+          return Collections.emptyList();\n+        }\n+      }\n+      List<Namespace> ret = new ArrayList<>();\n+      asNamespaceCatalog.listNamespaces(namespace).forEach(n -> ret.addAll(listAllNamespaces(n)));\n+      return ret;\n+    }\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog == null) {\n+      if (!getDefaultDatabase().equals(databaseName)) {\n+        throw new DatabaseNotExistException(getName(), databaseName);\n+      } else {\n+        return new CatalogDatabaseImpl(new HashMap<>(), \"\");\n+      }\n+    } else {\n+      try {\n+        Map<String, String> metadata =\n+            new HashMap<>(asNamespaceCatalog.loadNamespaceMetadata(toNamespace(databaseName)));\n+        String comment = metadata.remove(\"comment\");\n+        return new CatalogDatabaseImpl(metadata, comment);\n+      } catch (NoSuchNamespaceException e) {\n+        throw new DatabaseNotExistException(getName(), databaseName, e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean databaseExists(String databaseName) throws CatalogException {\n+    try {\n+      getDatabase(databaseName);\n+      return true;\n+    } catch (DatabaseNotExistException ignore) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)\n+      throws DatabaseAlreadyExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        asNamespaceCatalog.createNamespace(\n+            toNamespace(name),\n+            mergeComment(database.getProperties(), database.getComment()));\n+      } catch (AlreadyExistsException e) {\n+        if (!ignoreIfExists) {\n+          throw new DatabaseAlreadyExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      throw new UnsupportedOperationException(\"Namespaces are not supported by catalog: \" + getName());\n+    }\n+  }\n+\n+  private Map<String, String> mergeComment(Map<String, String> metadata, String comment) {\n+    Map<String, String> ret = new HashMap<>(metadata);\n+    if (metadata.containsKey(\"comment\")) {\n+      throw new CatalogException(\"Database properties should not contain key: 'comment'.\");\n+    }\n+    if (!StringUtils.isNullOrWhitespaceOnly(comment)) {\n+      ret.put(\"comment\", comment);\n+    }\n+    return ret;\n+  }\n+\n+  @Override\n+  public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)\n+      throws DatabaseNotExistException, DatabaseNotEmptyException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      try {\n+        boolean success = asNamespaceCatalog.dropNamespace(toNamespace(name));\n+        if (!success && !ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name);\n+        }\n+      } catch (NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      } catch (NamespaceNotEmptyException e) {\n+        throw new DatabaseNotEmptyException(getName(), name, e);\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void alterDatabase(String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)\n+      throws DatabaseNotExistException, CatalogException {\n+    if (asNamespaceCatalog != null) {\n+      Namespace namespace = toNamespace(name);\n+      Map<String, String> updates = Maps.newHashMap();\n+      Set<String> removals = Sets.newHashSet();\n+\n+      try {\n+        Map<String, String> oldOptions = asNamespaceCatalog.loadNamespaceMetadata(namespace);\n+        Map<String, String> newOptions = mergeComment(newDatabase.getProperties(), newDatabase.getComment());\n+\n+        for (String key : oldOptions.keySet()) {\n+          if (!newOptions.containsKey(key)) {\n+            removals.add(key);\n+          }\n+        }\n+\n+        for (Map.Entry<String, String> entry : newOptions.entrySet()) {\n+          if (!entry.getValue().equals(oldOptions.get(entry.getKey()))) {\n+            updates.put(entry.getKey(), entry.getValue());\n+          }\n+        }\n+\n+        if (!updates.isEmpty()) {\n+          asNamespaceCatalog.setProperties(namespace, updates);\n+        }\n+\n+        if (!removals.isEmpty()) {\n+          asNamespaceCatalog.removeProperties(namespace, removals);\n+        }\n+\n+      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {\n+        if (!ignoreIfNotExists) {\n+          throw new DatabaseNotExistException(getName(), name, e);\n+        }\n+      }\n+    } else {\n+      if (!ignoreIfNotExists) {\n+        throw new DatabaseNotExistException(getName(), name);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<String> listTables(String databaseName) throws DatabaseNotExistException, CatalogException {\n+    try {\n+      return icebergCatalog.listTables(toNamespace(databaseName)).stream()\n+          .map(TableIdentifier::name)\n+          .collect(Collectors.toList());\n+    } catch (NoSuchNamespaceException e) {\n+      throw new DatabaseNotExistException(getName(), databaseName, e);\n+    }\n+  }\n+\n+  @Override\n+  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    try {\n+      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+\n+      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n+      // catalog table.\n+      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n+      return new CatalogTableImpl(tableSchema, table.properties(), \"\");\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new TableNotExistException(getName(), tablePath, e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean tableExists(ObjectPath tablePath) throws CatalogException {\n+    return icebergCatalog.tableExists(toIdentifier(tablePath));\n+  }\n+\n+  @Override\n+  public void dropTable(ObjectPath tablePath, boolean ignoreIfNotExists)\n+      throws TableNotExistException, CatalogException {\n+    try {\n+      icebergCatalog.dropTable(toIdentifier(tablePath));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new TableNotExistException(getName(), tablePath, e);\n+    }\n+  }\n+\n+  @Override\n+  public void renameTable(ObjectPath tablePath, String newTableName, boolean ignoreIfNotExists)\n+      throws TableNotExistException, TableAlreadyExistException, CatalogException {\n+    try {\n+      icebergCatalog.renameTable(\n+          toIdentifier(tablePath),\n+          toIdentifier(new ObjectPath(tablePath.getDatabaseName(), newTableName)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new TableNotExistException(getName(), tablePath, e);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n+  }\n+\n+  /**\n+   * TODO Implement DDL-string parser for PartitionSpec.\n+   */\n+  @Override\n+  public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n+      throws CatalogException {\n+    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+  }\n+\n+  @Override\n+  public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n+      throws CatalogException {\n+    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+  }\n+\n+  // ------------------------------ Unsupported methods ---------------------------------------------\n+\n+  @Override\n+  public List<String> listViews(String databaseName) throws CatalogException {\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public CatalogPartition getPartition(\n+      ObjectPath tablePath, CatalogPartitionSpec partitionSpec\n+  ) throws CatalogException {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean partitionExists(ObjectPath tablePath, CatalogPartitionSpec partitionSpec) throws CatalogException {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void createPartition(\n+      ObjectPath tablePath, CatalogPartitionSpec partitionSpec, CatalogPartition partition, boolean ignoreIfExists\n+  ) throws CatalogException {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSpec, boolean ignoreIfNotExists)\n+      throws CatalogException {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void alterPartition(\n+      ObjectPath tablePath, CatalogPartitionSpec partitionSpec, CatalogPartition newPartition, boolean ignoreIfNotExists\n+  ) throws CatalogException {", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQwMDU2OA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453400568", "bodyText": "Got it.", "author": "JingsongLi", "createdAt": "2020-07-13T02:14:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex 0a48f486e..eea4c07d1 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n\n@@ -21,9 +21,7 @@ package org.apache.iceberg.flink;\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMDY3MA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453110670", "bodyText": "This should also be labelled (Hadoop catalog only) because the Hive catalog supports only database.", "author": "rdblue", "createdAt": "2020-07-10T22:51:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.configuration.GlobalConfiguration;\n+import org.apache.flink.runtime.util.HadoopUtils;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.table.descriptors.CatalogDescriptorValidator;\n+import org.apache.flink.table.factories.CatalogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+\n+/**\n+ * A Flink Catalog factory implementation that creates {@link FlinkCatalog}.\n+ * <p>\n+ * This supports the following catalog configuration options:\n+ * <ul>\n+ *   <li><tt>type</tt> - Flink catalog factory key, should be \"iceberg\"</li>\n+ *   <li><tt>catalog-type</tt> - iceberg catalog type, \"hive\" or \"hadoop\"</li>\n+ *   <li><tt>uri</tt> - the Hive Metastore URI (Hive catalog only)</li>\n+ *   <li><tt>clients</tt> - the Hive Client Pool Size (Hive catalog only)</li>\n+ *   <li><tt>warehouse</tt> - the warehouse path (Hadoop catalog only)</li>\n+ *   <li><tt>default-database</tt> - a database name to use as the default</li>\n+ *   <li><tt>base-namespace</tt> - a base namespace as the prefix for all databases</li>", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java\nindex 2f8feb2a4..59f4e8201 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java\n\n@@ -19,8 +19,6 @@\n \n package org.apache.iceberg.flink;\n \n-import java.util.ArrayList;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import org.apache.flink.configuration.GlobalConfiguration;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMDk4Mw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453110983", "bodyText": "Can you be more specific about this? What is a case where information is lost?", "author": "rdblue", "createdAt": "2020-07-10T22:52:13Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -21,10 +21,19 @@\n \n import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.types.FieldsDataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n \n+/**\n+ * Converter between Flink types and Iceberg type.\n+ * The conversion is not a 1:1 mapping that not allows back-and-forth conversion. So some information might get lost\n+ * during the back-and-forth conversion.", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MzQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453793475", "bodyText": "If I understand correctly, this is lossy because Iceberg doesn't represent some types that Flink supports, like CHAR(N). Is that right?", "author": "rdblue", "createdAt": "2020-07-13T16:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMDk4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1MTczNw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r454051737", "bodyText": "Yes", "author": "JingsongLi", "createdAt": "2020-07-14T01:57:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMDk4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1MTk4Mg==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r454051982", "bodyText": "Iceberg to Flink: will loss UUID.\nFlink to Iceberg: will loss precisions.", "author": "JingsongLi", "createdAt": "2020-07-14T01:58:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMDk4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\nindex 0cd103071..e1b6c5f1d 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\n\n@@ -33,6 +33,17 @@ import org.apache.iceberg.types.TypeUtil;\n  * Converter between Flink types and Iceberg type.\n  * The conversion is not a 1:1 mapping that not allows back-and-forth conversion. So some information might get lost\n  * during the back-and-forth conversion.\n+ * <p>\n+ * This inconsistent types:\n+ * <ul>\n+ *   <li>map Iceberg UUID type to Flink BinaryType(16)</li>\n+ *   <li>map Flink VarCharType and CharType to Iceberg String type (lost precision)</li>\n+ *   <li>map Flink VarBinaryType to Iceberg Binary type (lost precision)</li>\n+ *   <li>map Flink TimeType to Iceberg Time type (lost precision)</li>\n+ *   <li>map Flink TimestampType to Iceberg Timestamp without zone type (lost precision)</li>\n+ *   <li>map Flink LocalZonedTimestampType to Iceberg Timestamp with zone type (lost precision)</li>\n+ * </ul>\n+ * <p>\n  */\n public class FlinkSchemaUtil {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMTQxOA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453111418", "bodyText": "Char? Wouldn't this be fixed-length binary?", "author": "rdblue", "createdAt": "2020-07-10T22:54:12Z", "path": "flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.BinaryType;\n+import org.apache.flink.table.types.logical.BooleanType;\n+import org.apache.flink.table.types.logical.CharType;\n+import org.apache.flink.table.types.logical.DateType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.DoubleType;\n+import org.apache.flink.table.types.logical.FloatType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.LocalZonedTimestampType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimeType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarBinaryType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+\n+class TypeToFlinkType extends TypeUtil.SchemaVisitor<LogicalType> {\n+  TypeToFlinkType() {\n+  }\n+\n+  @Override\n+  public LogicalType schema(Schema schema, LogicalType structType) {\n+    return structType;\n+  }\n+\n+  @Override\n+  public LogicalType struct(Types.StructType struct, List<LogicalType> fieldResults) {\n+    List<Types.NestedField> fields = struct.fields();\n+\n+    List<RowType.RowField> flinkFields = Lists.newArrayListWithExpectedSize(fieldResults.size());\n+    for (int i = 0; i < fields.size(); i += 1) {\n+      Types.NestedField field = fields.get(i);\n+      LogicalType type = fieldResults.get(i);\n+      RowType.RowField flinkField = new RowType.RowField(\n+          field.name(), type.copy(field.isOptional()), field.doc());\n+      flinkFields.add(flinkField);\n+    }\n+\n+    return new RowType(flinkFields);\n+  }\n+\n+  @Override\n+  public LogicalType field(Types.NestedField field, LogicalType fieldResult) {\n+    return fieldResult;\n+  }\n+\n+  @Override\n+  public LogicalType list(Types.ListType list, LogicalType elementResult) {\n+    return new ArrayType(elementResult.copy(list.isElementOptional()));\n+  }\n+\n+  @Override\n+  public LogicalType map(Types.MapType map, LogicalType keyResult, LogicalType valueResult) {\n+    // keys in map are not allowed to be null.\n+    return new MapType(keyResult.copy(false), valueResult.copy(map.isValueOptional()));\n+  }\n+\n+  @Override\n+  public LogicalType primitive(Type.PrimitiveType primitive) {\n+    switch (primitive.typeId()) {\n+      case BOOLEAN:\n+        return new BooleanType();\n+      case INTEGER:\n+        return new IntType();\n+      case LONG:\n+        return new BigIntType();\n+      case FLOAT:\n+        return new FloatType();\n+      case DOUBLE:\n+        return new DoubleType();\n+      case DATE:\n+        return new DateType();\n+      case TIME:\n+        // MICROS\n+        return new TimeType(6);\n+      case TIMESTAMP:\n+        Types.TimestampType timestamp = (Types.TimestampType) primitive;\n+        if (timestamp.shouldAdjustToUTC()) {\n+          // MICROS\n+          return new LocalZonedTimestampType(6);\n+        } else {\n+          // MICROS\n+          return new TimestampType(6);\n+        }\n+      case STRING:\n+        return new VarCharType(VarCharType.MAX_LENGTH);\n+      case UUID:\n+        // UUID length is 16\n+        return new CharType(16);", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQwNDA2MQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453404061", "bodyText": "I thought UUID should be a Char with 36 precision because:\n\nIn Spark, UUID function returns StringType.\nIn Flink, UUID function returns CharType with 36 precision.\n\nBut you are right, in Orc and Parquet, UUID just be treated as a fixed-length binary.", "author": "JingsongLi", "createdAt": "2020-07-13T02:31:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMTQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mzk0MA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453793940", "bodyText": "I think either CHAR(36) or VARBINARY(16) would work, but not CHAR(16).", "author": "rdblue", "createdAt": "2020-07-13T16:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMTQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1NDkxOQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r454054919", "bodyText": "I choose fixed-length binary(16).", "author": "JingsongLi", "createdAt": "2020-07-14T02:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMTQxOA=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java b/flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java\nindex 449093d94..dfd8ffb96 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java\n\n@@ -24,7 +24,6 @@ import org.apache.flink.table.types.logical.ArrayType;\n import org.apache.flink.table.types.logical.BigIntType;\n import org.apache.flink.table.types.logical.BinaryType;\n import org.apache.flink.table.types.logical.BooleanType;\n-import org.apache.flink.table.types.logical.CharType;\n import org.apache.flink.table.types.logical.DateType;\n import org.apache.flink.table.types.logical.DecimalType;\n import org.apache.flink.table.types.logical.DoubleType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjA3OA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453112078", "bodyText": "Won't this close the catalog after every test method?", "author": "rdblue", "createdAt": "2020-07-10T22:56:51Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.util.ArrayUtils;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class FlinkCatalogTestBase extends FlinkTestBase {\n+\n+  protected static final String DATABASE = \"db\";\n+  private static File warehouse = null;\n+\n+  @BeforeClass\n+  public static void createWarehouse() throws IOException {\n+    FlinkCatalogTestBase.warehouse = File.createTempFile(\"warehouse\", null);\n+    Assert.assertTrue(warehouse.delete());\n+  }\n+\n+  @AfterClass\n+  public static void dropWarehouse() {\n+    if (warehouse != null && warehouse.exists()) {\n+      warehouse.delete();\n+    }\n+  }\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"testhive\", new String[0] },\n+        new Object[] { \"testhadoop\", new String[0] },\n+        new Object[] { \"testhadoop\", new String[] { \"l0\", \"l1\" }},\n+    };\n+  }\n+\n+  protected final TableEnvironment tEnv =\n+      TableEnvironment.create(EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n+\n+  protected final String catalogName;\n+  protected final String[] baseNamespace;\n+  protected final Catalog validationCatalog;\n+  protected final SupportsNamespaces validationNamespaceCatalog;\n+  protected final org.apache.flink.table.catalog.Catalog flinkCatalog;\n+\n+  protected final String flinkIdentifier;\n+  protected final Namespace icebergNamespace;\n+  protected final boolean isHadoopCatalog;\n+\n+  public FlinkCatalogTestBase(String catalogName, String[] baseNamespace) {\n+    this.catalogName = catalogName;\n+    this.baseNamespace = baseNamespace;\n+    this.isHadoopCatalog = catalogName.equals(\"testhadoop\");\n+    this.validationCatalog = isHadoopCatalog ?\n+        new HadoopCatalog(hiveConf, \"file:\" + warehouse) :\n+        catalog;\n+    this.validationNamespaceCatalog = (SupportsNamespaces) validationCatalog;\n+\n+    Map<String, String> config = new HashMap<>();\n+    config.put(\"type\", \"iceberg\");\n+    config.put(FlinkCatalogFactory.ICEBERG_CATALOG_TYPE, isHadoopCatalog ? \"hadoop\" : \"hive\");\n+    config.put(FlinkCatalogFactory.HADOOP_WAREHOUSE_LOCATION, \"file:\" + warehouse);\n+    if (baseNamespace.length > 0) {\n+      config.put(FlinkCatalogFactory.BASE_NAMESPACE, Joiner.on(\".\").join(baseNamespace));\n+    }\n+\n+    FlinkCatalogFactory factory = new FlinkCatalogFactory() {\n+      @Override\n+      protected Catalog buildIcebergCatalog(String name, Map<String, String> options) {\n+        // Flink hadoop configuration depends on system env, it is quiet hard to set from testing. So directly pass\n+        // correct hadoop configuration.\n+        return super.buildIcebergCatalog(name, options, hiveConf);\n+      }\n+    };\n+    flinkCatalog = factory.createCatalog(catalogName, config);\n+    tEnv.registerCatalog(catalogName, flinkCatalog);\n+\n+    this.flinkIdentifier = catalogName + \".\" + DATABASE;\n+    this.icebergNamespace = Namespace.of(ArrayUtils.concat(baseNamespace, new String[] { DATABASE }));\n+  }\n+\n+  @After", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQxNzk0Ng==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453417946", "bodyText": "Yes, every test method will create a new catalog too.\nBut it seems we can reuse them by catalog name.", "author": "JingsongLi", "createdAt": "2020-07-13T03:38:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjA3OA=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java\nindex 295e89ce9..743325f4d 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java\n\n@@ -21,7 +21,6 @@ package org.apache.iceberg.flink;\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.HashMap;\n import java.util.Map;\n import org.apache.flink.table.api.EnvironmentSettings;\n import org.apache.flink.table.api.TableEnvironment;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjU4Mw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453112583", "bodyText": "This sounds like a bug in the Hadoop catalog. Can we fix it instead of ignoring this test case?", "author": "rdblue", "createdAt": "2020-07-10T22:59:15Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.util.Map;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {\n+\n+  public TestFlinkCatalogDatabase(String catalogName, String[] baseNamepace) {\n+    super(catalogName, baseNamepace);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.tl\", flinkIdentifier);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testCreateNamespace() {\n+    Assert.assertFalse(\n+        \"Database should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Database should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDefaultDatabase() {\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertEquals(\"Should use the current catalog\", tEnv.getCurrentCatalog(), catalogName);\n+    Assert.assertEquals(\"Should use the configured default namespace\", tEnv.getCurrentDatabase(), \"default\");\n+  }\n+\n+  @Test\n+  public void testDropEmptyDatabase() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"DROP DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertFalse(\n+        \"Namespace should have been dropped\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDropNonEmptyNamespace() {\n+    Assume.assumeFalse(\"Hadoop catalog throws IOException: Directory is not empty.\", isHadoopCatalog);", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODE2Mw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453428163", "bodyText": "I'll modify in this PR. Tell me if I need create a new PR for fixing hadoop catalog.", "author": "JingsongLi", "createdAt": "2020-07-13T04:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjU4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDQxOQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453794419", "bodyText": "I think it would be better to fix the Hadoop catalog in a separate PR and leave this one with the Assume until it is merged.", "author": "rdblue", "createdAt": "2020-07-13T16:59:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjU4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1MTUxNQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r454051515", "bodyText": "I'll create it.", "author": "JingsongLi", "createdAt": "2020-07-14T01:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjU4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\nindex 1abebf7e9..d9d4f0d71 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n\n@@ -26,7 +26,6 @@ import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.types.Types;\n import org.junit.After;\n import org.junit.Assert;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMjg1OQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453112859", "bodyText": "It isn't clear from this name that this is for a database. How about renaming it to flinkDatabase?", "author": "rdblue", "createdAt": "2020-07-10T23:00:27Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.util.ArrayUtils;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class FlinkCatalogTestBase extends FlinkTestBase {\n+\n+  protected static final String DATABASE = \"db\";\n+  private static File warehouse = null;\n+\n+  @BeforeClass\n+  public static void createWarehouse() throws IOException {\n+    FlinkCatalogTestBase.warehouse = File.createTempFile(\"warehouse\", null);\n+    Assert.assertTrue(warehouse.delete());\n+  }\n+\n+  @AfterClass\n+  public static void dropWarehouse() {\n+    if (warehouse != null && warehouse.exists()) {\n+      warehouse.delete();\n+    }\n+  }\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"testhive\", new String[0] },\n+        new Object[] { \"testhadoop\", new String[0] },\n+        new Object[] { \"testhadoop\", new String[] { \"l0\", \"l1\" }},\n+    };\n+  }\n+\n+  protected final TableEnvironment tEnv =\n+      TableEnvironment.create(EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n+\n+  protected final String catalogName;\n+  protected final String[] baseNamespace;\n+  protected final Catalog validationCatalog;\n+  protected final SupportsNamespaces validationNamespaceCatalog;\n+  protected final org.apache.flink.table.catalog.Catalog flinkCatalog;\n+\n+  protected final String flinkIdentifier;", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java\nindex 295e89ce9..743325f4d 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java\n\n@@ -21,7 +21,6 @@ package org.apache.iceberg.flink;\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.HashMap;\n import java.util.Map;\n import org.apache.flink.table.api.EnvironmentSettings;\n import org.apache.flink.table.api.TableEnvironment;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMzE2NA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453113164", "bodyText": "Why does this require assertThrowsCause? Is it wrapped in a generic SQL failure exception?", "author": "rdblue", "createdAt": "2020-07-10T23:01:35Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.util.Map;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {\n+\n+  public TestFlinkCatalogDatabase(String catalogName, String[] baseNamepace) {\n+    super(catalogName, baseNamepace);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.tl\", flinkIdentifier);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testCreateNamespace() {\n+    Assert.assertFalse(\n+        \"Database should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Database should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDefaultDatabase() {\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertEquals(\"Should use the current catalog\", tEnv.getCurrentCatalog(), catalogName);\n+    Assert.assertEquals(\"Should use the configured default namespace\", tEnv.getCurrentDatabase(), \"default\");\n+  }\n+\n+  @Test\n+  public void testDropEmptyDatabase() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"DROP DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertFalse(\n+        \"Namespace should have been dropped\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDropNonEmptyNamespace() {\n+    Assume.assumeFalse(\"Hadoop catalog throws IOException: Directory is not empty.\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"),\n+        new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get())));\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+    Assert.assertTrue(\"Table should exist\", validationCatalog.tableExists(TableIdentifier.of(icebergNamespace, \"tl\")));\n+\n+    AssertHelpers.assertThrowsCause(", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyOTAyMg==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453429022", "bodyText": "Yes, in Flink, will wrap an exception like: throw new ValidationException(\"Could not execute DROP DATABASE\", e);, in the top exception, there is no the message that we want to check.", "author": "JingsongLi", "createdAt": "2020-07-13T04:31:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMzE2NA=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\nindex 1abebf7e9..d9d4f0d71 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n\n@@ -26,7 +26,6 @@ import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.types.Types;\n import org.junit.After;\n import org.junit.Assert;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMzUwMg==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453113502", "bodyText": "Should this call SHOW TABLES?", "author": "rdblue", "createdAt": "2020-07-10T23:02:54Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.util.Map;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {\n+\n+  public TestFlinkCatalogDatabase(String catalogName, String[] baseNamepace) {\n+    super(catalogName, baseNamepace);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.tl\", flinkIdentifier);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testCreateNamespace() {\n+    Assert.assertFalse(\n+        \"Database should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Database should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDefaultDatabase() {\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertEquals(\"Should use the current catalog\", tEnv.getCurrentCatalog(), catalogName);\n+    Assert.assertEquals(\"Should use the configured default namespace\", tEnv.getCurrentDatabase(), \"default\");\n+  }\n+\n+  @Test\n+  public void testDropEmptyDatabase() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"DROP DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertFalse(\n+        \"Namespace should have been dropped\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDropNonEmptyNamespace() {\n+    Assume.assumeFalse(\"Hadoop catalog throws IOException: Directory is not empty.\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"),\n+        new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get())));\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+    Assert.assertTrue(\"Table should exist\", validationCatalog.tableExists(TableIdentifier.of(icebergNamespace, \"tl\")));\n+\n+    AssertHelpers.assertThrowsCause(\n+        \"Should fail if trying to delete a non-empty database\",\n+        DatabaseNotEmptyException.class,\n+        String.format(\"Database %s in catalog %s is not empty.\", DATABASE, catalogName),\n+        () -> sql(\"DROP DATABASE %s\", flinkIdentifier));\n+\n+    sql(\"DROP TABLE %s.tl\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testListTables() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Assert.assertEquals(\"Should not list any tables\", 0, tEnv.listTables().length);", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyOTMwMw==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453429303", "bodyText": "In Flink 1.10, not support DDL SHOW TABLES. It is supported in 1.11.", "author": "JingsongLi", "createdAt": "2020-07-13T04:32:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExMzUwMg=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\nindex 1abebf7e9..d9d4f0d71 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n\n@@ -26,7 +26,6 @@ import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.types.Types;\n import org.junit.After;\n import org.junit.Assert;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExNDUyOQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453114529", "bodyText": "What is TEMPORARY_FOLDER? I don't see it elsewhere in this PR.", "author": "rdblue", "createdAt": "2020-07-10T23:06:32Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.util.Map;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {\n+\n+  public TestFlinkCatalogDatabase(String catalogName, String[] baseNamepace) {\n+    super(catalogName, baseNamepace);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.tl\", flinkIdentifier);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testCreateNamespace() {\n+    Assert.assertFalse(\n+        \"Database should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Database should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDefaultDatabase() {\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertEquals(\"Should use the current catalog\", tEnv.getCurrentCatalog(), catalogName);\n+    Assert.assertEquals(\"Should use the configured default namespace\", tEnv.getCurrentDatabase(), \"default\");\n+  }\n+\n+  @Test\n+  public void testDropEmptyDatabase() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"DROP DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertFalse(\n+        \"Namespace should have been dropped\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDropNonEmptyNamespace() {\n+    Assume.assumeFalse(\"Hadoop catalog throws IOException: Directory is not empty.\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"),\n+        new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get())));\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+    Assert.assertTrue(\"Table should exist\", validationCatalog.tableExists(TableIdentifier.of(icebergNamespace, \"tl\")));\n+\n+    AssertHelpers.assertThrowsCause(\n+        \"Should fail if trying to delete a non-empty database\",\n+        DatabaseNotEmptyException.class,\n+        String.format(\"Database %s in catalog %s is not empty.\", DATABASE, catalogName),\n+        () -> sql(\"DROP DATABASE %s\", flinkIdentifier));\n+\n+    sql(\"DROP TABLE %s.tl\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testListTables() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Assert.assertEquals(\"Should not list any tables\", 0, tEnv.listTables().length);\n+\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"),\n+        new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get())));\n+\n+    Assert.assertEquals(\"Only 1 table\", 1, tEnv.listTables().length);\n+    Assert.assertEquals(\"Table name should match\", \"tl\", tEnv.listTables()[0]);\n+  }\n+\n+  @Test\n+  public void testListNamespace() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    String[] databases = tEnv.listDatabases();\n+\n+    if (isHadoopCatalog) {\n+      Assert.assertEquals(\"Should have 1 database\", 1, databases.length);\n+      Assert.assertEquals(\"Should have only db database\", \"db\", databases[0]);\n+\n+      if (baseNamespace.length > 0) {\n+        // test namespace not belongs to this catalog\n+        validationNamespaceCatalog.createNamespace(Namespace.of(baseNamespace[0], \"UNKNOWN_NAMESPACE\"));\n+        databases = tEnv.listDatabases();\n+        Assert.assertEquals(\"Should have 1 database\", 1, databases.length);\n+        Assert.assertEquals(\"Should have only db database\", \"db\", databases[0]);\n+      }\n+    } else {\n+      Assert.assertEquals(\"Should have 2 databases\", 2, databases.length);\n+      Assert.assertEquals(\n+          \"Should have default and db databases\",\n+          ImmutableSet.of(\"default\", \"db\"),\n+          ImmutableSet.copyOf(databases));\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateNamespaceWithMetadata() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s WITH ('prop'='value')\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(icebergNamespace);\n+\n+    Assert.assertEquals(\"Namespace should have expected prop value\", \"value\", nsMetadata.get(\"prop\"));\n+  }\n+\n+  @Test\n+  public void testCreateNamespaceWithComment() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s COMMENT 'namespace doc'\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(icebergNamespace);\n+\n+    Assert.assertEquals(\"Namespace should have expected comment\", \"namespace doc\", nsMetadata.get(\"comment\"));\n+  }\n+\n+  @Test\n+  public void testCreateNamespaceWithLocation() throws Exception {\n+    Assume.assumeFalse(\"HadoopCatalog does not support namespace locations\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    File location = TEMPORARY_FOLDER.newFile();", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyOTU0MA==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453429540", "bodyText": "TEMPORARY_FOLDER is from Flink test base class AbstractTestBase.", "author": "JingsongLi", "createdAt": "2020-07-13T04:34:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExNDUyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\nindex 1abebf7e9..d9d4f0d71 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n\n@@ -26,7 +26,6 @@ import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.types.Types;\n import org.junit.After;\n import org.junit.Assert;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzExNDgzMQ==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r453114831", "bodyText": "Do we need a test to validate that the CREATE DATABASE statement fails for Hadoop?", "author": "rdblue", "createdAt": "2020-07-10T23:07:33Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.util.Map;\n+import org.apache.flink.table.catalog.exceptions.DatabaseNotEmptyException;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {\n+\n+  public TestFlinkCatalogDatabase(String catalogName, String[] baseNamepace) {\n+    super(catalogName, baseNamepace);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.tl\", flinkIdentifier);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testCreateNamespace() {\n+    Assert.assertFalse(\n+        \"Database should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Database should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDefaultDatabase() {\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertEquals(\"Should use the current catalog\", tEnv.getCurrentCatalog(), catalogName);\n+    Assert.assertEquals(\"Should use the configured default namespace\", tEnv.getCurrentDatabase(), \"default\");\n+  }\n+\n+  @Test\n+  public void testDropEmptyDatabase() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"DROP DATABASE %s\", flinkIdentifier);\n+\n+    Assert.assertFalse(\n+        \"Namespace should have been dropped\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+  }\n+\n+  @Test\n+  public void testDropNonEmptyNamespace() {\n+    Assume.assumeFalse(\"Hadoop catalog throws IOException: Directory is not empty.\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"),\n+        new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get())));\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+    Assert.assertTrue(\"Table should exist\", validationCatalog.tableExists(TableIdentifier.of(icebergNamespace, \"tl\")));\n+\n+    AssertHelpers.assertThrowsCause(\n+        \"Should fail if trying to delete a non-empty database\",\n+        DatabaseNotEmptyException.class,\n+        String.format(\"Database %s in catalog %s is not empty.\", DATABASE, catalogName),\n+        () -> sql(\"DROP DATABASE %s\", flinkIdentifier));\n+\n+    sql(\"DROP TABLE %s.tl\", flinkIdentifier);\n+  }\n+\n+  @Test\n+  public void testListTables() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Assert.assertEquals(\"Should not list any tables\", 0, tEnv.listTables().length);\n+\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"),\n+        new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get())));\n+\n+    Assert.assertEquals(\"Only 1 table\", 1, tEnv.listTables().length);\n+    Assert.assertEquals(\"Table name should match\", \"tl\", tEnv.listTables()[0]);\n+  }\n+\n+  @Test\n+  public void testListNamespace() {\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s\", flinkIdentifier);\n+    sql(\"USE CATALOG %s\", catalogName);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    String[] databases = tEnv.listDatabases();\n+\n+    if (isHadoopCatalog) {\n+      Assert.assertEquals(\"Should have 1 database\", 1, databases.length);\n+      Assert.assertEquals(\"Should have only db database\", \"db\", databases[0]);\n+\n+      if (baseNamespace.length > 0) {\n+        // test namespace not belongs to this catalog\n+        validationNamespaceCatalog.createNamespace(Namespace.of(baseNamespace[0], \"UNKNOWN_NAMESPACE\"));\n+        databases = tEnv.listDatabases();\n+        Assert.assertEquals(\"Should have 1 database\", 1, databases.length);\n+        Assert.assertEquals(\"Should have only db database\", \"db\", databases[0]);\n+      }\n+    } else {\n+      Assert.assertEquals(\"Should have 2 databases\", 2, databases.length);\n+      Assert.assertEquals(\n+          \"Should have default and db databases\",\n+          ImmutableSet.of(\"default\", \"db\"),\n+          ImmutableSet.copyOf(databases));\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateNamespaceWithMetadata() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s WITH ('prop'='value')\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(icebergNamespace);\n+\n+    Assert.assertEquals(\"Namespace should have expected prop value\", \"value\", nsMetadata.get(\"prop\"));\n+  }\n+\n+  @Test\n+  public void testCreateNamespaceWithComment() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+\n+    Assert.assertFalse(\n+        \"Namespace should not already exist\",\n+        validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    sql(\"CREATE DATABASE %s COMMENT 'namespace doc'\", flinkIdentifier);\n+\n+    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(icebergNamespace));\n+\n+    Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(icebergNamespace);\n+\n+    Assert.assertEquals(\"Namespace should have expected comment\", \"namespace doc\", nsMetadata.get(\"comment\"));\n+  }\n+\n+  @Test\n+  public void testCreateNamespaceWithLocation() throws Exception {\n+    Assume.assumeFalse(\"HadoopCatalog does not support namespace locations\", isHadoopCatalog);", "originalCommit": "248278f444044d11233d41b81bfcb999f63dadb5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\nindex 1abebf7e9..d9d4f0d71 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java\n\n@@ -26,7 +26,6 @@ import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.types.Types;\n import org.junit.After;\n import org.junit.Assert;\n"}}, {"oid": "6a8faaa1568601075b7d2927dd676dd17812c3e2", "url": "https://github.com/apache/iceberg/commit/6a8faaa1568601075b7d2927dd676dd17812c3e2", "message": "Try to fix TestHiveMetastore", "committedDate": "2020-07-14T02:36:36Z", "type": "forcePushed"}, {"oid": "aed759616b235b3397ab65b980e0f2a9b92d34d6", "url": "https://github.com/apache/iceberg/commit/aed759616b235b3397ab65b980e0f2a9b92d34d6", "message": "Address comments", "committedDate": "2020-07-14T03:24:27Z", "type": "forcePushed"}, {"oid": "6a4a84afa204ca3f10129ec70f6bda0af40d1097", "url": "https://github.com/apache/iceberg/commit/6a4a84afa204ca3f10129ec70f6bda0af40d1097", "message": "Integrate Iceberg catalog to Flink catalog", "committedDate": "2020-07-16T02:37:15Z", "type": "forcePushed"}, {"oid": "835e0ba93555b252e4dea4b188f548e3103000cf", "url": "https://github.com/apache/iceberg/commit/835e0ba93555b252e4dea4b188f548e3103000cf", "message": "Integrate Iceberg catalog to Flink catalog", "committedDate": "2020-07-16T02:39:04Z", "type": "commit"}, {"oid": "14283d28990de535059d0d683c627cf04ba7beb3", "url": "https://github.com/apache/iceberg/commit/14283d28990de535059d0d683c627cf04ba7beb3", "message": "Rebase & Update to 1.11", "committedDate": "2020-07-16T02:57:21Z", "type": "commit"}, {"oid": "14283d28990de535059d0d683c627cf04ba7beb3", "url": "https://github.com/apache/iceberg/commit/14283d28990de535059d0d683c627cf04ba7beb3", "message": "Rebase & Update to 1.11", "committedDate": "2020-07-16T02:57:21Z", "type": "forcePushed"}, {"oid": "e57e5348000db438437315b6aa800a863eeedf09", "url": "https://github.com/apache/iceberg/commit/e57e5348000db438437315b6aa800a863eeedf09", "message": "Revert gradle-wrapper", "committedDate": "2020-07-16T06:09:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMjMyMg==", "url": "https://github.com/apache/iceberg/pull/1182#discussion_r457712322", "bodyText": "Nit: the method names weren't updated.", "author": "rdblue", "createdAt": "2020-07-20T21:51:40Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.concurrent.ConcurrentMap;\n+import org.apache.flink.table.catalog.Catalog;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+public abstract class FlinkTestBase extends AbstractTestBase {\n+\n+  private static TestHiveMetastore metastore = null;\n+  protected static HiveConf hiveConf = null;\n+  protected static HiveCatalog catalog = null;\n+  protected static ConcurrentMap<String, Catalog> flinkCatalogs;\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {", "originalCommit": "e57e5348000db438437315b6aa800a863eeedf09", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}