{"pr_number": 1443, "pr_title": "Core: Fix table load error on corrupted version-hint.text file", "pr_createdAt": "2020-09-11T12:56:58Z", "pr_url": "https://github.com/apache/iceberg/pull/1443", "timeline": [{"oid": "64a736d8275c0493081c385a59514b072b134b18", "url": "https://github.com/apache/iceberg/commit/64a736d8275c0493081c385a59514b072b134b18", "message": "Core: Fix corrupted version-hint.text preventing table load", "committedDate": "2020-09-11T12:54:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MjM5OA==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487272398", "bodyText": "Should we move Util.getFs out of this try/catch block? An IOException from not being able to load the FS should probably fail instead of returning a hint.", "author": "rdblue", "createdAt": "2020-09-11T20:23:24Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);", "originalCommit": "64a736d8275c0493081c385a59514b072b134b18", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc1MDUyMw==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487750523", "bodyText": "Done", "author": "pvary", "createdAt": "2020-09-14T08:47:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MjM5OA=="}], "type": "inlineReview", "revised_code": {"commit": "a178ce70ca5ea2ac4b63e14d536b7daaf5f5c40c", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\nindex a1704786f..dc818bb6e 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\n\n@@ -279,13 +279,11 @@ public class HadoopTableOperations implements TableOperations {\n     }\n   }\n \n-  private int readVersionHint() {\n+  @VisibleForTesting\n+  int readVersionHint() {\n     Path versionHintFile = versionHintFile();\n     try {\n       FileSystem fs = Util.getFs(versionHintFile, conf);\n-      if (!fs.exists(versionHintFile)) {\n-        return 0;\n-      }\n \n       try (InputStreamReader fsr = new InputStreamReader(fs.open(versionHintFile), StandardCharsets.UTF_8);\n            BufferedReader in = new BufferedReader(fsr)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MzA3MA==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487273070", "bodyText": "I don't think this is correct. If the hint file is missing, the table should still be readable.", "author": "rdblue", "createdAt": "2020-09-11T20:25:03Z", "path": "core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java", "diffHunk": "@@ -441,4 +447,70 @@ public void testDropNamespace() throws IOException {\n     FileSystem fs = Util.getFs(new Path(metaLocation), conf);\n     Assert.assertFalse(fs.isDirectory(new Path(metaLocation)));\n   }\n+\n+  @Test\n+  public void testVersionHintFile() throws Exception {\n+    Configuration conf = new Configuration();\n+    String warehousePath = temp.newFolder().getAbsolutePath();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);\n+\n+    // Create a test table with multiple versions\n+    TableIdentifier tableId = TableIdentifier.of(\"tbl\");\n+    Table table = catalog.createTable(tableId, SCHEMA, PartitionSpec.unpartitioned());\n+\n+    DataFile dataFile1 = DataFiles.builder(SPEC)\n+        .withPath(\"/a.parquet\")\n+        .withFileSizeInBytes(10)\n+        .withRecordCount(1)\n+        .build();\n+\n+    DataFile dataFile2 = DataFiles.builder(SPEC)\n+        .withPath(\"/b.parquet\")\n+        .withFileSizeInBytes(10)\n+        .withRecordCount(1)\n+        .build();\n+\n+    table.newAppend().appendFile(dataFile1).commit();\n+    table.newAppend().appendFile(dataFile2).commit();\n+    long secondSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    // Get the version-hint.text file location\n+    String versionHintLocation = ((HadoopTableOperations) catalog.newTableOps(tableId)).versionHintFile().toString();\n+\n+    // Write old data to confirm that we are writing the correct file\n+    FileIO io = new HadoopFileIO(conf);\n+    io.deleteFile(versionHintLocation);\n+    try (PositionOutputStream stream = io.newOutputFile(versionHintLocation).create()) {\n+      stream.write(\"1\".getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Write newer data to confirm that we are writing the correct file\n+    io.deleteFile(versionHintLocation);\n+    try (PositionOutputStream stream = io.newOutputFile(versionHintLocation).create()) {\n+      stream.write(\"3\".getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Write an empty version hint file\n+    io.deleteFile(versionHintLocation);\n+    io.newOutputFile(versionHintLocation).create().close();\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Just delete the file - double check that we have manipulated the correct file\n+    io.deleteFile(versionHintLocation);\n+\n+    // Check that exception is thrown\n+    AssertHelpers.assertThrows(\n+        \"Should not be able to find the table\",\n+        NoSuchTableException.class,\n+        \"Table does not exist: tbl\",\n+        () -> catalog.loadTable(tableId));", "originalCommit": "64a736d8275c0493081c385a59514b072b134b18", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc1MTk4Nw==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487751987", "bodyText": "Thanks for the review @rdblue!\nThat was another codepath which I did not changed to keep the changes minimal, but I agree with you comment, and corrected that path as well.", "author": "pvary", "createdAt": "2020-09-14T08:50:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MzA3MA=="}], "type": "inlineReview", "revised_code": {"commit": "a178ce70ca5ea2ac4b63e14d536b7daaf5f5c40c", "chunk": "diff --git a/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java b/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java\nindex e886776e9..6aafe1e15 100644\n--- a/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java\n\n@@ -457,6 +456,7 @@ public class TestHadoopCatalog extends HadoopTableTestBase {\n     // Create a test table with multiple versions\n     TableIdentifier tableId = TableIdentifier.of(\"tbl\");\n     Table table = catalog.createTable(tableId, SCHEMA, PartitionSpec.unpartitioned());\n+    HadoopTableOperations tableOperations = (HadoopTableOperations) catalog.newTableOps(tableId);\n \n     DataFile dataFile1 = DataFiles.builder(SPEC)\n         .withPath(\"/a.parquet\")\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487500479", "bodyText": "Should this return 0 instead? Similar to when the version hint file does not exist?", "author": "shardulm94", "createdAt": "2020-09-13T08:39:57Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      // We just assume corrupted metadata and start to read from the first version file\n+      return 1;", "originalCommit": "64a736d8275c0493081c385a59514b072b134b18", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzczNjgxNQ==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487736815", "bodyText": "When we are returning 0 the caller will end up throwing the following exception:\nTable does not exist: tbl\norg.apache.iceberg.exceptions.NoSuchTableException: Table does not exist: tbl\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:108)\n\tat org.apache.iceberg.hadoop.TestHadoopCatalog.testVersionHintFile(TestHadoopCatalog.java:508)\n\nThe goal of the PR is to recover from the corrupted version-hint.txt file, so I have returned 1 instead.\nBut this question made me wonder if returning 1 is a good idea or not, so I will check with Ryan", "author": "pvary", "createdAt": "2020-09-14T08:25:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc1NjUwMw==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487756503", "bodyText": "@rdblue: I have checked @shardulm94's comment and started to think about the possible problems/solutions.\nThe main question is:\n\nAre the metadataFiles kept forever? Or are they cleaned-up with compaction, or whatever other process?\n\nIf they are kept forever then it could be ok to start from 1. If they could be removed somehow then we might end up failing to read the removed version file and failing anyway. Could this be a problem? Could it be a good solution to list the metadata files for this case and trying to recover using that listing?\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-09-14T08:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA4ODI0NQ==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r488088245", "bodyText": "0 is currently used to signal that the version hint doesn't exist. The refresh  method checks whether the version hint was missing so that it can throw an exception when the metadata is missing: if there is no metadata file and the version hint was 0, then the table doesn't exist yet. If the version hint is non-zero, then the problem is that the metadata file is missing.\nI like the solution here that does some recovery to find the right version. We should probably rename the method since it is doing recovery, but it's fine for now.\nThe recovery that is done in readVersionHint is something we can improve on. We could use file listing if the v1 metadata file doesn't exist. That would be a fairly safe way of recovering the version hint. I think that means there should be 2 follow-ups:\n\nImprove how we write the hint to avoid corruption in HDFS by writing a file to a unique name, then renaming it to version-hint.txt. That requires overwriting with the rename, so I'm not sure if we can actually do this.\nImprove version recovery when metadata files have been cleaned up, probably using list operations.", "author": "rdblue", "createdAt": "2020-09-14T17:02:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM2Nzg3MA==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r489367870", "bodyText": "#1465 is created for the 2nd point", "author": "pvary", "createdAt": "2020-09-16T11:35:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "a178ce70ca5ea2ac4b63e14d536b7daaf5f5c40c", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\nindex a1704786f..dc818bb6e 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\n\n@@ -279,13 +279,11 @@ public class HadoopTableOperations implements TableOperations {\n     }\n   }\n \n-  private int readVersionHint() {\n+  @VisibleForTesting\n+  int readVersionHint() {\n     Path versionHintFile = versionHintFile();\n     try {\n       FileSystem fs = Util.getFs(versionHintFile, conf);\n-      if (!fs.exists(versionHintFile)) {\n-        return 0;\n-      }\n \n       try (InputStreamReader fsr = new InputStreamReader(fs.open(versionHintFile), StandardCharsets.UTF_8);\n            BufferedReader in = new BufferedReader(fsr)) {\n"}}, {"oid": "a178ce70ca5ea2ac4b63e14d536b7daaf5f5c40c", "url": "https://github.com/apache/iceberg/commit/a178ce70ca5ea2ac4b63e14d536b7daaf5f5c40c", "message": "Adding recovery for more cases", "committedDate": "2020-09-14T08:02:05Z", "type": "commit"}, {"oid": "68d5c482e1e71e9f87a3064beeb69703c50a9343", "url": "https://github.com/apache/iceberg/commit/68d5c482e1e71e9f87a3064beeb69703c50a9343", "message": "Removed Util.getFs from the try/catch so we throw an exception if there is a problem with the path/fs", "committedDate": "2020-09-14T08:48:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA3OTgwMQ==", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r488079801", "bodyText": "Nit: unnecessary newline.", "author": "rdblue", "createdAt": "2020-09-14T16:50:21Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -277,21 +279,30 @@ private void writeVersionHint(int versionToWrite) {\n     }\n   }\n \n-  private int readVersionHint() {\n+  @VisibleForTesting\n+  int readVersionHint() {\n     Path versionHintFile = versionHintFile();\n-    try {\n-      FileSystem fs = Util.getFs(versionHintFile, conf);\n-      if (!fs.exists(versionHintFile)) {\n-        return 0;\n-      }\n+    FileSystem fs = Util.getFs(versionHintFile, conf);\n+\n+    try (InputStreamReader fsr = new InputStreamReader(fs.open(versionHintFile), StandardCharsets.UTF_8);\n+         BufferedReader in = new BufferedReader(fsr)) {\n+      return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n \n-      try (InputStreamReader fsr = new InputStreamReader(fs.open(versionHintFile), StandardCharsets.UTF_8);\n-           BufferedReader in = new BufferedReader(fsr)) {\n-        return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      try {\n+        if (getMetadataFile(1) != null) {\n+          // We just assume corrupted metadata and start to read from the first version file\n+          return 1;\n+", "originalCommit": "68d5c482e1e71e9f87a3064beeb69703c50a9343", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8056f7fcd91e7da438c6366a8d5faa1e33f132df", "chunk": "diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\nindex 1d83ccd19..3add626f3 100644\n--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java\n\n@@ -294,7 +294,6 @@ public class HadoopTableOperations implements TableOperations {\n         if (getMetadataFile(1) != null) {\n           // We just assume corrupted metadata and start to read from the first version file\n           return 1;\n-\n         }\n       } catch (IOException io) {\n         // We log this error only on debug level since this is just a problem in recovery path\n"}}, {"oid": "8056f7fcd91e7da438c6366a8d5faa1e33f132df", "url": "https://github.com/apache/iceberg/commit/8056f7fcd91e7da438c6366a8d5faa1e33f132df", "message": "Update HadoopTableOperations.java", "committedDate": "2020-09-14T17:03:02Z", "type": "commit"}]}