{"pr_number": 1936, "pr_title": "Flink: Support inferring parallelism for batch read.", "pr_createdAt": "2020-12-15T04:56:27Z", "pr_url": "https://github.com/apache/iceberg/pull/1936", "timeline": [{"oid": "d1b1b50dfd2f1d11bea431ca281e2c1864407ad2", "url": "https://github.com/apache/iceberg/commit/d1b1b50dfd2f1d11bea431ca281e2c1864407ad2", "message": "add parallelism optimize for IcebergTableSource", "committedDate": "2020-12-17T07:24:02Z", "type": "forcePushed"}, {"oid": "5efeef410f78f99fab909907128ff7ec070b7869", "url": "https://github.com/apache/iceberg/commit/5efeef410f78f99fab909907128ff7ec070b7869", "message": "add parallelism optimize for IcebergTableSource", "committedDate": "2020-12-18T03:00:48Z", "type": "forcePushed"}, {"oid": "adb96ec63828743c191fd66baaf66aab055f0636", "url": "https://github.com/apache/iceberg/commit/adb96ec63828743c191fd66baaf66aab055f0636", "message": "add parallelism optimize for IcebergTableSource", "committedDate": "2020-12-18T03:09:27Z", "type": "forcePushed"}, {"oid": "9cfe082290d3eaa30b5cb533ef00fba232520234", "url": "https://github.com/apache/iceberg/commit/9cfe082290d3eaa30b5cb533ef00fba232520234", "message": "add parallelism optimize for IcebergTableSource", "committedDate": "2020-12-18T08:20:10Z", "type": "forcePushed"}, {"oid": "af2df6f04785246a1b2c0677019fedd267360925", "url": "https://github.com/apache/iceberg/commit/af2df6f04785246a1b2c0677019fedd267360925", "message": "generate data by sql", "committedDate": "2020-12-21T05:58:14Z", "type": "forcePushed"}, {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "url": "https://github.com/apache/iceberg/commit/6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "message": "generate data by sql", "committedDate": "2021-01-11T01:34:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk1MDY5Mw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554950693", "bodyText": "The defaultValue(true) says deprecated now.  Let's change it to:\n  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");\nThe similar thing in TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.", "author": "openinx", "createdAt": "2021-01-11T10:31:17Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").defaultValue(true)", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\nindex eed6dc13f..44610c5c6 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\n\n@@ -29,11 +29,11 @@ public class FlinkTableOptions {\n   }\n \n   public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n-      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").defaultValue(true)\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n           .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n               \"If is true, source parallelism is inferred according to splits number.\\n\");\n \n   public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n-      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").defaultValue(100)\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").intType().defaultValue(100)\n           .withDescription(\"Sets max infer parallelism for source operator.\");\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk1NDA4Mg==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554954082", "bodyText": "How about moving those lines into a separate method ?", "author": "openinx", "createdAt": "2021-01-11T10:37:06Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex c11ff44a8..fc9d1bf67 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -205,33 +204,35 @@ public class FlinkSource {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-          if (max < 1) {\n-            throw new IllegalConfigurationException(\n-                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-          }\n-\n-          int splitNum = 0;\n-          try {\n-            FlinkInputSplit[] splits = format.createInputSplits(0);\n-            splitNum = splits.length;\n-          } catch (IOException e) {\n-            throw new RuntimeException(\"get input split  error.\", e);\n-          }\n-\n-          parallelism = Math.min(splitNum, max);\n-        }\n-\n-        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;\n-        parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n-\n-        return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+        return createInputDataStream(format);\n       } else {\n         throw new UnsupportedOperationException(\"The Unbounded mode is not supported yet\");\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = limit > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) limit;\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;\n+      parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n+\n+      return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+    }\n   }\n \n   private static boolean isBounded(ScanContext context) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk3ODk5MA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554978990", "bodyText": "Nit: use UncheckedIOException here.\n            throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);", "author": "openinx", "createdAt": "2021-01-11T11:21:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex c11ff44a8..fc9d1bf67 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -205,33 +204,35 @@ public class FlinkSource {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-          if (max < 1) {\n-            throw new IllegalConfigurationException(\n-                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-          }\n-\n-          int splitNum = 0;\n-          try {\n-            FlinkInputSplit[] splits = format.createInputSplits(0);\n-            splitNum = splits.length;\n-          } catch (IOException e) {\n-            throw new RuntimeException(\"get input split  error.\", e);\n-          }\n-\n-          parallelism = Math.min(splitNum, max);\n-        }\n-\n-        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;\n-        parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n-\n-        return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+        return createInputDataStream(format);\n       } else {\n         throw new UnsupportedOperationException(\"The Unbounded mode is not supported yet\");\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = limit > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) limit;\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;\n+      parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n+\n+      return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+    }\n   }\n \n   private static boolean isBounded(ScanContext context) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk3OTM0NA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554979344", "bodyText": "nit: Preconditions.checkState ?", "author": "openinx", "createdAt": "2021-01-11T11:22:29Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex c11ff44a8..fc9d1bf67 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -205,33 +204,35 @@ public class FlinkSource {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-          if (max < 1) {\n-            throw new IllegalConfigurationException(\n-                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-          }\n-\n-          int splitNum = 0;\n-          try {\n-            FlinkInputSplit[] splits = format.createInputSplits(0);\n-            splitNum = splits.length;\n-          } catch (IOException e) {\n-            throw new RuntimeException(\"get input split  error.\", e);\n-          }\n-\n-          parallelism = Math.min(splitNum, max);\n-        }\n-\n-        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;\n-        parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n-\n-        return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+        return createInputDataStream(format);\n       } else {\n         throw new UnsupportedOperationException(\"The Unbounded mode is not supported yet\");\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = limit > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) limit;\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;\n+      parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n+\n+      return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+    }\n   }\n \n   private static boolean isBounded(ScanContext context) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MDU3Mw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554980573", "bodyText": "It may be overflow when casting the long limit to integer  ?  I'd like to use (int) Math.min(parallelism, limit).", "author": "openinx", "createdAt": "2021-01-11T11:25:10Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);\n+          }\n+\n+          parallelism = Math.min(splitNum, max);\n+        }\n+\n+        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ2NzQ2NA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r555467464", "bodyText": "the parallelism is int type and the  limit is long type,  Math.min(parallelism, limit) will throws an exception,I add a judgment to prevent overflow.\n      int limitInt = limit > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) limit;\n      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;", "author": "zhangjun0x01", "createdAt": "2021-01-12T02:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MDU3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex c11ff44a8..fc9d1bf67 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -205,33 +204,35 @@ public class FlinkSource {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-          if (max < 1) {\n-            throw new IllegalConfigurationException(\n-                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-          }\n-\n-          int splitNum = 0;\n-          try {\n-            FlinkInputSplit[] splits = format.createInputSplits(0);\n-            splitNum = splits.length;\n-          } catch (IOException e) {\n-            throw new RuntimeException(\"get input split  error.\", e);\n-          }\n-\n-          parallelism = Math.min(splitNum, max);\n-        }\n-\n-        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;\n-        parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n-\n-        return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+        return createInputDataStream(format);\n       } else {\n         throw new UnsupportedOperationException(\"The Unbounded mode is not supported yet\");\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = limit > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) limit;\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;\n+      parallelism = Math.max(1, parallelism); // make sure that parallelism is at least 1\n+\n+      return env.createInput(format, rowTypeInfo).setParallelism(parallelism);\n+    }\n   }\n \n   private static boolean isBounded(ScanContext context) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MzQyNA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554983424", "bodyText": "Is there another way to assert the parallelism as expected value ?  Here we're using flink's planner to get the ExecNode ,  I'm concerning that we're using flink's Internal codes which would be a big trouble when upgrading the flink version.  Pls see this PR #1956", "author": "openinx", "createdAt": "2021-01-11T11:30:30Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n+\n+    // make sure to generate 2 CombinedScanTasks\n+    org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n+    Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n+    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Assert.assertTrue(fileScanTaskOptional.isPresent());\n+    long maxFileLen = fileScanTaskOptional.get().length();\n+    sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n+\n+    // 2 splits ,the parallelism is  2\n+    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n+\n+    // 2 splits  and limit is 1 ,the parallelism is  1\n+    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n+  }\n+\n+  private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTMwMTA3MQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r559301071", "bodyText": "Flink 1.12 does refactor ExecNode, I found an easier way to assert parallelism, I will update it later", "author": "zhangjun0x01", "createdAt": "2021-01-18T03:40:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MzQyNA=="}], "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\nindex 429b4fbbb..6335e8b43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n\n@@ -126,33 +126,37 @@ public class TestFlinkTableSource extends FlinkCatalogTestBase {\n   }\n \n   @Test\n-  public void testParallelismOptimize() {\n+  public void testInferedParallelism() {\n     sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n     sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n \n     TableEnvironment tenv = getTableEnv();\n \n     // empty table ,parallelism at least 1\n-    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    Table tableEmpty = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n \n     // make sure to generate 2 CombinedScanTasks\n     org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n     Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n-    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Optional<FileScanTask> fileScanTaskOptional = stream.max(Comparator.comparing(FileScanTask::length));\n     Assert.assertTrue(fileScanTaskOptional.isPresent());\n     long maxFileLen = fileScanTaskOptional.get().length();\n     sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n \n     // 2 splits ,the parallelism is  2\n-    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    Table tableSelect = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n \n     // 2 splits  and limit is 1 ,the parallelism is  1\n-    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    Table tableLimit = sqlQuery(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n   }\n \n+  private Table sqlQuery(String sql, Object... args) {\n+    return getTableEnv().sqlQuery(String.format(sql, args));\n+  }\n+\n   private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {\n     PlannerBase planner = (PlannerBase) ((TableEnvironmentImpl) tEnv).getPlanner();\n     RelNode relNode = planner.optimize(toRelNode(table));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4NDYzNw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554984637", "bodyText": "nit:  testParallelismOptimize -> testInferedParallelism", "author": "openinx", "createdAt": "2021-01-11T11:32:59Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\nindex 429b4fbbb..6335e8b43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n\n@@ -126,33 +126,37 @@ public class TestFlinkTableSource extends FlinkCatalogTestBase {\n   }\n \n   @Test\n-  public void testParallelismOptimize() {\n+  public void testInferedParallelism() {\n     sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n     sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n \n     TableEnvironment tenv = getTableEnv();\n \n     // empty table ,parallelism at least 1\n-    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    Table tableEmpty = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n \n     // make sure to generate 2 CombinedScanTasks\n     org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n     Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n-    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Optional<FileScanTask> fileScanTaskOptional = stream.max(Comparator.comparing(FileScanTask::length));\n     Assert.assertTrue(fileScanTaskOptional.isPresent());\n     long maxFileLen = fileScanTaskOptional.get().length();\n     sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n \n     // 2 splits ,the parallelism is  2\n-    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    Table tableSelect = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n \n     // 2 splits  and limit is 1 ,the parallelism is  1\n-    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    Table tableLimit = sqlQuery(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n   }\n \n+  private Table sqlQuery(String sql, Object... args) {\n+    return getTableEnv().sqlQuery(String.format(sql, args));\n+  }\n+\n   private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {\n     PlannerBase planner = (PlannerBase) ((TableEnvironmentImpl) tEnv).getPlanner();\n     RelNode relNode = planner.optimize(toRelNode(table));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4ODI2NQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554988265", "bodyText": "nit: how about introducing a small method:\n private Table sqlQuery(String sql, Object... args) {\n    return getTableEnv().sqlQuery(String.format(sql, args));\n  }", "author": "openinx", "createdAt": "2021-01-11T11:39:53Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));", "originalCommit": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "21ba46c286c0304bdcda28f717269114d1702d79", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\nindex 429b4fbbb..6335e8b43 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n\n@@ -126,33 +126,37 @@ public class TestFlinkTableSource extends FlinkCatalogTestBase {\n   }\n \n   @Test\n-  public void testParallelismOptimize() {\n+  public void testInferedParallelism() {\n     sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n     sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n \n     TableEnvironment tenv = getTableEnv();\n \n     // empty table ,parallelism at least 1\n-    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    Table tableEmpty = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n \n     // make sure to generate 2 CombinedScanTasks\n     org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n     Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n-    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Optional<FileScanTask> fileScanTaskOptional = stream.max(Comparator.comparing(FileScanTask::length));\n     Assert.assertTrue(fileScanTaskOptional.isPresent());\n     long maxFileLen = fileScanTaskOptional.get().length();\n     sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n \n     // 2 splits ,the parallelism is  2\n-    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    Table tableSelect = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n \n     // 2 splits  and limit is 1 ,the parallelism is  1\n-    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    Table tableLimit = sqlQuery(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME);\n     testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n   }\n \n+  private Table sqlQuery(String sql, Object... args) {\n+    return getTableEnv().sqlQuery(String.format(sql, args));\n+  }\n+\n   private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {\n     PlannerBase planner = (PlannerBase) ((TableEnvironmentImpl) tEnv).getPlanner();\n     RelNode relNode = planner.optimize(toRelNode(table));\n"}}, {"oid": "21ba46c286c0304bdcda28f717269114d1702d79", "url": "https://github.com/apache/iceberg/commit/21ba46c286c0304bdcda28f717269114d1702d79", "message": "fix some issues", "committedDate": "2021-01-12T02:13:06Z", "type": "forcePushed"}, {"oid": "5d3ca11b065155a0ced27e2712821d671751843d", "url": "https://github.com/apache/iceberg/commit/5d3ca11b065155a0ced27e2712821d671751843d", "message": "merge from master", "committedDate": "2021-01-14T08:25:28Z", "type": "forcePushed"}, {"oid": "4769b00c7df53e6d3beb305caf147781dfd771a2", "url": "https://github.com/apache/iceberg/commit/4769b00c7df53e6d3beb305caf147781dfd771a2", "message": "add assert context", "committedDate": "2021-01-16T11:34:11Z", "type": "forcePushed"}, {"oid": "b646fb3358ad4ba017a79d1c9ecc8c933dbf56a4", "url": "https://github.com/apache/iceberg/commit/b646fb3358ad4ba017a79d1c9ecc8c933dbf56a4", "message": "add assert context", "committedDate": "2021-01-16T14:12:32Z", "type": "forcePushed"}, {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "url": "https://github.com/apache/iceberg/commit/a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "message": "fix conflict", "committedDate": "2021-01-19T07:16:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxNDg4MA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560714880", "bodyText": "Nit:  I'd like to change this builder chain like the following ( That's more easy to read the change):\n  @Override\n  public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n    return FlinkSource.forRowData()\n        .env(execEnv)\n        .tableLoader(loader)\n        .project(getProjectedSchema())\n        .limit(limit)\n        .filters(filters)\n        .flinkConf(readableConfig)\n        .properties(properties)\n        .build();\n  }", "author": "openinx", "createdAt": "2021-01-20T06:49:06Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -79,13 +84,13 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters);\n+    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters, readableConfig);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n     return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).properties(properties).build();\n+        .filters(filters).flinkConf(readableConfig).properties(properties).build();", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\nindex fe719cff4..cba0b0e38 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\n\n@@ -89,8 +89,15 @@ public class IcebergTableSource\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n-    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).flinkConf(readableConfig).properties(properties).build();\n+    return FlinkSource.forRowData()\n+        .env(execEnv)\n+        .tableLoader(loader)\n+        .project(getProjectedSchema())\n+        .limit(limit)\n+        .filters(filters)\n+        .flinkConf(readableConfig)\n+        .properties(properties)\n+        .build();\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxODg2OA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560718868", "bodyText": "Nit: it's more clear to make each option definition into a separate line:\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\")\n          .booleanType()\n          .defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");", "author": "openinx", "createdAt": "2021-01-20T06:59:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\nindex 44610c5c6..145190c9d 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\n\n@@ -29,11 +29,15 @@ public class FlinkTableOptions {\n   }\n \n   public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n-      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\")\n+          .booleanType()\n+          .defaultValue(true)\n           .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n               \"If is true, source parallelism is inferred according to splits number.\\n\");\n \n   public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n-      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").intType().defaultValue(100)\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\")\n+          .intType()\n+          .defaultValue(100)\n           .withDescription(\"Sets max infer parallelism for source operator.\");\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxODk0Mw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560718943", "bodyText": "ditto", "author": "openinx", "createdAt": "2021-01-20T06:59:47Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n+          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n+              \"If is true, source parallelism is inferred according to splits number.\\n\");\n+\n+  public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").intType().defaultValue(100)", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\nindex 44610c5c6..145190c9d 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java\n\n@@ -29,11 +29,15 @@ public class FlinkTableOptions {\n   }\n \n   public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n-      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\")\n+          .booleanType()\n+          .defaultValue(true)\n           .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n               \"If is true, source parallelism is inferred according to splits number.\\n\");\n \n   public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n-      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").intType().defaultValue(100)\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\")\n+          .intType()\n+          .defaultValue(100)\n           .withDescription(\"Sets max infer parallelism for source operator.\");\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNDA3NA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560724074", "bodyText": "In this comment, I think I did not describe the things  clearly.   I mean  we could move the inferParallelism into a separate method, don't have to contains the DataStream constructing or chaining methods.\nprivate int inferParallelism(FlinkInputFormat format, ScanContext context) {\n   // ....\n}", "author": "openinx", "createdAt": "2021-01-20T07:13:30Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -197,7 +206,7 @@ public FlinkInputFormat buildFormat() {\n       TypeInformation<RowData> typeInfo = RowDataTypeInfo.of(FlinkSchemaUtil.convert(context.project()));\n \n       if (!context.isStreaming()) {\n-        return env.createInput(format, typeInfo);\n+        return createInputDataStream(format, context, typeInfo);", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 7ddc0729e..48922b82e 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -206,7 +206,8 @@ public class FlinkSource {\n       TypeInformation<RowData> typeInfo = RowDataTypeInfo.of(FlinkSchemaUtil.convert(context.project()));\n \n       if (!context.isStreaming()) {\n-        return createInputDataStream(format, context, typeInfo);\n+        int parallelism = inferParallelism(format, context);\n+        return env.createInput(format, typeInfo).setParallelism(parallelism);\n       } else {\n         StreamingMonitorFunction function = new StreamingMonitorFunction(tableLoader, context);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNTY1NQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560725655", "bodyText": "Nit:  I'd like to make this code more readable:\n      if (context.limit() > 0) {\n        int limit = context.limit() >= Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n        parallelism = Math.min(parallelism, limit);\n      }\n\n     // parallelism must be positive.\n      parallelism = Math.max(1, parallelism);", "author": "openinx", "createdAt": "2021-01-20T07:17:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = context.limit() > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 7ddc0729e..48922b82e 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -218,14 +219,13 @@ public class FlinkSource {\n       }\n     }\n \n-    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n-                                                      TypeInformation<RowData> typeInfo) {\n-      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-        Preconditions.checkState(max >= 1,\n+    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n+      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(maxInterParallelism >= 1,\n             FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-        int splitNum = 0;\n+        int splitNum;\n         try {\n           FlinkInputSplit[] splits = format.createInputSplits(0);\n           splitNum = splits.length;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560727935", "bodyText": "For those users that write flink batch jobs in Java API ,  they will always pass a flink's Configuration, right ?   So how about defining this as  org.apache.flink.configuration.Configuraiton  ?", "author": "openinx", "createdAt": "2021-01-20T07:22:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig flinkConf;", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUwMjI0MQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561502241", "bodyText": "When we construct the IcebergTableSource, we use the TableSourceFactory.Context#getConfiguration method to get the configuration. This method returns a ReadableConfig, so we use ReadableConfig instead of Configuration. In addition, Configuration is the implementation class of the ReadableConfig interface, so I think ReadableConfig should not has a problem", "author": "zhangjun0x01", "createdAt": "2021-01-21T02:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUzMTA4OQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561531089", "bodyText": "Got your point,  I'd prefer to use flink's Configuration  because  it will be exposed to flink developers as an API in  FlinkSource,  using the unified Configuration will be more straightforward for them.  But as you said,  the  TableSourceFactory#Context is exposing the ReadableConfig,  I also did not find a correct way to convert ReadableConfig to Configuration.   OK, I think we could use ReadableConfig here,  thanks.", "author": "openinx", "createdAt": "2021-01-21T03:03:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 7ddc0729e..48922b82e 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -73,7 +73,7 @@ public class FlinkSource {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n-    private ReadableConfig flinkConf;\n+    private ReadableConfig readableConfig;\n     private final ScanContext.Builder contextBuilder = ScanContext.builder();\n \n     public Builder tableLoader(TableLoader newLoader) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyODg3Nw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560728877", "bodyText": "Nit: use maxInterParallelism pls.", "author": "openinx", "createdAt": "2021-01-20T07:24:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 7ddc0729e..48922b82e 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -218,14 +219,13 @@ public class FlinkSource {\n       }\n     }\n \n-    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n-                                                      TypeInformation<RowData> typeInfo) {\n-      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-        Preconditions.checkState(max >= 1,\n+    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n+      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(maxInterParallelism >= 1,\n             FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-        int splitNum = 0;\n+        int splitNum;\n         try {\n           FlinkInputSplit[] splits = format.createInputSplits(0);\n           splitNum = splits.length;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyOTcxNg==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560729716", "bodyText": "Nit:  this assignment is redundant ( from intellij).", "author": "openinx", "createdAt": "2021-01-20T07:26:52Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 7ddc0729e..48922b82e 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -218,14 +219,13 @@ public class FlinkSource {\n       }\n     }\n \n-    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n-                                                      TypeInformation<RowData> typeInfo) {\n-      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-        Preconditions.checkState(max >= 1,\n+    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n+      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(maxInterParallelism >= 1,\n             FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-        int splitNum = 0;\n+        int splitNum;\n         try {\n           FlinkInputSplit[] splits = format.createInputSplits(0);\n           splitNum = splits.length;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560737410", "bodyText": "Shouldn't the inferParallelism only affect the batch job (See FlinkSource#Builder#build)?  So there's no reason that providing unit test in streaming  mode ?\nIn my mind,  Providing unit tests to check whether the inferParallelism() is returning the expected parallelism value is enough for this changes.   Seems like The ITCase is validating the behavior of DataStreamSource#setParallelism ,  we could think it's always correct because it's a basic API in flink.", "author": "openinx", "createdAt": "2021-01-20T07:43:59Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "originalCommit": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczODQ2OQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560738469", "bodyText": "In this way, we don't have to change so many codes in this class. Maybe we could just add unit tests in TestFlinkScan.java", "author": "openinx", "createdAt": "2021-01-20T07:46:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDg1NzU0OQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560857549", "bodyText": "I found that in this test method,I use the flink streaming mode,but it still enter the batch mode (here), I check the code,found that FlinkSource.Builder#build mthod judge streaming mode or batch mode by the conf of ScanContext instead of flink conf. will this confuse users?", "author": "zhangjun0x01", "createdAt": "2021-01-20T10:37:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk4MzU0Ng==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560983546", "bodyText": "Okay, that's a great point.  I think it will confuse users,  the correct way is :   Set the ScanContext's properties firstly (use the following fromFlinkConf) if someone provides a flink configuration,  that is similar to the ScanContext#fromProperties:\ndiff --git a/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java b/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java\nindex 2896efb3..c56e3311 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java\n@@ -292,10 +292,7 @@ class ScanContext implements Serializable {\n       return this;\n     }\n \n-    Builder fromProperties(Map<String, String> properties) {\n-      Configuration config = new Configuration();\n-      properties.forEach(config::setString);\n-\n+    Builder fromFlinkConf(Configuration config) {\n       return this.useSnapshotId(config.get(SNAPSHOT_ID))\n           .caseSensitive(config.get(CASE_SENSITIVE))\n           .asOfTimestamp(config.get(AS_OF_TIMESTAMP))\n@@ -305,7 +302,14 @@ class ScanContext implements Serializable {\n           .splitLookback(config.get(SPLIT_LOOKBACK))\n           .splitOpenFileCost(config.get(SPLIT_FILE_OPEN_COST))\n           .streaming(config.get(STREAMING))\n-          .monitorInterval(config.get(MONITOR_INTERVAL))\n+          .monitorInterval(config.get(MONITOR_INTERVAL));\n+    }\n+\n+    Builder fromProperties(Map<String, String> properties) {\n+      Configuration config = new Configuration();\n+      properties.forEach(config::setString);\n+\n+      return fromFlinkConf(config)\n           .nameMapping(properties.get(DEFAULT_NAME_MAPPING));\n     }", "author": "openinx", "createdAt": "2021-01-20T14:05:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDk5MjM3OA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560992378", "bodyText": "If someone provides both flink Configuration and iceberg's properties, then we should use the flink's Configuration values overwrite the iceberg's properties because  properties is a table-level settings  while the flink's Configuration is a job-level settings. It is reasonable for fine-grained configuration to ovewrite coarse-grained configuration.", "author": "openinx", "createdAt": "2021-01-20T14:16:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTQ2MTY3MA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561461670", "bodyText": "I think these are two different concepts.\nIn flink, whether using batch mode or streaming mode, we can read batch data. Flink treats batch jobs as bounded streaming jobs, so there should be no problem whether it is using batch mode or streaming mode to read batch data.  In addition, flink will use StreamExecutionEnvironment (DataStream) to do batch tasks and stream tasks uniformly (the  doc link) . The batch mode may expire, so I think we should also use StreamExecutionEnvironment (DataStream) for batch tasks as much as possible.\nWhen we use StreamExecutionEnvironment , in FlinkSource.Builder#build method,\nthe if and else block in this method are both streaming jobs, if block is a bounded streaming jobs, maybe we can rename ScanContext#isStreaming field to ScanContext#isStreamingRead, which will be easier to understand. If code block is bounded stream job (batch), else code block to do long-running stream job.\n    if (!context.isStreaming()) {\n        int parallelism = inferParallelism(format, context);\n        return env.createInput(format, typeInfo).setParallelism(parallelism);\n      } else {\n        StreamingMonitorFunction function = new StreamingMonitorFunction(tableLoader, context);\n\n        String monitorFunctionName = String.format(\"Iceberg table (%s) monitor\", table);\n        String readerOperatorName = String.format(\"Iceberg table (%s) reader\", table);\n\n        return env.addSource(function, monitorFunctionName)\n            .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n      }", "author": "zhangjun0x01", "createdAt": "2021-01-21T01:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTQ5MTI4OA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561491288", "bodyText": "In my mind, Providing unit tests to check whether the inferParallelism() is returning the expected parallelism value is enough for this changes. Seems like The ITCase is validating the behavior of DataStreamSource#setParallelism , we could think it's always correct because it's a basic API in flink.\n\nI think it\u2019s better not to use the inferParallelism method to get the parallelism to do assertion, because the inferParallelism method is private and is an internal method of iceberg. Just as you commented that it is best not to use the internal code of flink, I think we should try to use public APIs to get information.\nThe current TestFlinkTableSource class uses batch mode for unit test. In order not to modify too much code, we can move the testInferedParallelism method to other test classes, such as TestFlinkScan.java.\nSo I think we can use DataStream.getTransformation().getParallelism(); to get the parallelism of the flink operator. This method is public api of flink. Even if flink is upgraded in the future, it should not be modified. What do you think?", "author": "zhangjun0x01", "createdAt": "2021-01-21T02:24:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUwNjMwMA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561506300", "bodyText": "The isStreaming  indicate whether the flink source is a streaming source (In our mind) ,  not say it's a streaming job or batch job.  The hive table source also has the similar configure key :\n    public static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n            key(\"streaming-source.enable\")\n                    .booleanType()\n                    .defaultValue(false)\n                    .withDescription(\n                            \"Enable streaming source or not.\\n\"\n                                    + \" NOTES: Please make sure that each partition/file should be written\"\n                                    + \" atomically, otherwise the reader may get incomplete data.\");\nIf we think this iceberg configure key is not very clear,  I think we could propose another separate PR to align with hive configure key.  Let's focus on this parallelism issue here,  what do you think ?", "author": "openinx", "createdAt": "2021-01-21T02:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUxMzAxNQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561513015", "bodyText": "yes,it should be a streaming source, like kafka. If necessary, we can open a separate PR to discuss this.", "author": "zhangjun0x01", "createdAt": "2021-01-21T02:45:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTUxNTE1Ng==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561515156", "bodyText": "Just as you commented that it is best not to use the internal code of flink, I think we should try to use public APIs to get information.\n\nThe comment that saying we'd better not use flink's Internal API because that would introduce extra upgrade complexity (new flink version may breaks those internal API so we iceberg have to adjust the codes,  finally maintaining different versions of flink will bring us a lot of burden).\nWriting iceberg unit tests based on our iceberg's non-public ( we usually use package-access ) method is OK because there's no extra burden .", "author": "openinx", "createdAt": "2021-01-21T02:47:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MTYzNjYxNg==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r561636616", "bodyText": "I update the pr,move the testInferedParallelism method to TestFlinkScanSql,use  FlinkSource.Builder#inferParallelism method to do the assertion", "author": "zhangjun0x01", "createdAt": "2021-01-21T06:46:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}], "type": "inlineReview", "revised_code": {"commit": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\nindex 8addd29ba..229812a51 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n\n@@ -782,60 +685,4 @@ public class TestFlinkTableSource extends FlinkTestBase {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n-\n-  /**\n-   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n-   * Table to flink DataStream, so we only use streaming mode here.\n-   *\n-   * @throws TableNotExistException table not exist exception\n-   */\n-  @Test\n-  public void testInferedParallelism() throws TableNotExistException {\n-    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);\n-\n-    // Empty table ,parallelism at least 1\n-    Table tableEmpty = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n-    assertParallelismEquals(tableEmpty, 1);\n-\n-    sql(\"INSERT INTO %s  VALUES (1,'hello',10.0)\", TABLE_NAME);\n-    sql(\"INSERT INTO %s  VALUES (2,'iceberg',20.0)\", TABLE_NAME);\n-\n-    // Make sure to generate 2 CombinedScanTasks\n-    Optional<Catalog> catalog = tEnv.getCatalog(CATALOG_NAME);\n-    Assert.assertTrue(\"Conversion should succeed\", catalog.isPresent());\n-    FlinkCatalog flinkCatalog = (FlinkCatalog) catalog.get();\n-    org.apache.iceberg.Table table = flinkCatalog.loadIcebergTable(new ObjectPath(DATABASE_NAME, TABLE_NAME));\n-    Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n-    Optional<FileScanTask> fileScanTaskOptional = stream.max(Comparator.comparing(FileScanTask::length));\n-    Assert.assertTrue(\"Conversion should succeed\", fileScanTaskOptional.isPresent());\n-    long maxFileLen = fileScanTaskOptional.get().length();\n-    sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME,\n-        maxFileLen);\n-\n-    // 2 splits ,the parallelism is  2\n-    Table tableSelect = sqlQuery(\"SELECT * FROM %s\", TABLE_NAME);\n-    assertParallelismEquals(tableSelect, 2);\n-\n-    // 2 splits  and limit is 1 ,the parallelism is  1\n-    Table tableLimit = sqlQuery(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME);\n-    assertParallelismEquals(tableLimit, 1);\n-  }\n-\n-  private Table sqlQuery(String sql, Object... args) {\n-    return getTableEnv().sqlQuery(String.format(sql, args));\n-  }\n-\n-  private void assertParallelismEquals(Table table, int expected) {\n-    Assert.assertTrue(\"The table environment should be StreamTableEnvironment\",\n-        getTableEnv() instanceof StreamTableEnvironment);\n-    StreamTableEnvironment stenv = (StreamTableEnvironment) getTableEnv();\n-    DataStream<Row> ds = stenv.toAppendStream(table, Row.class);\n-    int parallelism = ds.getTransformation().getParallelism();\n-    Assert.assertEquals(\"Should produce the expected parallelism \", expected, parallelism);\n-  }\n-\n-  private void useBatchModeAndInsertData() {\n-    Assume.assumeFalse(\"Should  be supported on batch mode\", isStreamingJob);\n-    sql(\"INSERT INTO %s VALUES (1,'iceberg',10),(2,'b',20),(3,CAST(NULL AS VARCHAR),30)\", TABLE_NAME);\n-  }\n }\n"}}, {"oid": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "url": "https://github.com/apache/iceberg/commit/cba8621ea0891c9abcdae9ab34821b15e953d0d8", "message": "add test case in TestFlinkScanSql", "committedDate": "2021-01-21T06:38:55Z", "type": "forcePushed"}, {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "url": "https://github.com/apache/iceberg/commit/47ae11a8a83dee8adebc3a66301a21d3652f0092", "message": "add test case in TestFlinkScanSql", "committedDate": "2021-01-21T07:19:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMzNTU2OA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562335568", "bodyText": "Nit:  maxInterParallelism -> maxInferParallelism,  seems like it's a typo ?", "author": "openinx", "createdAt": "2021-01-22T02:23:29Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +218,33 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n+      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 48922b82e..95a7ba936 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -218,33 +208,6 @@ public class FlinkSource {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n-\n-    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n-      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n-      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n-        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n-        Preconditions.checkState(maxInterParallelism >= 1,\n-            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n-        int splitNum;\n-        try {\n-          FlinkInputSplit[] splits = format.createInputSplits(0);\n-          splitNum = splits.length;\n-        } catch (IOException e) {\n-          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n-        }\n-\n-        parallelism = Math.min(splitNum, maxInterParallelism);\n-      }\n-\n-      if (context.limit() > 0) {\n-        int limit = context.limit() >= Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n-        parallelism = Math.min(parallelism, limit);\n-      }\n-\n-      // parallelism must be positive.\n-      parallelism = Math.max(1, parallelism);\n-      return parallelism;\n-    }\n   }\n \n   public static boolean isBounded(Map<String, String> properties) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMzODI3NQ==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562338275", "bodyText": "Nit:  Let's move this line to line60,  so that the assignment order of IcebergTableSource constructor could align with these definitions.", "author": "openinx", "createdAt": "2021-01-22T02:27:20Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -51,25 +52,29 @@\n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n+  private final ReadableConfig readableConfig;", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\nindex cba0b0e38..c3606be28 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\n\n@@ -58,9 +58,8 @@ public class IcebergTableSource\n   private final long limit;\n   private final List<org.apache.iceberg.expressions.Expression> filters;\n \n-  public IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties,\n-                            ReadableConfig readableConfig) {\n-    this(loader, schema, properties, null, false, -1, ImmutableList.of(), readableConfig);\n+  public IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties) {\n+    this(loader, schema, properties, null, false, -1, ImmutableList.of(), null);\n   }\n \n   private IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM0MDA2MA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562340060", "bodyText": "Nit:  maybe we'd better also align the orders as above commented.", "author": "openinx", "createdAt": "2021-01-22T02:29:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -79,13 +84,20 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters);\n+    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters, readableConfig);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n-    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).properties(properties).build();\n+    return FlinkSource.forRowData()", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\nindex cba0b0e38..c3606be28 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java\n\n@@ -89,15 +88,8 @@ public class IcebergTableSource\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n-    return FlinkSource.forRowData()\n-        .env(execEnv)\n-        .tableLoader(loader)\n-        .project(getProjectedSchema())\n-        .limit(limit)\n-        .filters(filters)\n-        .flinkConf(readableConfig)\n-        .properties(properties)\n-        .build();\n+    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n+        .filters(filters).flinkConf(readableConfig).properties(properties).build();\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MjA1Ng==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562372056", "bodyText": "We can disable the table.exec.iceberg.infer-source-parallelism  for all the batch tests by default, then we don't have to change all cases from this file.   Actually,  we have wrote many unit tests which depends on the parallelism, for example  this PR #2064.  Using the inter-parallelism for batch unit tests will introduce extra complexity and instability,  so I recommend to disable the infer parallelism in our batch unit tests by default:\ndiff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\nindex 5b8e58cf..ab3d56ea 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n@@ -62,10 +62,17 @@ public abstract class FlinkTestBase extends AbstractTestBase {\n     if (tEnv == null) {\n       synchronized (this) {\n         if (tEnv == null) {\n-          this.tEnv = TableEnvironment.create(EnvironmentSettings\n+          EnvironmentSettings settings = EnvironmentSettings\n               .newInstance()\n               .useBlinkPlanner()\n-              .inBatchMode().build());\n+              .inBatchMode()\n+              .build();\n+\n+          TableEnvironment env = TableEnvironment.create(settings);\n+          env.getConfig().getConfiguration()\n+              .set(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM, false);\n+\n+          tEnv = env;\n         }\n       }\n     }", "author": "openinx", "createdAt": "2021-01-22T03:54:04Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -137,7 +136,10 @@ public void testFilterPushDownEqual() {\n     Assert.assertEquals(\"Should have 1 record\", 1, result.size());\n     Assert.assertArrayEquals(\"Should produce the expected record\", expectRecord, result.get(0));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n+    // Because we add infer parallelism, all data files will be scanned first.\n+    // Flink will call FlinkInputFormat#createInputSplits method to scan the data files,\n+    // plus the operation to get the execution plan, so there are three scan event.\n+    Assert.assertEquals(\"Should create 3 scans\", 3, scanEventCount);", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ1MDQ3Mw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562450473", "bodyText": "yes,I update it", "author": "zhangjun0x01", "createdAt": "2021-01-22T08:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MjA1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\nindex ab3b0622f..d8099189c 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java\n\n@@ -136,10 +163,7 @@ public class TestFlinkTableSource extends FlinkTestBase {\n     Assert.assertEquals(\"Should have 1 record\", 1, result.size());\n     Assert.assertArrayEquals(\"Should produce the expected record\", expectRecord, result.get(0));\n \n-    // Because we add infer parallelism, all data files will be scanned first.\n-    // Flink will call FlinkInputFormat#createInputSplits method to scan the data files,\n-    // plus the operation to get the execution plan, so there are three scan event.\n-    Assert.assertEquals(\"Should create 3 scans\", 3, scanEventCount);\n+    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n     Assert.assertEquals(\"Should contain the push down filter\", expectedFilter, lastScanEvent.filter().toString());\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzIxNA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562373214", "bodyText": "Nit:  inter parallelism should be at least 1.", "author": "openinx", "createdAt": "2021-01-22T03:58:11Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\nindex 393eb6d19..4231e335b 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\n\n@@ -109,51 +106,6 @@ public class TestFlinkScanSql extends TestFlinkScan {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n-  @Test\n-  public void testInferedParallelism() throws IOException {\n-    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n-\n-    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n-    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n-    ScanContext scanContext = ScanContext.builder().build();\n-\n-    // Empty table ,parallelism at least 1\n-    int parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, scanContext);\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n-\n-    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n-    writeRecords.get(0).set(1, 123L);\n-    writeRecords.get(0).set(2, \"2020-03-20\");\n-    writeRecords.get(1).set(1, 456L);\n-    writeRecords.get(1).set(2, \"2020-03-20\");\n-\n-    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n-\n-    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n-    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n-        RandomGenericData.generate(SCHEMA, 2, 0L));\n-    helper.appendToTable(dataFile1, dataFile2);\n-\n-    // Make sure to generate 2 CombinedScanTasks\n-    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n-    executeSQL(String\n-        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n-\n-    // 2 splits ,the parallelism is  2\n-    parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, scanContext);\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 2, parallelism);\n-\n-    // 2 splits  and limit is 1 ,the parallelism is  1\n-    parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, ScanContext.builder().limit(1).build());\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n-  }\n-\n   private List<Row> executeSQL(String sql) {\n     return Lists.newArrayList(tEnv.executeSql(sql).collect());\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzgxMw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562373813", "bodyText": "Should we provide a new Configuration()  for this variable ?  Otherwise,  it will just throw NPE if people forget to provide a flinkConf in FlinkSource#Builder because we don't check the nullable in interParallelism.", "author": "openinx", "createdAt": "2021-01-22T04:00:29Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig readableConfig;", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ1MDM3Mw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562450373", "bodyText": "yes,I add the new Configuration() for default.", "author": "zhangjun0x01", "createdAt": "2021-01-22T08:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzgxMw=="}], "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\nindex 48922b82e..95a7ba936 100644\n--- a/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n+++ b/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\n@@ -73,7 +70,6 @@ public class FlinkSource {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n-    private ReadableConfig readableConfig;\n     private final ScanContext.Builder contextBuilder = ScanContext.builder();\n \n     public Builder tableLoader(TableLoader newLoader) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTE4Mw==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562375183", "bodyText": "Those random generated records will be located in partition 2020-03-21 ?   I guess it's not.", "author": "openinx", "createdAt": "2021-01-22T04:06:32Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ1MDAzNA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562450034", "bodyText": "At first I copy the code from TestFlinkScanSql#testResiduals method to gererate 2 datafiles.\nI think there should be no problem about the partition. writeRecords will write to the partition 2020-03-20, and randomly generate two records into the partition 2020-03-21.\nBut for simplicity, I modified the code to randomly generate two records for each partition.", "author": "zhangjun0x01", "createdAt": "2021-01-22T08:00:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTE4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\nindex 393eb6d19..4231e335b 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\n\n@@ -109,51 +106,6 @@ public class TestFlinkScanSql extends TestFlinkScan {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n-  @Test\n-  public void testInferedParallelism() throws IOException {\n-    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n-\n-    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n-    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n-    ScanContext scanContext = ScanContext.builder().build();\n-\n-    // Empty table ,parallelism at least 1\n-    int parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, scanContext);\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n-\n-    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n-    writeRecords.get(0).set(1, 123L);\n-    writeRecords.get(0).set(2, \"2020-03-20\");\n-    writeRecords.get(1).set(1, 456L);\n-    writeRecords.get(1).set(2, \"2020-03-20\");\n-\n-    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n-\n-    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n-    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n-        RandomGenericData.generate(SCHEMA, 2, 0L));\n-    helper.appendToTable(dataFile1, dataFile2);\n-\n-    // Make sure to generate 2 CombinedScanTasks\n-    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n-    executeSQL(String\n-        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n-\n-    // 2 splits ,the parallelism is  2\n-    parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, scanContext);\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 2, parallelism);\n-\n-    // 2 splits  and limit is 1 ,the parallelism is  1\n-    parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, ScanContext.builder().limit(1).build());\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n-  }\n-\n   private List<Row> executeSQL(String sql) {\n     return Lists.newArrayList(tEnv.executeSql(sql).collect());\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTk0NA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562375944", "bodyText": "I think there're other test cases that we don't cover, it's good to cover those tests.\n\ntable.exec.iceberg.infer-source-parallelism=false;\ntable.exec.iceberg.infer-source-parallelism.max <= numberOfSplits;\ntable.exec.iceberg.infer-source-parallelism.max > numberOfSplits;\ntable.exec.iceberg.infer-source-parallelism.max > limit;\ntable.exec.iceberg.infer-source-parallelism.max <= limit;\n\nDivide those cases into small method if necessary.", "author": "openinx", "createdAt": "2021-01-22T04:09:31Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));\n+    helper.appendToTable(dataFile1, dataFile2);\n+\n+    // Make sure to generate 2 CombinedScanTasks\n+    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n+    executeSQL(String\n+        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n+\n+    // 2 splits ,the parallelism is  2\n+    parallelism = FlinkSource.forRowData()", "originalCommit": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjQ0NjkwOA==", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562446908", "bodyText": "I add the test case , but I did not split these test cases into different methods because they share a lot of code. If they are split, there may be a lot of duplicate code.", "author": "zhangjun0x01", "createdAt": "2021-01-22T07:54:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTk0NA=="}], "type": "inlineReview", "revised_code": {"commit": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "chunk": "diff --git a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\nindex 393eb6d19..4231e335b 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java\n\n@@ -109,51 +106,6 @@ public class TestFlinkScanSql extends TestFlinkScan {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n-  @Test\n-  public void testInferedParallelism() throws IOException {\n-    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n-\n-    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n-    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n-    ScanContext scanContext = ScanContext.builder().build();\n-\n-    // Empty table ,parallelism at least 1\n-    int parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, scanContext);\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n-\n-    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n-    writeRecords.get(0).set(1, 123L);\n-    writeRecords.get(0).set(2, \"2020-03-20\");\n-    writeRecords.get(1).set(1, 456L);\n-    writeRecords.get(1).set(2, \"2020-03-20\");\n-\n-    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n-\n-    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n-    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n-        RandomGenericData.generate(SCHEMA, 2, 0L));\n-    helper.appendToTable(dataFile1, dataFile2);\n-\n-    // Make sure to generate 2 CombinedScanTasks\n-    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n-    executeSQL(String\n-        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n-\n-    // 2 splits ,the parallelism is  2\n-    parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, scanContext);\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 2, parallelism);\n-\n-    // 2 splits  and limit is 1 ,the parallelism is  1\n-    parallelism = FlinkSource.forRowData()\n-        .flinkConf(new Configuration())\n-        .inferParallelism(flinkInputFormat, ScanContext.builder().limit(1).build());\n-    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n-  }\n-\n   private List<Row> executeSQL(String sql) {\n     return Lists.newArrayList(tEnv.executeSql(sql).collect());\n   }\n"}}, {"oid": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "url": "https://github.com/apache/iceberg/commit/ef7083111d0b0a3eba31c73f0eb1438b050633ad", "message": "add parallelism optimize for IcebergTableSource", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "a58e0dd7fdd19097f6e4941f10c360592bb7e19e", "url": "https://github.com/apache/iceberg/commit/a58e0dd7fdd19097f6e4941f10c360592bb7e19e", "message": "generate data by sql", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "92a6983540b0bb2e9cfa38bc327564885d2aedb1", "url": "https://github.com/apache/iceberg/commit/92a6983540b0bb2e9cfa38bc327564885d2aedb1", "message": "fix some issues", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "2abc15e811cb9ef161ff4d4da00800a052a04e4f", "url": "https://github.com/apache/iceberg/commit/2abc15e811cb9ef161ff4d4da00800a052a04e4f", "message": "merge from master", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "dd92b66becd8f3966c52c6dc3a3335ffd434842a", "url": "https://github.com/apache/iceberg/commit/dd92b66becd8f3966c52c6dc3a3335ffd434842a", "message": "add assert context", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "f73d8c62173abb632c5f7eee900b7439146e6f52", "url": "https://github.com/apache/iceberg/commit/f73d8c62173abb632c5f7eee900b7439146e6f52", "message": "get the parallelism simply", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "e31f3193233fe8ac539764aa83634b5aac6dc4e4", "url": "https://github.com/apache/iceberg/commit/e31f3193233fe8ac539764aa83634b5aac6dc4e4", "message": "fix conflict", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "8d925daaeef5b06bb12c9c0db2331294c5f926b3", "url": "https://github.com/apache/iceberg/commit/8d925daaeef5b06bb12c9c0db2331294c5f926b3", "message": "add test case in TestFlinkScanSql", "committedDate": "2021-01-22T06:02:02Z", "type": "commit"}, {"oid": "5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "url": "https://github.com/apache/iceberg/commit/5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "message": "fix some issues , add test case", "committedDate": "2021-01-22T07:51:51Z", "type": "commit"}, {"oid": "5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "url": "https://github.com/apache/iceberg/commit/5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "message": "fix some issues , add test case", "committedDate": "2021-01-22T07:51:51Z", "type": "forcePushed"}]}