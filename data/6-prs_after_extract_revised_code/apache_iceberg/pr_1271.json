{"pr_number": 1271, "pr_title": "Align the records written by GenericOrcWriter and SparkOrcWriter", "pr_createdAt": "2020-07-30T09:41:13Z", "pr_url": "https://github.com/apache/iceberg/pull/1271", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTU1OA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464729558", "bodyText": "data.getNano() always returns positive integer, so is this change required?", "author": "shardulm94", "createdAt": "2020-08-04T00:23:19Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -288,8 +289,10 @@ public void nonNullWrite(int rowId, LocalDate data, ColumnVector output) {\n     @Override\n     public void nonNullWrite(int rowId, OffsetDateTime data, ColumnVector output) {\n       TimestampColumnVector cv = (TimestampColumnVector) output;\n-      cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n-      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      // millis\n+      cv.time[rowId] = data.toInstant().toEpochMilli();\n+      // truncate nanos to only keep microsecond precision\n+      cv.nanos[rowId] = Math.floorDiv(data.getNano(), 1_000) * 1_000;", "originalCommit": "95c361026979dcd63758e80878e3d6917bcae505", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMjI1OQ==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464832259", "bodyText": "OK, I saw that the javadoc says the nano-of-second is from 0 to 999,999,999.  you're right, we don't need the floorDiv here.", "author": "openinx", "createdAt": "2020-08-04T06:41:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTU1OA=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\nindex e46e5a03..12d70f52 100644\n--- a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\n+++ b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\n\n@@ -292,7 +294,7 @@ public class GenericOrcWriters {\n       // millis\n       cv.time[rowId] = data.toInstant().toEpochMilli();\n       // truncate nanos to only keep microsecond precision\n-      cv.nanos[rowId] = Math.floorDiv(data.getNano(), 1_000) * 1_000;\n+      cv.nanos[rowId] = data.getNano() / 1_000 * 1_000;\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM1Nw==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464763357", "bodyText": "Nit: precision <= 18 check can be moved into the constructor", "author": "shardulm94", "createdAt": "2020-08-04T02:33:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 18,", "originalCommit": "95c361026979dcd63758e80878e3d6917bcae505", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNTE3Nw==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464835177", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-08-04T06:48:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\nindex ebb51d65..70e8b08f 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n\n@@ -196,12 +197,15 @@ public class SparkOrcValueReaders {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());\n \n-      Preconditions.checkArgument(value.precision() <= precision && precision <= 18,\n+      // The scale of decimal read from hive ORC file may be not equals to the expected scale. For data type\n+      // decimal(10,3) and the value 10.100, the hive ORC writer will remove its trailing zero and store it\n+      // as 101*10^(-1), its scale will adjust from 3 to 1. So here we could not assert that value.scale() == scale.\n+      // we also need to convert the hive orc decimal to a decimal with expected precision and scale.\n+      Preconditions.checkArgument(value.precision() <= precision,\n           \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);\n \n-      return new Decimal().set(new scala.math.BigDecimal(decimal), precision, scale);\n+      return new Decimal().set(value.serialize64(scale), precision, scale);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM4NA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464763384", "bodyText": "Nit: precision <= 38 check can be moved into the constructor", "author": "shardulm94", "createdAt": "2020-08-04T02:33:54Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -212,6 +218,10 @@ public Decimal nonNullRead(ColumnVector vector, int row) {\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       BigDecimal value = ((DecimalColumnVector) vector).vector[row]\n           .getHiveDecimal().bigDecimalValue();\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 38,", "originalCommit": "95c361026979dcd63758e80878e3d6917bcae505", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNjkzMw==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464836933", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-08-04T06:52:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM4NA=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\nindex ebb51d65..70e8b08f 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n\n@@ -219,7 +223,7 @@ public class SparkOrcValueReaders {\n       BigDecimal value = ((DecimalColumnVector) vector).vector[row]\n           .getHiveDecimal().bigDecimalValue();\n \n-      Preconditions.checkArgument(value.precision() <= precision && precision <= 38,\n+      Preconditions.checkArgument(value.precision() <= precision,\n           \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);\n \n       return new Decimal().set(new scala.math.BigDecimal(value), precision, scale);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464774002", "bodyText": "value.serialize64() will take in an expected scale as a parameter, so I think the only change required to the original code is to pass our expected reader scale into value.serialize64() instead of passing value.scale() and passing expected precision and scale to Decimal.set.\nSo this would look like return new Decimal().set(value.serialize64(scale), precision, scale);", "author": "shardulm94", "createdAt": "2020-08-04T03:15:13Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "originalCommit": "95c361026979dcd63758e80878e3d6917bcae505", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNTA3OA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464835078", "bodyText": "Sounds great.  The essential purpose here is to construct a Decimal with the correct precision and scale ( instead of the value.precision() and value.scale().", "author": "openinx", "createdAt": "2020-08-04T06:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg0OTUyMA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464849520", "bodyText": "Oh,  seems it's still incorrect.  Because the value.serialize64(scale) is still encoded by value.precision()  and value.scale(). we use the given precision and scale to parse this long value,  it will be messed up.  Notice, the value.precision is not equals to precision, similar to scale.\nThe correct way should be:\nDecimal decimal = new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\ndecimal.changePrecision(precision, scale);", "author": "openinx", "createdAt": "2020-08-04T07:19:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5OTg4Mg==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r465199882", "bodyText": "I believe value.serialize64 returns the raw long value adjusted for the requested scale (and since precision <= 18, it always fits in long), I don't think it is tied to any precision. That being said, I am not very familiar with using decimals, so maybe I am missing something. Can you give an example of the case you are referring to?", "author": "shardulm94", "createdAt": "2020-08-04T17:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyNjM1OA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r465426358", "bodyText": "Checked this again,  I wrongly used the return new Decimal().set(value.serialize64(value.scale()), precision, scale) to construct the decimal before, which broken the unit tests.  You are right,  the long value is not tied to any precision.  Sorry for the noisy.", "author": "openinx", "createdAt": "2020-08-05T01:55:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\nindex ebb51d65..70e8b08f 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n\n@@ -196,12 +197,15 @@ public class SparkOrcValueReaders {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());\n \n-      Preconditions.checkArgument(value.precision() <= precision && precision <= 18,\n+      // The scale of decimal read from hive ORC file may be not equals to the expected scale. For data type\n+      // decimal(10,3) and the value 10.100, the hive ORC writer will remove its trailing zero and store it\n+      // as 101*10^(-1), its scale will adjust from 3 to 1. So here we could not assert that value.scale() == scale.\n+      // we also need to convert the hive orc decimal to a decimal with expected precision and scale.\n+      Preconditions.checkArgument(value.precision() <= precision,\n           \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);\n \n-      return new Decimal().set(new scala.math.BigDecimal(decimal), precision, scale);\n+      return new Decimal().set(value.serialize64(scale), precision, scale);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU0Nw==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464775547", "bodyText": "Nit: precision <= 18 check can be moved into the constructor", "author": "shardulm94", "createdAt": "2020-08-04T03:21:43Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -324,14 +329,24 @@ public void nonNullWrite(int rowId, LocalDateTime data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 18,", "originalCommit": "95c361026979dcd63758e80878e3d6917bcae505", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMzAwMA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464833000", "bodyText": "the precision <=18  can be removed now, because we've checked it here.", "author": "openinx", "createdAt": "2020-08-04T06:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\nindex e46e5a03..12d70f52 100644\n--- a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\n+++ b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\n\n@@ -331,8 +333,8 @@ public class GenericOrcWriters {\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n       Preconditions.checkArgument(data.scale() == scale,\n           \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n-      Preconditions.checkArgument(data.precision() <= precision && precision <= 18,\n-          \"Cannot write value as decimal(%s,%s), too large: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, data);\n \n       ((DecimalColumnVector) output).vector[rowId]\n           .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU5Ng==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464775596", "bodyText": "Nit: precision <= 38 check can be moved into the constructor", "author": "shardulm94", "createdAt": "2020-08-04T03:21:53Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -340,7 +355,11 @@ public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 38,", "originalCommit": "95c361026979dcd63758e80878e3d6917bcae505", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMzY0Ng==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464833646", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-08-04T06:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\nindex e46e5a03..12d70f52 100644\n--- a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\n+++ b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java\n\n@@ -357,8 +359,8 @@ public class GenericOrcWriters {\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n       Preconditions.checkArgument(data.scale() == scale,\n           \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n-      Preconditions.checkArgument(data.precision() <= precision && precision <= 38,\n-          \"Cannot write value as decimal(%s,%s), exceed the range: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision,\n+          \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, data);\n \n       ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzM5MQ==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464777391", "bodyText": "This check seems redundant to me. If we are already passing our expected precision and scale to data.getDecimal(), wont the scale and precision of the returned decimal always match?", "author": "shardulm94", "createdAt": "2020-08-04T03:29:14Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -237,9 +239,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.noNulls = false;\n         output.isNull[rowId] = true;\n       } else {\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 18,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", decimal);", "originalCommit": "042670ffd50e8b8a111bba93a2875ee357509137", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzODk3NA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464838974", "bodyText": "Make sense.  they could be removed now.", "author": "openinx", "createdAt": "2020-08-04T06:57:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzM5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java\nindex f4757b13..4508a102 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java\n\n@@ -46,385 +39,102 @@ import org.apache.spark.sql.types.Decimal;\n  */\n public class SparkOrcWriter implements OrcRowWriter<InternalRow> {\n \n-  private final Converter[] converters;\n+  private final SparkOrcValueWriter writer;\n \n-  public SparkOrcWriter(TypeDescription schema) {\n-    converters = buildConverters(schema);\n+  public SparkOrcWriter(Schema iSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    writer = OrcSchemaWithTypeVisitor.visit(iSchema, orcSchema, new WriteBuilder());\n   }\n \n   @Override\n   public void write(InternalRow value, VectorizedRowBatch output) {\n-    int row = output.size++;\n-    for (int c = 0; c < converters.length; ++c) {\n-      converters[c].addValue(row, c, value, output.cols[c]);\n-    }\n-  }\n+    Preconditions.checkArgument(writer instanceof StructWriter, \"writer must be StructWriter\");\n \n-  /**\n-   * The interface for the conversion from Spark's SpecializedGetters to\n-   * ORC's ColumnVectors.\n-   */\n-  interface Converter {\n-    /**\n-     * Take a value from the Spark data value and add it to the ORC output.\n-     * @param rowId the row in the ColumnVector\n-     * @param column either the column number or element number\n-     * @param data either an InternalRow or ArrayData\n-     * @param output the ColumnVector to put the value into\n-     */\n-    void addValue(int rowId, int column, SpecializedGetters data,\n-                  ColumnVector output);\n-  }\n-\n-  static class BooleanConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getBoolean(column) ? 1 : 0;\n-      }\n+    int row = output.size;\n+    output.size += 1;\n+    List<SparkOrcValueWriter> writers = ((StructWriter) writer).writers();\n+    for (int c = 0; c < writers.size(); c++) {\n+      SparkOrcValueWriter child = writers.get(c);\n+      child.write(row, c, value, output.cols[c]);\n     }\n   }\n \n-  static class ByteConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getByte(column);\n-      }\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<SparkOrcValueWriter> {\n+    private WriteBuilder() {\n     }\n-  }\n \n-  static class ShortConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getShort(column);\n-      }\n+    public SparkOrcValueWriter record(Types.StructType iStruct, TypeDescription record,\n+                                      List<String> names, List<SparkOrcValueWriter> fields) {\n+      return new StructWriter(fields);\n     }\n-  }\n \n-  static class IntConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getInt(column);\n-      }\n+    public SparkOrcValueWriter list(Types.ListType iList, TypeDescription array,\n+                                    SparkOrcValueWriter element) {\n+      return SparkOrcValueWriters.list(element);\n     }\n-  }\n \n-  static class LongConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getLong(column);\n-      }\n+    public SparkOrcValueWriter map(Types.MapType iMap, TypeDescription map,\n+                                   SparkOrcValueWriter key, SparkOrcValueWriter value) {\n+      return SparkOrcValueWriters.map(key, value);\n     }\n-  }\n \n-  static class FloatConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data.getFloat(column);\n-      }\n+    public SparkOrcValueWriter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          return SparkOrcValueWriters.booleans();\n+        case BYTE:\n+          return SparkOrcValueWriters.bytes();\n+        case SHORT:\n+          return SparkOrcValueWriters.shorts();\n+        case DATE:\n+        case INT:\n+          return SparkOrcValueWriters.ints();\n+        case LONG:\n+          return SparkOrcValueWriters.longs();\n+        case FLOAT:\n+          return SparkOrcValueWriters.floats();\n+        case DOUBLE:\n+          return SparkOrcValueWriters.doubles();\n+        case BINARY:\n+          return SparkOrcValueWriters.byteArrays();\n+        case STRING:\n+        case CHAR:\n+        case VARCHAR:\n+          return SparkOrcValueWriters.strings();\n+        case DECIMAL:\n+          return SparkOrcValueWriters.decimal(primitive.getPrecision(), primitive.getScale());\n+        case TIMESTAMP_INSTANT:\n+          return SparkOrcValueWriters.timestampTz();\n+        default:\n+          throw new IllegalArgumentException(\"Unhandled type \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class StructWriter implements SparkOrcValueWriter {\n+    private final List<SparkOrcValueWriter> writers;\n+\n+    StructWriter(List<SparkOrcValueWriter> writers) {\n+      this.writers = writers;\n+    }\n+\n+    List<SparkOrcValueWriter> writers() {\n+      return writers;\n     }\n-  }\n \n-  static class DoubleConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data.getDouble(column);\n+    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n+      InternalRow value = data.getStruct(column, writers.size());\n+      StructColumnVector cv = (StructColumnVector) output;\n+      for (int c = 0; c < writers.size(); ++c) {\n+        writers.get(c).write(rowId, c, value, cv.fields[c]);\n       }\n     }\n   }\n-\n-  static class StringConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        byte[] value = data.getUTF8String(column).getBytes();\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n-    }\n-  }\n-\n-  static class BytesConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        // getBinary always makes a copy, so we don't need to worry about it\n-        // being changed behind our back.\n-        byte[] value = data.getBinary(column);\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n-    }\n-  }\n-\n-  static class TimestampTzConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        TimestampColumnVector cv = (TimestampColumnVector) output;\n-        long micros = data.getLong(column); // it could be negative.\n-        cv.time[rowId] = Math.floorDiv(micros, 1_000); // millis\n-        cv.nanos[rowId] = (int) (Math.floorMod(micros, 1_000_000)) * 1_000; // nanos\n-      }\n-    }\n-  }\n-\n-  static class Decimal18Converter implements Converter {\n-    private final int precision;\n-    private final int scale;\n-\n-    Decimal18Converter(TypeDescription schema) {\n-      precision = schema.getPrecision();\n-      scale = schema.getScale();\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        Decimal decimal = data.getDecimal(column, precision, scale);\n-        Preconditions.checkArgument(scale == decimal.scale(),\n-            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 18,\n-            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", decimal);\n-\n-        output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].setFromLongAndScale(decimal.toUnscaledLong(), scale);\n-      }\n-    }\n-  }\n-\n-  static class Decimal38Converter implements Converter {\n-    private final int precision;\n-    private final int scale;\n-\n-    Decimal38Converter(TypeDescription schema) {\n-      precision = schema.getPrecision();\n-      scale = schema.getScale();\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-\n-        Decimal decimal = data.getDecimal(column, precision, scale);\n-        Preconditions.checkArgument(scale == decimal.scale(),\n-            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 38,\n-            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, decimal);\n-\n-        ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(decimal.toJavaBigDecimal()));\n-      }\n-    }\n-  }\n-\n-  static class StructConverter implements Converter {\n-    private final Converter[] children;\n-\n-    StructConverter(TypeDescription schema) {\n-      children = new Converter[schema.getChildren().size()];\n-      for (int c = 0; c < children.length; ++c) {\n-        children[c] = buildConverter(schema.getChildren().get(c));\n-      }\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        InternalRow value = data.getStruct(column, children.length);\n-        StructColumnVector cv = (StructColumnVector) output;\n-        for (int c = 0; c < children.length; ++c) {\n-          children[c].addValue(rowId, c, value, cv.fields[c]);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class ListConverter implements Converter {\n-    private final Converter children;\n-\n-    ListConverter(TypeDescription schema) {\n-      children = buildConverter(schema.getChildren().get(0));\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ArrayData value = data.getArray(column);\n-        ListColumnVector cv = (ListColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.numElements();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.child.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          children.addValue((int) (e + cv.offsets[rowId]), e, value, cv.child);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class MapConverter implements Converter {\n-    private final Converter keyConverter;\n-    private final Converter valueConverter;\n-\n-    MapConverter(TypeDescription schema) {\n-      keyConverter = buildConverter(schema.getChildren().get(0));\n-      valueConverter = buildConverter(schema.getChildren().get(1));\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        MapData map = data.getMap(column);\n-        ArrayData key = map.keyArray();\n-        ArrayData value = map.valueArray();\n-        MapColumnVector cv = (MapColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.numElements();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.keys.ensureSize(cv.childCount, true);\n-        cv.values.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          int pos = (int) (e + cv.offsets[rowId]);\n-          keyConverter.addValue(pos, e, key, cv.keys);\n-          valueConverter.addValue(pos, e, value, cv.values);\n-        }\n-      }\n-    }\n-  }\n-\n-  private static Converter buildConverter(TypeDescription schema) {\n-    switch (schema.getCategory()) {\n-      case BOOLEAN:\n-        return new BooleanConverter();\n-      case BYTE:\n-        return new ByteConverter();\n-      case SHORT:\n-        return new ShortConverter();\n-      case DATE:\n-      case INT:\n-        return new IntConverter();\n-      case LONG:\n-        return new LongConverter();\n-      case FLOAT:\n-        return new FloatConverter();\n-      case DOUBLE:\n-        return new DoubleConverter();\n-      case BINARY:\n-        return new BytesConverter();\n-      case STRING:\n-      case CHAR:\n-      case VARCHAR:\n-        return new StringConverter();\n-      case DECIMAL:\n-        return schema.getPrecision() <= 18 ?\n-            new Decimal18Converter(schema) :\n-            new Decimal38Converter(schema);\n-      case TIMESTAMP_INSTANT:\n-        return new TimestampTzConverter();\n-      case STRUCT:\n-        return new StructConverter(schema);\n-      case LIST:\n-        return new ListConverter(schema);\n-      case MAP:\n-        return new MapConverter(schema);\n-    }\n-    throw new IllegalArgumentException(\"Unhandled type \" + schema);\n-  }\n-\n-  private static Converter[] buildConverters(TypeDescription schema) {\n-    if (schema.getCategory() != TypeDescription.Category.STRUCT) {\n-      throw new IllegalArgumentException(\"Top level must be a struct \" + schema);\n-    }\n-    List<TypeDescription> children = schema.getChildren();\n-    Converter[] result = new Converter[children.size()];\n-    for (int c = 0; c < children.size(); ++c) {\n-      result[c] = buildConverter(children.get(c));\n-    }\n-    return result;\n-  }\n-\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzQzOQ==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464777439", "bodyText": "This check seems redundant to me. If we are already passing our expected precision and scale to data.getDecimal(), wont the scale and precision of the returned decimal always match?", "author": "shardulm94", "createdAt": "2020-08-04T03:29:24Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -261,9 +268,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.isNull[rowId] = true;\n       } else {\n         output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].set(\n-            HiveDecimal.create(data.getDecimal(column, precision, scale)\n-                .toJavaBigDecimal()));\n+\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 38,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, decimal);", "originalCommit": "042670ffd50e8b8a111bba93a2875ee357509137", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java\nindex f4757b13..4508a102 100644\n--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java\n+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java\n\n@@ -46,385 +39,102 @@ import org.apache.spark.sql.types.Decimal;\n  */\n public class SparkOrcWriter implements OrcRowWriter<InternalRow> {\n \n-  private final Converter[] converters;\n+  private final SparkOrcValueWriter writer;\n \n-  public SparkOrcWriter(TypeDescription schema) {\n-    converters = buildConverters(schema);\n+  public SparkOrcWriter(Schema iSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    writer = OrcSchemaWithTypeVisitor.visit(iSchema, orcSchema, new WriteBuilder());\n   }\n \n   @Override\n   public void write(InternalRow value, VectorizedRowBatch output) {\n-    int row = output.size++;\n-    for (int c = 0; c < converters.length; ++c) {\n-      converters[c].addValue(row, c, value, output.cols[c]);\n-    }\n-  }\n+    Preconditions.checkArgument(writer instanceof StructWriter, \"writer must be StructWriter\");\n \n-  /**\n-   * The interface for the conversion from Spark's SpecializedGetters to\n-   * ORC's ColumnVectors.\n-   */\n-  interface Converter {\n-    /**\n-     * Take a value from the Spark data value and add it to the ORC output.\n-     * @param rowId the row in the ColumnVector\n-     * @param column either the column number or element number\n-     * @param data either an InternalRow or ArrayData\n-     * @param output the ColumnVector to put the value into\n-     */\n-    void addValue(int rowId, int column, SpecializedGetters data,\n-                  ColumnVector output);\n-  }\n-\n-  static class BooleanConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getBoolean(column) ? 1 : 0;\n-      }\n+    int row = output.size;\n+    output.size += 1;\n+    List<SparkOrcValueWriter> writers = ((StructWriter) writer).writers();\n+    for (int c = 0; c < writers.size(); c++) {\n+      SparkOrcValueWriter child = writers.get(c);\n+      child.write(row, c, value, output.cols[c]);\n     }\n   }\n \n-  static class ByteConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getByte(column);\n-      }\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<SparkOrcValueWriter> {\n+    private WriteBuilder() {\n     }\n-  }\n \n-  static class ShortConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getShort(column);\n-      }\n+    public SparkOrcValueWriter record(Types.StructType iStruct, TypeDescription record,\n+                                      List<String> names, List<SparkOrcValueWriter> fields) {\n+      return new StructWriter(fields);\n     }\n-  }\n \n-  static class IntConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getInt(column);\n-      }\n+    public SparkOrcValueWriter list(Types.ListType iList, TypeDescription array,\n+                                    SparkOrcValueWriter element) {\n+      return SparkOrcValueWriters.list(element);\n     }\n-  }\n \n-  static class LongConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getLong(column);\n-      }\n+    public SparkOrcValueWriter map(Types.MapType iMap, TypeDescription map,\n+                                   SparkOrcValueWriter key, SparkOrcValueWriter value) {\n+      return SparkOrcValueWriters.map(key, value);\n     }\n-  }\n \n-  static class FloatConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data.getFloat(column);\n-      }\n+    public SparkOrcValueWriter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          return SparkOrcValueWriters.booleans();\n+        case BYTE:\n+          return SparkOrcValueWriters.bytes();\n+        case SHORT:\n+          return SparkOrcValueWriters.shorts();\n+        case DATE:\n+        case INT:\n+          return SparkOrcValueWriters.ints();\n+        case LONG:\n+          return SparkOrcValueWriters.longs();\n+        case FLOAT:\n+          return SparkOrcValueWriters.floats();\n+        case DOUBLE:\n+          return SparkOrcValueWriters.doubles();\n+        case BINARY:\n+          return SparkOrcValueWriters.byteArrays();\n+        case STRING:\n+        case CHAR:\n+        case VARCHAR:\n+          return SparkOrcValueWriters.strings();\n+        case DECIMAL:\n+          return SparkOrcValueWriters.decimal(primitive.getPrecision(), primitive.getScale());\n+        case TIMESTAMP_INSTANT:\n+          return SparkOrcValueWriters.timestampTz();\n+        default:\n+          throw new IllegalArgumentException(\"Unhandled type \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class StructWriter implements SparkOrcValueWriter {\n+    private final List<SparkOrcValueWriter> writers;\n+\n+    StructWriter(List<SparkOrcValueWriter> writers) {\n+      this.writers = writers;\n+    }\n+\n+    List<SparkOrcValueWriter> writers() {\n+      return writers;\n     }\n-  }\n \n-  static class DoubleConverter implements Converter {\n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data.getDouble(column);\n+    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n+      InternalRow value = data.getStruct(column, writers.size());\n+      StructColumnVector cv = (StructColumnVector) output;\n+      for (int c = 0; c < writers.size(); ++c) {\n+        writers.get(c).write(rowId, c, value, cv.fields[c]);\n       }\n     }\n   }\n-\n-  static class StringConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        byte[] value = data.getUTF8String(column).getBytes();\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n-    }\n-  }\n-\n-  static class BytesConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        // getBinary always makes a copy, so we don't need to worry about it\n-        // being changed behind our back.\n-        byte[] value = data.getBinary(column);\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n-    }\n-  }\n-\n-  static class TimestampTzConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        TimestampColumnVector cv = (TimestampColumnVector) output;\n-        long micros = data.getLong(column); // it could be negative.\n-        cv.time[rowId] = Math.floorDiv(micros, 1_000); // millis\n-        cv.nanos[rowId] = (int) (Math.floorMod(micros, 1_000_000)) * 1_000; // nanos\n-      }\n-    }\n-  }\n-\n-  static class Decimal18Converter implements Converter {\n-    private final int precision;\n-    private final int scale;\n-\n-    Decimal18Converter(TypeDescription schema) {\n-      precision = schema.getPrecision();\n-      scale = schema.getScale();\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        Decimal decimal = data.getDecimal(column, precision, scale);\n-        Preconditions.checkArgument(scale == decimal.scale(),\n-            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 18,\n-            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", decimal);\n-\n-        output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].setFromLongAndScale(decimal.toUnscaledLong(), scale);\n-      }\n-    }\n-  }\n-\n-  static class Decimal38Converter implements Converter {\n-    private final int precision;\n-    private final int scale;\n-\n-    Decimal38Converter(TypeDescription schema) {\n-      precision = schema.getPrecision();\n-      scale = schema.getScale();\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-\n-        Decimal decimal = data.getDecimal(column, precision, scale);\n-        Preconditions.checkArgument(scale == decimal.scale(),\n-            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n-        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 38,\n-            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, decimal);\n-\n-        ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(decimal.toJavaBigDecimal()));\n-      }\n-    }\n-  }\n-\n-  static class StructConverter implements Converter {\n-    private final Converter[] children;\n-\n-    StructConverter(TypeDescription schema) {\n-      children = new Converter[schema.getChildren().size()];\n-      for (int c = 0; c < children.length; ++c) {\n-        children[c] = buildConverter(schema.getChildren().get(c));\n-      }\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        InternalRow value = data.getStruct(column, children.length);\n-        StructColumnVector cv = (StructColumnVector) output;\n-        for (int c = 0; c < children.length; ++c) {\n-          children[c].addValue(rowId, c, value, cv.fields[c]);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class ListConverter implements Converter {\n-    private final Converter children;\n-\n-    ListConverter(TypeDescription schema) {\n-      children = buildConverter(schema.getChildren().get(0));\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ArrayData value = data.getArray(column);\n-        ListColumnVector cv = (ListColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.numElements();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.child.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          children.addValue((int) (e + cv.offsets[rowId]), e, value, cv.child);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class MapConverter implements Converter {\n-    private final Converter keyConverter;\n-    private final Converter valueConverter;\n-\n-    MapConverter(TypeDescription schema) {\n-      keyConverter = buildConverter(schema.getChildren().get(0));\n-      valueConverter = buildConverter(schema.getChildren().get(1));\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        MapData map = data.getMap(column);\n-        ArrayData key = map.keyArray();\n-        ArrayData value = map.valueArray();\n-        MapColumnVector cv = (MapColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.numElements();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.keys.ensureSize(cv.childCount, true);\n-        cv.values.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          int pos = (int) (e + cv.offsets[rowId]);\n-          keyConverter.addValue(pos, e, key, cv.keys);\n-          valueConverter.addValue(pos, e, value, cv.values);\n-        }\n-      }\n-    }\n-  }\n-\n-  private static Converter buildConverter(TypeDescription schema) {\n-    switch (schema.getCategory()) {\n-      case BOOLEAN:\n-        return new BooleanConverter();\n-      case BYTE:\n-        return new ByteConverter();\n-      case SHORT:\n-        return new ShortConverter();\n-      case DATE:\n-      case INT:\n-        return new IntConverter();\n-      case LONG:\n-        return new LongConverter();\n-      case FLOAT:\n-        return new FloatConverter();\n-      case DOUBLE:\n-        return new DoubleConverter();\n-      case BINARY:\n-        return new BytesConverter();\n-      case STRING:\n-      case CHAR:\n-      case VARCHAR:\n-        return new StringConverter();\n-      case DECIMAL:\n-        return schema.getPrecision() <= 18 ?\n-            new Decimal18Converter(schema) :\n-            new Decimal38Converter(schema);\n-      case TIMESTAMP_INSTANT:\n-        return new TimestampTzConverter();\n-      case STRUCT:\n-        return new StructConverter(schema);\n-      case LIST:\n-        return new ListConverter(schema);\n-      case MAP:\n-        return new MapConverter(schema);\n-    }\n-    throw new IllegalArgumentException(\"Unhandled type \" + schema);\n-  }\n-\n-  private static Converter[] buildConverters(TypeDescription schema) {\n-    if (schema.getCategory() != TypeDescription.Category.STRUCT) {\n-      throw new IllegalArgumentException(\"Top level must be a struct \" + schema);\n-    }\n-    List<TypeDescription> children = schema.getChildren();\n-    Converter[] result = new Converter[children.size()];\n-    for (int c = 0; c < children.size(); ++c) {\n-      result[c] = buildConverter(children.get(c));\n-    }\n-    return result;\n-  }\n-\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzMxMg==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466007312", "bodyText": "I'm not sure we need to check the precision either. If we read a value, then we should return it, right?", "author": "rdblue", "createdAt": "2020-08-05T21:13:14Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +197,15 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+\n+      // The scale of decimal read from hive ORC file may be not equals to the expected scale. For data type\n+      // decimal(10,3) and the value 10.100, the hive ORC writer will remove its trailing zero and store it\n+      // as 101*10^(-1), its scale will adjust from 3 to 1. So here we could not assert that value.scale() == scale.\n+      // we also need to convert the hive orc decimal to a decimal with expected precision and scale.\n+      Preconditions.checkArgument(value.precision() <= precision,\n+          \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);", "originalCommit": "d1f7a489efa28ae39dc367230fa81882f78d2cc3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwMzAwMQ==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466203001", "bodyText": "It is necessary to do this check. we need to make sure that there's no bug when written a decimal into ORC. For example,  for decimal(3, 0) data type we encounter a hive decimal 10000 (whose precision is 5), that should be something wrong.  Throwing an exception is the correct way in that case.", "author": "openinx", "createdAt": "2020-08-06T07:33:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzMxMg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNDY1MA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466014650", "bodyText": "Validation should be done against this data, not data that has been read from a file. That way the test won't be broken by a problem with the reader or writer that produces the expected rows. To validate against these, use the GenericsHelpers.assertEqualsUnsafe methods.", "author": "rdblue", "createdAt": "2020-08-05T21:28:23Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.orc.GenericOrcWriter;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.Assert;\n+\n+public class TestSparkRecordOrcReaderWriter extends AvroDataTest {\n+  private static final int NUM_RECORDS = 200;\n+\n+  @Override\n+  protected void writeAndValidate(Schema schema) throws IOException {\n+    List<Record> records = RandomGenericData.generate(schema, NUM_RECORDS, 1992L);", "originalCommit": "d1f7a489efa28ae39dc367230fa81882f78d2cc3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5ODkwOA==", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466198908", "bodyText": "It make sense.", "author": "openinx", "createdAt": "2020-08-06T07:24:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNDY1MA=="}], "type": "inlineReview", "revised_code": {"commit": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "chunk": "diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java\nindex d2d41a66..1e7430d1 100644\n--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java\n+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java\n\n@@ -21,10 +21,12 @@ package org.apache.iceberg.spark.data;\n \n import java.io.File;\n import java.io.IOException;\n+import java.math.BigDecimal;\n import java.util.Iterator;\n import java.util.List;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.RandomGenericData;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.data.orc.GenericOrcReader;\n"}}, {"oid": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "url": "https://github.com/apache/iceberg/commit/b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "message": "Align the records between GenericOrcWriter and SparkOrcWriter", "committedDate": "2020-08-07T02:05:54Z", "type": "commit"}, {"oid": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "url": "https://github.com/apache/iceberg/commit/b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "message": "Align the records between GenericOrcWriter and SparkOrcWriter", "committedDate": "2020-08-07T02:05:54Z", "type": "forcePushed"}]}