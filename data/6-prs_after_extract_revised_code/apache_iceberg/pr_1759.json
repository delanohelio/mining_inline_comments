{"pr_number": 1759, "pr_title": "Spark: Implement RollbackToSnapshotProcedure", "pr_createdAt": "2020-11-12T20:57:12Z", "pr_url": "https://github.com/apache/iceberg/pull/1759", "timeline": [{"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "url": "https://github.com/apache/iceberg/commit/2eca4484d4def23719c8fa90ddadaab3f8b69212", "message": "Spark: Implement RollbackToSnapshotProcedure", "committedDate": "2020-11-12T20:54:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjA4OA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522422088", "bodyText": "Initially, I added a single test suite for all operations that manage snapshots. We may have a suite per procedure now.", "author": "aokolnychyi", "createdAt": "2020-11-12T20:58:28Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1ODM5NQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522458395", "bodyText": "Fixed.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:11:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjA4OA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nsimilarity index 90%\nrename from spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\nrename to spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 01bab9d60..ae356430f 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n\n@@ -34,9 +34,9 @@ import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n import org.junit.After;\n import org.junit.Test;\n \n-public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n \n-  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+  public TestRollbackToSnapshotProcedure(String catalogName, String implementation, Map<String, String> config) {\n     super(catalogName, implementation, config);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjg0MQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522422841", "bodyText": "Our table catalog is always case sensitive right now.", "author": "aokolnychyi", "createdAt": "2020-11-12T20:59:51Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import org.apache.iceberg.spark.procedures.SparkProcedure;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureCatalog;\n+\n+abstract class BaseCatalog implements StagingTableCatalog, ProcedureCatalog, SupportsNamespaces {\n+\n+  @Override\n+  public Procedure loadProcedure(Identifier ident) throws NoSuchProcedureException {\n+    String[] namespace = ident.namespace();\n+    String name = ident.name();\n+\n+    // namespace resolution is case sensitive to match how we resolve namespaces for tables right now\n+    if (namespace.length == 1 && namespace[0].equals(\"system\")) {", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzOTg5NQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522439895", "bodyText": "I'd be fine with case insensitive here. It's not like we are defining other namespaces.", "author": "rdblue", "createdAt": "2020-11-12T21:33:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0OTE3Ng==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522449176", "bodyText": "Let's make it case insensitive too until we have a way to configure case sensitivity in our catalogs.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:52:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MDA4Nw==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522460087", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:13:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjg0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java b/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java\nindex 09f5166a8..791e95030 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java\n\n@@ -35,8 +35,8 @@ abstract class BaseCatalog implements StagingTableCatalog, ProcedureCatalog, Sup\n     String[] namespace = ident.namespace();\n     String name = ident.name();\n \n-    // namespace resolution is case sensitive to match how we resolve namespaces for tables right now\n-    if (namespace.length == 1 && namespace[0].equals(\"system\")) {\n+    // namespace resolution is case insensitive until we have a way to configure case sensitivity in catalogs\n+    if (namespace.length == 1 && namespace[0].equalsIgnoreCase(\"system\")) {\n       try {\n         // procedure resolution is case insensitive to match the existing Spark behavior for functions\n         // SimpleFunctionRegistry normalizes function names but leaves namespace resolution to the caller\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjkyOA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522422928", "bodyText": "This is debatable.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:00:01Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import org.apache.iceberg.spark.procedures.SparkProcedure;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureCatalog;\n+\n+abstract class BaseCatalog implements StagingTableCatalog, ProcedureCatalog, SupportsNamespaces {\n+\n+  @Override\n+  public Procedure loadProcedure(Identifier ident) throws NoSuchProcedureException {\n+    String[] namespace = ident.namespace();\n+    String name = ident.name();\n+\n+    // namespace resolution is case sensitive to match how we resolve namespaces for tables right now\n+    if (namespace.length == 1 && namespace[0].equals(\"system\")) {\n+      try {\n+        // procedure resolution is case insensitive to match the existing Spark behavior for functions\n+        // SimpleFunctionRegistry normalizes function names but leaves namespace resolution to the caller\n+        SparkProcedure procedure = SparkProcedure.valueOf(name.toUpperCase(Locale.ROOT));", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzOTY3Ng==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522439676", "bodyText": "I'm fine with it being case insensitive.", "author": "rdblue", "createdAt": "2020-11-12T21:33:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjkyOA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java b/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java\nindex 09f5166a8..791e95030 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java\n\n@@ -35,8 +35,8 @@ abstract class BaseCatalog implements StagingTableCatalog, ProcedureCatalog, Sup\n     String[] namespace = ident.namespace();\n     String name = ident.name();\n \n-    // namespace resolution is case sensitive to match how we resolve namespaces for tables right now\n-    if (namespace.length == 1 && namespace[0].equals(\"system\")) {\n+    // namespace resolution is case insensitive until we have a way to configure case sensitivity in catalogs\n+    if (namespace.length == 1 && namespace[0].equalsIgnoreCase(\"system\")) {\n       try {\n         // procedure resolution is case insensitive to match the existing Spark behavior for functions\n         // SimpleFunctionRegistry normalizes function names but leaves namespace resolution to the caller\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522424355", "bodyText": "This place requires discussion. Right now, I allow calling procedures like this:\nCALL cat.system.rollback_to_snapshot_id('`name.space`', '`table.name`', snapshot_id)\n\nWhere the namespace and table names are strings but they may be quoted.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:02:45Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MDk3MA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522440970", "bodyText": "If they are strings, then they should be passed using string escapes. I would expect back-tick to be preserved.", "author": "rdblue", "createdAt": "2020-11-12T21:36:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NDQyNQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522444425", "bodyText": "Thinking about this a bit more, maybe this is okay. I can't think of another way to quote name.space if it is an identifier.", "author": "rdblue", "createdAt": "2020-11-12T21:43:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTgxMw==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522445813", "bodyText": "Will we be able to construct a valid namespace then? I think the tricky part is that . is not a special character but may indicate a namespace boundary.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:46:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NjUyOQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522446529", "bodyText": "I'd love to find a better approach, though.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2Mzc1MQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522463751", "bodyText": "This is fine for now.", "author": "rdblue", "createdAt": "2020-11-12T22:16:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex f664019dd..21a54a3e5 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -46,8 +46,8 @@ abstract class BaseProcedure implements Procedure {\n   }\n \n   protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n-    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), \"Table name cannot be empty\");\n \n     Identifier ident = toIdentifier(namespace, tableName);\n     SparkTable sparkTable = loadSparkTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNTM1NA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522425354", "bodyText": "Here is the test for quoted identifiers.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:04:42Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+\n+  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",\n+        catalogName, firstSnapshot.snapshotId(), tableIdent.namespace(), tableIdent.name());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotRefreshesRelationCache() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    Dataset<Row> query = spark.sql(\"SELECT * FROM \" + tableName + \" WHERE id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    spark.sql(\"CACHE TABLE tmp\");\n+\n+    assertEquals(\"View should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(namespace => '%s', table => '%s', snapshot_id => %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"View cache must be invalidated\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    sql(\"UNCACHE TABLE tmp\");\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotWithQuotedIdentifiers() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    StringBuilder quotedNamespaceBuilder = new StringBuilder();", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzODM0NA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522438344", "bodyText": "Why would the additional quotes be removed? This seems incorrect to me.", "author": "rdblue", "createdAt": "2020-11-12T21:30:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNTM1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MTgxMg==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522441812", "bodyText": "They would be parsed as name parts so that we can construct a valid Identifier.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:37:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNTM1NA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nsimilarity index 90%\nrename from spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\nrename to spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 01bab9d60..ae356430f 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n\n@@ -34,9 +34,9 @@ import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n import org.junit.After;\n import org.junit.Test;\n \n-public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n \n-  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+  public TestRollbackToSnapshotProcedure(String catalogName, String implementation, Map<String, String> config) {\n     super(catalogName, implementation, config);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNjM2Mw==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522436363", "bodyText": "It looks like this line is the only reason for this test class. What about adding a way to pass additional Spark properties to the base class instead? Like protected Map<String, String> sparkConfig().", "author": "rdblue", "createdAt": "2020-11-12T21:26:47Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.junit.BeforeClass;\n+\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTOREURIS;\n+\n+public abstract class SparkExtensionsTestBase extends SparkCatalogTestBase {\n+\n+  public SparkExtensionsTestBase(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {\n+    SparkTestBase.metastore = new TestHiveMetastore();\n+    metastore.start();\n+    SparkTestBase.hiveConf = metastore.hiveConf();\n+\n+    SparkTestBase.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzODc1NA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522438754", "bodyText": "I tried that in the first place. The problem is that startMetastoreAndSpark is static.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:31:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNjM2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2NTkyMQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522465921", "bodyText": "Alright, then this works.", "author": "rdblue", "createdAt": "2020-11-12T22:19:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNjM2Mw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNzQzMA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522437430", "bodyText": "Is %dL required or could it be %d? I think because of the casting that was added, it could be %d.", "author": "rdblue", "createdAt": "2020-11-12T21:28:54Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+\n+  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzOTcxNg==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522439716", "bodyText": "It is not required, we have testRollbackToSnapshotWithQuotedIdentifiers for that. I don't think our cast logic is working there, though. I think Spark is smart enough to parse snapshot id as long since the value is too large to be int.\nI'll add a test for the casting logic as soon as it applies.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:33:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNzQzMA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nsimilarity index 90%\nrename from spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\nrename to spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 01bab9d60..ae356430f 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n\n@@ -34,9 +34,9 @@ import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n import org.junit.After;\n import org.junit.Test;\n \n-public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n \n-  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+  public TestRollbackToSnapshotProcedure(String catalogName, String implementation, Map<String, String> config) {\n     super(catalogName, implementation, config);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzOTI0Mg==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522439242", "bodyText": "There are a couple more cases:\n\nPositional arguments missing table name\nPositional arguments missing namespace", "author": "rdblue", "createdAt": "2020-11-12T21:32:26Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+\n+  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",\n+        catalogName, firstSnapshot.snapshotId(), tableIdent.namespace(), tableIdent.name());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotRefreshesRelationCache() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    Dataset<Row> query = spark.sql(\"SELECT * FROM \" + tableName + \" WHERE id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    spark.sql(\"CACHE TABLE tmp\");\n+\n+    assertEquals(\"View should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(namespace => '%s', table => '%s', snapshot_id => %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"View cache must be invalidated\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    sql(\"UNCACHE TABLE tmp\");\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotWithQuotedIdentifiers() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    StringBuilder quotedNamespaceBuilder = new StringBuilder();\n+    for (String level : tableIdent.namespace().levels()) {\n+      quotedNamespaceBuilder.append(\"`\");\n+      quotedNamespaceBuilder.append(level);\n+      quotedNamespaceBuilder.append(\"`\");\n+    }\n+    String quotedNamespace = quotedNamespaceBuilder.toString();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '`%s`', %d)\",\n+        catalogName, quotedNamespace, tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToInvalidSnapshot() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+\n+    Namespace namespace = tableIdent.namespace();\n+    String tableName = tableIdent.name();\n+\n+    AssertHelpers.assertThrows(\"Should reject invalid snapshot id\",\n+        ValidationException.class, \"Cannot roll back to unknown snapshot id\",\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot('%s', '%s', -1L)\", catalogName, namespace, tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidRollbackToSnapshotCases() {", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1ODcyOQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522458729", "bodyText": "Added more tests.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:11:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzOTI0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nsimilarity index 90%\nrename from spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\nrename to spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 01bab9d60..ae356430f 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n\n@@ -34,9 +34,9 @@ import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n import org.junit.After;\n import org.junit.Test;\n \n-public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n \n-  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+  public TestRollbackToSnapshotProcedure(String catalogName, String implementation, Map<String, String> config) {\n     super(catalogName, implementation, config);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MDU1NQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522440555", "bodyText": "What about namespace is null?", "author": "rdblue", "createdAt": "2020-11-12T21:35:07Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1MDM3NA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522450374", "bodyText": "Both namespace and table names are required params so Spark would validate that for us. It would not harm to add an extra validation here too. Let me do that.", "author": "aokolnychyi", "createdAt": "2020-11-12T21:54:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MDU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1ODgzNQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522458835", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:11:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MDU1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex f664019dd..21a54a3e5 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -46,8 +46,8 @@ abstract class BaseProcedure implements Procedure {\n   }\n \n   protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n-    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), \"Table name cannot be empty\");\n \n     Identifier ident = toIdentifier(namespace, tableName);\n     SparkTable sparkTable = loadSparkTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MTgwOA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522441808", "bodyText": "Even in a private method, I don't think it is a good practice to use Scala Seq because of compatibility problems. It would be safer to pass this directly into a converter to get a List.", "author": "rdblue", "createdAt": "2020-11-12T21:37:45Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {\n+    Seq<String> namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+    String[] namespace = new String[namespaceParts.size()];\n+    namespaceParts.copyToArray(namespace);\n+\n+    Seq<String> nameParts = parseMultipartIdentifier(name);\n+    Preconditions.checkArgument(nameParts.size() == 1, \"Name must consist of one part: %s\", name);\n+\n+    return Identifier.of(namespace, nameParts.head());\n+  }\n+\n+  private Seq<String> parseMultipartIdentifier(String identifierAsString) {", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1MTgyMQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522451821", "bodyText": "Do you mean change the return type to List?", "author": "aokolnychyi", "createdAt": "2020-11-12T21:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MTgwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MzQ5OA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522463498", "bodyText": "Yes. Seq is not safe.", "author": "rdblue", "createdAt": "2020-11-12T22:16:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MTgwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwNDQ5Mw==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522704493", "bodyText": "I updated it to return an array.", "author": "aokolnychyi", "createdAt": "2020-11-13T07:02:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MTgwOA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex f664019dd..21a54a3e5 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -46,8 +46,8 @@ abstract class BaseProcedure implements Procedure {\n   }\n \n   protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n-    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), \"Table name cannot be empty\");\n \n     Identifier ident = toIdentifier(namespace, tableName);\n     SparkTable sparkTable = loadSparkTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MjI2NQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522442265", "bodyText": "Why not let the NoSuchTableException propagate?", "author": "rdblue", "createdAt": "2020-11-12T21:38:41Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {\n+    Seq<String> namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+    String[] namespace = new String[namespaceParts.size()];\n+    namespaceParts.copyToArray(namespace);\n+\n+    Seq<String> nameParts = parseMultipartIdentifier(name);\n+    Preconditions.checkArgument(nameParts.size() == 1, \"Name must consist of one part: %s\", name);\n+\n+    return Identifier.of(namespace, nameParts.head());\n+  }\n+\n+  private Seq<String> parseMultipartIdentifier(String identifierAsString) {\n+    try {\n+      ParserInterface parser = spark.sessionState().sqlParser();\n+      return parser.parseMultipartIdentifier(identifierAsString);\n+    } catch (ParseException e) {\n+      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+    }\n+  }\n+\n+  protected SparkTable loadSparkTable(Identifier ident) {\n+    try {\n+      Table table = catalog.loadTable(ident);\n+      ValidationException.check(table instanceof SparkTable, \"%s is not %s\", ident, SparkTable.class.getName());\n+      return (SparkTable) table;\n+    } catch (NoSuchTableException e) {\n+      throw new RuntimeException(String.format(\"Couldn't load table '%s' in catalog '%s'\", ident, catalog.name()), e);", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1MjgzMg==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522452832", "bodyText": "Unfortunately, NoSuchTableException is a checked exception. We will have to declare in all method that rely on this one. I am ok with that but I am not sure it is worth it.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:00:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MjI2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MzMxMA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522463310", "bodyText": "Nah, this is fine.", "author": "rdblue", "createdAt": "2020-11-12T22:16:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MjI2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\nindex f664019dd..21a54a3e5 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java\n\n@@ -46,8 +46,8 @@ abstract class BaseProcedure implements Procedure {\n   }\n \n   protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n-    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), \"Table name cannot be empty\");\n \n     Identifier ident = toIdentifier(namespace, tableName);\n     SparkTable sparkTable = loadSparkTable(ident);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0Mjk5NQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522442995", "bodyText": "Why not static?", "author": "rdblue", "createdAt": "2020-11-12T21:40:08Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1ODk4OA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522458988", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0Mjk5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\nindex a0e82fb90..f7e1e713a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n\n@@ -31,16 +31,16 @@ import org.apache.spark.sql.types.StructType;\n \n public class RollbackToSnapshotProcedure extends BaseProcedure {\n \n-  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+  private static final ProcedureParameter[] parameters = new ProcedureParameter[]{\n       ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n       ProcedureParameter.required(\"table\", DataTypes.StringType),\n       ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n   };\n-  private final StructField[] outputFields = new StructField[]{\n-      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+\n+  private static final StructType outputType = new StructType(new StructField[]{\n+      new StructField(\"previous_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n       new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n-  };\n-  private final StructType outputType = new StructType(outputFields);\n+  });\n \n   public RollbackToSnapshotProcedure(TableCatalog catalog) {\n     super(catalog);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MzEwNQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522443105", "bodyText": "Same here. This could be static right?", "author": "rdblue", "createdAt": "2020-11-12T21:40:21Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+  private final StructField[] outputFields = new StructField[]{", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1OTA5Ng==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522459096", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:12:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MzEwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\nindex a0e82fb90..f7e1e713a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n\n@@ -31,16 +31,16 @@ import org.apache.spark.sql.types.StructType;\n \n public class RollbackToSnapshotProcedure extends BaseProcedure {\n \n-  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+  private static final ProcedureParameter[] parameters = new ProcedureParameter[]{\n       ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n       ProcedureParameter.required(\"table\", DataTypes.StringType),\n       ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n   };\n-  private final StructField[] outputFields = new StructField[]{\n-      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+\n+  private static final StructType outputType = new StructType(new StructField[]{\n+      new StructField(\"previous_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n       new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n-  };\n-  private final StructType outputType = new StructType(outputFields);\n+  });\n \n   public RollbackToSnapshotProcedure(TableCatalog catalog) {\n     super(catalog);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MzczMA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522443730", "bodyText": "What about \"previous_snapshot_id\"? I think that's clear enough. Or at least as clear as \"previous_current\".", "author": "rdblue", "createdAt": "2020-11-12T21:41:39Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+  private final StructField[] outputFields = new StructField[]{\n+      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1OTIyNA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522459224", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:12:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MzczMA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\nindex a0e82fb90..f7e1e713a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n\n@@ -31,16 +31,16 @@ import org.apache.spark.sql.types.StructType;\n \n public class RollbackToSnapshotProcedure extends BaseProcedure {\n \n-  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+  private static final ProcedureParameter[] parameters = new ProcedureParameter[]{\n       ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n       ProcedureParameter.required(\"table\", DataTypes.StringType),\n       ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n   };\n-  private final StructField[] outputFields = new StructField[]{\n-      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+\n+  private static final StructType outputType = new StructType(new StructField[]{\n+      new StructField(\"previous_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n       new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n-  };\n-  private final StructType outputType = new StructType(outputFields);\n+  });\n \n   public RollbackToSnapshotProcedure(TableCatalog catalog) {\n     super(catalog);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NDgxNA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522444814", "bodyText": "It may be cleaner to use new StructType().addField(...).addField(...).", "author": "rdblue", "createdAt": "2020-11-12T21:43:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+  private final StructField[] outputFields = new StructField[]{\n+      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+      new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n+  };\n+  private final StructType outputType = new StructType(outputFields);", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1OTQzMg==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522459432", "bodyText": "I went for an in-line definition.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:12:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NDgxNA=="}], "type": "inlineReview", "revised_code": {"commit": "2b73b16c56b4e7432d33d58468e6f7657a137298", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\nindex a0e82fb90..f7e1e713a 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n\n@@ -31,16 +31,16 @@ import org.apache.spark.sql.types.StructType;\n \n public class RollbackToSnapshotProcedure extends BaseProcedure {\n \n-  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+  private static final ProcedureParameter[] parameters = new ProcedureParameter[]{\n       ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n       ProcedureParameter.required(\"table\", DataTypes.StringType),\n       ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n   };\n-  private final StructField[] outputFields = new StructField[]{\n-      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+\n+  private static final StructType outputType = new StructType(new StructField[]{\n+      new StructField(\"previous_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n       new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n-  };\n-  private final StructType outputType = new StructType(outputFields);\n+  });\n \n   public RollbackToSnapshotProcedure(TableCatalog catalog) {\n     super(catalog);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTk2MA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522445960", "bodyText": "It seems strange that the builder passes catalog to build, rather than withTableCatalog.", "author": "rdblue", "createdAt": "2020-11-12T21:46:25Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedure.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+\n+public enum SparkProcedure {\n+  ROLLBACK_TO_SNAPSHOT(RollbackToSnapshotProcedure::new);\n+\n+  private final ProcedureBuilder builder;\n+\n+  SparkProcedure(ProcedureBuilder builder) {\n+    this.builder = builder;\n+  }\n+\n+  public Procedure build(TableCatalog catalog) {\n+    return builder.build(catalog);\n+  }\n+\n+  private interface ProcedureBuilder {\n+    Procedure build(TableCatalog catalog);", "originalCommit": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1NjY2OA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522456668", "bodyText": "Do you mean the overall approach or the method name?", "author": "aokolnychyi", "createdAt": "2020-11-12T22:07:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTk2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MjI3MQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522462271", "bodyText": "I'm assuming that there will be more options that will be passed to the builder, but there is just TableCatalog for now. I would expect this to support cases that don't need a TableCatalog, so it doesn't seem like something that we should tie to the build method. Instead, I'd use a withTableCatalog method and always build the procedure with build()", "author": "rdblue", "createdAt": "2020-11-12T22:15:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTk2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ3MTE2MA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522471160", "bodyText": "Let me rework this part.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:26:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTk2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwNDIzMQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522704231", "bodyText": "Updated. Let me know what you think, @rdblue.", "author": "aokolnychyi", "createdAt": "2020-11-13T07:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTk2MA=="}], "type": "inlineReview", "revised_code": {"commit": "027076348afcae569967b3f4b5923e36d1f01fa4", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedure.java\ndeleted file mode 100644\nindex 5636277e0..000000000\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedure.java\n+++ /dev/null\n\n@@ -1,41 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iceberg.spark.procedures;\n-\n-import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n-\n-public enum SparkProcedure {\n-  ROLLBACK_TO_SNAPSHOT(RollbackToSnapshotProcedure::new);\n-\n-  private final ProcedureBuilder builder;\n-\n-  SparkProcedure(ProcedureBuilder builder) {\n-    this.builder = builder;\n-  }\n-\n-  public Procedure build(TableCatalog catalog) {\n-    return builder.build(catalog);\n-  }\n-\n-  private interface ProcedureBuilder {\n-    Procedure build(TableCatalog catalog);\n-  }\n-}\n"}}, {"oid": "2b73b16c56b4e7432d33d58468e6f7657a137298", "url": "https://github.com/apache/iceberg/commit/2b73b16c56b4e7432d33d58468e6f7657a137298", "message": "Some fixes", "committedDate": "2020-11-12T22:09:00Z", "type": "commit"}, {"oid": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20", "url": "https://github.com/apache/iceberg/commit/cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20", "message": "Remove extra line", "committedDate": "2020-11-12T22:10:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MzAwNQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522463005", "bodyText": "Should be PARAMETERS for checkstyle. Same with OUTPUT_TYPE.", "author": "rdblue", "createdAt": "2020-11-12T22:16:00Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private static final ProcedureParameter[] parameters = new ProcedureParameter[]{", "originalCommit": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwMzY0OQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522703649", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-13T07:01:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MzAwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "027076348afcae569967b3f4b5923e36d1f01fa4", "chunk": "diff --git a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\nindex f7e1e713a..cfaba8618 100644\n--- a/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n+++ b/spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java\n\n@@ -20,6 +20,7 @@\n package org.apache.iceberg.spark.procedures;\n \n import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2NDY1Mg==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522464652", "bodyText": "Just to make sure: this works without an explicit catalog right? I should be able to CALL system.rollback_to_snapshot(...) and it uses the current catalog?", "author": "rdblue", "createdAt": "2020-11-12T22:17:55Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n+\n+  public TestRollbackToSnapshotProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",\n+        catalogName, firstSnapshot.snapshotId(), tableIdent.namespace(), tableIdent.name());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotRefreshesRelationCache() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    Dataset<Row> query = spark.sql(\"SELECT * FROM \" + tableName + \" WHERE id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    spark.sql(\"CACHE TABLE tmp\");\n+\n+    assertEquals(\"View should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(namespace => '%s', table => '%s', snapshot_id => %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"View cache must be invalidated\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    sql(\"UNCACHE TABLE tmp\");\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotWithQuotedIdentifiers() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    StringBuilder quotedNamespaceBuilder = new StringBuilder();\n+    for (String level : tableIdent.namespace().levels()) {\n+      quotedNamespaceBuilder.append(\"`\");\n+      quotedNamespaceBuilder.append(level);\n+      quotedNamespaceBuilder.append(\"`\");\n+    }\n+    String quotedNamespace = quotedNamespaceBuilder.toString();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '`%s`', %d)\",", "originalCommit": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2NzcyMA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522467720", "bodyText": "Let me add a test for this since I changed the resolution logic recently.", "author": "aokolnychyi", "createdAt": "2020-11-12T22:21:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2NDY1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwMzQ1OQ==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522703459", "bodyText": "I added a test for this and it works correctly.", "author": "aokolnychyi", "createdAt": "2020-11-13T07:01:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2NDY1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "027076348afcae569967b3f4b5923e36d1f01fa4", "chunk": "diff --git a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex ae356430f..04e4faa54 100644\n--- a/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n+++ b/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n\n@@ -32,6 +32,7 @@ import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n import org.junit.After;\n+import org.junit.Assume;\n import org.junit.Test;\n \n public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n"}}, {"oid": "027076348afcae569967b3f4b5923e36d1f01fa4", "url": "https://github.com/apache/iceberg/commit/027076348afcae569967b3f4b5923e36d1f01fa4", "message": "Minor updates", "committedDate": "2020-11-13T07:00:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA1Nzg5OA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r523057898", "bodyText": "We may need to reason about concurrent operations on this table instance if caching is enabled. Here, there is a possibility there will be an operation in between we got previousSnapshot and before we committed so the output may not be precise. It may be even more important in other procedures later.\n@rdblue, thoughts?", "author": "aokolnychyi", "createdAt": "2020-11-13T16:22:30Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * A procedure that rollbacks a table to a specific snapshot id.\n+ */\n+class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"previous_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+      new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  public static ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<RollbackToSnapshotProcedure>() {\n+      @Override\n+      public RollbackToSnapshotProcedure doBuild() {\n+        return new RollbackToSnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  private RollbackToSnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String namespace = args.getString(0);\n+    String tableName = args.getString(1);\n+    long snapshotId = args.getLong(2);\n+\n+    return modifyIcebergTable(namespace, tableName, table -> {\n+      Snapshot previousSnapshot = table.currentSnapshot();", "originalCommit": "027076348afcae569967b3f4b5923e36d1f01fa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MTc5MA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r523161790", "bodyText": "We can add a validation to set requiredCurrentSnapshotId in the operation. Right now, we always roll back as long as it is an ancestor of the current state. Can be done as a follow-up though.", "author": "rdblue", "createdAt": "2020-11-13T18:57:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA1Nzg5OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MjQyOA==", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r523162428", "bodyText": "Why not make build abstract?", "author": "rdblue", "createdAt": "2020-11-13T18:58:31Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog tableCatalog;\n+\n+  protected BaseProcedure(TableCatalog tableCatalog) {\n+    this.spark = SparkSession.active();\n+    this.tableCatalog = tableCatalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {\n+    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+\n+    String[] nameParts = parseMultipartIdentifier(name);\n+    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n+\n+    return Identifier.of(namespaceParts, nameParts[0]);\n+  }\n+\n+  private String[] parseMultipartIdentifier(String identifierAsString) {\n+    try {\n+      ParserInterface parser = spark.sessionState().sqlParser();\n+      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n+      String[] nameParts = new String[namePartsSeq.size()];\n+      namePartsSeq.copyToArray(nameParts);\n+      return nameParts;\n+    } catch (ParseException e) {\n+      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+    }\n+  }\n+\n+  protected SparkTable loadSparkTable(Identifier ident) {\n+    try {\n+      Table table = tableCatalog.loadTable(ident);\n+      ValidationException.check(table instanceof SparkTable, \"%s is not %s\", ident, SparkTable.class.getName());\n+      return (SparkTable) table;\n+    } catch (NoSuchTableException e) {\n+      String errMsg = String.format(\"Couldn't load table '%s' in catalog '%s'\", ident, tableCatalog.name());\n+      throw new RuntimeException(errMsg, e);\n+    }\n+  }\n+\n+  protected void refreshSparkCache(Identifier ident, Table table) {\n+    CacheManager cacheManager = spark.sharedState().cacheManager();\n+    DataSourceV2Relation relation = DataSourceV2Relation.create(table, Option.apply(tableCatalog), Option.apply(ident));\n+    cacheManager.recacheByPlan(spark, relation);\n+  }\n+\n+  protected abstract static class Builder<T extends BaseProcedure> implements ProcedureBuilder {\n+    private TableCatalog tableCatalog;\n+\n+    @Override\n+    public Builder<T> withTableCatalog(TableCatalog newTableCatalog) {\n+      this.tableCatalog = newTableCatalog;\n+      return this;\n+    }\n+\n+    @Override\n+    public T build() {\n+      return doBuild();\n+    }", "originalCommit": "027076348afcae569967b3f4b5923e36d1f01fa4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}