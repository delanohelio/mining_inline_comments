{"pr_number": 4005, "pr_title": "[Cache][BE] LRU cache for sql/partition cache #2581", "pr_createdAt": "2020-07-02T10:09:25Z", "pr_url": "https://github.com/apache/incubator-doris/pull/4005", "timeline": [{"oid": "311dccb3e579b2993280f345a54ac82fe982a1b1", "url": "https://github.com/apache/incubator-doris/commit/311dccb3e579b2993280f345a54ac82fe982a1b1", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581", "committedDate": "2020-08-03T10:14:30Z", "type": "forcePushed"}, {"oid": "a28524639aad6fba25479179c3ca95e72188eb13", "url": "https://github.com/apache/incubator-doris/commit/a28524639aad6fba25479179c3ca95e72188eb13", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581", "committedDate": "2020-08-10T07:58:36Z", "type": "forcePushed"}, {"oid": "3ee304e2f72bcb8f10959b2d226ea6e49ad47e59", "url": "https://github.com/apache/incubator-doris/commit/3ee304e2f72bcb8f10959b2d226ea6e49ad47e59", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581\n1. Cache data by sql key and partition key\n2. Support fetch/update/clear operator\n3. Hit cache by key, version and vertion_time", "committedDate": "2020-08-10T12:05:38Z", "type": "forcePushed"}, {"oid": "0015dcaff193e0dfef3270683ba5a546bf974b10", "url": "https://github.com/apache/incubator-doris/commit/0015dcaff193e0dfef3270683ba5a546bf974b10", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581\n1. Cache data by sql key and partition key\n2. Support fetch/update/clear operator\n3. Hit cache by key, version and vertion_time", "committedDate": "2020-08-21T11:11:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTUzOTM0MA==", "url": "https://github.com/apache/incubator-doris/pull/4005#discussion_r475539340", "bodyText": "This file has been deleted?", "author": "morningman", "createdAt": "2020-08-24T11:40:30Z", "path": "fe/fe-core/src/main/java/org/apache/doris/common/Config.java", "diffHunk": "@@ -1,1221 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.common;", "originalCommit": "0015dcaff193e0dfef3270683ba5a546bf974b10", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b0ad2e27a7dfa30ca55b2601eca335c30c041b59", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/common/Config.java b/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\nnew file mode 100644\nindex 000000000..b88bc0635\n--- /dev/null\n+++ b/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\n\n@@ -0,0 +1,1241 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.common;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.http.HttpServer;\n+\n+public class Config extends ConfigBase {\n+    \n+    /**\n+     * The max size of one sys log and audit log\n+     */\n+    @ConfField public static int log_roll_size_mb = 1024; // 1 GB\n+\n+    /**\n+     * sys_log_dir:\n+     *      This specifies FE log dir. FE will produces 2 log files:\n+     *      fe.log:      all logs of FE process.\n+     *      fe.warn.log  all WARNING and ERROR log of FE process.\n+     *      \n+     * sys_log_level:\n+     *      INFO, WARNING, ERROR, FATAL\n+     *      \n+     * sys_log_roll_num:\n+     *      Maximal FE log files to be kept within an sys_log_roll_interval.\n+     *      default is 10, which means there will be at most 10 log files in a day\n+     *      \n+     * sys_log_verbose_modules:\n+     *      Verbose modules. VERBOSE level is implemented by log4j DEBUG level.\n+     *      eg:\n+     *          sys_log_verbose_modules = org.apache.doris.catalog\n+     *      This will only print debug log of files in package org.apache.doris.catalog and all its sub packages.\n+     *      \n+     * sys_log_roll_interval:\n+     *      DAY:  log suffix is yyyyMMdd\n+     *      HOUR: log suffix is yyyyMMddHH\n+     *      \n+     * sys_log_delete_age:\n+     *      default is 7 days, if log's last modify time is 7 days ago, it will be deleted.\n+     *      support format:\n+     *          7d      7 days\n+     *          10h     10 hours\n+     *          60m     60 mins\n+     *          120s    120 seconds\n+     */\n+    @ConfField\n+    public static String sys_log_dir = PaloFe.DORIS_HOME_DIR + \"/log\";\n+    @ConfField public static String sys_log_level = \"INFO\"; \n+    @ConfField public static int sys_log_roll_num = 10;\n+    @ConfField public static String[] sys_log_verbose_modules = {};\n+    @ConfField public static String sys_log_roll_interval = \"DAY\";\n+    @ConfField public static String sys_log_delete_age = \"7d\";\n+    @Deprecated\n+    @ConfField public static String sys_log_roll_mode = \"SIZE-MB-1024\";\n+\n+    /**\n+     * audit_log_dir:\n+     *      This specifies FE audit log dir.\n+     *      Audit log fe.audit.log contains all requests with related infos such as user, host, cost, status, etc.\n+     * \n+     * audit_log_roll_num:\n+     *      Maximal FE audit log files to be kept within an audit_log_roll_interval.\n+     *      \n+     * audit_log_modules:\n+     *       Slow query contains all queries which cost exceed *qe_slow_log_ms*\n+     *       \n+     * qe_slow_log_ms:\n+     *      If the response time of a query exceed this threshold, it will be recored in audit log as slow_query.\n+     *      \n+     * audit_log_roll_interval:\n+     *      DAY:  log suffix is yyyyMMdd\n+     *      HOUR: log suffix is yyyyMMddHH\n+     *      \n+     * audit_log_delete_age:\n+     *      default is 30 days, if log's last modify time is 30 days ago, it will be deleted.\n+     *      support format:\n+     *          7d      7 days\n+     *          10h     10 hours\n+     *          60m     60 mins\n+     *          120s    120 seconds\n+     */\n+    @ConfField public static String audit_log_dir = PaloFe.DORIS_HOME_DIR + \"/log\";\n+    @ConfField public static int audit_log_roll_num = 90;\n+    @ConfField public static String[] audit_log_modules = {\"slow_query\", \"query\"};\n+    @ConfField(mutable = true) public static long qe_slow_log_ms = 5000;\n+    @ConfField public static String audit_log_roll_interval = \"DAY\";\n+    @ConfField public static String audit_log_delete_age = \"30d\";\n+    @Deprecated\n+    @ConfField public static String audit_log_roll_mode = \"TIME-DAY\";\n+\n+    /**\n+     * plugin_dir:\n+     *      plugin install directory\n+     */\n+    @ConfField public static String plugin_dir = System.getenv(\"DORIS_HOME\") + \"/plugins\";\n+\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean plugin_enable = true;\n+\n+    /**\n+     * Labels of finished or cancelled load jobs will be removed after *label_keep_max_second*\n+     * The removed labels can be reused.\n+     * Set a short time will lower the FE memory usage.\n+     * (Because all load jobs' info is kept in memory before being removed)\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int label_keep_max_second = 3 * 24 * 3600; // 3 days\n+  \n+    /**\n+     * The max keep time of some kind of jobs.\n+     * like schema change job and rollup job.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int history_job_keep_max_second = 7 * 24 * 3600; // 7 days\n+  \n+    /**\n+     * Load label cleaner will run every *label_clean_interval_second* to clean the outdated jobs.\n+     */\n+    @ConfField public static int label_clean_interval_second = 4 * 3600; // 4 hours\n+  \n+    /**\n+     * the transaction will be cleaned after transaction_clean_interval_second seconds if the transaction is visible or aborted\n+     * we should make this interval as short as possible and each clean cycle as soon as possible\n+     */\n+    @ConfField public static int transaction_clean_interval_second = 30;\n+\n+    // Configurations for meta data durability\n+    /**\n+     * Doris meta data will be saved here.\n+     * The storage of this dir is highly recommended as to be:\n+     * 1. High write performance (SSD)\n+     * 2. Safe (RAID)\n+     */\n+    @ConfField public static String meta_dir = PaloFe.DORIS_HOME_DIR + \"/doris-meta\";\n+    \n+    /**\n+     * temp dir is used to save intermediate results of some process, such as backup and restore process.\n+     * file in this dir will be cleaned after these process is finished.\n+     */\n+    @ConfField public static String tmp_dir = PaloFe.DORIS_HOME_DIR + \"/temp_dir\";\n+    \n+    /**\n+     * Edit log type.\n+     * BDB: write log to bdbje\n+     * LOCAL: deprecated.\n+     */\n+    @ConfField\n+    public static String edit_log_type = \"BDB\";\n+  \n+    /**\n+     * bdbje port\n+     */\n+    @ConfField\n+    public static int edit_log_port = 9010;\n+  \n+    /**\n+     * Master FE will save image every *edit_log_roll_num* meta journals.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int edit_log_roll_num = 50000;\n+      \n+    /**\n+     * Non-master FE will stop offering service\n+     * if meta data delay gap exceeds *meta_delay_toleration_second*\n+     */\n+    @ConfField public static int meta_delay_toleration_second = 300;    // 5 min\n+  \n+    /**\n+     * Master FE sync policy of bdbje.\n+     * If you only deploy one Follower FE, set this to 'SYNC'. If you deploy more than 3 Follower FE,\n+     * you can set this and the following 'replica_sync_policy' to WRITE_NO_SYNC.\n+     * more info, see: http://docs.oracle.com/cd/E17277_02/html/java/com/sleepycat/je/Durability.SyncPolicy.html\n+     */\n+    @ConfField public static String master_sync_policy = \"SYNC\"; // SYNC, NO_SYNC, WRITE_NO_SYNC\n+  \n+    /**\n+     * Follower FE sync policy of bdbje.\n+     */\n+    @ConfField public static String replica_sync_policy = \"SYNC\"; // SYNC, NO_SYNC, WRITE_NO_SYNC\n+  \n+    /**\n+     * Replica ack policy of bdbje.\n+     * more info, see: http://docs.oracle.com/cd/E17277_02/html/java/com/sleepycat/je/Durability.ReplicaAckPolicy.html\n+     */\n+    @ConfField public static String replica_ack_policy = \"SIMPLE_MAJORITY\"; // ALL, NONE, SIMPLE_MAJORITY\n+    \n+    /**\n+     * The heartbeat timeout of bdbje between master and follower.\n+     * the default is 30 seconds, which is same as default value in bdbje.\n+     * If the network is experiencing transient problems, of some unexpected long java GC annoying you,\n+     * you can try to increase this value to decrease the chances of false timeouts\n+     */\n+    @ConfField public static int bdbje_heartbeat_timeout_second = 30;\n+\n+    /**\n+     * The lock timeout of bdbje operation\n+     * If there are many LockTimeoutException in FE WARN log, you can try to increase this value\n+     */\n+    @ConfField\n+    public static int bdbje_lock_timeout_second = 1;\n+\n+    /**\n+     * num of thread to handle heartbeat events in heartbeat_mgr.\n+     */\n+    @ConfField(masterOnly = true)\n+    public static int heartbeat_mgr_threads_num = 8;\n+\n+    /**\n+     * blocking queue size to store heartbeat task in heartbeat_mgr.\n+     */\n+    @ConfField(masterOnly = true)\n+    public static int heartbeat_mgr_blocking_queue_size = 1024;\n+\n+    /**\n+     * max num of thread to handle agent task in agent task thread-pool.\n+     */\n+    @ConfField(masterOnly = true)\n+    public static int max_agent_task_threads_num = 4096;\n+\n+    /**\n+     * the max txn number which bdbje can rollback when trying to rejoin the group\n+     */\n+    @ConfField public static int txn_rollback_limit = 100;\n+\n+    /**\n+     * Specified an IP for frontend, instead of the ip get by *InetAddress.getByName*.\n+     * This can be used when *InetAddress.getByName* get an unexpected IP address.\n+     * Default is \"0.0.0.0\", which means not set.\n+     * CAN NOT set this as a hostname, only IP.\n+     */\n+    @ConfField public static String frontend_address = \"0.0.0.0\";\n+\n+    /**\n+     * Declare a selection strategy for those servers have many ips.\n+     * Note that there should at most one ip match this list.\n+     * this is a list in semicolon-delimited format, in CIDR notation, e.g. 10.10.10.0/24\n+     * If no ip match this rule, will choose one randomly.\n+     */\n+    @ConfField public static String priority_networks = \"\";\n+\n+    /**\n+     * If true, FE will reset bdbje replication group(that is, to remove all electable nodes info)\n+     * and is supposed to start as Master.\n+     * If all the electable nodes can not start, we can copy the meta data\n+     * to another node and set this config to true to try to restart the FE.\n+     */\n+    @ConfField public static String metadata_failure_recovery = \"false\";\n+\n+    /**\n+     * If true, non-master FE will ignore the meta data delay gap between Master FE and its self,\n+     * even if the metadata delay gap exceeds *meta_delay_toleration_second*.\n+     * Non-master FE will still offer read service.\n+     *\n+     * This is helpful when you try to stop the Master FE for a relatively long time for some reason,\n+     * but still wish the non-master FE can offer read service.\n+     */\n+    @ConfField(mutable = true)\n+    public static boolean ignore_meta_check = false;\n+\n+    /**\n+     * Set the maximum acceptable clock skew between non-master FE to Master FE host.\n+     * This value is checked whenever a non-master FE establishes a connection to master FE via BDBJE.\n+     * The connection is abandoned if the clock skew is larger than this value.\n+     */\n+    @ConfField public static long max_bdbje_clock_delta_ms = 5000; // 5s\n+\n+    /**\n+     * Fe http port\n+     * Currently, all FEs' http port must be same.\n+     */\n+    @ConfField public static int http_port = 8030;\n+\n+    /*\n+     * Netty http param\n+     */\n+    @ConfField public static int http_max_line_length = HttpServer.DEFAULT_MAX_LINE_LENGTH;\n+\n+    @ConfField public static int http_max_header_size = HttpServer.DEFAULT_MAX_HEADER_SIZE;\n+\n+    @ConfField public static int http_max_chunk_size = HttpServer.DEFAULT_MAX_CHUNK_SIZE;\n+\n+    /**\n+     * The backlog_num for netty http server\n+     * When you enlarge this backlog_num, you should ensure it's value larger than\n+     * the linux /proc/sys/net/core/somaxconn config\n+     */\n+    @ConfField public static int http_backlog_num = 1024;\n+\n+    /**\n+     * The connection timeout and socket timeout config for thrift server\n+     * The value for thrift_client_timeout_ms is set to be larger than zero to prevent\n+     * some hang up problems in java.net.SocketInputStream.socketRead0\n+     */\n+    @ConfField public static int thrift_client_timeout_ms = 30000;\n+\n+    /**\n+     * The backlog_num for thrift server\n+     * When you enlarge this backlog_num, you should ensure it's value larger than\n+     * the linux /proc/sys/net/core/somaxconn config\n+     */\n+    @ConfField public static int thrift_backlog_num = 1024;\n+\n+    /**\n+     * FE thrift server port\n+     */\n+    @ConfField public static int rpc_port = 9020;\n+  \n+    /**\n+     * FE mysql server port\n+     */\n+    @ConfField public static int query_port = 9030;\n+\n+    /**\n+     * mysql service nio option.\n+     */\n+    @ConfField public static boolean mysql_service_nio_enabled = true;\n+\n+    /**\n+     * num of thread to handle io events in mysql.\n+     */\n+    @ConfField public static int mysql_service_io_threads_num = 4;\n+\n+    /**\n+     * max num of thread to handle task in mysql.\n+     */\n+    @ConfField public static int max_mysql_service_task_threads_num = 4096;\n+\n+    /**\n+     * Cluster name will be shown as the title of web page\n+     */\n+    @ConfField public static String cluster_name = \"Baidu Palo\";\n+  \n+    /**\n+     * node(FE or BE) will be considered belonging to the same Palo cluster if they have same cluster id.\n+     * Cluster id is usually a random integer generated when master FE start at first time.\n+     * You can also sepecify one.\n+     */\n+    @ConfField public static int cluster_id = -1;\n+  \n+    /**\n+     * Cluster token used for internal authentication.\n+     */\n+    @ConfField public static String auth_token = \"\";\n+\n+    // Configurations for load, clone, create table, alter table etc. We will rarely change them\n+    /**\n+     * Maximal waiting time for creating a single replica.\n+     * eg.\n+     *      if you create a table with #m tablets and #n replicas for each tablet,\n+     *      the create table request will run at most (m * n * tablet_create_timeout_second) before timeout.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int tablet_create_timeout_second = 1;\n+  \n+    /**\n+     * In order not to wait too long for create table(index), set a max timeout.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_create_table_timeout_second = 60;\n+    \n+    /**\n+     * Maximal waiting time for all publish version tasks of one transaction to be finished\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int publish_version_timeout_second = 30; // 30 seconds\n+    \n+    /**\n+     * minimal intervals between two publish version action\n+     */\n+    @ConfField public static int publish_version_interval_ms = 10;\n+\n+    /**\n+     * The thrift server max worker threads\n+     */\n+    @ConfField public static int thrift_server_max_worker_threads = 4096;\n+\n+    /**\n+     * Maximal wait seconds for straggler node in load\n+     * eg.\n+     *      there are 3 replicas A, B, C\n+     *      load is already quorum finished(A,B) at t1 and C is not finished\n+     *      if (current_time - t1) > 300s, then palo will treat C as a failure node\n+     *      will call transaction manager to commit the transaction and tell transaction manager \n+     *      that C is failed\n+     * \n+     * This is also used when waiting for publish tasks\n+     * \n+     * TODO this parameter is the default value for all job and the DBA could specify it for separate job\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int load_straggler_wait_second = 300;\n+    \n+    /**\n+     * Maximal memory layout length of a row. default is 100 KB.\n+     * In BE, the maximal size of a RowBlock is 100MB(Configure as max_unpacked_row_block_size in be.conf).\n+     * And each RowBlock contains 1024 rows. So the maximal size of a row is approximately 100 KB.\n+     * \n+     * eg.\n+     *      schema: k1(int), v1(decimal), v2(varchar(2000))\n+     *      then the memory layout length of a row is: 8(int) + 40(decimal) + 2000(varchar) = 2048 (Bytes)\n+     *      \n+     * See memory layout length of all types, run 'help create table' in mysql-client.\n+     * \n+     * If you want to increase this number to support more columns in a row, you also need to increase the \n+     * max_unpacked_row_block_size in be.conf. But the performance impact is unknown.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_layout_length_per_row = 100000; // 100k\n+\n+    /**\n+     * The load scheduler running interval.\n+     * A load job will transfer its state from PENDING to LOADING to FINISHED.\n+     * The load scheduler will transfer load job from PENDING to LOADING\n+     *      while the txn callback will transfer load job from LOADING to FINISHED.\n+     * So a load job will cost at most one interval to finish when the concurrency has not reached the upper limit.\n+     */\n+    @ConfField public static int load_checker_interval_second = 5;\n+\n+    /**\n+     * Concurrency of HIGH priority pending load jobs.\n+     * Load job priority is defined as HIGH or NORMAL.\n+     * All mini batch load jobs are HIGH priority, other types of load jobs are NORMAL priority.\n+     * Priority is set to avoid that a slow load job occupies a thread for a long time.\n+     * This is just a internal optimized scheduling policy.\n+     * Currently, you can not specified the job priority manually,\n+     * and do not change this if you know what you are doing.\n+     */\n+    @ConfField public static int load_pending_thread_num_high_priority = 3;\n+    /**\n+     * Concurrency of NORMAL priority pending load jobs.\n+     * Do not change this if you know what you are doing.\n+     */\n+    @ConfField public static int load_pending_thread_num_normal_priority = 10;\n+    /**\n+     * Concurrency of HIGH priority etl load jobs.\n+     * Do not change this if you know what you are doing.\n+     */\n+    @ConfField public static int load_etl_thread_num_high_priority = 3;\n+    /**\n+     * Concurrency of NORMAL priority etl load jobs.\n+     * Do not change this if you know what you are doing.\n+     */\n+    @ConfField public static int load_etl_thread_num_normal_priority = 10;\n+    /**\n+     * Concurrency of delete jobs.\n+     */\n+    @ConfField public static int delete_thread_num = 10;\n+    /**\n+     * Not available.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int load_input_size_limit_gb = 0; // GB, 0 is no limit\n+    /**\n+     * Not available.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int load_running_job_num_limit = 0; // 0 is no limit\n+    /**\n+     * Default broker load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int broker_load_default_timeout_second = 14400; // 4 hour\n+\n+    /**\n+     * Default non-streaming mini load timeout\n+     */\n+    @Deprecated\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int mini_load_default_timeout_second = 3600; // 1 hour\n+    \n+    /**\n+     * Default insert load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int insert_load_default_timeout_second = 3600; // 1 hour\n+    \n+    /**\n+     * Default stream load and streaming mini load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int stream_load_default_timeout_second = 600; // 600s\n+\n+    /**\n+     * Max load timeout applicable to all type of load except for stream load\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_load_timeout_second = 259200; // 3days\n+\n+    /**\n+     * Max stream load and streaming mini load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_stream_load_timeout_second = 259200; // 3days\n+\n+    /**\n+     * Min stream load timeout applicable to all type of load\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int min_load_timeout_second = 1; // 1s\n+\n+    /**\n+     * Default hadoop load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int hadoop_load_default_timeout_second = 86400 * 3; // 3 day\n+\n+    // Configurations for spark load\n+    /**\n+     * Default spark dpp version\n+     */\n+    @ConfField\n+    public static String spark_dpp_version = \"1.0.0\";\n+    /**\n+     * Default spark load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int spark_load_default_timeout_second = 86400; // 1 day\n+\n+    /**\n+     * Default spark home dir\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static String spark_home_default_dir = PaloFe.DORIS_HOME_DIR + \"/lib/spark2x\";\n+\n+    /**\n+     * Default spark dependencies path\n+     */\n+    @ConfField\n+    public static String spark_resource_path = \"\";\n+\n+    /**\n+     * The specified spark launcher log dir\n+     */\n+    @ConfField\n+    public static String spark_launcher_log_dir = sys_log_dir + \"/spark_launcher_log\";\n+\n+    /**\n+     * Default yarn client path\n+     */\n+    @ConfField\n+    public static String yarn_client_path = PaloFe.DORIS_HOME_DIR + \"/lib/yarn-client/hadoop/bin/yarn\";\n+\n+    /**\n+     * Default yarn config file directory\n+     * Each time before running the yarn command, we need to check that the\n+     * config file exists under this path, and if not, create them.\n+     */\n+    @ConfField\n+    public static String yarn_config_dir = PaloFe.DORIS_HOME_DIR + \"/lib/yarn-config\";\n+\n+    /**\n+     * Default number of waiting jobs for routine load and version 2 of load\n+     * This is a desired number.\n+     * In some situation, such as switch the master, the current number is maybe more then desired_max_waiting_jobs\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int desired_max_waiting_jobs = 100;\n+  \n+    /**\n+     * maximun concurrent running txn num including prepare, commit txns under a single db\n+     * txn manager will reject coming txns\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_running_txn_num_per_db = 100;\n+\n+    /**\n+     * The load task executor pool size. This pool size limits the max running load tasks.\n+     * Currently, it only limits the load task of broker load, pending and loading phases.\n+     * It should be less than 'max_running_txn_num_per_db'\n+     */\n+    @ConfField(mutable = false, masterOnly = true)\n+    public static int async_load_task_pool_size = 10;\n+\n+    /**\n+     * Same meaning as *tablet_create_timeout_second*, but used when delete a tablet.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int tablet_delete_timeout_second = 2;\n+    /**\n+     * Clone checker's running interval.\n+     */\n+    @ConfField public static int clone_checker_interval_second = 300;\n+    /**\n+     * Default timeout of a single clone job. Set long enough to fit your replica size.\n+     * The larger the replica data size is, the more time is will cost to finish clone.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int clone_job_timeout_second = 7200; // 2h\n+    /**\n+     * Concurrency of LOW priority clone jobs.\n+     * Concurrency of High priority clone jobs is currently unlimit.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int clone_max_job_num = 100;\n+    /**\n+     * LOW priority clone job's delay trigger time.\n+     * A clone job contains a tablet which need to be cloned(recovery or migration).\n+     * If the priority is LOW, it will be delayed *clone_low_priority_delay_second*\n+     * after the job creation and then be executed.\n+     * This is to avoid a large number of clone jobs running at same time only because a host is down for a short time.\n+     *\n+     * NOTICE that this config(and *clone_normal_priority_delay_second* as well)\n+     * will not work if it's smaller then *clone_checker_interval_second*\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int clone_low_priority_delay_second = 600;\n+    /**\n+     * NORMAL priority clone job's delay trigger time.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int clone_normal_priority_delay_second = 300;\n+    /**\n+     * HIGH priority clone job's delay trigger time.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int clone_high_priority_delay_second = 0;\n+    /**\n+     * the minimal delay seconds between a replica is failed and fe try to recovery it using clone.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int replica_delay_recovery_second = 0;\n+    /**\n+     * Balance threshold of data size in BE.\n+     * The balance algorithm is:\n+     * 1. Calculate the average used capacity(AUC) of the entire cluster. (total data size / total backends num)\n+     * 2. The high water level is (AUC * (1 + clone_capacity_balance_threshold))\n+     * 3. The low water level is (AUC * (1 - clone_capacity_balance_threshold))\n+     * The Clone checker will try to move replica from high water level BE to low water level BE.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static double clone_capacity_balance_threshold = 0.2;\n+    /**\n+     * Balance threshold of num of replicas in Backends.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static double clone_distribution_balance_threshold = 0.2;\n+    /**\n+     * The high water of disk capacity used percent.\n+     * This is used for calculating load score of a backend.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static double capacity_used_percent_high_water = 0.75;\n+    /**\n+     * Maximal timeout of ALTER TABLE request. Set long enough to fit your table data size.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int alter_table_timeout_second = 86400; // 1day\n+    /**\n+     * If a backend is down for *max_backend_down_time_second*, a BACKEND_DOWN event will be triggered.\n+     * Do not set this if you know what you are doing.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_backend_down_time_second = 3600; // 1h\n+    /**\n+     * When create a table(or partition), you can specify its storage medium(HDD or SSD).\n+     * If not set, this specifies the default medium when creat.\n+     */\n+    @ConfField public static String default_storage_medium = \"HDD\";\n+    /**\n+     * When create a table(or partition), you can specify its storage medium(HDD or SSD).\n+     * If set to SSD, this specifies the default duration that tablets will stay on SSD.\n+     * After that, tablets will be moved to HDD automatically.\n+     * You can set storage cooldown time in CREATE TABLE stmt.\n+     */\n+    @ConfField public static long storage_cooldown_second = 30 * 24 * 3600L; // 30 days\n+    /**\n+     * After dropping database(table/partition), you can recover it by using RECOVER stmt.\n+     * And this specifies the maximal data retention time. After time, the data will be deleted permanently.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long catalog_trash_expire_second = 86400L; // 1day\n+    /**\n+     * Maximal bytes that a single broker scanner will read.\n+     * Do not set this if you know what you are doing.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long min_bytes_per_broker_scanner = 67108864L; // 64MB\n+    /**\n+     * Maximal concurrency of broker scanners.\n+     * Do not set this if you know what you are doing.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_broker_concurrency = 10;\n+\n+    /**\n+     * Export checker's running interval.\n+     */\n+    @ConfField public static int export_checker_interval_second = 5;\n+    /**\n+     * Limitation of the concurrency of running export jobs.\n+     * Default is 5.\n+     * 0 is unlimited\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int export_running_job_num_limit = 5;\n+    /**\n+     * Default timeout of export jobs.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int export_task_default_timeout_second = 2 * 3600; // 2h\n+    /**\n+     * Number of tablets per export query plan\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int export_tablet_num_per_task = 5;\n+\n+    // Configurations for consistency check\n+    /**\n+     * Consistency checker will run from *consistency_check_start_time* to *consistency_check_end_time*.\n+     * Default is from 23:00 to 04:00\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static String consistency_check_start_time = \"23\";\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static String consistency_check_end_time = \"4\";\n+    /**\n+     * Default timeout of a single consistency check task. Set long enough to fit your tablet size.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long check_consistency_default_timeout_second = 600; // 10 min\n+\n+    // Configurations for query engine\n+    /**\n+     * Maximal number of connections per FE.\n+     */\n+    @ConfField public static int qe_max_connection = 1024;\n+\n+    /**\n+     * Maximal number of thread in connection-scheduler-pool.\n+     */\n+    @ConfField public static int max_connection_scheduler_threads_num = 4096;\n+\n+    /**\n+     * The memory_limit for colocote join PlanFragment instance =\n+     * exec_mem_limit / min (query_colocate_join_memory_limit_penalty_factor, instance_num)\n+     */\n+    @ConfField(mutable = true)\n+    public static int query_colocate_join_memory_limit_penalty_factor = 8;\n+\n+    /**\n+     * Deprecated after 0.10\n+     */\n+    @ConfField\n+    public static boolean disable_colocate_join = false;\n+    /**\n+     * The default user resource publishing timeout.\n+     */\n+    @ConfField public static int meta_publish_timeout_ms = 1000;\n+    @ConfField public static boolean proxy_auth_enable = false;\n+    @ConfField public static String proxy_auth_magic_prefix = \"x@8\";\n+    /**\n+     * Limit on the number of expr children of an expr tree.\n+     * Exceed this limit may cause long analysis time while holding database read lock.\n+     * Do not set this if you know what you are doing.\n+     */\n+    @ConfField(mutable = true)\n+    public static int expr_children_limit = 10000;\n+    /**\n+     * Limit on the depth of an expr tree.\n+     * Exceed this limit may cause long analysis time while holding db read lock.\n+     * Do not set this if you know what you are doing.\n+     */\n+    @ConfField(mutable = true)\n+    public static int expr_depth_limit = 3000;\n+\n+    // Configurations for backup and restore\n+    /**\n+     * Plugins' path for BACKUP and RESTORE operations. Currently deprecated.\n+     */\n+    @Deprecated\n+    @ConfField public static String backup_plugin_path = \"/tools/trans_file_tool/trans_files.sh\";\n+\n+    // Configurations for hadoop dpp\n+    /**\n+     * The following configurations are not available.\n+     */\n+    @ConfField public static String dpp_hadoop_client_path = \"/lib/hadoop-client/hadoop/bin/hadoop\";\n+    @ConfField public static long dpp_bytes_per_reduce = 100 * 1024 * 1024L; // 100M\n+    @ConfField public static String dpp_default_cluster = \"palo-dpp\";\n+    @ConfField public static String dpp_default_config_str = \"\"\n+            + \"{\"\n+            + \"hadoop_configs : '\"\n+            + \"mapred.job.priority=NORMAL;\"\n+            + \"mapred.job.map.capacity=50;\"\n+            + \"mapred.job.reduce.capacity=50;\"\n+            + \"mapred.hce.replace.streaming=false;\"\n+            + \"abaci.long.stored.job=true;\"\n+            + \"dce.shuffle.enable=false;\"\n+            + \"dfs.client.authserver.force_stop=true;\"\n+            + \"dfs.client.auth.method=0\"\n+            + \"'}\";\n+    @ConfField public static String dpp_config_str = \"\"\n+            + \"{palo-dpp : {\"\n+            + \"hadoop_palo_path : '/dir',\"\n+            + \"hadoop_configs : '\"\n+            + \"fs.default.name=hdfs://host:port;\"\n+            + \"mapred.job.tracker=host:port;\"\n+            + \"hadoop.job.ugi=user,password\"\n+            + \"'}\"\n+            + \"}\";\n+\n+    // For forward compatibility, will be removed later.\n+    // check token when download image file.\n+    @ConfField public static boolean enable_token_check = true;\n+\n+    /**\n+     * Set to true if you deploy Palo using thirdparty deploy manager\n+     * Valid options are:\n+     *      disable:    no deploy manager\n+     *      k8s:        Kubernetes\n+     *      ambari:     Ambari\n+     *      local:      Local File (for test or Boxer2 BCC version)\n+     */\n+    @ConfField public static String enable_deploy_manager = \"disable\";\n+    \n+    // If use k8s deploy manager locally, set this to true and prepare the certs files\n+    @ConfField public static boolean with_k8s_certs = false;\n+    \n+    // Set runtime locale when exec some cmds\n+    @ConfField public static String locale = \"zh_CN.UTF-8\";\n+\n+    // default timeout of backup job\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int backup_job_default_timeout_ms = 86400 * 1000; // 1 day\n+    \n+    /**\n+     * 'storage_high_watermark_usage_percent' limit the max capacity usage percent of a Backend storage path.\n+     * 'storage_min_left_capacity_bytes' limit the minimum left capacity of a Backend storage path.\n+     * If both limitations are reached, this storage path can not be chose as tablet balance destination.\n+     * But for tablet recovery, we may exceed these limit for keeping data integrity as much as possible.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int storage_high_watermark_usage_percent = 85;\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long storage_min_left_capacity_bytes = 2 * 1024 * 1024 * 1024; // 2G\n+\n+    /**\n+     * If capacity of disk reach the 'storage_flood_stage_usage_percent' and 'storage_flood_stage_left_capacity_bytes',\n+     * the following operation will be rejected:\n+     * 1. load job\n+     * 2. restore job\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int storage_flood_stage_usage_percent = 95;\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long storage_flood_stage_left_capacity_bytes = 1 * 1024 * 1024 * 1024; // 100MB\n+\n+    // update interval of tablet stat\n+    // All frontends will get tablet stat from all backends at each interval\n+    @ConfField public static int tablet_stat_update_interval_second = 300;  // 5 min\n+\n+    // May be necessary to modify the following BRPC configurations in high concurrency scenarios. \n+    // The number of concurrent requests BRPC can processed\n+    @ConfField public static int brpc_number_of_concurrent_requests_processed = 4096;\n+\n+    // BRPC idle wait time (ms)\n+    @ConfField public static int brpc_idle_wait_max_time = 10000;\n+    \n+    /**\n+     * if set to false, auth check will be disable, in case some goes wrong with the new privilege system. \n+     */\n+    @ConfField public static boolean enable_auth_check = true;\n+    \n+    /**\n+     * Max bytes a broker scanner can process in one broker load job.\n+     * Commonly, each Backends has one broker scanner.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long max_bytes_per_broker_scanner = 3 * 1024 * 1024 * 1024L; // 3G\n+    \n+    /**\n+     * Max number of load jobs, include PENDING\u3001ETL\u3001LOADING\u3001QUORUM_FINISHED.\n+     * If exceed this number, load job is not allowed to be submitted.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long max_unfinished_load_job = 1000;\n+    \n+    /**\n+     * If set to true, Planner will try to select replica of tablet on same host as this Frontend.\n+     * This may reduce network transmission in following case:\n+     * 1. N hosts with N Backends and N Frontends deployed.\n+     * 2. The data has N replicas.\n+     * 3. High concurrency queries are sent to all Frontends evenly\n+     * In this case, all Frontends can only use local replicas to do the query.\n+     */\n+    @ConfField(mutable = true)\n+    public static boolean enable_local_replica_selection = false;\n+    \n+    /**\n+     * The timeout of executing async remote fragment.\n+     * In normal case, the async remote fragment will be executed in a short time. If system are under high load\n+     * condition\uff0ctry to set this timeout longer.\n+     */\n+    @ConfField(mutable = true)\n+    public static long remote_fragment_exec_timeout_ms = 5000; // 5 sec\n+    \n+    /**\n+     * The number of query retries. \n+     * A query may retry if we encounter RPC exception and no result has been sent to user.\n+     * You may reduce this number to avoid Avalanche disaster.\n+     */\n+    @ConfField(mutable = true)\n+    public static int max_query_retry_time = 2;\n+\n+    /**\n+     * The tryLock timeout configuration of catalog lock.\n+     * Normally it does not need to change, unless you need to test something.\n+     */\n+    @ConfField(mutable = true)\n+    public static long catalog_try_lock_timeout_ms = 5000; // 5 sec\n+    \n+    /**\n+     * if this is set to true\n+     *    all pending load job will failed when call begin txn api\n+     *    all prepare load job will failed when call commit txn api\n+     *    all committed load job will waiting to be published\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean disable_load_job = false;\n+\n+    /*\n+     * One master daemon thread will update database used data quota for db txn manager every db_used_data_quota_update_interval_secs\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int db_used_data_quota_update_interval_secs = 300;\n+    \n+    /**\n+     * Load using hadoop cluster will be deprecated in future.\n+     * Set to true to disable this kind of load.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean disable_hadoop_load = false;\n+    \n+    /**\n+     * fe will call es api to get es index shard info every es_state_sync_interval_secs\n+     */\n+    @ConfField\n+    public static long es_state_sync_interval_second = 10;\n+    \n+    /**\n+     * the factor of delay time before deciding to repair tablet.\n+     * if priority is VERY_HIGH, repair it immediately.\n+     * HIGH, delay tablet_repair_delay_factor_second * 1;\n+     * NORMAL: delay tablet_repair_delay_factor_second * 2;\n+     * LOW: delay tablet_repair_delay_factor_second * 3;\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long tablet_repair_delay_factor_second = 60;\n+    \n+    /**\n+     * the default slot number per path in tablet scheduler\n+     * TODO(cmy): remove this config and dynamically adjust it by clone task statistic\n+     */\n+    @ConfField public static int schedule_slot_num_per_path = 2;\n+    \n+    /**\n+     * Deprecated after 0.10\n+     */\n+    @ConfField public static boolean use_new_tablet_scheduler = true;\n+\n+    /**\n+     * the threshold of cluster balance score, if a backend's load score is 10% lower than average score,\n+     * this backend will be marked as LOW load, if load score is 10% higher than average score, HIGH load\n+     * will be marked.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static double balance_load_score_threshold = 0.1; // 10%\n+\n+    /**\n+     * if set to true, TabletScheduler will not do balance.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean disable_balance = false;\n+\n+    // if the number of scheduled tablets in TabletScheduler exceed max_scheduling_tablets\n+    // skip checking.\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_scheduling_tablets = 2000;\n+\n+    // if the number of balancing tablets in TabletScheduler exceed max_balancing_tablets,\n+    // no more balance check\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_balancing_tablets = 100;\n+\n+    // This threshold is to avoid piling up too many report task in FE, which may cause OOM exception.\n+    // In some large Doris cluster, eg: 100 Backends with ten million replicas, a tablet report may cost\n+    // several seconds after some modification of metadata(drop partition, etc..).\n+    // And one Backend will report tablets info every 1 min, so unlimited receiving reports is unacceptable.\n+    // TODO(cmy): we will optimize the processing speed of tablet report in future, but now, just discard\n+    // the report if queue size exceeding limit.\n+    // Some online time cost:\n+    // 1. disk report: 0-1 ms\n+    // 2. task report: 0-1 ms\n+    // 3. tablet report \n+    //      10000 replicas: 200ms\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int report_queue_size = 100;\n+    \n+    /**\n+     * If set to true, metric collector will be run as a daemon timer to collect metrics at fix interval\n+     */\n+    @ConfField public static boolean enable_metric_calculator = true;\n+\n+    /**\n+     * the max routine load job num, including NEED_SCHEDULED, RUNNING, PAUSE\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_routine_load_job_num = 100;\n+\n+    /**\n+     * the max concurrent routine load task num of a single routine load job\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_routine_load_task_concurrent_num = 5;\n+\n+    /**\n+     * the max concurrent routine load task num per BE.\n+     * This is to limit the num of routine load tasks sending to a BE, and it should also less\n+     * than BE config 'routine_load_thread_pool_size'(default 10),\n+     * which is the routine load task thread pool size on BE.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_routine_load_task_num_per_be = 5;\n+\n+    /**\n+     * The max number of files store in SmallFileMgr \n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_small_file_number = 100;\n+\n+    /**\n+     * The max size of a single file store in SmallFileMgr \n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_small_file_size_bytes = 1024 * 1024; // 1MB\n+\n+    /**\n+     * Save small files\n+     */\n+    @ConfField public static String small_file_dir = PaloFe.DORIS_HOME_DIR + \"/small_files\";\n+    \n+    /**\n+     * The following 2 configs can set to true to disable the automatic colocate tables's relocate and balance.\n+     * if 'disable_colocate_relocate' is set to true, ColocateTableBalancer will not relocate colocate tables when Backend unavailable.\n+     * if 'disable_colocate_balance' is set to true, ColocateTableBalancer will not balance colocate tables.\n+     */\n+    @ConfField(mutable = true, masterOnly = true) public static boolean disable_colocate_relocate = false;\n+    @ConfField(mutable = true, masterOnly = true) public static boolean disable_colocate_balance = false;\n+\n+    /**\n+     * If set to true, the insert stmt with processing error will still return a label to user.\n+     * And user can use this label to check the load job's status.\n+     * The default value is false, which means if insert operation encounter errors,\n+     * exception will be thrown to user client directly without load label.\n+     */\n+    @ConfField(mutable = true, masterOnly = true) public static boolean using_old_load_usage_pattern = false;\n+\n+    /**\n+     * This will limit the max recursion depth of hash distribution pruner.\n+     * eg: where a in (5 elements) and b in (4 elements) and c in (3 elements) and d in (2 elements).\n+     * a/b/c/d are distribution columns, so the recursion depth will be 5 * 4 * 3 * 2 = 120, larger than 100,\n+     * So that distribution pruner will no work and just return all buckets.\n+     * \n+     * Increase the depth can support distribution pruning for more elements, but may cost more CPU.\n+     */\n+    @ConfField(mutable = true, masterOnly = false)\n+    public static int max_distribution_pruner_recursion_depth = 100;\n+\n+    /**\n+     * If the jvm memory used percent(heap or old mem pool) exceed this threshold, checkpoint thread will\n+     * not work to avoid OOM.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long metadata_checkopoint_memory_threshold = 60;\n+\n+    /**\n+     * If set to true, the checkpoint thread will make the checkpoint regardless of the jvm memory used percent.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean force_do_metadata_checkpoint = false;\n+\n+    /**\n+     * The multi cluster feature will be deprecated in version 0.12\n+     * set this config to true will disable all operations related to cluster feature, include:\n+     *   create/drop cluster\n+     *   add free backend/add backend to cluster/decommission cluster balance\n+     *   change the backends num of cluster\n+     *   link/migration db\n+     */\n+    @ConfField(mutable = true)\n+    public static boolean disable_cluster_feature = true;\n+\n+    /**\n+     * Decide how often to check dynamic partition\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long dynamic_partition_check_interval_seconds = 600;\n+\n+    /**\n+     * If set to true, dynamic partition feature will open\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean dynamic_partition_enable = true;\n+\n+    /**\n+     * control rollup job concurrent limit\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_running_rollup_job_num_per_table = 1;\n+\n+    /**\n+     * If set to true, Doris will check if the compiled and running versions of Java are compatible\n+     */\n+    @ConfField\n+    public static boolean check_java_version = true;\n+\n+    /**\n+     * control materialized view\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean enable_materialized_view = true;\n+\n+    /**\n+     * it can't auto-resume routine load job as long as one of the backends is down\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_tolerable_backend_down_num = 0;\n+\n+    /**\n+     * a period for auto resume routine load\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int period_of_auto_resume_min = 5;\n+\n+    /**\n+     * If set to true, the backend will be automatically dropped after finishing decommission.\n+     * If set to false, the backend will not be dropped and remaining in DECOMMISSION state.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean drop_backend_after_decommission = true;\n+\n+    /**\n+     * If set to true, FE will check backend available capacity by storage medium when create table\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean enable_strict_storage_medium_check = false;\n+\n+    /**\n+     * enable spark load for temporary use\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean enable_spark_load = false;\n+\n+    /**\n+     * Define thrift server's server model, default is TThreadPoolServer model\n+     */\n+    @ConfField\n+    public static String thrift_server_type = ThriftServer.THREAD_POOL;\n+\n+    /**\n+     * This config will decide whether to resend agent task when create_time for agent_task is set,\n+     * only when current_time - create_time > agent_task_resend_wait_time_ms can ReportHandler do resend agent task\n+     */\n+    @ConfField (mutable = true, masterOnly = true)\n+    public static long agent_task_resend_wait_time_ms = 5000;\n+\n+    /**\n+     * min_clone_task_timeout_sec and max_clone_task_timeout_sec is to limit the\n+     * min and max timeout of a clone task.\n+     * Under normal circumstances, the timeout of a clone task is estimated by\n+     * the amount of data and the minimum transmission speed(5MB/s).\n+     * But in special cases, you may need to manually set these two configs\n+     * to ensure that the clone task will not fail due to timeout.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long min_clone_task_timeout_sec = 3 * 60; // 3min\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static long max_clone_task_timeout_sec = 2 * 60 * 60; // 2h\n+\n+    /** \n+     * If set to true, fe will enable sql result cache\n+     * This option is suitable for offline data update scenarios\n+     *                              case1   case2   case3   case4\n+     * enable_sql_cache             false   true    true    false\n+     * enable_partition_cache       false   false   true    true\n+     */\n+    @ConfField(mutable = true, masterOnly = false)\n+    public static boolean cache_enable_sql_mode = true;\n+\n+    /**\n+     * If set to true, fe will get data from be cache,\n+     * This option is suitable for real-time updating of partial partitions.\n+     */\n+    @ConfField(mutable = true, masterOnly = false)\n+    public static boolean cache_enable_partition_mode = true;\n+\n+    /**\n+     *  Minimum interval between last version when caching results,\n+     *  This parameter distinguishes between offline and real-time updates\n+     */\n+    @ConfField(mutable = true, masterOnly = false)\n+    public static int cache_last_version_interval_second = 900;\n+\n+    /**\n+     * Set the maximum number of rows that can be cached\n+     */\n+    @ConfField(mutable = true, masterOnly = false)\n+    public static int cache_result_max_row_count = 3000;\n+    \n+    /**\n+     * Used to limit element num of InPredicate in delete statement.\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int max_allowed_in_element_num_of_delete = 1024;\n+\n+    /**\n+     * In some cases, some tablets may have all replicas damaged or lost.\n+     * At this time, the data has been lost, and the damaged tablets\n+     * will cause the entire query to fail, and the remaining healthy tablets cannot be queried.\n+     * In this case, you can set this configuration to true.\n+     * The system will replace damaged tablets with empty tablets to ensure that the query\n+     * can be executed. (but at this time the data has been lost, so the query results may be inaccurate)\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean recover_with_empty_tablet = false;\n+}\n"}}, {"oid": "b0ad2e27a7dfa30ca55b2601eca335c30c041b59", "url": "https://github.com/apache/incubator-doris/commit/b0ad2e27a7dfa30ca55b2601eca335c30c041b59", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581\n1. Cache data by sql key and partition key\n2. Support fetch/update/clear operator\n3. Hit cache by key, version and vertion_time", "committedDate": "2020-09-02T14:10:29Z", "type": "forcePushed"}, {"oid": "2ffa081f9227911e5fac8942a2f84611cf4a3da3", "url": "https://github.com/apache/incubator-doris/commit/2ffa081f9227911e5fac8942a2f84611cf4a3da3", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581\n1. Cache data by sql key and partition key\n2. Support fetch/update/clear operator\n3. Hit cache by key, version and vertion_time", "committedDate": "2020-09-17T06:30:37Z", "type": "commit"}, {"oid": "2ffa081f9227911e5fac8942a2f84611cf4a3da3", "url": "https://github.com/apache/incubator-doris/commit/2ffa081f9227911e5fac8942a2f84611cf4a3da3", "message": "[Feature][Cache] LRU cache for sql/partition cache #2581\n1. Cache data by sql key and partition key\n2. Support fetch/update/clear operator\n3. Hit cache by key, version and vertion_time", "committedDate": "2020-09-17T06:30:37Z", "type": "forcePushed"}, {"oid": "a951bc4a50d58f3235e6e9fb03a50a5144c624bd", "url": "https://github.com/apache/incubator-doris/commit/a951bc4a50d58f3235e6e9fb03a50a5144c624bd", "message": "fixed memory leak in partition_cache_test.cpp", "committedDate": "2020-09-18T06:52:19Z", "type": "forcePushed"}, {"oid": "24d78f2cf627a8a828e2c552092dd9f8c83735d7", "url": "https://github.com/apache/incubator-doris/commit/24d78f2cf627a8a828e2c552092dd9f8c83735d7", "message": "fixed memory leak in partition_cache_test.cpp", "committedDate": "2020-09-18T10:07:29Z", "type": "forcePushed"}, {"oid": "768367fbe7e54d365c313817727a4e68ec2e99c2", "url": "https://github.com/apache/incubator-doris/commit/768367fbe7e54d365c313817727a4e68ec2e99c2", "message": "fixed memory leak in partition_cache_test.cpp", "committedDate": "2020-09-19T04:09:43Z", "type": "forcePushed"}, {"oid": "6afcb27fb5cf8714a68dbd60bad2b39aa580ef3e", "url": "https://github.com/apache/incubator-doris/commit/6afcb27fb5cf8714a68dbd60bad2b39aa580ef3e", "message": "fixed memory leak in partition_cache_test.cpp", "committedDate": "2020-09-19T10:12:18Z", "type": "commit"}, {"oid": "6afcb27fb5cf8714a68dbd60bad2b39aa580ef3e", "url": "https://github.com/apache/incubator-doris/commit/6afcb27fb5cf8714a68dbd60bad2b39aa580ef3e", "message": "fixed memory leak in partition_cache_test.cpp", "committedDate": "2020-09-19T10:12:18Z", "type": "forcePushed"}]}